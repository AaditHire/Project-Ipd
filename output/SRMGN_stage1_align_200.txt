/opt/conda/envs/srmgn/lib/python3.10/site-packages/torch/distributed/launch.py:180: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
------------ Options -------------
PBAFN_gen_checkpoint: None
PBAFN_warp_checkpoint: None
PFAFN_gen_checkpoint: None
PFAFN_warp_checkpoint: None
batchSize: 64
beta1: 0.5
checkpoints_dir: checkpoints
continue_train: False
data_type: 32
dataroot: ../dataset/SPLIT-VITON/VITON_train
debug: False
display_freq: 100
display_winsize: 512
fineSize: 512
gpu_ids: [2]
input_nc: 3
isTrain: True
label_nc: 14
lambda_feat: 10.0
launcher: pytorch
loadSize: 512
load_pretrain: 
local_rank: 0
lr: 5e-05
max_dataset_size: inf
nThreads: 1
n_blocks_global: 4
n_blocks_local: 3
n_downsample_global: 4
n_layers_D: 3
n_local_enhancers: 1
name: SRMGN_stage1_align_200_3
ndf: 64
netG: global
ngf: 64
niter: 50
niter_decay: 150
niter_fix_global: 0
no_flip: False
no_ganFeat_loss: False
no_html: False
no_lsgan: False
no_vgg_loss: False
norm: instance
num_D: 2
num_gpus: 1
output_nc: 3
phase: train
pool_size: 0
print_freq: 100
resize_or_crop: None
save_epoch_freq: 5
save_latest_freq: 1000
serial_batches: False
tf_log: False
tv_weight: 0.1
use_dropout: False
valroot: ../dataset/SPLIT-VITON/VITON_val
verbose: False
which_epoch: latest
-------------- End ----------------
------------ Options -------------
PBAFN_gen_checkpoint: None
PBAFN_warp_checkpoint: None
PFAFN_gen_checkpoint: None
PFAFN_warp_checkpoint: None
batchSize: 64
beta1: 0.5
checkpoints_dir: checkpoints
continue_train: False
data_type: 32
dataroot: ../dataset/SPLIT-VITON/VITON_train
debug: False
display_freq: 100
display_winsize: 512
fineSize: 512
gpu_ids: [2]
input_nc: 3
isTrain: True
label_nc: 14
lambda_feat: 10.0
launcher: pytorch
loadSize: 512
load_pretrain: 
local_rank: 0
lr: 5e-05
max_dataset_size: inf
nThreads: 1
n_blocks_global: 4
n_blocks_local: 3
n_downsample_global: 4
n_layers_D: 3
n_local_enhancers: 1
name: SRMGN_stage1_align_200_3
ndf: 64
netG: global
ngf: 64
niter: 50
niter_decay: 150
niter_fix_global: 0
no_flip: False
no_ganFeat_loss: False
no_html: False
no_lsgan: False
no_vgg_loss: False
norm: instance
num_D: 2
num_gpus: 1
output_nc: 3
phase: train
pool_size: 0
print_freq: 100
resize_or_crop: None
save_epoch_freq: 5
save_latest_freq: 1000
serial_batches: False
tf_log: False
tv_weight: 0.1
use_dropout: False
valroot: ../dataset/SPLIT-VITON/VITON_val
verbose: False
which_epoch: latest
-------------- End ----------------
dataset [AlignedDataset] was created
../dataset/SPLIT-VITON/VITON_train/train_label label
../dataset/SPLIT-VITON/VITON_train/train_img img
../dataset/SPLIT-VITON/VITON_train/train_edge edge
../dataset/SPLIT-VITON/VITON_train/train_color color
dataset [AlignedDataset] was created
../dataset/SPLIT-VITON/VITON_val/val_label label
../dataset/SPLIT-VITON/VITON_val/val_img img
../dataset/SPLIT-VITON/VITON_val/val_edge edge
../dataset/SPLIT-VITON/VITON_val/val_color color
/opt/conda/envs/srmgn/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
2023.02.18-22:59:39:100:[step-100/42400: 0.24%]--[loss-7.346315]--[lr-0.000050]--[ETA-11:14:55]
2023.02.18-23:01:14:200:[step-200/42400: 0.47%]--[loss-6.797468]--[lr-0.000050]--[ETA-10:59:32]
Saving best at epoch 1
End of epoch 1 / 200: train_loss: 7.248 	 val_loss: 7.794 	 time: 218 sec
2023.02.18-23:03:02:88:[step-300/42400: 0.71%]--[loss-5.799109]--[lr-0.000050]--[ETA-11:00:42]
2023.02.18-23:04:37:188:[step-400/42400: 0.94%]--[loss-5.474601]--[lr-0.000050]--[ETA-11:02:30]
Saving best at epoch 2
End of epoch 2 / 200: train_loss: 5.983 	 val_loss: 5.704 	 time: 213 sec
2023.02.18-23:06:25:76:[step-500/42400: 1.18%]--[loss-5.951210]--[lr-0.000050]--[ETA-11:00:10]
2023.02.18-23:08:00:176:[step-600/42400: 1.42%]--[loss-5.322080]--[lr-0.000050]--[ETA-11:00:35]
End of epoch 3 / 200: train_loss: 5.584 	 val_loss: 6.129 	 time: 214 sec
2023.02.18-23:09:48:64:[step-700/42400: 1.65%]--[loss-5.265948]--[lr-0.000050]--[ETA-11:05:45]
2023.02.18-23:11:23:164:[step-800/42400: 1.89%]--[loss-5.765962]--[lr-0.000050]--[ETA-10:54:13]
Saving best at epoch 4
End of epoch 4 / 200: train_loss: 5.364 	 val_loss: 5.470 	 time: 214 sec
2023.02.18-23:13:12:52:[step-900/42400: 2.12%]--[loss-5.597797]--[lr-0.000050]--[ETA-10:59:37]
2023.02.18-23:14:46:152:[step-1000/42400: 2.36%]--[loss-5.683784]--[lr-0.000050]--[ETA-10:53:13]
Saving best at epoch 5
End of epoch 5 / 200: train_loss: 5.207 	 val_loss: 5.376 	 time: 215 sec
Saving the model at the end of epoch 5, iters 1060
2023.02.18-23:16:36:40:[step-1100/42400: 2.59%]--[loss-5.257910]--[lr-0.000050]--[ETA-10:51:04]
2023.02.18-23:18:12:140:[step-1200/42400: 2.83%]--[loss-5.453606]--[lr-0.000050]--[ETA-10:50:49]
End of epoch 6 / 200: train_loss: 5.086 	 val_loss: 5.577 	 time: 216 sec
2023.02.18-23:20:01:28:[step-1300/42400: 3.07%]--[loss-5.177114]--[lr-0.000050]--[ETA-11:03:05]
2023.02.18-23:21:36:128:[step-1400/42400: 3.30%]--[loss-4.866273]--[lr-0.000050]--[ETA-10:44:59]
End of epoch 7 / 200: train_loss: 5.005 	 val_loss: 5.379 	 time: 215 sec
2023.02.18-23:23:25:16:[step-1500/42400: 3.54%]--[loss-5.066357]--[lr-0.000050]--[ETA-10:46:28]
2023.02.18-23:25:00:116:[step-1600/42400: 3.77%]--[loss-4.700865]--[lr-0.000050]--[ETA-10:48:41]
Saving best at epoch 8
End of epoch 8 / 200: train_loss: 4.917 	 val_loss: 5.171 	 time: 214 sec
2023.02.18-23:26:49:4:[step-1700/42400: 4.01%]--[loss-4.762296]--[lr-0.000050]--[ETA-10:37:03]
2023.02.18-23:28:23:104:[step-1800/42400: 4.25%]--[loss-4.300873]--[lr-0.000050]--[ETA-10:39:13]
2023.02.18-23:29:58:204:[step-1900/42400: 4.48%]--[loss-4.699599]--[lr-0.000050]--[ETA-10:36:04]
End of epoch 9 / 200: train_loss: 4.848 	 val_loss: 5.263 	 time: 214 sec
2023.02.18-23:31:46:92:[step-2000/42400: 4.72%]--[loss-4.701024]--[lr-0.000050]--[ETA-10:40:07]
2023.02.18-23:33:21:192:[step-2100/42400: 4.95%]--[loss-4.703127]--[lr-0.000050]--[ETA-10:35:02]
End of epoch 10 / 200: train_loss: 4.781 	 val_loss: 5.882 	 time: 214 sec
Saving the model at the end of epoch 10, iters 2120
2023.02.18-23:35:10:80:[step-2200/42400: 5.19%]--[loss-5.008627]--[lr-0.000050]--[ETA-10:28:12]
2023.02.18-23:36:44:180:[step-2300/42400: 5.42%]--[loss-4.956215]--[lr-0.000050]--[ETA-10:32:39]
Saving best at epoch 11
End of epoch 11 / 200: train_loss: 4.721 	 val_loss: 4.899 	 time: 214 sec
2023.02.18-23:38:33:68:[step-2400/42400: 5.66%]--[loss-4.658783]--[lr-0.000050]--[ETA-10:27:47]
2023.02.18-23:40:08:168:[step-2500/42400: 5.90%]--[loss-4.855653]--[lr-0.000050]--[ETA-10:28:53]
End of epoch 12 / 200: train_loss: 4.658 	 val_loss: 5.078 	 time: 214 sec
2023.02.18-23:41:57:56:[step-2600/42400: 6.13%]--[loss-4.600049]--[lr-0.000050]--[ETA-10:26:21]
2023.02.18-23:43:32:156:[step-2700/42400: 6.37%]--[loss-4.447239]--[lr-0.000050]--[ETA-10:35:34]
Saving best at epoch 13
End of epoch 13 / 200: train_loss: 4.613 	 val_loss: 4.770 	 time: 215 sec
2023.02.18-23:45:21:44:[step-2800/42400: 6.60%]--[loss-5.102203]--[lr-0.000050]--[ETA-10:19:06]
2023.02.18-23:46:55:144:[step-2900/42400: 6.84%]--[loss-4.378785]--[lr-0.000050]--[ETA-10:25:23]
End of epoch 14 / 200: train_loss: 4.585 	 val_loss: 4.817 	 time: 214 sec
2023.02.18-23:48:44:32:[step-3000/42400: 7.08%]--[loss-4.816708]--[lr-0.000050]--[ETA-10:29:39]
2023.02.18-23:50:19:132:[step-3100/42400: 7.31%]--[loss-4.354402]--[lr-0.000050]--[ETA-10:20:03]
End of epoch 15 / 200: train_loss: 4.553 	 val_loss: 4.878 	 time: 215 sec
Saving the model at the end of epoch 15, iters 3180
2023.02.18-23:52:08:20:[step-3200/42400: 7.55%]--[loss-4.417242]--[lr-0.000050]--[ETA-10:28:03]
2023.02.18-23:53:43:120:[step-3300/42400: 7.78%]--[loss-4.391123]--[lr-0.000050]--[ETA-10:12:42]
End of epoch 16 / 200: train_loss: 4.526 	 val_loss: 6.986 	 time: 214 sec
2023.02.18-23:55:31:8:[step-3400/42400: 8.02%]--[loss-5.034622]--[lr-0.000050]--[ETA-10:15:33]
2023.02.18-23:57:06:108:[step-3500/42400: 8.25%]--[loss-4.489249]--[lr-0.000050]--[ETA-10:08:27]
2023.02.18-23:58:40:208:[step-3600/42400: 8.49%]--[loss-4.040045]--[lr-0.000050]--[ETA-10:05:33]
End of epoch 17 / 200: train_loss: 4.538 	 val_loss: 4.938 	 time: 214 sec
2023.02.19-00:00:29:96:[step-3700/42400: 8.73%]--[loss-4.287255]--[lr-0.000050]--[ETA-10:10:47]
2023.02.19-00:02:04:196:[step-3800/42400: 8.96%]--[loss-4.351568]--[lr-0.000050]--[ETA-10:10:40]
End of epoch 18 / 200: train_loss: 4.463 	 val_loss: 4.907 	 time: 215 sec
2023.02.19-00:03:53:84:[step-3900/42400: 9.20%]--[loss-4.426886]--[lr-0.000050]--[ETA-10:03:38]
2023.02.19-00:05:28:184:[step-4000/42400: 9.43%]--[loss-4.323486]--[lr-0.000050]--[ETA-10:09:30]
End of epoch 19 / 200: train_loss: 4.440 	 val_loss: 4.788 	 time: 215 sec
2023.02.19-00:07:17:72:[step-4100/42400: 9.67%]--[loss-4.509444]--[lr-0.000050]--[ETA-10:02:34]
2023.02.19-00:08:52:172:[step-4200/42400: 9.91%]--[loss-4.601532]--[lr-0.000050]--[ETA-10:01:52]
Saving best at epoch 20
End of epoch 20 / 200: train_loss: 4.402 	 val_loss: 4.595 	 time: 215 sec
Saving the model at the end of epoch 20, iters 4240
2023.02.19-00:10:42:60:[step-4300/42400: 10.14%]--[loss-4.312460]--[lr-0.000050]--[ETA-9:59:23]
2023.02.19-00:12:17:160:[step-4400/42400: 10.38%]--[loss-4.912541]--[lr-0.000050]--[ETA-9:54:15]
End of epoch 21 / 200: train_loss: 4.393 	 val_loss: 4.677 	 time: 215 sec
2023.02.19-00:14:05:48:[step-4500/42400: 10.61%]--[loss-4.595721]--[lr-0.000050]--[ETA-9:55:14]
2023.02.19-00:15:39:148:[step-4600/42400: 10.85%]--[loss-4.157529]--[lr-0.000050]--[ETA-9:52:29]
End of epoch 22 / 200: train_loss: 4.360 	 val_loss: 5.865 	 time: 214 sec
2023.02.19-00:17:28:36:[step-4700/42400: 11.08%]--[loss-4.499187]--[lr-0.000050]--[ETA-9:50:51]
2023.02.19-00:19:02:136:[step-4800/42400: 11.32%]--[loss-3.942397]--[lr-0.000050]--[ETA-9:49:21]
End of epoch 23 / 200: train_loss: 4.368 	 val_loss: 6.731 	 time: 213 sec
2023.02.19-00:20:51:24:[step-4900/42400: 11.56%]--[loss-4.565419]--[lr-0.000050]--[ETA-9:54:37]
2023.02.19-00:22:25:124:[step-5000/42400: 11.79%]--[loss-4.210757]--[lr-0.000050]--[ETA-9:46:00]
End of epoch 24 / 200: train_loss: 4.370 	 val_loss: 4.793 	 time: 214 sec
2023.02.19-00:24:14:12:[step-5100/42400: 12.03%]--[loss-4.709368]--[lr-0.000050]--[ETA-9:50:03]
2023.02.19-00:25:49:112:[step-5200/42400: 12.26%]--[loss-4.248717]--[lr-0.000050]--[ETA-9:47:56]
2023.02.19-00:27:24:212:[step-5300/42400: 12.50%]--[loss-3.865446]--[lr-0.000050]--[ETA-1:53:49]
End of epoch 25 / 200: train_loss: 4.329 	 val_loss: 4.675 	 time: 216 sec
Saving the model at the end of epoch 25, iters 5300
2023.02.19-00:29:14:100:[step-5400/42400: 12.74%]--[loss-4.183923]--[lr-0.000050]--[ETA-9:41:07]
2023.02.19-00:30:48:200:[step-5500/42400: 12.97%]--[loss-4.106508]--[lr-0.000050]--[ETA-9:35:54]
End of epoch 26 / 200: train_loss: 4.300 	 val_loss: 4.627 	 time: 214 sec
2023.02.19-00:32:37:88:[step-5600/42400: 13.21%]--[loss-4.157652]--[lr-0.000050]--[ETA-9:37:34]
2023.02.19-00:34:11:188:[step-5700/42400: 13.44%]--[loss-4.002355]--[lr-0.000050]--[ETA-9:37:35]
End of epoch 27 / 200: train_loss: 4.289 	 val_loss: 4.715 	 time: 214 sec
2023.02.19-00:36:00:76:[step-5800/42400: 13.68%]--[loss-4.043351]--[lr-0.000050]--[ETA-9:33:57]
2023.02.19-00:37:34:176:[step-5900/42400: 13.92%]--[loss-3.974412]--[lr-0.000050]--[ETA-9:29:56]
Saving best at epoch 28
End of epoch 28 / 200: train_loss: 4.260 	 val_loss: 4.551 	 time: 214 sec
2023.02.19-00:39:24:64:[step-6000/42400: 14.15%]--[loss-4.278255]--[lr-0.000050]--[ETA-9:31:49]
2023.02.19-00:40:59:164:[step-6100/42400: 14.39%]--[loss-4.286215]--[lr-0.000050]--[ETA-9:34:57]
Saving best at epoch 29
End of epoch 29 / 200: train_loss: 4.240 	 val_loss: 4.544 	 time: 216 sec
2023.02.19-00:42:49:52:[step-6200/42400: 14.62%]--[loss-4.110563]--[lr-0.000050]--[ETA-9:37:44]
2023.02.19-00:44:23:152:[step-6300/42400: 14.86%]--[loss-4.038917]--[lr-0.000050]--[ETA-9:28:19]
End of epoch 30 / 200: train_loss: 4.240 	 val_loss: 4.567 	 time: 215 sec
Saving the model at the end of epoch 30, iters 6360
2023.02.19-00:46:12:40:[step-6400/42400: 15.09%]--[loss-4.011748]--[lr-0.000050]--[ETA-9:30:53]
2023.02.19-00:47:47:140:[step-6500/42400: 15.33%]--[loss-4.026826]--[lr-0.000050]--[ETA-9:27:04]
End of epoch 31 / 200: train_loss: 4.231 	 val_loss: 6.103 	 time: 214 sec
2023.02.19-00:49:36:28:[step-6600/42400: 15.57%]--[loss-3.622374]--[lr-0.000050]--[ETA-9:36:56]
2023.02.19-00:51:11:128:[step-6700/42400: 15.80%]--[loss-4.263039]--[lr-0.000050]--[ETA-9:19:57]
Saving best at epoch 32
End of epoch 32 / 200: train_loss: 4.231 	 val_loss: 4.498 	 time: 215 sec
2023.02.19-00:53:00:16:[step-6800/42400: 16.04%]--[loss-4.095262]--[lr-0.000050]--[ETA-9:24:31]
2023.02.19-00:54:36:116:[step-6900/42400: 16.27%]--[loss-4.010926]--[lr-0.000050]--[ETA-9:21:32]
End of epoch 33 / 200: train_loss: 4.190 	 val_loss: 4.774 	 time: 215 sec
2023.02.19-00:56:25:4:[step-7000/42400: 16.51%]--[loss-4.334006]--[lr-0.000050]--[ETA-9:27:12]
2023.02.19-00:58:00:104:[step-7100/42400: 16.75%]--[loss-4.347995]--[lr-0.000050]--[ETA-9:14:32]
2023.02.19-00:59:35:204:[step-7200/42400: 16.98%]--[loss-4.196327]--[lr-0.000050]--[ETA-9:12:14]
End of epoch 34 / 200: train_loss: 4.212 	 val_loss: 4.816 	 time: 215 sec
2023.02.19-01:01:23:92:[step-7300/42400: 17.22%]--[loss-4.033719]--[lr-0.000050]--[ETA-9:16:34]
2023.02.19-01:02:58:192:[step-7400/42400: 17.45%]--[loss-4.080818]--[lr-0.000050]--[ETA-9:18:13]
End of epoch 35 / 200: train_loss: 4.197 	 val_loss: 4.539 	 time: 215 sec
Saving the model at the end of epoch 35, iters 7420
2023.02.19-01:04:47:80:[step-7500/42400: 17.69%]--[loss-4.268740]--[lr-0.000050]--[ETA-9:11:30]
2023.02.19-01:06:22:180:[step-7600/42400: 17.92%]--[loss-4.441845]--[lr-0.000050]--[ETA-9:10:39]
End of epoch 36 / 200: train_loss: 4.164 	 val_loss: 6.473 	 time: 215 sec
2023.02.19-01:08:12:68:[step-7700/42400: 18.16%]--[loss-3.974121]--[lr-0.000050]--[ETA-9:11:25]
2023.02.19-01:09:46:168:[step-7800/42400: 18.40%]--[loss-4.129361]--[lr-0.000050]--[ETA-9:04:33]
Saving best at epoch 37
End of epoch 37 / 200: train_loss: 4.218 	 val_loss: 4.343 	 time: 215 sec
2023.02.19-01:11:36:56:[step-7900/42400: 18.63%]--[loss-4.097554]--[lr-0.000050]--[ETA-9:07:08]
2023.02.19-01:13:10:156:[step-8000/42400: 18.87%]--[loss-3.843966]--[lr-0.000050]--[ETA-9:04:35]
Saving best at epoch 38
End of epoch 38 / 200: train_loss: 4.127 	 val_loss: 4.299 	 time: 215 sec
2023.02.19-01:14:59:44:[step-8100/42400: 19.10%]--[loss-4.153071]--[lr-0.000050]--[ETA-9:05:09]
2023.02.19-01:16:34:144:[step-8200/42400: 19.34%]--[loss-4.403984]--[lr-0.000050]--[ETA-8:58:48]
End of epoch 39 / 200: train_loss: 4.122 	 val_loss: 4.425 	 time: 214 sec
2023.02.19-01:18:23:32:[step-8300/42400: 19.58%]--[loss-4.620297]--[lr-0.000050]--[ETA-8:53:17]
2023.02.19-01:19:58:132:[step-8400/42400: 19.81%]--[loss-3.903555]--[lr-0.000050]--[ETA-8:53:39]
End of epoch 40 / 200: train_loss: 4.115 	 val_loss: 4.347 	 time: 214 sec
Saving the model at the end of epoch 40, iters 8480
2023.02.19-01:21:47:20:[step-8500/42400: 20.05%]--[loss-4.073136]--[lr-0.000050]--[ETA-8:57:18]
2023.02.19-01:23:22:120:[step-8600/42400: 20.28%]--[loss-4.073502]--[lr-0.000050]--[ETA-8:53:07]
End of epoch 41 / 200: train_loss: 4.113 	 val_loss: 4.537 	 time: 215 sec
2023.02.19-01:25:10:8:[step-8700/42400: 20.52%]--[loss-4.235230]--[lr-0.000050]--[ETA-8:52:37]
2023.02.19-01:26:45:108:[step-8800/42400: 20.75%]--[loss-4.107605]--[lr-0.000050]--[ETA-8:56:06]
2023.02.19-01:28:19:208:[step-8900/42400: 20.99%]--[loss-3.918874]--[lr-0.000050]--[ETA-8:42:47]
End of epoch 42 / 200: train_loss: 4.125 	 val_loss: 4.803 	 time: 214 sec
2023.02.19-01:30:09:96:[step-9000/42400: 21.23%]--[loss-4.300583]--[lr-0.000050]--[ETA-8:50:37]
2023.02.19-01:31:44:196:[step-9100/42400: 21.46%]--[loss-4.041615]--[lr-0.000050]--[ETA-8:49:37]
End of epoch 43 / 200: train_loss: 4.109 	 val_loss: 4.321 	 time: 215 sec
2023.02.19-01:33:33:84:[step-9200/42400: 21.70%]--[loss-4.043524]--[lr-0.000050]--[ETA-8:41:59]
2023.02.19-01:35:07:184:[step-9300/42400: 21.93%]--[loss-3.878420]--[lr-0.000050]--[ETA-8:44:36]
End of epoch 44 / 200: train_loss: 4.087 	 val_loss: 4.536 	 time: 214 sec
2023.02.19-01:36:57:72:[step-9400/42400: 22.17%]--[loss-3.754386]--[lr-0.000050]--[ETA-8:38:45]
2023.02.19-01:38:32:172:[step-9500/42400: 22.41%]--[loss-4.029446]--[lr-0.000050]--[ETA-8:39:46]
End of epoch 45 / 200: train_loss: 4.077 	 val_loss: 4.352 	 time: 215 sec
Saving the model at the end of epoch 45, iters 9540
2023.02.19-01:40:21:60:[step-9600/42400: 22.64%]--[loss-4.010653]--[lr-0.000050]--[ETA-8:36:36]
2023.02.19-01:41:56:160:[step-9700/42400: 22.88%]--[loss-3.654036]--[lr-0.000050]--[ETA-8:36:37]
End of epoch 46 / 200: train_loss: 4.076 	 val_loss: 4.377 	 time: 214 sec
2023.02.19-01:43:44:48:[step-9800/42400: 23.11%]--[loss-3.922477]--[lr-0.000050]--[ETA-8:33:25]
2023.02.19-01:45:19:148:[step-9900/42400: 23.35%]--[loss-3.778427]--[lr-0.000050]--[ETA-8:33:56]
End of epoch 47 / 200: train_loss: 4.061 	 val_loss: 4.478 	 time: 214 sec
2023.02.19-01:47:08:36:[step-10000/42400: 23.58%]--[loss-3.875564]--[lr-0.000050]--[ETA-8:34:24]
2023.02.19-01:48:44:136:[step-10100/42400: 23.82%]--[loss-3.879336]--[lr-0.000050]--[ETA-8:31:40]
End of epoch 48 / 200: train_loss: 4.057 	 val_loss: 4.341 	 time: 216 sec
2023.02.19-01:50:33:24:[step-10200/42400: 24.06%]--[loss-3.872772]--[lr-0.000050]--[ETA-8:24:59]
2023.02.19-01:52:07:124:[step-10300/42400: 24.29%]--[loss-3.977227]--[lr-0.000050]--[ETA-8:26:16]
End of epoch 49 / 200: train_loss: 4.049 	 val_loss: 4.431 	 time: 215 sec
2023.02.19-01:53:56:12:[step-10400/42400: 24.53%]--[loss-4.096142]--[lr-0.000050]--[ETA-8:21:38]
2023.02.19-01:55:30:112:[step-10500/42400: 24.76%]--[loss-3.855881]--[lr-0.000050]--[ETA-8:23:07]
2023.02.19-01:57:04:212:[step-10600/42400: 25.00%]--[loss-3.486311]--[lr-0.000050]--[ETA-1:35:21]
End of epoch 50 / 200: train_loss: 4.023 	 val_loss: 4.332 	 time: 213 sec
Saving the model at the end of epoch 50, iters 10600
2023.02.19-01:58:54:100:[step-10700/42400: 25.24%]--[loss-4.177763]--[lr-0.000050]--[ETA-8:17:30]
2023.02.19-02:00:29:200:[step-10800/42400: 25.47%]--[loss-3.944031]--[lr-0.000050]--[ETA-8:14:18]
End of epoch 51 / 200: train_loss: 4.017 	 val_loss: 4.303 	 time: 215 sec
2023.02.19-02:02:19:88:[step-10900/42400: 25.71%]--[loss-3.967458]--[lr-0.000050]--[ETA-8:19:09]
2023.02.19-02:03:54:188:[step-11000/42400: 25.94%]--[loss-3.936959]--[lr-0.000050]--[ETA-8:13:24]
Saving best at epoch 52
End of epoch 52 / 200: train_loss: 4.016 	 val_loss: 4.289 	 time: 217 sec
2023.02.19-02:05:43:76:[step-11100/42400: 26.18%]--[loss-3.852590]--[lr-0.000049]--[ETA-8:13:11]
2023.02.19-02:07:18:176:[step-11200/42400: 26.42%]--[loss-4.093908]--[lr-0.000049]--[ETA-8:11:24]
End of epoch 53 / 200: train_loss: 4.024 	 val_loss: 4.320 	 time: 214 sec
2023.02.19-02:09:07:64:[step-11300/42400: 26.65%]--[loss-4.241940]--[lr-0.000049]--[ETA-8:07:58]
2023.02.19-02:10:41:164:[step-11400/42400: 26.89%]--[loss-3.666756]--[lr-0.000049]--[ETA-8:07:41]
End of epoch 54 / 200: train_loss: 3.997 	 val_loss: 4.398 	 time: 214 sec
2023.02.19-02:12:30:52:[step-11500/42400: 27.12%]--[loss-3.773252]--[lr-0.000049]--[ETA-8:03:29]
2023.02.19-02:14:05:152:[step-11600/42400: 27.36%]--[loss-3.885622]--[lr-0.000049]--[ETA-8:04:27]
End of epoch 55 / 200: train_loss: 3.991 	 val_loss: 4.350 	 time: 214 sec
Saving the model at the end of epoch 55, iters 11660
2023.02.19-02:15:54:40:[step-11700/42400: 27.59%]--[loss-3.805279]--[lr-0.000048]--[ETA-8:11:28]
2023.02.19-02:17:29:140:[step-11800/42400: 27.83%]--[loss-3.923716]--[lr-0.000048]--[ETA-7:59:29]
End of epoch 56 / 200: train_loss: 3.994 	 val_loss: 4.306 	 time: 215 sec
2023.02.19-02:19:17:28:[step-11900/42400: 28.07%]--[loss-3.867075]--[lr-0.000048]--[ETA-8:06:36]
2023.02.19-02:20:53:128:[step-12000/42400: 28.30%]--[loss-4.118843]--[lr-0.000048]--[ETA-7:59:41]
End of epoch 57 / 200: train_loss: 3.971 	 val_loss: 4.403 	 time: 215 sec
2023.02.19-02:22:41:16:[step-12100/42400: 28.54%]--[loss-4.295460]--[lr-0.000048]--[ETA-8:00:25]
2023.02.19-02:24:16:116:[step-12200/42400: 28.77%]--[loss-3.845198]--[lr-0.000048]--[ETA-7:54:31]
End of epoch 58 / 200: train_loss: 3.972 	 val_loss: 4.346 	 time: 214 sec
2023.02.19-02:26:05:4:[step-12300/42400: 29.01%]--[loss-3.998544]--[lr-0.000047]--[ETA-7:50:54]
2023.02.19-02:27:40:104:[step-12400/42400: 29.25%]--[loss-4.016521]--[lr-0.000047]--[ETA-7:51:07]
2023.02.19-02:29:14:204:[step-12500/42400: 29.48%]--[loss-4.264781]--[lr-0.000047]--[ETA-7:46:36]
Saving best at epoch 59
End of epoch 59 / 200: train_loss: 3.950 	 val_loss: 4.259 	 time: 215 sec
2023.02.19-02:31:04:92:[step-12600/42400: 29.72%]--[loss-4.173120]--[lr-0.000047]--[ETA-7:49:33]
2023.02.19-02:32:39:192:[step-12700/42400: 29.95%]--[loss-4.013073]--[lr-0.000047]--[ETA-7:48:25]
End of epoch 60 / 200: train_loss: 3.941 	 val_loss: 4.389 	 time: 215 sec
Saving the model at the end of epoch 60, iters 12720
2023.02.19-02:34:29:80:[step-12800/42400: 30.19%]--[loss-3.936702]--[lr-0.000047]--[ETA-7:48:45]
2023.02.19-02:36:04:180:[step-12900/42400: 30.42%]--[loss-3.991005]--[lr-0.000047]--[ETA-7:52:02]
End of epoch 61 / 200: train_loss: 3.946 	 val_loss: 4.363 	 time: 216 sec
2023.02.19-02:37:54:68:[step-13000/42400: 30.66%]--[loss-3.924541]--[lr-0.000046]--[ETA-7:42:58]
2023.02.19-02:39:29:168:[step-13100/42400: 30.90%]--[loss-3.976418]--[lr-0.000046]--[ETA-7:42:32]
End of epoch 62 / 200: train_loss: 3.937 	 val_loss: 4.442 	 time: 215 sec
2023.02.19-02:41:17:56:[step-13200/42400: 31.13%]--[loss-4.315403]--[lr-0.000046]--[ETA-7:38:47]
2023.02.19-02:42:52:156:[step-13300/42400: 31.37%]--[loss-3.951769]--[lr-0.000046]--[ETA-7:40:59]
End of epoch 63 / 200: train_loss: 3.947 	 val_loss: 5.518 	 time: 214 sec
2023.02.19-02:44:41:44:[step-13400/42400: 31.60%]--[loss-4.035259]--[lr-0.000046]--[ETA-7:35:56]
2023.02.19-02:46:16:144:[step-13500/42400: 31.84%]--[loss-3.774193]--[lr-0.000046]--[ETA-7:33:32]
End of epoch 64 / 200: train_loss: 3.956 	 val_loss: 4.399 	 time: 214 sec
2023.02.19-02:48:04:32:[step-13600/42400: 32.08%]--[loss-3.713775]--[lr-0.000045]--[ETA-7:33:37]
2023.02.19-02:49:40:132:[step-13700/42400: 32.31%]--[loss-3.758750]--[lr-0.000045]--[ETA-7:34:16]
Saving best at epoch 65
End of epoch 65 / 200: train_loss: 3.918 	 val_loss: 4.190 	 time: 216 sec
Saving the model at the end of epoch 65, iters 13780
2023.02.19-02:51:30:20:[step-13800/42400: 32.55%]--[loss-4.109158]--[lr-0.000045]--[ETA-7:34:27]
2023.02.19-02:53:05:120:[step-13900/42400: 32.78%]--[loss-3.812407]--[lr-0.000045]--[ETA-7:32:54]
End of epoch 66 / 200: train_loss: 3.900 	 val_loss: 4.191 	 time: 215 sec
2023.02.19-02:54:53:8:[step-14000/42400: 33.02%]--[loss-4.029912]--[lr-0.000045]--[ETA-7:27:18]
2023.02.19-02:56:28:108:[step-14100/42400: 33.25%]--[loss-3.780576]--[lr-0.000045]--[ETA-7:29:36]
2023.02.19-02:58:02:208:[step-14200/42400: 33.49%]--[loss-3.971508]--[lr-0.000045]--[ETA-7:19:40]
End of epoch 67 / 200: train_loss: 3.896 	 val_loss: 4.242 	 time: 214 sec
2023.02.19-02:59:52:96:[step-14300/42400: 33.73%]--[loss-3.791281]--[lr-0.000044]--[ETA-7:22:22]
2023.02.19-03:01:27:196:[step-14400/42400: 33.96%]--[loss-4.050100]--[lr-0.000044]--[ETA-7:21:18]
End of epoch 68 / 200: train_loss: 3.889 	 val_loss: 4.210 	 time: 215 sec
2023.02.19-03:03:16:84:[step-14500/42400: 34.20%]--[loss-3.844024]--[lr-0.000044]--[ETA-7:17:06]
2023.02.19-03:04:50:184:[step-14600/42400: 34.43%]--[loss-3.526629]--[lr-0.000044]--[ETA-7:18:26]
End of epoch 69 / 200: train_loss: 3.883 	 val_loss: 4.292 	 time: 214 sec
2023.02.19-03:06:38:72:[step-14700/42400: 34.67%]--[loss-3.607105]--[lr-0.000044]--[ETA-7:18:04]
2023.02.19-03:08:13:172:[step-14800/42400: 34.91%]--[loss-3.776516]--[lr-0.000044]--[ETA-7:14:38]
End of epoch 70 / 200: train_loss: 3.872 	 val_loss: 4.219 	 time: 214 sec
Saving the model at the end of epoch 70, iters 14840
2023.02.19-03:10:02:60:[step-14900/42400: 35.14%]--[loss-3.841008]--[lr-0.000043]--[ETA-7:17:15]
2023.02.19-03:11:37:160:[step-15000/42400: 35.38%]--[loss-3.759706]--[lr-0.000043]--[ETA-7:09:58]
End of epoch 71 / 200: train_loss: 3.875 	 val_loss: 4.258 	 time: 215 sec
2023.02.19-03:13:26:48:[step-15100/42400: 35.61%]--[loss-4.100842]--[lr-0.000043]--[ETA-7:15:13]
2023.02.19-03:15:00:148:[step-15200/42400: 35.85%]--[loss-3.648400]--[lr-0.000043]--[ETA-7:05:13]
Saving best at epoch 72
End of epoch 72 / 200: train_loss: 3.869 	 val_loss: 4.144 	 time: 214 sec
2023.02.19-03:16:49:36:[step-15300/42400: 36.08%]--[loss-3.821220]--[lr-0.000043]--[ETA-7:09:00]
2023.02.19-03:18:24:136:[step-15400/42400: 36.32%]--[loss-4.054279]--[lr-0.000043]--[ETA-7:01:46]
End of epoch 73 / 200: train_loss: 3.855 	 val_loss: 4.281 	 time: 214 sec
2023.02.19-03:20:12:24:[step-15500/42400: 36.56%]--[loss-3.636512]--[lr-0.000042]--[ETA-7:01:10]
2023.02.19-03:21:47:124:[step-15600/42400: 36.79%]--[loss-3.830548]--[lr-0.000042]--[ETA-7:03:11]
Saving best at epoch 74
End of epoch 74 / 200: train_loss: 3.849 	 val_loss: 4.104 	 time: 214 sec
2023.02.19-03:23:37:12:[step-15700/42400: 37.03%]--[loss-3.656284]--[lr-0.000042]--[ETA-7:00:20]
2023.02.19-03:25:12:112:[step-15800/42400: 37.26%]--[loss-3.736539]--[lr-0.000042]--[ETA-6:58:01]
2023.02.19-03:26:46:212:[step-15900/42400: 37.50%]--[loss-3.496029]--[lr-0.000042]--[ETA-1:23:32]
End of epoch 75 / 200: train_loss: 3.845 	 val_loss: 4.140 	 time: 215 sec
Saving the model at the end of epoch 75, iters 15900
2023.02.19-03:28:35:100:[step-16000/42400: 37.74%]--[loss-4.015224]--[lr-0.000042]--[ETA-6:58:49]
2023.02.19-03:30:10:200:[step-16100/42400: 37.97%]--[loss-3.505573]--[lr-0.000042]--[ETA-6:51:25]
End of epoch 76 / 200: train_loss: 3.836 	 val_loss: 4.178 	 time: 215 sec
2023.02.19-03:31:59:88:[step-16200/42400: 38.21%]--[loss-3.767206]--[lr-0.000041]--[ETA-6:51:50]
2023.02.19-03:33:34:188:[step-16300/42400: 38.44%]--[loss-4.079707]--[lr-0.000041]--[ETA-6:50:36]
Saving best at epoch 77
End of epoch 77 / 200: train_loss: 3.825 	 val_loss: 4.079 	 time: 214 sec
2023.02.19-03:35:22:76:[step-16400/42400: 38.68%]--[loss-3.676553]--[lr-0.000041]--[ETA-6:46:54]
2023.02.19-03:36:57:176:[step-16500/42400: 38.92%]--[loss-3.631981]--[lr-0.000041]--[ETA-6:49:13]
End of epoch 78 / 200: train_loss: 3.824 	 val_loss: 4.174 	 time: 214 sec
2023.02.19-03:38:46:64:[step-16600/42400: 39.15%]--[loss-4.049003]--[lr-0.000041]--[ETA-6:41:48]
2023.02.19-03:40:21:164:[step-16700/42400: 39.39%]--[loss-3.628087]--[lr-0.000041]--[ETA-6:43:35]
Saving best at epoch 79
End of epoch 79 / 200: train_loss: 3.825 	 val_loss: 4.073 	 time: 215 sec
2023.02.19-03:42:10:52:[step-16800/42400: 39.62%]--[loss-3.649357]--[lr-0.000040]--[ETA-6:45:06]
2023.02.19-03:43:45:152:[step-16900/42400: 39.86%]--[loss-3.772520]--[lr-0.000040]--[ETA-6:40:35]
End of epoch 80 / 200: train_loss: 3.807 	 val_loss: 4.145 	 time: 215 sec
Saving the model at the end of epoch 80, iters 16960
2023.02.19-03:45:33:40:[step-17000/42400: 40.09%]--[loss-3.534390]--[lr-0.000040]--[ETA-6:37:08]
2023.02.19-03:47:08:140:[step-17100/42400: 40.33%]--[loss-3.849976]--[lr-0.000040]--[ETA-6:44:42]
End of epoch 81 / 200: train_loss: 3.809 	 val_loss: 4.781 	 time: 214 sec
2023.02.19-03:48:57:28:[step-17200/42400: 40.57%]--[loss-3.842910]--[lr-0.000040]--[ETA-6:34:59]
2023.02.19-03:50:32:128:[step-17300/42400: 40.80%]--[loss-3.465724]--[lr-0.000040]--[ETA-6:40:09]
Saving best at epoch 82
End of epoch 82 / 200: train_loss: 3.817 	 val_loss: 4.042 	 time: 215 sec
2023.02.19-03:52:21:16:[step-17400/42400: 41.04%]--[loss-3.766305]--[lr-0.000039]--[ETA-6:34:38]
2023.02.19-03:53:56:116:[step-17500/42400: 41.27%]--[loss-3.671766]--[lr-0.000039]--[ETA-6:32:46]
End of epoch 83 / 200: train_loss: 3.792 	 val_loss: 4.099 	 time: 214 sec
2023.02.19-03:55:45:4:[step-17600/42400: 41.51%]--[loss-3.820577]--[lr-0.000039]--[ETA-6:32:48]
2023.02.19-03:57:20:104:[step-17700/42400: 41.75%]--[loss-3.869420]--[lr-0.000039]--[ETA-6:27:26]
2023.02.19-03:58:55:204:[step-17800/42400: 41.98%]--[loss-3.483559]--[lr-0.000039]--[ETA-6:26:27]
End of epoch 84 / 200: train_loss: 3.791 	 val_loss: 4.056 	 time: 215 sec
2023.02.19-04:00:44:92:[step-17900/42400: 42.22%]--[loss-3.738308]--[lr-0.000039]--[ETA-6:24:43]
2023.02.19-04:02:19:192:[step-18000/42400: 42.45%]--[loss-3.919503]--[lr-0.000039]--[ETA-6:27:31]
End of epoch 85 / 200: train_loss: 3.785 	 val_loss: 4.062 	 time: 215 sec
Saving the model at the end of epoch 85, iters 18020
2023.02.19-04:04:08:80:[step-18100/42400: 42.69%]--[loss-3.770524]--[lr-0.000038]--[ETA-6:23:52]
2023.02.19-04:05:43:180:[step-18200/42400: 42.92%]--[loss-3.649806]--[lr-0.000038]--[ETA-6:28:44]
End of epoch 86 / 200: train_loss: 3.782 	 val_loss: 4.132 	 time: 215 sec
2023.02.19-04:07:33:68:[step-18300/42400: 43.16%]--[loss-3.804043]--[lr-0.000038]--[ETA-6:22:10]
2023.02.19-04:09:08:168:[step-18400/42400: 43.40%]--[loss-3.983634]--[lr-0.000038]--[ETA-6:18:22]
Saving best at epoch 87
End of epoch 87 / 200: train_loss: 3.774 	 val_loss: 4.018 	 time: 215 sec
2023.02.19-04:10:58:56:[step-18500/42400: 43.63%]--[loss-3.408078]--[lr-0.000038]--[ETA-6:23:22]
2023.02.19-04:12:32:156:[step-18600/42400: 43.87%]--[loss-3.617150]--[lr-0.000038]--[ETA-6:14:09]
End of epoch 88 / 200: train_loss: 3.773 	 val_loss: 4.118 	 time: 215 sec
2023.02.19-04:14:22:44:[step-18700/42400: 44.10%]--[loss-3.320965]--[lr-0.000037]--[ETA-6:15:49]
2023.02.19-04:15:56:144:[step-18800/42400: 44.34%]--[loss-4.259398]--[lr-0.000037]--[ETA-6:11:29]
End of epoch 89 / 200: train_loss: 3.801 	 val_loss: 4.273 	 time: 215 sec
2023.02.19-04:17:45:32:[step-18900/42400: 44.58%]--[loss-3.854635]--[lr-0.000037]--[ETA-6:11:31]
2023.02.19-04:19:20:132:[step-19000/42400: 44.81%]--[loss-3.713405]--[lr-0.000037]--[ETA-6:07:03]
End of epoch 90 / 200: train_loss: 3.791 	 val_loss: 4.256 	 time: 214 sec
Saving the model at the end of epoch 90, iters 19080
2023.02.19-04:21:09:20:[step-19100/42400: 45.05%]--[loss-3.575784]--[lr-0.000037]--[ETA-6:09:36]
2023.02.19-04:22:44:120:[step-19200/42400: 45.28%]--[loss-3.637859]--[lr-0.000037]--[ETA-6:04:16]
End of epoch 91 / 200: train_loss: 3.766 	 val_loss: 4.074 	 time: 215 sec
2023.02.19-04:24:33:8:[step-19300/42400: 45.52%]--[loss-4.295708]--[lr-0.000036]--[ETA-6:04:35]
2023.02.19-04:26:08:108:[step-19400/42400: 45.75%]--[loss-3.815442]--[lr-0.000036]--[ETA-6:06:09]
2023.02.19-04:27:43:208:[step-19500/42400: 45.99%]--[loss-3.639919]--[lr-0.000036]--[ETA-5:59:24]
End of epoch 92 / 200: train_loss: 3.760 	 val_loss: 4.296 	 time: 215 sec
2023.02.19-04:29:31:96:[step-19600/42400: 46.23%]--[loss-3.846780]--[lr-0.000036]--[ETA-6:02:35]
2023.02.19-04:31:06:196:[step-19700/42400: 46.46%]--[loss-3.538839]--[lr-0.000036]--[ETA-5:55:34]
End of epoch 93 / 200: train_loss: 3.758 	 val_loss: 4.034 	 time: 214 sec
2023.02.19-04:32:56:84:[step-19800/42400: 46.70%]--[loss-4.238721]--[lr-0.000036]--[ETA-5:58:38]
2023.02.19-04:34:31:184:[step-19900/42400: 46.93%]--[loss-4.027387]--[lr-0.000036]--[ETA-5:53:28]
End of epoch 94 / 200: train_loss: 3.743 	 val_loss: 4.083 	 time: 216 sec
2023.02.19-04:36:20:72:[step-20000/42400: 47.17%]--[loss-3.873906]--[lr-0.000035]--[ETA-5:49:37]
2023.02.19-04:37:54:172:[step-20100/42400: 47.41%]--[loss-3.998503]--[lr-0.000035]--[ETA-5:52:28]
End of epoch 95 / 200: train_loss: 3.738 	 val_loss: 4.152 	 time: 215 sec
Saving the model at the end of epoch 95, iters 20140
2023.02.19-04:39:44:60:[step-20200/42400: 47.64%]--[loss-3.640835]--[lr-0.000035]--[ETA-5:48:51]
2023.02.19-04:41:18:160:[step-20300/42400: 47.88%]--[loss-3.391044]--[lr-0.000035]--[ETA-5:49:26]
End of epoch 96 / 200: train_loss: 3.732 	 val_loss: 4.045 	 time: 215 sec
2023.02.19-04:43:08:48:[step-20400/42400: 48.11%]--[loss-3.818019]--[lr-0.000035]--[ETA-5:46:36]
2023.02.19-04:44:43:148:[step-20500/42400: 48.35%]--[loss-4.056777]--[lr-0.000035]--[ETA-5:48:16]
Saving best at epoch 97
End of epoch 97 / 200: train_loss: 3.733 	 val_loss: 4.001 	 time: 216 sec
2023.02.19-04:46:33:36:[step-20600/42400: 48.58%]--[loss-3.625259]--[lr-0.000034]--[ETA-5:45:13]
2023.02.19-04:48:08:136:[step-20700/42400: 48.82%]--[loss-3.633960]--[lr-0.000034]--[ETA-5:44:31]
Saving best at epoch 98
End of epoch 98 / 200: train_loss: 3.720 	 val_loss: 3.984 	 time: 216 sec
2023.02.19-04:49:58:24:[step-20800/42400: 49.06%]--[loss-3.771828]--[lr-0.000034]--[ETA-5:44:33]
2023.02.19-04:51:33:124:[step-20900/42400: 49.29%]--[loss-3.453778]--[lr-0.000034]--[ETA-5:40:30]
End of epoch 99 / 200: train_loss: 3.719 	 val_loss: 4.022 	 time: 215 sec
2023.02.19-04:53:22:12:[step-21000/42400: 49.53%]--[loss-3.839587]--[lr-0.000034]--[ETA-5:42:54]
2023.02.19-04:54:57:112:[step-21100/42400: 49.76%]--[loss-3.721554]--[lr-0.000034]--[ETA-5:34:23]
2023.02.19-04:56:31:212:[step-21200/42400: 50.00%]--[loss-3.378736]--[lr-0.000034]--[ETA-1:02:10]
End of epoch 100 / 200: train_loss: 3.713 	 val_loss: 4.084 	 time: 215 sec
Saving the model at the end of epoch 100, iters 21200
2023.02.19-04:58:22:100:[step-21300/42400: 50.24%]--[loss-3.629432]--[lr-0.000033]--[ETA-5:35:23]
2023.02.19-04:59:57:200:[step-21400/42400: 50.47%]--[loss-3.950476]--[lr-0.000033]--[ETA-5:31:27]
End of epoch 101 / 200: train_loss: 3.714 	 val_loss: 4.004 	 time: 216 sec
2023.02.19-05:01:47:88:[step-21500/42400: 50.71%]--[loss-3.647505]--[lr-0.000033]--[ETA-5:28:40]
2023.02.19-05:03:22:188:[step-21600/42400: 50.94%]--[loss-3.694662]--[lr-0.000033]--[ETA-5:31:34]
Saving best at epoch 102
End of epoch 102 / 200: train_loss: 3.702 	 val_loss: 3.969 	 time: 216 sec
2023.02.19-05:05:12:76:[step-21700/42400: 51.18%]--[loss-3.549706]--[lr-0.000033]--[ETA-5:23:45]
2023.02.19-05:06:46:176:[step-21800/42400: 51.42%]--[loss-3.960522]--[lr-0.000033]--[ETA-5:23:39]
End of epoch 103 / 200: train_loss: 3.698 	 val_loss: 4.022 	 time: 214 sec
2023.02.19-05:08:35:64:[step-21900/42400: 51.65%]--[loss-3.653723]--[lr-0.000032]--[ETA-5:23:13]
2023.02.19-05:10:10:164:[step-22000/42400: 51.89%]--[loss-3.685186]--[lr-0.000032]--[ETA-5:23:37]
Saving best at epoch 104
End of epoch 104 / 200: train_loss: 3.688 	 val_loss: 3.963 	 time: 216 sec
2023.02.19-05:11:59:52:[step-22100/42400: 52.12%]--[loss-3.981909]--[lr-0.000032]--[ETA-5:16:33]
2023.02.19-05:13:34:152:[step-22200/42400: 52.36%]--[loss-3.473212]--[lr-0.000032]--[ETA-5:18:22]
End of epoch 105 / 200: train_loss: 3.705 	 val_loss: 4.007 	 time: 214 sec
Saving the model at the end of epoch 105, iters 22260
2023.02.19-05:15:23:40:[step-22300/42400: 52.59%]--[loss-3.705703]--[lr-0.000032]--[ETA-5:20:07]
2023.02.19-05:16:58:140:[step-22400/42400: 52.83%]--[loss-3.546782]--[lr-0.000032]--[ETA-5:15:40]
End of epoch 106 / 200: train_loss: 3.694 	 val_loss: 3.967 	 time: 215 sec
2023.02.19-05:18:47:28:[step-22500/42400: 53.07%]--[loss-3.573068]--[lr-0.000031]--[ETA-5:15:58]
2023.02.19-05:20:22:128:[step-22600/42400: 53.30%]--[loss-3.484562]--[lr-0.000031]--[ETA-5:11:29]
End of epoch 107 / 200: train_loss: 3.679 	 val_loss: 4.040 	 time: 214 sec
2023.02.19-05:22:11:16:[step-22700/42400: 53.54%]--[loss-3.598316]--[lr-0.000031]--[ETA-5:11:16]
2023.02.19-05:23:46:116:[step-22800/42400: 53.77%]--[loss-3.932922]--[lr-0.000031]--[ETA-5:08:50]
End of epoch 108 / 200: train_loss: 3.679 	 val_loss: 4.056 	 time: 215 sec
2023.02.19-05:25:35:4:[step-22900/42400: 54.01%]--[loss-3.745210]--[lr-0.000031]--[ETA-5:09:56]
2023.02.19-05:27:10:104:[step-23000/42400: 54.25%]--[loss-3.849976]--[lr-0.000031]--[ETA-5:05:04]
2023.02.19-05:28:45:204:[step-23100/42400: 54.48%]--[loss-3.580322]--[lr-0.000031]--[ETA-5:03:00]
Saving best at epoch 109
End of epoch 109 / 200: train_loss: 3.678 	 val_loss: 3.960 	 time: 216 sec
2023.02.19-05:30:34:92:[step-23200/42400: 54.72%]--[loss-4.013887]--[lr-0.000030]--[ETA-5:02:56]
2023.02.19-05:32:09:192:[step-23300/42400: 54.95%]--[loss-3.666466]--[lr-0.000030]--[ETA-5:00:53]
End of epoch 110 / 200: train_loss: 3.663 	 val_loss: 3.968 	 time: 214 sec
Saving the model at the end of epoch 110, iters 23320
2023.02.19-05:33:57:80:[step-23400/42400: 55.19%]--[loss-3.675429]--[lr-0.000030]--[ETA-4:58:08]
2023.02.19-05:35:32:180:[step-23500/42400: 55.42%]--[loss-3.580950]--[lr-0.000030]--[ETA-4:57:10]
End of epoch 111 / 200: train_loss: 3.667 	 val_loss: 4.229 	 time: 214 sec
2023.02.19-05:37:21:68:[step-23600/42400: 55.66%]--[loss-3.701907]--[lr-0.000030]--[ETA-4:54:24]
2023.02.19-05:38:56:168:[step-23700/42400: 55.90%]--[loss-3.805714]--[lr-0.000030]--[ETA-4:56:50]
End of epoch 112 / 200: train_loss: 3.671 	 val_loss: 3.966 	 time: 216 sec
2023.02.19-05:40:46:56:[step-23800/42400: 56.13%]--[loss-3.498982]--[lr-0.000029]--[ETA-4:52:47]
2023.02.19-05:42:20:156:[step-23900/42400: 56.37%]--[loss-3.380454]--[lr-0.000029]--[ETA-4:50:13]
End of epoch 113 / 200: train_loss: 3.656 	 val_loss: 4.018 	 time: 214 sec
2023.02.19-05:44:09:44:[step-24000/42400: 56.60%]--[loss-4.000437]--[lr-0.000029]--[ETA-4:53:08]
2023.02.19-05:45:44:144:[step-24100/42400: 56.84%]--[loss-3.703274]--[lr-0.000029]--[ETA-4:52:03]
End of epoch 114 / 200: train_loss: 3.657 	 val_loss: 3.964 	 time: 215 sec
2023.02.19-05:47:34:32:[step-24200/42400: 57.08%]--[loss-3.579121]--[lr-0.000029]--[ETA-4:49:31]
2023.02.19-05:49:09:132:[step-24300/42400: 57.31%]--[loss-3.435685]--[lr-0.000029]--[ETA-4:48:13]
End of epoch 115 / 200: train_loss: 3.656 	 val_loss: 4.150 	 time: 215 sec
Saving the model at the end of epoch 115, iters 24380
2023.02.19-05:50:58:20:[step-24400/42400: 57.55%]--[loss-3.521995]--[lr-0.000028]--[ETA-4:45:01]
2023.02.19-05:52:34:120:[step-24500/42400: 57.78%]--[loss-3.600540]--[lr-0.000028]--[ETA-4:44:35]
End of epoch 116 / 200: train_loss: 3.652 	 val_loss: 4.051 	 time: 216 sec
2023.02.19-05:54:23:8:[step-24600/42400: 58.02%]--[loss-3.920607]--[lr-0.000028]--[ETA-4:41:57]
2023.02.19-05:55:58:108:[step-24700/42400: 58.25%]--[loss-3.845575]--[lr-0.000028]--[ETA-4:38:15]
2023.02.19-05:57:32:208:[step-24800/42400: 58.49%]--[loss-3.501106]--[lr-0.000028]--[ETA-4:34:11]
Saving best at epoch 117
End of epoch 117 / 200: train_loss: 3.649 	 val_loss: 3.947 	 time: 215 sec
2023.02.19-05:59:22:96:[step-24900/42400: 58.73%]--[loss-3.837086]--[lr-0.000028]--[ETA-4:36:04]
2023.02.19-06:00:57:196:[step-25000/42400: 58.96%]--[loss-3.263628]--[lr-0.000028]--[ETA-4:36:33]
Saving best at epoch 118
End of epoch 118 / 200: train_loss: 3.643 	 val_loss: 3.940 	 time: 216 sec
2023.02.19-06:02:47:84:[step-25100/42400: 59.20%]--[loss-3.389143]--[lr-0.000027]--[ETA-4:33:54]
2023.02.19-06:04:22:184:[step-25200/42400: 59.43%]--[loss-3.753132]--[lr-0.000027]--[ETA-4:30:07]
End of epoch 119 / 200: train_loss: 3.642 	 val_loss: 4.051 	 time: 215 sec
2023.02.19-06:06:11:72:[step-25300/42400: 59.67%]--[loss-3.663199]--[lr-0.000027]--[ETA-4:28:23]
2023.02.19-06:07:46:172:[step-25400/42400: 59.91%]--[loss-3.755968]--[lr-0.000027]--[ETA-4:28:47]
End of epoch 120 / 200: train_loss: 3.639 	 val_loss: 3.948 	 time: 216 sec
Saving the model at the end of epoch 120, iters 25440
2023.02.19-06:09:35:60:[step-25500/42400: 60.14%]--[loss-3.600010]--[lr-0.000027]--[ETA-4:23:49]
2023.02.19-06:11:10:160:[step-25600/42400: 60.38%]--[loss-3.632940]--[lr-0.000027]--[ETA-4:24:59]
End of epoch 121 / 200: train_loss: 3.626 	 val_loss: 3.950 	 time: 214 sec
2023.02.19-06:12:59:48:[step-25700/42400: 60.61%]--[loss-3.671464]--[lr-0.000026]--[ETA-4:22:01]
2023.02.19-06:14:34:148:[step-25800/42400: 60.85%]--[loss-3.604577]--[lr-0.000026]--[ETA-4:21:57]
End of epoch 122 / 200: train_loss: 3.623 	 val_loss: 3.985 	 time: 215 sec
2023.02.19-06:16:23:36:[step-25900/42400: 61.08%]--[loss-3.386248]--[lr-0.000026]--[ETA-4:23:02]
2023.02.19-06:17:57:136:[step-26000/42400: 61.32%]--[loss-3.560849]--[lr-0.000026]--[ETA-4:18:56]
End of epoch 123 / 200: train_loss: 3.626 	 val_loss: 3.945 	 time: 215 sec
2023.02.19-06:19:47:24:[step-26100/42400: 61.56%]--[loss-3.422326]--[lr-0.000026]--[ETA-4:17:19]
2023.02.19-06:21:22:124:[step-26200/42400: 61.79%]--[loss-3.938675]--[lr-0.000026]--[ETA-4:14:58]
Saving best at epoch 124
End of epoch 124 / 200: train_loss: 3.611 	 val_loss: 3.916 	 time: 215 sec
2023.02.19-06:23:11:12:[step-26300/42400: 62.03%]--[loss-3.554773]--[lr-0.000025]--[ETA-4:14:08]
2023.02.19-06:24:46:112:[step-26400/42400: 62.26%]--[loss-3.400314]--[lr-0.000025]--[ETA-4:17:38]
2023.02.19-06:26:20:212:[step-26500/42400: 62.50%]--[loss-3.146688]--[lr-0.000025]--[ETA-0:47:24]
Saving best at epoch 125
End of epoch 125 / 200: train_loss: 3.609 	 val_loss: 3.902 	 time: 215 sec
Saving the model at the end of epoch 125, iters 26500
2023.02.19-06:28:11:100:[step-26600/42400: 62.74%]--[loss-3.552532]--[lr-0.000025]--[ETA-4:09:02]
2023.02.19-06:29:46:200:[step-26700/42400: 62.97%]--[loss-3.396448]--[lr-0.000025]--[ETA-4:06:15]
End of epoch 126 / 200: train_loss: 3.604 	 val_loss: 3.908 	 time: 215 sec
2023.02.19-06:31:35:88:[step-26800/42400: 63.21%]--[loss-3.328012]--[lr-0.000025]--[ETA-4:06:33]
2023.02.19-06:33:10:188:[step-26900/42400: 63.44%]--[loss-3.433643]--[lr-0.000025]--[ETA-4:05:02]
End of epoch 127 / 200: train_loss: 3.601 	 val_loss: 3.918 	 time: 215 sec
2023.02.19-06:35:00:76:[step-27000/42400: 63.68%]--[loss-4.067513]--[lr-0.000024]--[ETA-4:00:49]
2023.02.19-06:36:35:176:[step-27100/42400: 63.92%]--[loss-3.797546]--[lr-0.000024]--[ETA-4:01:33]
End of epoch 128 / 200: train_loss: 3.599 	 val_loss: 3.930 	 time: 215 sec
2023.02.19-06:38:24:64:[step-27200/42400: 64.15%]--[loss-3.287781]--[lr-0.000024]--[ETA-4:01:05]
2023.02.19-06:39:59:164:[step-27300/42400: 64.39%]--[loss-3.498724]--[lr-0.000024]--[ETA-3:59:36]
End of epoch 129 / 200: train_loss: 3.602 	 val_loss: 3.949 	 time: 216 sec
2023.02.19-06:41:49:52:[step-27400/42400: 64.62%]--[loss-3.722192]--[lr-0.000024]--[ETA-3:55:52]
2023.02.19-06:43:24:152:[step-27500/42400: 64.86%]--[loss-3.551760]--[lr-0.000024]--[ETA-3:55:43]
End of epoch 130 / 200: train_loss: 3.595 	 val_loss: 4.021 	 time: 215 sec
Saving the model at the end of epoch 130, iters 27560
2023.02.19-06:45:13:40:[step-27600/42400: 65.09%]--[loss-3.744264]--[lr-0.000023]--[ETA-3:53:51]
2023.02.19-06:46:48:140:[step-27700/42400: 65.33%]--[loss-3.590067]--[lr-0.000023]--[ETA-3:51:37]
Saving best at epoch 131
End of epoch 131 / 200: train_loss: 3.593 	 val_loss: 3.897 	 time: 215 sec
2023.02.19-06:48:37:28:[step-27800/42400: 65.57%]--[loss-3.481615]--[lr-0.000023]--[ETA-3:53:01]
2023.02.19-06:50:12:128:[step-27900/42400: 65.80%]--[loss-3.568235]--[lr-0.000023]--[ETA-3:49:11]
End of epoch 132 / 200: train_loss: 3.586 	 val_loss: 3.904 	 time: 215 sec
2023.02.19-06:52:02:16:[step-28000/42400: 66.04%]--[loss-3.467808]--[lr-0.000023]--[ETA-3:48:26]
2023.02.19-06:53:37:116:[step-28100/42400: 66.27%]--[loss-3.756193]--[lr-0.000023]--[ETA-3:46:59]
End of epoch 133 / 200: train_loss: 3.583 	 val_loss: 3.910 	 time: 216 sec
2023.02.19-06:55:26:4:[step-28200/42400: 66.51%]--[loss-3.485606]--[lr-0.000022]--[ETA-3:42:30]
2023.02.19-06:57:01:104:[step-28300/42400: 66.75%]--[loss-3.537878]--[lr-0.000022]--[ETA-3:43:34]
2023.02.19-06:58:36:204:[step-28400/42400: 66.98%]--[loss-3.765105]--[lr-0.000022]--[ETA-3:40:53]
Saving best at epoch 134
End of epoch 134 / 200: train_loss: 3.575 	 val_loss: 3.892 	 time: 214 sec
2023.02.19-07:00:26:92:[step-28500/42400: 67.22%]--[loss-3.466778]--[lr-0.000022]--[ETA-3:38:21]
2023.02.19-07:02:01:192:[step-28600/42400: 67.45%]--[loss-3.430016]--[lr-0.000022]--[ETA-3:38:03]
End of epoch 135 / 200: train_loss: 3.576 	 val_loss: 3.893 	 time: 215 sec
Saving the model at the end of epoch 135, iters 28620
2023.02.19-07:03:50:80:[step-28700/42400: 67.69%]--[loss-3.909188]--[lr-0.000022]--[ETA-3:37:20]
2023.02.19-07:05:24:180:[step-28800/42400: 67.92%]--[loss-3.519438]--[lr-0.000022]--[ETA-3:34:35]
Saving best at epoch 136
End of epoch 136 / 200: train_loss: 3.568 	 val_loss: 3.865 	 time: 215 sec
2023.02.19-07:07:14:68:[step-28900/42400: 68.16%]--[loss-3.489604]--[lr-0.000021]--[ETA-3:31:42]
2023.02.19-07:08:49:168:[step-29000/42400: 68.40%]--[loss-3.618260]--[lr-0.000021]--[ETA-3:31:18]
End of epoch 137 / 200: train_loss: 3.568 	 val_loss: 3.890 	 time: 215 sec
2023.02.19-07:10:39:56:[step-29100/42400: 68.63%]--[loss-3.328055]--[lr-0.000021]--[ETA-3:31:09]
2023.02.19-07:12:14:156:[step-29200/42400: 68.87%]--[loss-3.533314]--[lr-0.000021]--[ETA-3:27:20]
End of epoch 138 / 200: train_loss: 3.575 	 val_loss: 3.936 	 time: 215 sec
2023.02.19-07:14:03:44:[step-29300/42400: 69.10%]--[loss-3.409978]--[lr-0.000021]--[ETA-3:25:48]
2023.02.19-07:15:38:144:[step-29400/42400: 69.34%]--[loss-3.409879]--[lr-0.000021]--[ETA-3:24:25]
Saving best at epoch 139
End of epoch 139 / 200: train_loss: 3.569 	 val_loss: 3.865 	 time: 215 sec
2023.02.19-07:17:27:32:[step-29500/42400: 69.58%]--[loss-3.672092]--[lr-0.000020]--[ETA-3:26:39]
2023.02.19-07:19:02:132:[step-29600/42400: 69.81%]--[loss-3.167577]--[lr-0.000020]--[ETA-3:23:58]
Saving best at epoch 140
End of epoch 140 / 200: train_loss: 3.556 	 val_loss: 3.841 	 time: 216 sec
Saving the model at the end of epoch 140, iters 29680
2023.02.19-07:20:53:20:[step-29700/42400: 70.05%]--[loss-3.543005]--[lr-0.000020]--[ETA-3:21:14]
2023.02.19-07:22:27:120:[step-29800/42400: 70.28%]--[loss-3.696136]--[lr-0.000020]--[ETA-3:18:23]
End of epoch 141 / 200: train_loss: 3.557 	 val_loss: 3.868 	 time: 215 sec
2023.02.19-07:24:17:8:[step-29900/42400: 70.52%]--[loss-3.374240]--[lr-0.000020]--[ETA-3:20:05]
2023.02.19-07:25:52:108:[step-30000/42400: 70.75%]--[loss-3.523953]--[lr-0.000020]--[ETA-3:16:09]
2023.02.19-07:27:27:208:[step-30100/42400: 70.99%]--[loss-3.847827]--[lr-0.000020]--[ETA-3:11:48]
End of epoch 142 / 200: train_loss: 3.550 	 val_loss: 3.842 	 time: 216 sec
2023.02.19-07:29:16:96:[step-30200/42400: 71.23%]--[loss-3.741744]--[lr-0.000019]--[ETA-3:11:47]
2023.02.19-07:30:51:196:[step-30300/42400: 71.46%]--[loss-3.387689]--[lr-0.000019]--[ETA-3:11:58]
End of epoch 143 / 200: train_loss: 3.545 	 val_loss: 3.846 	 time: 215 sec
2023.02.19-07:32:41:84:[step-30400/42400: 71.70%]--[loss-3.445022]--[lr-0.000019]--[ETA-3:10:27]
2023.02.19-07:34:17:184:[step-30500/42400: 71.93%]--[loss-3.367763]--[lr-0.000019]--[ETA-3:09:24]
End of epoch 144 / 200: train_loss: 3.539 	 val_loss: 3.849 	 time: 217 sec
2023.02.19-07:36:07:72:[step-30600/42400: 72.17%]--[loss-3.400427]--[lr-0.000019]--[ETA-3:05:46]
2023.02.19-07:37:42:172:[step-30700/42400: 72.41%]--[loss-3.476697]--[lr-0.000019]--[ETA-3:06:01]
End of epoch 145 / 200: train_loss: 3.538 	 val_loss: 3.904 	 time: 216 sec
Saving the model at the end of epoch 145, iters 30740
2023.02.19-07:39:31:60:[step-30800/42400: 72.64%]--[loss-3.762841]--[lr-0.000018]--[ETA-3:01:56]
2023.02.19-07:41:06:160:[step-30900/42400: 72.88%]--[loss-3.393988]--[lr-0.000018]--[ETA-3:00:51]
Saving best at epoch 146
End of epoch 146 / 200: train_loss: 3.540 	 val_loss: 3.840 	 time: 215 sec
2023.02.19-07:42:56:48:[step-31000/42400: 73.11%]--[loss-3.355572]--[lr-0.000018]--[ETA-3:01:49]
2023.02.19-07:44:30:148:[step-31100/42400: 73.35%]--[loss-3.702755]--[lr-0.000018]--[ETA-2:58:23]
End of epoch 147 / 200: train_loss: 3.532 	 val_loss: 3.854 	 time: 215 sec
2023.02.19-07:46:19:36:[step-31200/42400: 73.58%]--[loss-3.420168]--[lr-0.000018]--[ETA-2:57:36]
2023.02.19-07:47:54:136:[step-31300/42400: 73.82%]--[loss-3.300612]--[lr-0.000018]--[ETA-2:55:34]
End of epoch 148 / 200: train_loss: 3.526 	 val_loss: 3.843 	 time: 215 sec
2023.02.19-07:49:43:24:[step-31400/42400: 74.06%]--[loss-3.751307]--[lr-0.000017]--[ETA-2:56:47]
2023.02.19-07:51:18:124:[step-31500/42400: 74.29%]--[loss-3.413690]--[lr-0.000017]--[ETA-2:51:26]
End of epoch 149 / 200: train_loss: 3.531 	 val_loss: 3.861 	 time: 214 sec
2023.02.19-07:53:07:12:[step-31600/42400: 74.53%]--[loss-3.666109]--[lr-0.000017]--[ETA-2:49:37]
2023.02.19-07:54:43:112:[step-31700/42400: 74.76%]--[loss-3.652523]--[lr-0.000017]--[ETA-2:51:10]
2023.02.19-07:56:17:212:[step-31800/42400: 75.00%]--[loss-3.271441]--[lr-0.000017]--[ETA-0:32:22]
Saving best at epoch 150
End of epoch 150 / 200: train_loss: 3.521 	 val_loss: 3.821 	 time: 216 sec
Saving the model at the end of epoch 150, iters 31800
2023.02.19-07:58:08:100:[step-31900/42400: 75.24%]--[loss-3.441044]--[lr-0.000017]--[ETA-2:44:51]
2023.02.19-07:59:43:200:[step-32000/42400: 75.47%]--[loss-3.466411]--[lr-0.000017]--[ETA-2:43:12]
End of epoch 151 / 200: train_loss: 3.517 	 val_loss: 3.829 	 time: 216 sec
2023.02.19-08:01:32:88:[step-32100/42400: 75.71%]--[loss-3.756513]--[lr-0.000016]--[ETA-2:42:09]
2023.02.19-08:03:06:188:[step-32200/42400: 75.94%]--[loss-3.137867]--[lr-0.000016]--[ETA-2:40:49]
End of epoch 152 / 200: train_loss: 3.518 	 val_loss: 3.822 	 time: 214 sec
2023.02.19-08:04:55:76:[step-32300/42400: 76.18%]--[loss-3.925548]--[lr-0.000016]--[ETA-2:40:00]
2023.02.19-08:06:29:176:[step-32400/42400: 76.42%]--[loss-3.740249]--[lr-0.000016]--[ETA-2:36:59]
Saving best at epoch 153
End of epoch 153 / 200: train_loss: 3.518 	 val_loss: 3.814 	 time: 214 sec
2023.02.19-08:08:18:64:[step-32500/42400: 76.65%]--[loss-3.444403]--[lr-0.000016]--[ETA-2:36:35]
2023.02.19-08:09:53:164:[step-32600/42400: 76.89%]--[loss-3.330712]--[lr-0.000016]--[ETA-2:34:07]
End of epoch 154 / 200: train_loss: 3.509 	 val_loss: 3.862 	 time: 215 sec
2023.02.19-08:11:42:52:[step-32700/42400: 77.12%]--[loss-3.462162]--[lr-0.000015]--[ETA-2:32:28]
2023.02.19-08:13:17:152:[step-32800/42400: 77.36%]--[loss-3.539565]--[lr-0.000015]--[ETA-2:31:28]
End of epoch 155 / 200: train_loss: 3.516 	 val_loss: 3.863 	 time: 214 sec
Saving the model at the end of epoch 155, iters 32860
2023.02.19-08:15:06:40:[step-32900/42400: 77.59%]--[loss-3.655842]--[lr-0.000015]--[ETA-2:30:19]
2023.02.19-08:16:41:140:[step-33000/42400: 77.83%]--[loss-3.707345]--[lr-0.000015]--[ETA-2:27:27]
End of epoch 156 / 200: train_loss: 3.505 	 val_loss: 3.842 	 time: 215 sec
2023.02.19-08:18:29:28:[step-33100/42400: 78.07%]--[loss-3.888524]--[lr-0.000015]--[ETA-2:27:05]
2023.02.19-08:20:04:128:[step-33200/42400: 78.30%]--[loss-3.465292]--[lr-0.000015]--[ETA-2:25:00]
Saving best at epoch 157
End of epoch 157 / 200: train_loss: 3.503 	 val_loss: 3.808 	 time: 215 sec
2023.02.19-08:21:54:16:[step-33300/42400: 78.54%]--[loss-3.399530]--[lr-0.000014]--[ETA-2:26:08]
2023.02.19-08:23:28:116:[step-33400/42400: 78.77%]--[loss-3.736091]--[lr-0.000014]--[ETA-2:21:54]
End of epoch 158 / 200: train_loss: 3.501 	 val_loss: 3.823 	 time: 215 sec
2023.02.19-08:25:18:4:[step-33500/42400: 79.01%]--[loss-3.652457]--[lr-0.000014]--[ETA-2:21:07]
2023.02.19-08:26:53:104:[step-33600/42400: 79.25%]--[loss-3.556550]--[lr-0.000014]--[ETA-2:17:39]
2023.02.19-08:28:27:204:[step-33700/42400: 79.48%]--[loss-3.446441]--[lr-0.000014]--[ETA-2:15:43]
End of epoch 159 / 200: train_loss: 3.500 	 val_loss: 3.860 	 time: 215 sec
2023.02.19-08:30:17:92:[step-33800/42400: 79.72%]--[loss-3.609076]--[lr-0.000014]--[ETA-2:14:45]
2023.02.19-08:31:52:192:[step-33900/42400: 79.95%]--[loss-3.345546]--[lr-0.000014]--[ETA-2:14:17]
Saving best at epoch 160
End of epoch 160 / 200: train_loss: 3.498 	 val_loss: 3.792 	 time: 216 sec
Saving the model at the end of epoch 160, iters 33920
2023.02.19-08:33:42:80:[step-34000/42400: 80.19%]--[loss-3.236268]--[lr-0.000013]--[ETA-2:12:38]
2023.02.19-08:35:17:180:[step-34100/42400: 80.42%]--[loss-3.316386]--[lr-0.000013]--[ETA-2:10:54]
End of epoch 161 / 200: train_loss: 3.486 	 val_loss: 3.821 	 time: 216 sec
2023.02.19-08:37:07:68:[step-34200/42400: 80.66%]--[loss-3.428632]--[lr-0.000013]--[ETA-2:11:41]
2023.02.19-08:38:42:168:[step-34300/42400: 80.90%]--[loss-3.323848]--[lr-0.000013]--[ETA-2:08:13]
End of epoch 162 / 200: train_loss: 3.488 	 val_loss: 3.812 	 time: 215 sec
2023.02.19-08:40:31:56:[step-34400/42400: 81.13%]--[loss-3.309171]--[lr-0.000013]--[ETA-2:05:51]
2023.02.19-08:42:07:156:[step-34500/42400: 81.37%]--[loss-3.357953]--[lr-0.000013]--[ETA-2:04:21]
End of epoch 163 / 200: train_loss: 3.484 	 val_loss: 3.842 	 time: 216 sec
2023.02.19-08:43:56:44:[step-34600/42400: 81.60%]--[loss-3.247553]--[lr-0.000012]--[ETA-2:02:57]
2023.02.19-08:45:31:144:[step-34700/42400: 81.84%]--[loss-3.262907]--[lr-0.000012]--[ETA-2:01:47]
End of epoch 164 / 200: train_loss: 3.485 	 val_loss: 3.793 	 time: 216 sec
2023.02.19-08:47:21:32:[step-34800/42400: 82.08%]--[loss-3.226823]--[lr-0.000012]--[ETA-1:59:58]
2023.02.19-08:48:56:132:[step-34900/42400: 82.31%]--[loss-3.377760]--[lr-0.000012]--[ETA-1:58:48]
End of epoch 165 / 200: train_loss: 3.473 	 val_loss: 3.796 	 time: 216 sec
Saving the model at the end of epoch 165, iters 34980
2023.02.19-08:50:47:20:[step-35000/42400: 82.55%]--[loss-3.586173]--[lr-0.000012]--[ETA-1:57:20]
2023.02.19-08:52:22:120:[step-35100/42400: 82.78%]--[loss-3.558692]--[lr-0.000012]--[ETA-1:56:20]
End of epoch 166 / 200: train_loss: 3.473 	 val_loss: 3.796 	 time: 216 sec
2023.02.19-08:54:12:8:[step-35200/42400: 83.02%]--[loss-3.550688]--[lr-0.000011]--[ETA-1:55:53]
2023.02.19-08:55:47:108:[step-35300/42400: 83.25%]--[loss-3.354436]--[lr-0.000011]--[ETA-1:51:15]
2023.02.19-08:57:21:208:[step-35400/42400: 83.49%]--[loss-3.502658]--[lr-0.000011]--[ETA-1:49:26]
Saving best at epoch 167
End of epoch 167 / 200: train_loss: 3.470 	 val_loss: 3.774 	 time: 216 sec
2023.02.19-08:59:12:96:[step-35500/42400: 83.73%]--[loss-3.355740]--[lr-0.000011]--[ETA-1:50:22]
2023.02.19-09:00:47:196:[step-35600/42400: 83.96%]--[loss-3.181296]--[lr-0.000011]--[ETA-1:47:06]
End of epoch 168 / 200: train_loss: 3.469 	 val_loss: 3.784 	 time: 216 sec
2023.02.19-09:02:37:84:[step-35700/42400: 84.20%]--[loss-3.452144]--[lr-0.000011]--[ETA-1:46:02]
2023.02.19-09:04:12:184:[step-35800/42400: 84.43%]--[loss-3.202763]--[lr-0.000011]--[ETA-1:44:46]
End of epoch 169 / 200: train_loss: 3.467 	 val_loss: 3.833 	 time: 216 sec
2023.02.19-09:06:02:72:[step-35900/42400: 84.67%]--[loss-3.814655]--[lr-0.000010]--[ETA-1:42:16]
2023.02.19-09:07:37:172:[step-36000/42400: 84.91%]--[loss-3.441311]--[lr-0.000010]--[ETA-1:41:10]
End of epoch 170 / 200: train_loss: 3.469 	 val_loss: 3.780 	 time: 216 sec
Saving the model at the end of epoch 170, iters 36040
2023.02.19-09:09:27:60:[step-36100/42400: 85.14%]--[loss-3.307009]--[lr-0.000010]--[ETA-1:39:20]
2023.02.19-09:11:02:160:[step-36200/42400: 85.38%]--[loss-3.479575]--[lr-0.000010]--[ETA-1:38:32]
Saving best at epoch 171
End of epoch 171 / 200: train_loss: 3.466 	 val_loss: 3.770 	 time: 216 sec
2023.02.19-09:12:51:48:[step-36300/42400: 85.61%]--[loss-3.280689]--[lr-0.000010]--[ETA-1:35:51]
2023.02.19-09:14:26:148:[step-36400/42400: 85.85%]--[loss-3.564645]--[lr-0.000010]--[ETA-1:34:54]
End of epoch 172 / 200: train_loss: 3.459 	 val_loss: 3.774 	 time: 214 sec
2023.02.19-09:16:15:36:[step-36500/42400: 86.08%]--[loss-3.356227]--[lr-0.000009]--[ETA-1:32:32]
2023.02.19-09:17:50:136:[step-36600/42400: 86.32%]--[loss-4.092027]--[lr-0.000009]--[ETA-1:31:27]
End of epoch 173 / 200: train_loss: 3.455 	 val_loss: 3.799 	 time: 215 sec
2023.02.19-09:19:38:24:[step-36700/42400: 86.56%]--[loss-3.478706]--[lr-0.000009]--[ETA-1:31:00]
2023.02.19-09:21:14:124:[step-36800/42400: 86.79%]--[loss-3.405009]--[lr-0.000009]--[ETA-1:28:15]
Saving best at epoch 174
End of epoch 174 / 200: train_loss: 3.455 	 val_loss: 3.761 	 time: 215 sec
2023.02.19-09:23:03:12:[step-36900/42400: 87.03%]--[loss-3.432619]--[lr-0.000009]--[ETA-1:27:01]
2023.02.19-09:24:38:112:[step-37000/42400: 87.26%]--[loss-3.327858]--[lr-0.000009]--[ETA-1:25:41]
2023.02.19-09:26:12:212:[step-37100/42400: 87.50%]--[loss-3.459993]--[lr-0.000009]--[ETA-0:15:36]
End of epoch 175 / 200: train_loss: 3.451 	 val_loss: 3.761 	 time: 215 sec
Saving the model at the end of epoch 175, iters 37100
2023.02.19-09:28:02:100:[step-37200/42400: 87.74%]--[loss-3.724417]--[lr-0.000008]--[ETA-1:22:39]
2023.02.19-09:29:37:200:[step-37300/42400: 87.97%]--[loss-3.372352]--[lr-0.000008]--[ETA-1:19:41]
Saving best at epoch 176
End of epoch 176 / 200: train_loss: 3.447 	 val_loss: 3.755 	 time: 215 sec
2023.02.19-09:31:26:88:[step-37400/42400: 88.21%]--[loss-3.771601]--[lr-0.000008]--[ETA-1:18:37]
2023.02.19-09:33:00:188:[step-37500/42400: 88.44%]--[loss-3.637659]--[lr-0.000008]--[ETA-1:17:31]
Saving best at epoch 177
End of epoch 177 / 200: train_loss: 3.442 	 val_loss: 3.748 	 time: 214 sec
2023.02.19-09:34:50:76:[step-37600/42400: 88.68%]--[loss-3.167918]--[lr-0.000008]--[ETA-1:16:16]
2023.02.19-09:36:24:176:[step-37700/42400: 88.92%]--[loss-3.698058]--[lr-0.000008]--[ETA-1:14:00]
End of epoch 178 / 200: train_loss: 3.438 	 val_loss: 3.756 	 time: 215 sec
2023.02.19-09:38:13:64:[step-37800/42400: 89.15%]--[loss-3.560089]--[lr-0.000007]--[ETA-1:12:13]
2023.02.19-09:39:48:164:[step-37900/42400: 89.39%]--[loss-3.356036]--[lr-0.000007]--[ETA-1:10:59]
Saving best at epoch 179
End of epoch 179 / 200: train_loss: 3.435 	 val_loss: 3.742 	 time: 215 sec
2023.02.19-09:41:38:52:[step-38000/42400: 89.62%]--[loss-3.592605]--[lr-0.000007]--[ETA-1:10:22]
2023.02.19-09:43:13:152:[step-38100/42400: 89.86%]--[loss-3.327126]--[lr-0.000007]--[ETA-1:08:13]
End of epoch 180 / 200: train_loss: 3.435 	 val_loss: 3.745 	 time: 216 sec
Saving the model at the end of epoch 180, iters 38160
2023.02.19-09:45:02:40:[step-38200/42400: 90.09%]--[loss-3.552944]--[lr-0.000007]--[ETA-1:06:48]
2023.02.19-09:46:37:140:[step-38300/42400: 90.33%]--[loss-3.235932]--[lr-0.000007]--[ETA-1:04:26]
End of epoch 181 / 200: train_loss: 3.436 	 val_loss: 3.749 	 time: 214 sec
2023.02.19-09:48:26:28:[step-38400/42400: 90.57%]--[loss-3.137158]--[lr-0.000006]--[ETA-1:03:40]
2023.02.19-09:50:00:128:[step-38500/42400: 90.80%]--[loss-3.296766]--[lr-0.000006]--[ETA-1:01:47]
End of epoch 182 / 200: train_loss: 3.431 	 val_loss: 3.760 	 time: 215 sec
2023.02.19-09:51:49:16:[step-38600/42400: 91.04%]--[loss-3.416672]--[lr-0.000006]--[ETA-0:59:44]
2023.02.19-09:53:24:116:[step-38700/42400: 91.27%]--[loss-3.219057]--[lr-0.000006]--[ETA-0:58:16]
End of epoch 183 / 200: train_loss: 3.431 	 val_loss: 3.744 	 time: 215 sec
2023.02.19-09:55:13:4:[step-38800/42400: 91.51%]--[loss-3.334945]--[lr-0.000006]--[ETA-0:57:20]
2023.02.19-09:56:48:104:[step-38900/42400: 91.75%]--[loss-3.549166]--[lr-0.000006]--[ETA-0:55:20]
2023.02.19-09:58:23:204:[step-39000/42400: 91.98%]--[loss-3.309422]--[lr-0.000006]--[ETA-0:53:07]
Saving best at epoch 184
End of epoch 184 / 200: train_loss: 3.428 	 val_loss: 3.735 	 time: 215 sec
2023.02.19-10:00:13:92:[step-39100/42400: 92.22%]--[loss-3.339100]--[lr-0.000005]--[ETA-0:52:21]
2023.02.19-10:01:48:192:[step-39200/42400: 92.45%]--[loss-3.520998]--[lr-0.000005]--[ETA-0:50:36]
End of epoch 185 / 200: train_loss: 3.425 	 val_loss: 3.750 	 time: 216 sec
Saving the model at the end of epoch 185, iters 39220
2023.02.19-10:03:38:80:[step-39300/42400: 92.69%]--[loss-3.374473]--[lr-0.000005]--[ETA-0:48:38]
2023.02.19-10:05:13:180:[step-39400/42400: 92.92%]--[loss-3.311761]--[lr-0.000005]--[ETA-0:47:48]
End of epoch 186 / 200: train_loss: 3.424 	 val_loss: 3.744 	 time: 216 sec
2023.02.19-10:07:03:68:[step-39500/42400: 93.16%]--[loss-3.116442]--[lr-0.000005]--[ETA-0:46:04]
2023.02.19-10:08:38:168:[step-39600/42400: 93.40%]--[loss-3.419302]--[lr-0.000005]--[ETA-0:44:17]
Saving best at epoch 187
End of epoch 187 / 200: train_loss: 3.424 	 val_loss: 3.726 	 time: 216 sec
2023.02.19-10:10:28:56:[step-39700/42400: 93.63%]--[loss-3.275778]--[lr-0.000004]--[ETA-0:42:36]
2023.02.19-10:12:03:156:[step-39800/42400: 93.87%]--[loss-3.267909]--[lr-0.000004]--[ETA-0:40:57]
End of epoch 188 / 200: train_loss: 3.417 	 val_loss: 3.732 	 time: 216 sec
2023.02.19-10:13:53:44:[step-39900/42400: 94.10%]--[loss-3.355949]--[lr-0.000004]--[ETA-0:40:16]
2023.02.19-10:15:28:144:[step-40000/42400: 94.34%]--[loss-3.430304]--[lr-0.000004]--[ETA-0:37:59]
End of epoch 189 / 200: train_loss: 3.417 	 val_loss: 3.728 	 time: 216 sec
2023.02.19-10:17:17:32:[step-40100/42400: 94.58%]--[loss-3.706028]--[lr-0.000004]--[ETA-0:36:39]
2023.02.19-10:18:52:132:[step-40200/42400: 94.81%]--[loss-3.493646]--[lr-0.000004]--[ETA-0:34:36]
End of epoch 190 / 200: train_loss: 3.417 	 val_loss: 3.735 	 time: 215 sec
Saving the model at the end of epoch 190, iters 40280
2023.02.19-10:20:41:20:[step-40300/42400: 95.05%]--[loss-3.453560]--[lr-0.000003]--[ETA-0:33:28]
2023.02.19-10:22:16:120:[step-40400/42400: 95.28%]--[loss-3.303453]--[lr-0.000003]--[ETA-0:31:36]
End of epoch 191 / 200: train_loss: 3.418 	 val_loss: 3.744 	 time: 215 sec
2023.02.19-10:24:05:8:[step-40500/42400: 95.52%]--[loss-3.190159]--[lr-0.000003]--[ETA-0:30:11]
2023.02.19-10:25:41:108:[step-40600/42400: 95.75%]--[loss-3.398860]--[lr-0.000003]--[ETA-0:28:43]
2023.02.19-10:27:16:208:[step-40700/42400: 95.99%]--[loss-3.353809]--[lr-0.000003]--[ETA-0:26:38]
End of epoch 192 / 200: train_loss: 3.416 	 val_loss: 3.737 	 time: 215 sec
2023.02.19-10:29:06:96:[step-40800/42400: 96.23%]--[loss-3.105476]--[lr-0.000003]--[ETA-0:25:09]
2023.02.19-10:30:41:196:[step-40900/42400: 96.46%]--[loss-3.241313]--[lr-0.000003]--[ETA-0:23:52]
Saving best at epoch 193
End of epoch 193 / 200: train_loss: 3.410 	 val_loss: 3.718 	 time: 217 sec
2023.02.19-10:32:31:84:[step-41000/42400: 96.70%]--[loss-3.469505]--[lr-0.000002]--[ETA-0:22:28]
2023.02.19-10:34:07:184:[step-41100/42400: 96.93%]--[loss-3.351572]--[lr-0.000002]--[ETA-0:20:39]
End of epoch 194 / 200: train_loss: 3.409 	 val_loss: 3.734 	 time: 216 sec
2023.02.19-10:35:57:72:[step-41200/42400: 97.17%]--[loss-4.103380]--[lr-0.000002]--[ETA-0:18:52]
2023.02.19-10:37:31:172:[step-41300/42400: 97.41%]--[loss-3.401689]--[lr-0.000002]--[ETA-0:17:22]
Saving best at epoch 195
End of epoch 195 / 200: train_loss: 3.408 	 val_loss: 3.717 	 time: 216 sec
Saving the model at the end of epoch 195, iters 41340
2023.02.19-10:39:22:60:[step-41400/42400: 97.64%]--[loss-3.406719]--[lr-0.000002]--[ETA-0:15:41]
2023.02.19-10:40:56:160:[step-41500/42400: 97.88%]--[loss-3.432229]--[lr-0.000002]--[ETA-0:14:11]
Saving best at epoch 196
End of epoch 196 / 200: train_loss: 3.406 	 val_loss: 3.717 	 time: 216 sec
2023.02.19-10:42:46:48:[step-41600/42400: 98.11%]--[loss-3.242262]--[lr-0.000001]--[ETA-0:12:34]
2023.02.19-10:44:21:148:[step-41700/42400: 98.35%]--[loss-3.281734]--[lr-0.000001]--[ETA-0:11:01]
Saving best at epoch 197
End of epoch 197 / 200: train_loss: 3.404 	 val_loss: 3.715 	 time: 215 sec
2023.02.19-10:46:11:36:[step-41800/42400: 98.58%]--[loss-3.431582]--[lr-0.000001]--[ETA-0:09:31]
2023.02.19-10:47:46:136:[step-41900/42400: 98.82%]--[loss-3.296202]--[lr-0.000001]--[ETA-0:08:02]
Saving best at epoch 198
End of epoch 198 / 200: train_loss: 3.404 	 val_loss: 3.712 	 time: 217 sec
2023.02.19-10:49:36:24:[step-42000/42400: 99.06%]--[loss-3.576872]--[lr-0.000001]--[ETA-0:06:19]
2023.02.19-10:51:12:124:[step-42100/42400: 99.29%]--[loss-3.239243]--[lr-0.000001]--[ETA-0:04:49]
Saving best at epoch 199
End of epoch 199 / 200: train_loss: 3.404 	 val_loss: 3.709 	 time: 216 sec
2023.02.19-10:53:01:12:[step-42200/42400: 99.53%]--[loss-3.322436]--[lr-0.000000]--[ETA-0:03:10]
2023.02.19-10:54:36:112:[step-42300/42400: 99.76%]--[loss-3.213508]--[lr-0.000000]--[ETA-0:01:34]
2023.02.19-10:56:09:212:[step-42400/42400: 100.00%]--[loss-3.664187]--[lr-0.000000]--[ETA-0:00:37]
Saving best at epoch 200
End of epoch 200 / 200: train_loss: 3.402 	 val_loss: 3.708 	 time: 214 sec
Saving the model at the end of epoch 200, iters 42400
