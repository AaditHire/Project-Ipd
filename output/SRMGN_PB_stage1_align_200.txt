/opt/conda/envs/srmgn/lib/python3.10/site-packages/torch/distributed/launch.py:180: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
------------ Options -------------
PBAFN_gen_checkpoint: None
PBAFN_warp_checkpoint: None
PFAFN_gen_checkpoint: None
PFAFN_warp_checkpoint: None
batchSize: 64
beta1: 0.5
checkpoints_dir: checkpoints
continue_train: False
data_type: 32
dataroot: ../dataset/Flow-Style-VTON/VITON_traindata
debug: False
display_freq: 100
display_winsize: 512
fineSize: 512
gpu_ids: [2]
input_nc: 3
isTrain: True
label_nc: 14
lambda_feat: 10.0
launcher: pytorch
loadSize: 512
load_pretrain: 
local_rank: 0
lr: 5e-05
max_dataset_size: inf
nThreads: 1
n_blocks_global: 4
n_blocks_local: 3
n_downsample_global: 4
n_layers_D: 3
n_local_enhancers: 1
name: SRMGN_PB_stage1_align_200
ndf: 64
netG: global
ngf: 64
niter: 50
niter_decay: 150
niter_fix_global: 0
no_flip: False
no_ganFeat_loss: False
no_html: False
no_lsgan: False
no_vgg_loss: False
norm: instance
num_D: 2
num_gpus: 1
output_nc: 3
phase: train
pool_size: 0
print_freq: 100
resize_or_crop: None
save_epoch_freq: 5
save_latest_freq: 1000
serial_batches: False
tf_log: False
tv_weight: 0.1
use_dropout: False
valroot: dataset/VITON_valdata/
verbose: False
which_epoch: latest
-------------- End ----------------
------------ Options -------------
PBAFN_gen_checkpoint: None
PBAFN_warp_checkpoint: None
PFAFN_gen_checkpoint: None
PFAFN_warp_checkpoint: None
batchSize: 64
beta1: 0.5
checkpoints_dir: checkpoints
continue_train: False
data_type: 32
dataroot: ../dataset/Flow-Style-VTON/VITON_traindata
debug: False
display_freq: 100
display_winsize: 512
fineSize: 512
gpu_ids: [2]
input_nc: 3
isTrain: True
label_nc: 14
lambda_feat: 10.0
launcher: pytorch
loadSize: 512
load_pretrain: 
local_rank: 0
lr: 5e-05
max_dataset_size: inf
nThreads: 1
n_blocks_global: 4
n_blocks_local: 3
n_downsample_global: 4
n_layers_D: 3
n_local_enhancers: 1
name: SRMGN_PB_stage1_align_200
ndf: 64
netG: global
ngf: 64
niter: 50
niter_decay: 150
niter_fix_global: 0
no_flip: False
no_ganFeat_loss: False
no_html: False
no_lsgan: False
no_vgg_loss: False
norm: instance
num_D: 2
num_gpus: 1
output_nc: 3
phase: train
pool_size: 0
print_freq: 100
resize_or_crop: None
save_epoch_freq: 5
save_latest_freq: 1000
serial_batches: False
tf_log: False
tv_weight: 0.1
use_dropout: False
valroot: dataset/VITON_valdata/
verbose: False
which_epoch: latest
-------------- End ----------------
dataset [AlignedDataset] was created
../dataset/Flow-Style-VTON/VITON_traindata/train_label label
../dataset/Flow-Style-VTON/VITON_traindata/train_img img
../dataset/Flow-Style-VTON/VITON_traindata/train_edge edge
../dataset/Flow-Style-VTON/VITON_traindata/train_color color
/opt/conda/envs/srmgn/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
2023.02.21-15:31:47:100:[step-100/44600: 0.22%]--[loss-6.961091]--[lr-0.000050]--[ETA-11:34:03]
2023.02.21-15:33:22:200:[step-200/44600: 0.45%]--[loss-6.705900]--[lr-0.000050]--[ETA-11:33:58]
End of epoch 1 / 200: train_loss: 7.152 	 time: 220 sec
2023.02.21-15:35:05:77:[step-300/44600: 0.67%]--[loss-6.508420]--[lr-0.000050]--[ETA-11:37:31]
2023.02.21-15:36:41:177:[step-400/44600: 0.90%]--[loss-5.192202]--[lr-0.000050]--[ETA-11:42:37]
End of epoch 2 / 200: train_loss: 5.917 	 time: 221 sec
2023.02.21-15:38:23:54:[step-500/44600: 1.12%]--[loss-5.576917]--[lr-0.000050]--[ETA-11:59:49]
2023.02.21-15:39:59:154:[step-600/44600: 1.35%]--[loss-5.377079]--[lr-0.000050]--[ETA-11:37:46]
End of epoch 3 / 200: train_loss: 5.529 	 time: 220 sec
2023.02.21-15:41:41:31:[step-700/44600: 1.57%]--[loss-5.124079]--[lr-0.000050]--[ETA-11:31:01]
2023.02.21-15:43:17:131:[step-800/44600: 1.79%]--[loss-5.030783]--[lr-0.000050]--[ETA-11:36:12]
End of epoch 4 / 200: train_loss: 5.314 	 time: 220 sec
2023.02.21-15:45:00:8:[step-900/44600: 2.02%]--[loss-5.568556]--[lr-0.000050]--[ETA-11:34:45]
2023.02.21-15:46:36:108:[step-1000/44600: 2.24%]--[loss-5.090511]--[lr-0.000050]--[ETA-11:57:13]
2023.02.21-15:48:12:208:[step-1100/44600: 2.47%]--[loss-5.564929]--[lr-0.000050]--[ETA-11:27:09]
End of epoch 5 / 200: train_loss: 5.169 	 time: 221 sec
Saving the model at the end of epoch 5, iters 1115
2023.02.21-15:49:55:85:[step-1200/44600: 2.69%]--[loss-5.113635]--[lr-0.000050]--[ETA-11:20:37]
2023.02.21-15:51:31:185:[step-1300/44600: 2.91%]--[loss-5.057083]--[lr-0.000050]--[ETA-11:32:48]
End of epoch 6 / 200: train_loss: 5.060 	 time: 220 sec
2023.02.21-15:53:14:62:[step-1400/44600: 3.14%]--[loss-5.100642]--[lr-0.000050]--[ETA-11:20:44]
2023.02.21-15:54:50:162:[step-1500/44600: 3.36%]--[loss-5.253727]--[lr-0.000050]--[ETA-11:30:51]
End of epoch 7 / 200: train_loss: 4.973 	 time: 220 sec
2023.02.21-15:56:31:39:[step-1600/44600: 3.59%]--[loss-4.829025]--[lr-0.000050]--[ETA-11:27:07]
2023.02.21-15:58:07:139:[step-1700/44600: 3.81%]--[loss-5.004260]--[lr-0.000050]--[ETA-11:24:46]
End of epoch 8 / 200: train_loss: 4.884 	 time: 219 sec
2023.02.21-15:59:51:16:[step-1800/44600: 4.04%]--[loss-5.052845]--[lr-0.000050]--[ETA-11:34:14]
2023.02.21-16:01:26:116:[step-1900/44600: 4.26%]--[loss-4.851922]--[lr-0.000050]--[ETA-11:20:18]
2023.02.21-16:03:02:216:[step-2000/44600: 4.48%]--[loss-4.671702]--[lr-0.000050]--[ETA-11:20:46]
End of epoch 9 / 200: train_loss: 4.814 	 time: 221 sec
2023.02.21-16:04:46:93:[step-2100/44600: 4.71%]--[loss-4.679792]--[lr-0.000050]--[ETA-11:27:40]
2023.02.21-16:06:23:193:[step-2200/44600: 4.93%]--[loss-4.515524]--[lr-0.000050]--[ETA-11:05:18]
End of epoch 10 / 200: train_loss: 4.754 	 time: 223 sec
Saving the model at the end of epoch 10, iters 2230
2023.02.21-16:08:07:70:[step-2300/44600: 5.16%]--[loss-4.704731]--[lr-0.000050]--[ETA-11:24:33]
2023.02.21-16:09:43:170:[step-2400/44600: 5.38%]--[loss-4.370886]--[lr-0.000050]--[ETA-11:20:51]
End of epoch 11 / 200: train_loss: 4.682 	 time: 222 sec
2023.02.21-16:11:31:47:[step-2500/44600: 5.61%]--[loss-4.579066]--[lr-0.000050]--[ETA-11:38:36]
2023.02.21-16:13:10:147:[step-2600/44600: 5.83%]--[loss-4.514266]--[lr-0.000050]--[ETA-11:21:32]
End of epoch 12 / 200: train_loss: 4.642 	 time: 229 sec
2023.02.21-16:14:55:24:[step-2700/44600: 6.05%]--[loss-4.382951]--[lr-0.000050]--[ETA-11:12:35]
2023.02.21-16:16:31:124:[step-2800/44600: 6.28%]--[loss-4.420538]--[lr-0.000050]--[ETA-11:19:46]
End of epoch 13 / 200: train_loss: 4.600 	 time: 221 sec
2023.02.21-16:18:14:1:[step-2900/44600: 6.50%]--[loss-4.601344]--[lr-0.000050]--[ETA-13:27:39]
2023.02.21-16:19:50:101:[step-3000/44600: 6.73%]--[loss-4.758121]--[lr-0.000050]--[ETA-11:18:20]
2023.02.21-16:21:27:201:[step-3100/44600: 6.95%]--[loss-5.007461]--[lr-0.000050]--[ETA-11:04:36]
End of epoch 14 / 200: train_loss: 4.557 	 time: 222 sec
2023.02.21-16:23:13:78:[step-3200/44600: 7.17%]--[loss-4.717978]--[lr-0.000050]--[ETA-12:10:18]
2023.02.21-16:24:51:178:[step-3300/44600: 7.40%]--[loss-4.424630]--[lr-0.000050]--[ETA-10:54:35]
End of epoch 15 / 200: train_loss: 4.515 	 time: 225 sec
Saving the model at the end of epoch 15, iters 3345
2023.02.21-16:26:38:55:[step-3400/44600: 7.62%]--[loss-4.225100]--[lr-0.000050]--[ETA-10:59:28]
2023.02.21-16:28:15:155:[step-3500/44600: 7.85%]--[loss-4.489948]--[lr-0.000050]--[ETA-11:07:08]
End of epoch 16 / 200: train_loss: 4.485 	 time: 227 sec
2023.02.21-16:30:01:32:[step-3600/44600: 8.07%]--[loss-4.602759]--[lr-0.000050]--[ETA-11:06:04]
2023.02.21-16:31:38:132:[step-3700/44600: 8.30%]--[loss-4.218860]--[lr-0.000050]--[ETA-11:15:15]
End of epoch 17 / 200: train_loss: 4.465 	 time: 225 sec
2023.02.21-16:33:26:9:[step-3800/44600: 8.52%]--[loss-4.870360]--[lr-0.000050]--[ETA-11:40:56]
2023.02.21-16:35:05:109:[step-3900/44600: 8.74%]--[loss-4.225045]--[lr-0.000050]--[ETA-10:58:44]
2023.02.21-16:36:42:209:[step-4000/44600: 8.97%]--[loss-4.626553]--[lr-0.000050]--[ETA-10:41:45]
End of epoch 18 / 200: train_loss: 4.430 	 time: 228 sec
2023.02.21-16:38:29:86:[step-4100/44600: 9.19%]--[loss-4.404493]--[lr-0.000050]--[ETA-10:43:03]
2023.02.21-16:40:07:186:[step-4200/44600: 9.42%]--[loss-4.721126]--[lr-0.000050]--[ETA-11:13:27]
End of epoch 19 / 200: train_loss: 4.407 	 time: 227 sec
2023.02.21-16:41:54:63:[step-4300/44600: 9.64%]--[loss-4.208262]--[lr-0.000050]--[ETA-10:55:54]
2023.02.21-16:43:31:163:[step-4400/44600: 9.87%]--[loss-4.591688]--[lr-0.000050]--[ETA-10:49:53]
End of epoch 20 / 200: train_loss: 4.387 	 time: 226 sec
Saving the model at the end of epoch 20, iters 4460
2023.02.21-16:45:18:40:[step-4500/44600: 10.09%]--[loss-4.312370]--[lr-0.000050]--[ETA-11:44:52]
2023.02.21-16:46:56:140:[step-4600/44600: 10.31%]--[loss-4.354141]--[lr-0.000050]--[ETA-10:46:13]
End of epoch 21 / 200: train_loss: 4.355 	 time: 226 sec
2023.02.21-16:48:41:17:[step-4700/44600: 10.54%]--[loss-4.336054]--[lr-0.000050]--[ETA-10:55:18]
2023.02.21-16:50:19:117:[step-4800/44600: 10.76%]--[loss-4.205443]--[lr-0.000050]--[ETA-10:50:05]
2023.02.21-16:52:01:217:[step-4900/44600: 10.99%]--[loss-4.267878]--[lr-0.000050]--[ETA-11:24:25]
End of epoch 22 / 200: train_loss: 4.334 	 time: 230 sec
2023.02.21-16:53:54:94:[step-5000/44600: 11.21%]--[loss-4.423552]--[lr-0.000050]--[ETA-11:23:58]
2023.02.21-16:55:40:194:[step-5100/44600: 11.43%]--[loss-4.476081]--[lr-0.000050]--[ETA-10:52:37]
End of epoch 23 / 200: train_loss: 4.311 	 time: 243 sec
2023.02.21-16:57:35:71:[step-5200/44600: 11.66%]--[loss-4.196490]--[lr-0.000050]--[ETA-11:08:13]
2023.02.21-16:59:19:171:[step-5300/44600: 11.88%]--[loss-4.173440]--[lr-0.000050]--[ETA-11:20:25]
End of epoch 24 / 200: train_loss: 4.293 	 time: 243 sec
2023.02.21-17:01:15:48:[step-5400/44600: 12.11%]--[loss-4.314645]--[lr-0.000050]--[ETA-11:13:58]
2023.02.21-17:03:01:148:[step-5500/44600: 12.33%]--[loss-4.158152]--[lr-0.000050]--[ETA-12:05:44]
End of epoch 25 / 200: train_loss: 4.286 	 time: 247 sec
Saving the model at the end of epoch 25, iters 5575
2023.02.21-17:04:57:25:[step-5600/44600: 12.56%]--[loss-4.474107]--[lr-0.000050]--[ETA-13:39:09]
2023.02.21-17:06:43:125:[step-5700/44600: 12.78%]--[loss-4.168011]--[lr-0.000050]--[ETA-13:05:17]
End of epoch 26 / 200: train_loss: 4.267 	 time: 246 sec
2023.02.21-17:08:38:2:[step-5800/44600: 13.00%]--[loss-4.668557]--[lr-0.000050]--[ETA-13:15:56]
2023.02.21-17:10:26:102:[step-5900/44600: 13.23%]--[loss-4.217674]--[lr-0.000050]--[ETA-10:45:19]
2023.02.21-17:12:12:202:[step-6000/44600: 13.45%]--[loss-4.454516]--[lr-0.000050]--[ETA-11:44:05]
End of epoch 27 / 200: train_loss: 4.250 	 time: 246 sec
2023.02.21-17:14:08:79:[step-6100/44600: 13.68%]--[loss-4.097780]--[lr-0.000050]--[ETA-10:19:24]
2023.02.21-17:15:54:179:[step-6200/44600: 13.90%]--[loss-4.498081]--[lr-0.000050]--[ETA-11:50:57]
End of epoch 28 / 200: train_loss: 4.231 	 time: 247 sec
2023.02.21-17:17:51:56:[step-6300/44600: 14.13%]--[loss-4.171717]--[lr-0.000050]--[ETA-10:46:37]
2023.02.21-17:19:40:156:[step-6400/44600: 14.35%]--[loss-3.954702]--[lr-0.000050]--[ETA-11:55:30]
End of epoch 29 / 200: train_loss: 4.220 	 time: 250 sec
2023.02.21-17:21:40:33:[step-6500/44600: 14.57%]--[loss-4.301128]--[lr-0.000050]--[ETA-11:27:55]
2023.02.21-17:23:30:133:[step-6600/44600: 14.80%]--[loss-4.224811]--[lr-0.000050]--[ETA-11:54:50]
End of epoch 30 / 200: train_loss: 4.218 	 time: 256 sec
Saving the model at the end of epoch 30, iters 6690
2023.02.21-17:25:28:10:[step-6700/44600: 15.02%]--[loss-4.118877]--[lr-0.000050]--[ETA-11:14:35]
2023.02.21-17:27:19:110:[step-6800/44600: 15.25%]--[loss-4.005270]--[lr-0.000050]--[ETA-11:37:01]
2023.02.21-17:29:09:210:[step-6900/44600: 15.47%]--[loss-4.233789]--[lr-0.000050]--[ETA-11:09:42]
End of epoch 31 / 200: train_loss: 4.194 	 time: 253 sec
2023.02.21-17:31:09:87:[step-7000/44600: 15.70%]--[loss-4.013856]--[lr-0.000050]--[ETA-11:59:19]
2023.02.21-17:32:58:187:[step-7100/44600: 15.92%]--[loss-4.160828]--[lr-0.000050]--[ETA-10:56:50]
End of epoch 32 / 200: train_loss: 4.177 	 time: 254 sec
2023.02.21-17:34:57:64:[step-7200/44600: 16.14%]--[loss-3.882424]--[lr-0.000050]--[ETA-11:42:31]
2023.02.21-17:36:50:164:[step-7300/44600: 16.37%]--[loss-4.088294]--[lr-0.000050]--[ETA-10:55:55]
End of epoch 33 / 200: train_loss: 4.170 	 time: 258 sec
2023.02.21-17:38:52:41:[step-7400/44600: 16.59%]--[loss-4.241616]--[lr-0.000050]--[ETA-12:38:27]
2023.02.21-17:40:45:141:[step-7500/44600: 16.82%]--[loss-4.136318]--[lr-0.000050]--[ETA-12:58:32]
End of epoch 34 / 200: train_loss: 4.154 	 time: 261 sec
2023.02.21-17:42:48:18:[step-7600/44600: 17.04%]--[loss-3.911691]--[lr-0.000050]--[ETA-13:52:18]
2023.02.21-17:44:42:118:[step-7700/44600: 17.26%]--[loss-4.328968]--[lr-0.000050]--[ETA-12:26:46]
2023.02.21-17:46:38:218:[step-7800/44600: 17.49%]--[loss-3.893888]--[lr-0.000050]--[ETA-12:32:10]
End of epoch 35 / 200: train_loss: 4.130 	 time: 266 sec
Saving the model at the end of epoch 35, iters 7805
2023.02.21-17:48:44:95:[step-7900/44600: 17.71%]--[loss-3.998663]--[lr-0.000050]--[ETA-10:52:14]
2023.02.21-17:50:40:195:[step-8000/44600: 17.94%]--[loss-4.131363]--[lr-0.000050]--[ETA-11:10:52]
End of epoch 36 / 200: train_loss: 4.119 	 time: 268 sec
2023.02.21-17:52:47:72:[step-8100/44600: 18.16%]--[loss-4.415613]--[lr-0.000050]--[ETA-11:30:27]
2023.02.21-17:54:44:172:[step-8200/44600: 18.39%]--[loss-4.358357]--[lr-0.000050]--[ETA-11:09:13]
End of epoch 37 / 200: train_loss: 4.110 	 time: 272 sec
2023.02.21-17:56:53:49:[step-8300/44600: 18.61%]--[loss-3.858115]--[lr-0.000050]--[ETA-9:44:58]
2023.02.21-17:58:53:149:[step-8400/44600: 18.83%]--[loss-4.166141]--[lr-0.000050]--[ETA-9:53:20]
End of epoch 38 / 200: train_loss: 4.105 	 time: 276 sec
2023.02.21-18:01:02:26:[step-8500/44600: 19.06%]--[loss-3.942314]--[lr-0.000050]--[ETA-12:34:17]
2023.02.21-18:03:04:126:[step-8600/44600: 19.28%]--[loss-4.458799]--[lr-0.000050]--[ETA-13:22:58]
End of epoch 39 / 200: train_loss: 4.106 	 time: 279 sec
2023.02.21-18:05:13:3:[step-8700/44600: 19.51%]--[loss-3.973427]--[lr-0.000050]--[ETA-13:39:54]
2023.02.21-18:06:57:103:[step-8800/44600: 19.73%]--[loss-3.856172]--[lr-0.000050]--[ETA-9:41:42]
2023.02.21-18:08:34:203:[step-8900/44600: 19.96%]--[loss-4.033434]--[lr-0.000050]--[ETA-9:33:26]
End of epoch 40 / 200: train_loss: 4.088 	 time: 232 sec
Saving the model at the end of epoch 40, iters 8920
2023.02.21-18:10:22:80:[step-9000/44600: 20.18%]--[loss-3.657269]--[lr-0.000050]--[ETA-9:44:54]
2023.02.21-18:12:00:180:[step-9100/44600: 20.40%]--[loss-3.973641]--[lr-0.000050]--[ETA-9:32:29]
End of epoch 41 / 200: train_loss: 4.090 	 time: 228 sec
2023.02.21-18:13:47:57:[step-9200/44600: 20.63%]--[loss-4.354014]--[lr-0.000050]--[ETA-9:24:05]
2023.02.21-18:15:25:157:[step-9300/44600: 20.85%]--[loss-4.216314]--[lr-0.000050]--[ETA-9:43:39]
End of epoch 42 / 200: train_loss: 4.067 	 time: 227 sec
2023.02.21-18:17:12:34:[step-9400/44600: 21.08%]--[loss-3.981709]--[lr-0.000050]--[ETA-9:20:40]
2023.02.21-18:18:49:134:[step-9500/44600: 21.30%]--[loss-3.946651]--[lr-0.000050]--[ETA-9:24:56]
End of epoch 43 / 200: train_loss: 4.069 	 time: 226 sec
2023.02.21-18:20:37:11:[step-9600/44600: 21.52%]--[loss-4.267204]--[lr-0.000050]--[ETA-9:36:14]
2023.02.21-18:22:14:111:[step-9700/44600: 21.75%]--[loss-4.264163]--[lr-0.000050]--[ETA-9:24:00]
2023.02.21-18:23:52:211:[step-9800/44600: 21.97%]--[loss-3.903880]--[lr-0.000050]--[ETA-9:26:33]
End of epoch 44 / 200: train_loss: 4.051 	 time: 228 sec
2023.02.21-18:25:38:88:[step-9900/44600: 22.20%]--[loss-3.674139]--[lr-0.000050]--[ETA-9:22:42]
2023.02.21-18:27:16:188:[step-10000/44600: 22.42%]--[loss-3.838362]--[lr-0.000050]--[ETA-9:15:00]
End of epoch 45 / 200: train_loss: 4.035 	 time: 226 sec
Saving the model at the end of epoch 45, iters 10035
2023.02.21-18:29:03:65:[step-10100/44600: 22.65%]--[loss-4.030534]--[lr-0.000050]--[ETA-9:29:36]
2023.02.21-18:30:40:165:[step-10200/44600: 22.87%]--[loss-3.817931]--[lr-0.000050]--[ETA-9:18:44]
End of epoch 46 / 200: train_loss: 4.030 	 time: 226 sec
2023.02.21-18:32:27:42:[step-10300/44600: 23.09%]--[loss-4.408396]--[lr-0.000050]--[ETA-9:21:11]
2023.02.21-18:34:04:142:[step-10400/44600: 23.32%]--[loss-4.266080]--[lr-0.000050]--[ETA-9:21:00]
End of epoch 47 / 200: train_loss: 4.022 	 time: 227 sec
2023.02.21-18:35:50:19:[step-10500/44600: 23.54%]--[loss-3.853154]--[lr-0.000050]--[ETA-9:07:53]
2023.02.21-18:37:28:119:[step-10600/44600: 23.77%]--[loss-3.954785]--[lr-0.000050]--[ETA-9:03:07]
2023.02.21-18:39:05:219:[step-10700/44600: 23.99%]--[loss-4.128746]--[lr-0.000050]--[ETA-9:10:18]
End of epoch 48 / 200: train_loss: 4.034 	 time: 225 sec
2023.02.21-18:40:50:96:[step-10800/44600: 24.22%]--[loss-3.964830]--[lr-0.000050]--[ETA-9:18:27]
2023.02.21-18:42:28:196:[step-10900/44600: 24.44%]--[loss-3.818201]--[lr-0.000050]--[ETA-8:58:51]
End of epoch 49 / 200: train_loss: 4.020 	 time: 224 sec
2023.02.21-18:44:14:73:[step-11000/44600: 24.66%]--[loss-3.867802]--[lr-0.000050]--[ETA-8:58:03]
2023.02.21-18:45:52:173:[step-11100/44600: 24.89%]--[loss-4.384593]--[lr-0.000050]--[ETA-9:00:04]
End of epoch 50 / 200: train_loss: 4.027 	 time: 226 sec
Saving the model at the end of epoch 50, iters 11150
2023.02.21-18:47:37:50:[step-11200/44600: 25.11%]--[loss-3.726574]--[lr-0.000050]--[ETA-8:59:23]
2023.02.21-18:49:14:150:[step-11300/44600: 25.34%]--[loss-3.689770]--[lr-0.000050]--[ETA-8:59:02]
End of epoch 51 / 200: train_loss: 3.999 	 time: 225 sec
2023.02.21-18:50:59:27:[step-11400/44600: 25.56%]--[loss-3.837627]--[lr-0.000050]--[ETA-8:54:38]
2023.02.21-18:52:37:127:[step-11500/44600: 25.78%]--[loss-3.801679]--[lr-0.000050]--[ETA-8:51:48]
End of epoch 52 / 200: train_loss: 3.988 	 time: 223 sec
2023.02.21-18:54:23:4:[step-11600/44600: 26.01%]--[loss-3.812394]--[lr-0.000049]--[ETA-9:10:02]
2023.02.21-18:56:00:104:[step-11700/44600: 26.23%]--[loss-4.170340]--[lr-0.000049]--[ETA-8:48:36]
2023.02.21-18:57:40:204:[step-11800/44600: 26.46%]--[loss-4.317706]--[lr-0.000049]--[ETA-9:21:08]
End of epoch 53 / 200: train_loss: 3.971 	 time: 230 sec
2023.02.21-18:59:33:81:[step-11900/44600: 26.68%]--[loss-4.156619]--[lr-0.000049]--[ETA-9:12:48]
2023.02.21-19:01:15:181:[step-12000/44600: 26.91%]--[loss-4.055973]--[lr-0.000049]--[ETA-9:38:23]
End of epoch 54 / 200: train_loss: 3.964 	 time: 238 sec
2023.02.21-19:03:10:58:[step-12100/44600: 27.13%]--[loss-3.789972]--[lr-0.000049]--[ETA-9:20:35]
2023.02.21-19:04:54:158:[step-12200/44600: 27.35%]--[loss-4.235740]--[lr-0.000049]--[ETA-9:30:14]
End of epoch 55 / 200: train_loss: 3.961 	 time: 242 sec
Saving the model at the end of epoch 55, iters 12265
2023.02.21-19:06:46:35:[step-12300/44600: 27.58%]--[loss-3.690139]--[lr-0.000048]--[ETA-9:57:51]
2023.02.21-19:08:29:135:[step-12400/44600: 27.80%]--[loss-4.065663]--[lr-0.000048]--[ETA-9:35:53]
End of epoch 56 / 200: train_loss: 3.947 	 time: 238 sec
2023.02.21-19:10:22:12:[step-12500/44600: 28.03%]--[loss-3.926989]--[lr-0.000048]--[ETA-9:09:36]
2023.02.21-19:12:06:112:[step-12600/44600: 28.25%]--[loss-3.884370]--[lr-0.000048]--[ETA-9:19:19]
2023.02.21-19:13:49:212:[step-12700/44600: 28.48%]--[loss-3.911814]--[lr-0.000048]--[ETA-9:20:24]
End of epoch 57 / 200: train_loss: 3.932 	 time: 239 sec
2023.02.21-19:15:42:89:[step-12800/44600: 28.70%]--[loss-3.881750]--[lr-0.000048]--[ETA-8:36:20]
2023.02.21-19:17:25:189:[step-12900/44600: 28.92%]--[loss-3.952625]--[lr-0.000048]--[ETA-9:23:04]
End of epoch 58 / 200: train_loss: 3.927 	 time: 240 sec
2023.02.21-19:19:16:66:[step-13000/44600: 29.15%]--[loss-3.819998]--[lr-0.000047]--[ETA-8:45:58]
2023.02.21-19:20:58:166:[step-13100/44600: 29.37%]--[loss-4.160741]--[lr-0.000047]--[ETA-8:47:37]
End of epoch 59 / 200: train_loss: 3.920 	 time: 235 sec
2023.02.21-19:22:52:43:[step-13200/44600: 29.60%]--[loss-3.661242]--[lr-0.000047]--[ETA-8:33:43]
2023.02.21-19:24:36:143:[step-13300/44600: 29.82%]--[loss-3.631532]--[lr-0.000047]--[ETA-9:08:19]
End of epoch 60 / 200: train_loss: 3.921 	 time: 241 sec
Saving the model at the end of epoch 60, iters 13380
2023.02.21-19:26:29:20:[step-13400/44600: 30.04%]--[loss-3.726522]--[lr-0.000047]--[ETA-9:45:50]
2023.02.21-19:28:12:120:[step-13500/44600: 30.27%]--[loss-4.019824]--[lr-0.000047]--[ETA-9:10:51]
2023.02.21-19:29:54:220:[step-13600/44600: 30.49%]--[loss-3.696010]--[lr-0.000047]--[ETA-8:30:35]
End of epoch 61 / 200: train_loss: 3.910 	 time: 240 sec
2023.02.21-19:31:48:97:[step-13700/44600: 30.72%]--[loss-4.040459]--[lr-0.000046]--[ETA-8:32:44]
2023.02.21-19:33:30:197:[step-13800/44600: 30.94%]--[loss-4.051802]--[lr-0.000046]--[ETA-8:43:11]
End of epoch 62 / 200: train_loss: 3.900 	 time: 238 sec
2023.02.21-19:35:23:74:[step-13900/44600: 31.17%]--[loss-4.045032]--[lr-0.000046]--[ETA-9:06:08]
2023.02.21-19:37:07:174:[step-14000/44600: 31.39%]--[loss-3.744169]--[lr-0.000046]--[ETA-8:39:33]
End of epoch 63 / 200: train_loss: 3.884 	 time: 241 sec
2023.02.21-19:39:02:51:[step-14100/44600: 31.61%]--[loss-3.423697]--[lr-0.000046]--[ETA-8:30:40]
2023.02.21-19:40:46:151:[step-14200/44600: 31.84%]--[loss-3.841258]--[lr-0.000046]--[ETA-8:33:43]
End of epoch 64 / 200: train_loss: 3.881 	 time: 243 sec
2023.02.21-19:42:42:28:[step-14300/44600: 32.06%]--[loss-3.651804]--[lr-0.000045]--[ETA-8:15:20]
2023.02.21-19:44:26:128:[step-14400/44600: 32.29%]--[loss-3.874500]--[lr-0.000045]--[ETA-8:12:06]
End of epoch 65 / 200: train_loss: 3.870 	 time: 242 sec
Saving the model at the end of epoch 65, iters 14495
2023.02.21-19:46:21:5:[step-14500/44600: 32.51%]--[loss-3.823163]--[lr-0.000045]--[ETA-8:47:43]
2023.02.21-19:48:08:105:[step-14600/44600: 32.74%]--[loss-4.154746]--[lr-0.000045]--[ETA-8:01:25]
2023.02.21-19:49:52:205:[step-14700/44600: 32.96%]--[loss-3.524218]--[lr-0.000045]--[ETA-9:06:45]
End of epoch 66 / 200: train_loss: 3.867 	 time: 246 sec
2023.02.21-19:51:47:82:[step-14800/44600: 33.18%]--[loss-3.719202]--[lr-0.000045]--[ETA-9:32:11]
2023.02.21-19:53:31:182:[step-14900/44600: 33.41%]--[loss-3.605897]--[lr-0.000045]--[ETA-8:30:16]
End of epoch 67 / 200: train_loss: 3.867 	 time: 242 sec
2023.02.21-19:55:24:59:[step-15000/44600: 33.63%]--[loss-3.847409]--[lr-0.000044]--[ETA-9:11:14]
2023.02.21-19:57:11:159:[step-15100/44600: 33.86%]--[loss-3.484075]--[lr-0.000044]--[ETA-9:15:54]
End of epoch 68 / 200: train_loss: 3.858 	 time: 245 sec
2023.02.21-19:59:05:36:[step-15200/44600: 34.08%]--[loss-4.056261]--[lr-0.000044]--[ETA-9:38:30]
2023.02.21-20:00:53:136:[step-15300/44600: 34.30%]--[loss-3.936943]--[lr-0.000044]--[ETA-7:46:12]
End of epoch 69 / 200: train_loss: 3.854 	 time: 246 sec
2023.02.21-20:02:50:13:[step-15400/44600: 34.53%]--[loss-3.734009]--[lr-0.000044]--[ETA-8:49:38]
2023.02.21-20:04:38:113:[step-15500/44600: 34.75%]--[loss-3.918555]--[lr-0.000044]--[ETA-8:49:19]
2023.02.21-20:06:17:213:[step-15600/44600: 34.98%]--[loss-3.656384]--[lr-0.000044]--[ETA-7:43:18]
End of epoch 70 / 200: train_loss: 3.845 	 time: 241 sec
Saving the model at the end of epoch 70, iters 15610
2023.02.21-20:08:01:90:[step-15700/44600: 35.20%]--[loss-4.030493]--[lr-0.000043]--[ETA-7:51:50]
2023.02.21-20:09:39:190:[step-15800/44600: 35.43%]--[loss-3.488296]--[lr-0.000043]--[ETA-7:53:29]
End of epoch 71 / 200: train_loss: 3.839 	 time: 224 sec
2023.02.21-20:11:25:67:[step-15900/44600: 35.65%]--[loss-4.262608]--[lr-0.000043]--[ETA-7:34:57]
2023.02.21-20:13:03:167:[step-16000/44600: 35.87%]--[loss-3.720608]--[lr-0.000043]--[ETA-7:44:31]
End of epoch 72 / 200: train_loss: 3.826 	 time: 226 sec
2023.02.21-20:14:49:44:[step-16100/44600: 36.10%]--[loss-4.197695]--[lr-0.000043]--[ETA-7:33:46]
2023.02.21-20:16:27:144:[step-16200/44600: 36.32%]--[loss-3.744146]--[lr-0.000043]--[ETA-7:33:55]
End of epoch 73 / 200: train_loss: 3.823 	 time: 225 sec
2023.02.21-20:18:11:21:[step-16300/44600: 36.55%]--[loss-3.890874]--[lr-0.000042]--[ETA-7:34:21]
2023.02.21-20:19:49:121:[step-16400/44600: 36.77%]--[loss-3.802366]--[lr-0.000042]--[ETA-7:36:27]
2023.02.21-20:21:26:221:[step-16500/44600: 37.00%]--[loss-3.639863]--[lr-0.000042]--[ETA-7:30:02]
End of epoch 74 / 200: train_loss: 3.826 	 time: 225 sec
2023.02.21-20:23:13:98:[step-16600/44600: 37.22%]--[loss-3.706520]--[lr-0.000042]--[ETA-7:28:17]
2023.02.21-20:24:50:198:[step-16700/44600: 37.44%]--[loss-4.249837]--[lr-0.000042]--[ETA-7:28:10]
End of epoch 75 / 200: train_loss: 3.823 	 time: 226 sec
Saving the model at the end of epoch 75, iters 16725
2023.02.21-20:26:36:75:[step-16800/44600: 37.67%]--[loss-3.514653]--[lr-0.000042]--[ETA-7:46:25]
2023.02.21-20:28:13:175:[step-16900/44600: 37.89%]--[loss-3.602592]--[lr-0.000042]--[ETA-7:26:36]
End of epoch 76 / 200: train_loss: 3.801 	 time: 225 sec
2023.02.21-20:29:59:52:[step-17000/44600: 38.12%]--[loss-3.830942]--[lr-0.000041]--[ETA-7:32:32]
2023.02.21-20:31:36:152:[step-17100/44600: 38.34%]--[loss-3.856333]--[lr-0.000041]--[ETA-7:24:15]
End of epoch 77 / 200: train_loss: 3.795 	 time: 225 sec
2023.02.21-20:33:23:29:[step-17200/44600: 38.57%]--[loss-4.085347]--[lr-0.000041]--[ETA-7:33:28]
2023.02.21-20:35:01:129:[step-17300/44600: 38.79%]--[loss-3.697524]--[lr-0.000041]--[ETA-7:21:21]
End of epoch 78 / 200: train_loss: 3.788 	 time: 226 sec
2023.02.21-20:36:48:6:[step-17400/44600: 39.01%]--[loss-3.628082]--[lr-0.000041]--[ETA-7:22:03]
2023.02.21-20:38:25:106:[step-17500/44600: 39.24%]--[loss-3.841030]--[lr-0.000041]--[ETA-7:19:50]
2023.02.21-20:40:03:206:[step-17600/44600: 39.46%]--[loss-3.828954]--[lr-0.000041]--[ETA-7:08:35]
End of epoch 79 / 200: train_loss: 3.787 	 time: 227 sec
2023.02.21-20:41:48:83:[step-17700/44600: 39.69%]--[loss-3.907215]--[lr-0.000040]--[ETA-7:07:26]
2023.02.21-20:43:26:183:[step-17800/44600: 39.91%]--[loss-3.490511]--[lr-0.000040]--[ETA-7:12:14]
End of epoch 80 / 200: train_loss: 3.783 	 time: 224 sec
Saving the model at the end of epoch 80, iters 17840
2023.02.21-20:45:10:60:[step-17900/44600: 40.13%]--[loss-3.965356]--[lr-0.000040]--[ETA-7:08:27]
2023.02.21-20:46:48:160:[step-18000/44600: 40.36%]--[loss-4.064859]--[lr-0.000040]--[ETA-7:04:57]
End of epoch 81 / 200: train_loss: 3.800 	 time: 225 sec
2023.02.21-20:48:34:37:[step-18100/44600: 40.58%]--[loss-3.672628]--[lr-0.000040]--[ETA-7:05:27]
2023.02.21-20:50:10:137:[step-18200/44600: 40.81%]--[loss-3.795804]--[lr-0.000040]--[ETA-7:01:48]
End of epoch 82 / 200: train_loss: 3.781 	 time: 222 sec
2023.02.21-20:51:53:14:[step-18300/44600: 41.03%]--[loss-3.943274]--[lr-0.000039]--[ETA-6:53:40]
2023.02.21-20:53:29:114:[step-18400/44600: 41.26%]--[loss-3.828822]--[lr-0.000039]--[ETA-7:01:17]
2023.02.21-20:55:05:214:[step-18500/44600: 41.48%]--[loss-3.815428]--[lr-0.000039]--[ETA-6:55:31]
End of epoch 83 / 200: train_loss: 3.769 	 time: 221 sec
2023.02.21-20:56:50:91:[step-18600/44600: 41.70%]--[loss-4.171648]--[lr-0.000039]--[ETA-7:06:46]
2023.02.21-20:58:26:191:[step-18700/44600: 41.93%]--[loss-3.990974]--[lr-0.000039]--[ETA-6:51:21]
End of epoch 84 / 200: train_loss: 3.759 	 time: 222 sec
2023.02.21-21:00:09:68:[step-18800/44600: 42.15%]--[loss-3.693393]--[lr-0.000039]--[ETA-7:00:01]
2023.02.21-21:01:45:168:[step-18900/44600: 42.38%]--[loss-3.771293]--[lr-0.000039]--[ETA-6:50:24]
End of epoch 85 / 200: train_loss: 3.752 	 time: 221 sec
Saving the model at the end of epoch 85, iters 18955
2023.02.21-21:03:29:45:[step-19000/44600: 42.60%]--[loss-3.882663]--[lr-0.000038]--[ETA-6:51:00]
2023.02.21-21:05:06:145:[step-19100/44600: 42.83%]--[loss-3.695124]--[lr-0.000038]--[ETA-6:45:56]
End of epoch 86 / 200: train_loss: 3.761 	 time: 221 sec
2023.02.21-21:06:47:22:[step-19200/44600: 43.05%]--[loss-3.595206]--[lr-0.000038]--[ETA-6:41:29]
2023.02.21-21:08:24:122:[step-19300/44600: 43.27%]--[loss-4.333030]--[lr-0.000038]--[ETA-6:40:19]
2023.02.21-21:09:59:222:[step-19400/44600: 43.50%]--[loss-4.019589]--[lr-0.000038]--[ETA-6:39:07]
End of epoch 87 / 200: train_loss: 3.741 	 time: 220 sec
2023.02.21-21:11:42:99:[step-19500/44600: 43.72%]--[loss-3.989603]--[lr-0.000038]--[ETA-6:44:54]
2023.02.21-21:13:18:199:[step-19600/44600: 43.95%]--[loss-3.522383]--[lr-0.000038]--[ETA-6:45:35]
End of epoch 88 / 200: train_loss: 3.737 	 time: 220 sec
2023.02.21-21:15:01:76:[step-19700/44600: 44.17%]--[loss-3.707503]--[lr-0.000037]--[ETA-6:40:00]
2023.02.21-21:16:37:176:[step-19800/44600: 44.39%]--[loss-4.083204]--[lr-0.000037]--[ETA-6:35:49]
End of epoch 89 / 200: train_loss: 3.729 	 time: 221 sec
2023.02.21-21:18:21:53:[step-19900/44600: 44.62%]--[loss-3.729354]--[lr-0.000037]--[ETA-6:34:18]
2023.02.21-21:19:57:153:[step-20000/44600: 44.84%]--[loss-3.564367]--[lr-0.000037]--[ETA-6:30:09]
End of epoch 90 / 200: train_loss: 3.730 	 time: 221 sec
Saving the model at the end of epoch 90, iters 20070
2023.02.21-21:21:41:30:[step-20100/44600: 45.07%]--[loss-3.653451]--[lr-0.000037]--[ETA-6:40:12]
2023.02.21-21:23:17:130:[step-20200/44600: 45.29%]--[loss-3.886796]--[lr-0.000037]--[ETA-6:28:12]
End of epoch 91 / 200: train_loss: 3.745 	 time: 221 sec
2023.02.21-21:25:00:7:[step-20300/44600: 45.52%]--[loss-4.381987]--[lr-0.000036]--[ETA-6:38:05]
2023.02.21-21:26:36:107:[step-20400/44600: 45.74%]--[loss-3.455184]--[lr-0.000036]--[ETA-6:28:11]
2023.02.21-21:28:12:207:[step-20500/44600: 45.96%]--[loss-3.774133]--[lr-0.000036]--[ETA-6:26:09]
End of epoch 92 / 200: train_loss: 3.724 	 time: 221 sec
2023.02.21-21:29:55:84:[step-20600/44600: 46.19%]--[loss-3.539366]--[lr-0.000036]--[ETA-6:28:40]
2023.02.21-21:31:31:184:[step-20700/44600: 46.41%]--[loss-3.656398]--[lr-0.000036]--[ETA-6:21:33]
End of epoch 93 / 200: train_loss: 3.720 	 time: 221 sec
2023.02.21-21:33:16:61:[step-20800/44600: 46.64%]--[loss-3.698045]--[lr-0.000036]--[ETA-6:13:02]
2023.02.21-21:34:51:161:[step-20900/44600: 46.86%]--[loss-3.682564]--[lr-0.000036]--[ETA-6:13:49]
End of epoch 94 / 200: train_loss: 3.712 	 time: 221 sec
2023.02.21-21:36:34:38:[step-21000/44600: 47.09%]--[loss-3.478950]--[lr-0.000035]--[ETA-6:20:03]
2023.02.21-21:38:11:138:[step-21100/44600: 47.31%]--[loss-3.552241]--[lr-0.000035]--[ETA-6:13:41]
End of epoch 95 / 200: train_loss: 3.709 	 time: 221 sec
Saving the model at the end of epoch 95, iters 21185
2023.02.21-21:39:55:15:[step-21200/44600: 47.53%]--[loss-3.649993]--[lr-0.000035]--[ETA-6:13:47]
2023.02.21-21:41:31:115:[step-21300/44600: 47.76%]--[loss-3.676059]--[lr-0.000035]--[ETA-6:12:37]
2023.02.21-21:43:06:215:[step-21400/44600: 47.98%]--[loss-3.640550]--[lr-0.000035]--[ETA-6:06:59]
End of epoch 96 / 200: train_loss: 3.707 	 time: 221 sec
2023.02.21-21:44:50:92:[step-21500/44600: 48.21%]--[loss-4.071440]--[lr-0.000035]--[ETA-6:11:55]
2023.02.21-21:46:26:192:[step-21600/44600: 48.43%]--[loss-3.391124]--[lr-0.000035]--[ETA-6:06:27]
End of epoch 97 / 200: train_loss: 3.706 	 time: 221 sec
2023.02.21-21:48:09:69:[step-21700/44600: 48.65%]--[loss-3.627310]--[lr-0.000034]--[ETA-6:06:02]
2023.02.21-21:49:44:169:[step-21800/44600: 48.88%]--[loss-3.569547]--[lr-0.000034]--[ETA-6:05:47]
End of epoch 98 / 200: train_loss: 3.694 	 time: 220 sec
2023.02.21-21:51:27:46:[step-21900/44600: 49.10%]--[loss-3.497837]--[lr-0.000034]--[ETA-6:01:31]
2023.02.21-21:53:03:146:[step-22000/44600: 49.33%]--[loss-3.841621]--[lr-0.000034]--[ETA-6:10:48]
End of epoch 99 / 200: train_loss: 3.686 	 time: 221 sec
2023.02.21-21:54:47:23:[step-22100/44600: 49.55%]--[loss-3.399560]--[lr-0.000034]--[ETA-5:57:28]
2023.02.21-21:56:23:123:[step-22200/44600: 49.78%]--[loss-3.641809]--[lr-0.000034]--[ETA-6:09:49]
2023.02.21-21:57:58:223:[step-22300/44600: 50.00%]--[loss-4.153337]--[lr-0.000034]--[ETA-1:43:22]
End of epoch 100 / 200: train_loss: 3.686 	 time: 221 sec
Saving the model at the end of epoch 100, iters 22300
2023.02.21-21:59:43:100:[step-22400/44600: 50.22%]--[loss-3.567020]--[lr-0.000033]--[ETA-5:54:38]
2023.02.21-22:01:19:200:[step-22500/44600: 50.45%]--[loss-3.364473]--[lr-0.000033]--[ETA-5:54:29]
End of epoch 101 / 200: train_loss: 3.680 	 time: 221 sec
2023.02.21-22:03:02:77:[step-22600/44600: 50.67%]--[loss-3.525109]--[lr-0.000033]--[ETA-5:55:59]
2023.02.21-22:04:38:177:[step-22700/44600: 50.90%]--[loss-3.678211]--[lr-0.000033]--[ETA-5:48:27]
End of epoch 102 / 200: train_loss: 3.678 	 time: 221 sec
2023.02.21-22:06:21:54:[step-22800/44600: 51.12%]--[loss-3.545835]--[lr-0.000033]--[ETA-5:52:18]
2023.02.21-22:07:57:154:[step-22900/44600: 51.35%]--[loss-3.492040]--[lr-0.000033]--[ETA-5:47:49]
End of epoch 103 / 200: train_loss: 3.682 	 time: 221 sec
2023.02.21-22:09:41:31:[step-23000/44600: 51.57%]--[loss-3.747389]--[lr-0.000032]--[ETA-5:43:53]
2023.02.21-22:11:17:131:[step-23100/44600: 51.79%]--[loss-3.590262]--[lr-0.000032]--[ETA-5:37:40]
End of epoch 104 / 200: train_loss: 3.667 	 time: 221 sec
2023.02.21-22:13:00:8:[step-23200/44600: 52.02%]--[loss-3.795896]--[lr-0.000032]--[ETA-5:39:37]
2023.02.21-22:14:36:108:[step-23300/44600: 52.24%]--[loss-3.672872]--[lr-0.000032]--[ETA-5:36:34]
2023.02.21-22:16:12:208:[step-23400/44600: 52.47%]--[loss-3.616932]--[lr-0.000032]--[ETA-5:34:54]
End of epoch 105 / 200: train_loss: 3.662 	 time: 221 sec
Saving the model at the end of epoch 105, iters 23415
2023.02.21-22:17:55:85:[step-23500/44600: 52.69%]--[loss-3.725785]--[lr-0.000032]--[ETA-5:41:00]
2023.02.21-22:19:31:185:[step-23600/44600: 52.91%]--[loss-3.658071]--[lr-0.000032]--[ETA-5:36:57]
End of epoch 106 / 200: train_loss: 3.658 	 time: 220 sec
2023.02.21-22:21:14:62:[step-23700/44600: 53.14%]--[loss-3.546027]--[lr-0.000031]--[ETA-5:36:28]
2023.02.21-22:22:50:162:[step-23800/44600: 53.36%]--[loss-3.744832]--[lr-0.000031]--[ETA-5:37:15]
End of epoch 107 / 200: train_loss: 3.661 	 time: 221 sec
2023.02.21-22:24:33:39:[step-23900/44600: 53.59%]--[loss-3.646593]--[lr-0.000031]--[ETA-5:38:48]
2023.02.21-22:26:09:139:[step-24000/44600: 53.81%]--[loss-3.780271]--[lr-0.000031]--[ETA-5:28:40]
End of epoch 108 / 200: train_loss: 3.650 	 time: 220 sec
2023.02.21-22:27:52:16:[step-24100/44600: 54.04%]--[loss-3.694495]--[lr-0.000031]--[ETA-5:24:41]
2023.02.21-22:29:29:116:[step-24200/44600: 54.26%]--[loss-3.552921]--[lr-0.000031]--[ETA-5:23:55]
2023.02.21-22:31:04:216:[step-24300/44600: 54.48%]--[loss-3.697272]--[lr-0.000031]--[ETA-5:20:13]
End of epoch 109 / 200: train_loss: 3.642 	 time: 221 sec
2023.02.21-22:32:49:93:[step-24400/44600: 54.71%]--[loss-3.622874]--[lr-0.000030]--[ETA-5:23:53]
2023.02.21-22:34:25:193:[step-24500/44600: 54.93%]--[loss-3.811268]--[lr-0.000030]--[ETA-5:19:00]
End of epoch 110 / 200: train_loss: 3.642 	 time: 222 sec
Saving the model at the end of epoch 110, iters 24530
2023.02.21-22:36:08:70:[step-24600/44600: 55.16%]--[loss-3.779614]--[lr-0.000030]--[ETA-5:21:04]
2023.02.21-22:37:44:170:[step-24700/44600: 55.38%]--[loss-3.646135]--[lr-0.000030]--[ETA-5:15:42]
End of epoch 111 / 200: train_loss: 3.640 	 time: 221 sec
2023.02.21-22:39:29:47:[step-24800/44600: 55.61%]--[loss-4.119030]--[lr-0.000030]--[ETA-5:18:43]
2023.02.21-22:41:05:147:[step-24900/44600: 55.83%]--[loss-3.612126]--[lr-0.000030]--[ETA-5:25:41]
End of epoch 112 / 200: train_loss: 3.633 	 time: 222 sec
2023.02.21-22:42:49:24:[step-25000/44600: 56.05%]--[loss-3.516931]--[lr-0.000029]--[ETA-5:09:42]
2023.02.21-22:44:25:124:[step-25100/44600: 56.28%]--[loss-3.716509]--[lr-0.000029]--[ETA-5:06:54]
End of epoch 113 / 200: train_loss: 3.627 	 time: 222 sec
2023.02.21-22:46:08:1:[step-25200/44600: 56.50%]--[loss-3.496368]--[lr-0.000029]--[ETA-6:38:47]
2023.02.21-22:47:44:101:[step-25300/44600: 56.73%]--[loss-3.414175]--[lr-0.000029]--[ETA-5:13:49]
2023.02.21-22:49:20:201:[step-25400/44600: 56.95%]--[loss-3.447104]--[lr-0.000029]--[ETA-5:02:52]
End of epoch 114 / 200: train_loss: 3.624 	 time: 221 sec
2023.02.21-22:51:03:78:[step-25500/44600: 57.17%]--[loss-3.594146]--[lr-0.000029]--[ETA-5:00:51]
2023.02.21-22:52:38:178:[step-25600/44600: 57.40%]--[loss-3.714590]--[lr-0.000029]--[ETA-5:02:39]
End of epoch 115 / 200: train_loss: 3.622 	 time: 220 sec
Saving the model at the end of epoch 115, iters 25645
2023.02.21-22:54:22:55:[step-25700/44600: 57.62%]--[loss-3.714286]--[lr-0.000028]--[ETA-5:08:09]
2023.02.21-22:55:58:155:[step-25800/44600: 57.85%]--[loss-3.454743]--[lr-0.000028]--[ETA-5:01:18]
End of epoch 116 / 200: train_loss: 3.616 	 time: 221 sec
2023.02.21-22:57:42:32:[step-25900/44600: 58.07%]--[loss-3.641247]--[lr-0.000028]--[ETA-4:56:47]
2023.02.21-22:59:18:132:[step-26000/44600: 58.30%]--[loss-3.734458]--[lr-0.000028]--[ETA-4:55:31]
End of epoch 117 / 200: train_loss: 3.615 	 time: 221 sec
2023.02.21-23:01:01:9:[step-26100/44600: 58.52%]--[loss-3.620193]--[lr-0.000028]--[ETA-5:00:35]
2023.02.21-23:02:38:109:[step-26200/44600: 58.74%]--[loss-3.646803]--[lr-0.000028]--[ETA-4:51:41]
2023.02.21-23:04:13:209:[step-26300/44600: 58.97%]--[loss-3.472735]--[lr-0.000028]--[ETA-4:49:05]
End of epoch 118 / 200: train_loss: 3.612 	 time: 221 sec
2023.02.21-23:05:58:86:[step-26400/44600: 59.19%]--[loss-3.570889]--[lr-0.000027]--[ETA-4:51:14]
2023.02.21-23:07:34:186:[step-26500/44600: 59.42%]--[loss-3.470299]--[lr-0.000027]--[ETA-4:47:58]
End of epoch 119 / 200: train_loss: 3.605 	 time: 222 sec
2023.02.21-23:09:18:63:[step-26600/44600: 59.64%]--[loss-3.408711]--[lr-0.000027]--[ETA-4:46:05]
2023.02.21-23:10:54:163:[step-26700/44600: 59.87%]--[loss-3.392914]--[lr-0.000027]--[ETA-4:44:48]
End of epoch 120 / 200: train_loss: 3.602 	 time: 221 sec
Saving the model at the end of epoch 120, iters 26760
2023.02.21-23:12:37:40:[step-26800/44600: 60.09%]--[loss-3.543410]--[lr-0.000027]--[ETA-4:40:45]
2023.02.21-23:14:12:140:[step-26900/44600: 60.31%]--[loss-3.592252]--[lr-0.000027]--[ETA-4:43:12]
End of epoch 121 / 200: train_loss: 3.596 	 time: 220 sec
2023.02.21-23:15:57:17:[step-27000/44600: 60.54%]--[loss-3.819548]--[lr-0.000026]--[ETA-4:39:39]
2023.02.21-23:17:33:117:[step-27100/44600: 60.76%]--[loss-3.647537]--[lr-0.000026]--[ETA-4:38:25]
2023.02.21-23:19:09:217:[step-27200/44600: 60.99%]--[loss-3.597059]--[lr-0.000026]--[ETA-4:41:15]
End of epoch 122 / 200: train_loss: 3.597 	 time: 222 sec
2023.02.21-23:20:53:94:[step-27300/44600: 61.21%]--[loss-3.634643]--[lr-0.000026]--[ETA-4:34:01]
2023.02.21-23:22:30:194:[step-27400/44600: 61.43%]--[loss-3.801357]--[lr-0.000026]--[ETA-4:34:01]
End of epoch 123 / 200: train_loss: 3.598 	 time: 222 sec
2023.02.21-23:24:12:71:[step-27500/44600: 61.66%]--[loss-3.493852]--[lr-0.000026]--[ETA-4:34:53]
2023.02.21-23:25:49:171:[step-27600/44600: 61.88%]--[loss-3.527473]--[lr-0.000026]--[ETA-4:31:22]
End of epoch 124 / 200: train_loss: 3.586 	 time: 221 sec
2023.02.21-23:27:34:48:[step-27700/44600: 62.11%]--[loss-3.544963]--[lr-0.000025]--[ETA-4:30:44]
2023.02.21-23:29:09:148:[step-27800/44600: 62.33%]--[loss-3.503737]--[lr-0.000025]--[ETA-4:27:12]
End of epoch 125 / 200: train_loss: 3.581 	 time: 222 sec
Saving the model at the end of epoch 125, iters 27875
2023.02.21-23:30:52:25:[step-27900/44600: 62.56%]--[loss-3.545907]--[lr-0.000025]--[ETA-4:25:11]
2023.02.21-23:32:28:125:[step-28000/44600: 62.78%]--[loss-3.617969]--[lr-0.000025]--[ETA-4:22:19]
End of epoch 126 / 200: train_loss: 3.577 	 time: 220 sec
2023.02.21-23:34:13:2:[step-28100/44600: 63.00%]--[loss-3.683042]--[lr-0.000025]--[ETA-4:41:48]
2023.02.21-23:35:49:102:[step-28200/44600: 63.23%]--[loss-3.429580]--[lr-0.000025]--[ETA-4:18:40]
2023.02.21-23:37:24:202:[step-28300/44600: 63.45%]--[loss-3.535845]--[lr-0.000025]--[ETA-4:17:17]
End of epoch 127 / 200: train_loss: 3.573 	 time: 222 sec
2023.02.21-23:39:09:79:[step-28400/44600: 63.68%]--[loss-3.783918]--[lr-0.000024]--[ETA-4:19:42]
2023.02.21-23:40:45:179:[step-28500/44600: 63.90%]--[loss-3.814528]--[lr-0.000024]--[ETA-4:16:03]
End of epoch 128 / 200: train_loss: 3.573 	 time: 221 sec
2023.02.21-23:42:28:56:[step-28600/44600: 64.13%]--[loss-3.352880]--[lr-0.000024]--[ETA-4:14:21]
2023.02.21-23:44:04:156:[step-28700/44600: 64.35%]--[loss-3.626647]--[lr-0.000024]--[ETA-4:12:42]
End of epoch 129 / 200: train_loss: 3.570 	 time: 221 sec
2023.02.21-23:45:47:33:[step-28800/44600: 64.57%]--[loss-3.424534]--[lr-0.000024]--[ETA-4:09:53]
2023.02.21-23:47:23:133:[step-28900/44600: 64.80%]--[loss-3.527643]--[lr-0.000024]--[ETA-4:09:22]
End of epoch 130 / 200: train_loss: 3.566 	 time: 221 sec
Saving the model at the end of epoch 130, iters 28990
2023.02.21-23:49:06:10:[step-29000/44600: 65.02%]--[loss-3.281464]--[lr-0.000023]--[ETA-4:07:25]
2023.02.21-23:50:42:110:[step-29100/44600: 65.25%]--[loss-3.441206]--[lr-0.000023]--[ETA-4:08:51]
2023.02.21-23:52:18:210:[step-29200/44600: 65.47%]--[loss-3.497977]--[lr-0.000023]--[ETA-4:10:05]
End of epoch 131 / 200: train_loss: 3.562 	 time: 221 sec
2023.02.21-23:54:02:87:[step-29300/44600: 65.70%]--[loss-3.367579]--[lr-0.000023]--[ETA-4:03:46]
2023.02.21-23:55:38:187:[step-29400/44600: 65.92%]--[loss-3.356724]--[lr-0.000023]--[ETA-4:00:23]
End of epoch 132 / 200: train_loss: 3.558 	 time: 221 sec
2023.02.21-23:57:21:64:[step-29500/44600: 66.14%]--[loss-3.446492]--[lr-0.000023]--[ETA-3:59:37]
2023.02.21-23:58:57:164:[step-29600/44600: 66.37%]--[loss-3.918099]--[lr-0.000023]--[ETA-3:57:39]
End of epoch 133 / 200: train_loss: 3.549 	 time: 220 sec
2023.02.22-00:00:40:41:[step-29700/44600: 66.59%]--[loss-3.404962]--[lr-0.000022]--[ETA-3:57:08]
2023.02.22-00:02:16:141:[step-29800/44600: 66.82%]--[loss-3.625129]--[lr-0.000022]--[ETA-4:02:39]
End of epoch 134 / 200: train_loss: 3.548 	 time: 220 sec
2023.02.22-00:03:59:18:[step-29900/44600: 67.04%]--[loss-3.412318]--[lr-0.000022]--[ETA-3:53:12]
2023.02.22-00:05:36:118:[step-30000/44600: 67.26%]--[loss-3.481316]--[lr-0.000022]--[ETA-3:50:25]
2023.02.22-00:07:11:218:[step-30100/44600: 67.49%]--[loss-3.627016]--[lr-0.000022]--[ETA-3:47:42]
End of epoch 135 / 200: train_loss: 3.546 	 time: 221 sec
Saving the model at the end of epoch 135, iters 30105
2023.02.22-00:08:55:95:[step-30200/44600: 67.71%]--[loss-3.494149]--[lr-0.000022]--[ETA-3:54:01]
2023.02.22-00:10:30:195:[step-30300/44600: 67.94%]--[loss-3.871440]--[lr-0.000022]--[ETA-3:46:27]
End of epoch 136 / 200: train_loss: 3.540 	 time: 221 sec
2023.02.22-00:12:13:72:[step-30400/44600: 68.16%]--[loss-3.274521]--[lr-0.000021]--[ETA-3:47:34]
2023.02.22-00:13:49:172:[step-30500/44600: 68.39%]--[loss-3.542302]--[lr-0.000021]--[ETA-3:49:15]
End of epoch 137 / 200: train_loss: 3.537 	 time: 220 sec
2023.02.22-00:15:31:49:[step-30600/44600: 68.61%]--[loss-3.341082]--[lr-0.000021]--[ETA-3:42:58]
2023.02.22-00:17:07:149:[step-30700/44600: 68.83%]--[loss-3.615001]--[lr-0.000021]--[ETA-3:43:05]
End of epoch 138 / 200: train_loss: 3.535 	 time: 220 sec
2023.02.22-00:18:50:26:[step-30800/44600: 69.06%]--[loss-3.470615]--[lr-0.000021]--[ETA-3:43:06]
2023.02.22-00:20:26:126:[step-30900/44600: 69.28%]--[loss-3.566294]--[lr-0.000021]--[ETA-3:42:03]
End of epoch 139 / 200: train_loss: 3.532 	 time: 220 sec
2023.02.22-00:22:09:3:[step-31000/44600: 69.51%]--[loss-3.666312]--[lr-0.000020]--[ETA-3:43:02]
2023.02.22-00:23:45:103:[step-31100/44600: 69.73%]--[loss-3.281600]--[lr-0.000020]--[ETA-3:36:21]
2023.02.22-00:25:20:203:[step-31200/44600: 69.96%]--[loss-3.608426]--[lr-0.000020]--[ETA-3:32:56]
End of epoch 140 / 200: train_loss: 3.527 	 time: 220 sec
Saving the model at the end of epoch 140, iters 31220
2023.02.22-00:27:02:80:[step-31300/44600: 70.18%]--[loss-3.535098]--[lr-0.000020]--[ETA-3:29:54]
2023.02.22-00:28:38:180:[step-31400/44600: 70.40%]--[loss-3.660720]--[lr-0.000020]--[ETA-3:27:34]
End of epoch 141 / 200: train_loss: 3.525 	 time: 219 sec
2023.02.22-00:30:22:57:[step-31500/44600: 70.63%]--[loss-3.977507]--[lr-0.000020]--[ETA-3:29:56]
2023.02.22-00:31:58:157:[step-31600/44600: 70.85%]--[loss-3.372523]--[lr-0.000020]--[ETA-3:27:02]
End of epoch 142 / 200: train_loss: 3.525 	 time: 222 sec
2023.02.22-00:33:42:34:[step-31700/44600: 71.08%]--[loss-3.422170]--[lr-0.000019]--[ETA-3:29:45]
2023.02.22-00:35:18:134:[step-31800/44600: 71.30%]--[loss-3.739852]--[lr-0.000019]--[ETA-3:22:03]
End of epoch 143 / 200: train_loss: 3.522 	 time: 221 sec
2023.02.22-00:37:01:11:[step-31900/44600: 71.52%]--[loss-3.207844]--[lr-0.000019]--[ETA-3:19:35]
2023.02.22-00:38:37:111:[step-32000/44600: 71.75%]--[loss-3.450882]--[lr-0.000019]--[ETA-3:21:20]
2023.02.22-00:40:13:211:[step-32100/44600: 71.97%]--[loss-3.436455]--[lr-0.000019]--[ETA-3:17:31]
End of epoch 144 / 200: train_loss: 3.516 	 time: 221 sec
2023.02.22-00:41:56:88:[step-32200/44600: 72.20%]--[loss-3.680091]--[lr-0.000019]--[ETA-3:24:22]
2023.02.22-00:43:32:188:[step-32300/44600: 72.42%]--[loss-3.596408]--[lr-0.000019]--[ETA-3:21:20]
End of epoch 145 / 200: train_loss: 3.514 	 time: 220 sec
Saving the model at the end of epoch 145, iters 32335
2023.02.22-00:45:14:65:[step-32400/44600: 72.65%]--[loss-3.527641]--[lr-0.000018]--[ETA-3:13:58]
2023.02.22-00:46:51:165:[step-32500/44600: 72.87%]--[loss-3.284474]--[lr-0.000018]--[ETA-3:12:20]
End of epoch 146 / 200: train_loss: 3.508 	 time: 220 sec
2023.02.22-00:48:34:42:[step-32600/44600: 73.09%]--[loss-3.432378]--[lr-0.000018]--[ETA-3:12:11]
2023.02.22-00:50:10:142:[step-32700/44600: 73.32%]--[loss-3.390714]--[lr-0.000018]--[ETA-3:11:28]
End of epoch 147 / 200: train_loss: 3.505 	 time: 221 sec
2023.02.22-00:51:54:19:[step-32800/44600: 73.54%]--[loss-3.324619]--[lr-0.000018]--[ETA-3:11:00]
2023.02.22-00:53:30:119:[step-32900/44600: 73.77%]--[loss-3.234815]--[lr-0.000018]--[ETA-3:05:56]
2023.02.22-00:55:06:219:[step-33000/44600: 73.99%]--[loss-3.279808]--[lr-0.000018]--[ETA-3:03:14]
End of epoch 148 / 200: train_loss: 3.500 	 time: 222 sec
2023.02.22-00:56:50:96:[step-33100/44600: 74.22%]--[loss-3.274048]--[lr-0.000017]--[ETA-3:03:13]
2023.02.22-00:58:26:196:[step-33200/44600: 74.44%]--[loss-3.471427]--[lr-0.000017]--[ETA-3:00:20]
End of epoch 149 / 200: train_loss: 3.499 	 time: 221 sec
2023.02.22-01:00:10:73:[step-33300/44600: 74.66%]--[loss-3.563766]--[lr-0.000017]--[ETA-3:04:55]
2023.02.22-01:01:46:173:[step-33400/44600: 74.89%]--[loss-3.904078]--[lr-0.000017]--[ETA-2:56:39]
End of epoch 150 / 200: train_loss: 3.496 	 time: 222 sec
Saving the model at the end of epoch 150, iters 33450
2023.02.22-01:03:30:50:[step-33500/44600: 75.11%]--[loss-3.627617]--[lr-0.000017]--[ETA-2:56:28]
2023.02.22-01:05:06:150:[step-33600/44600: 75.34%]--[loss-3.579409]--[lr-0.000017]--[ETA-2:54:25]
End of epoch 151 / 200: train_loss: 3.496 	 time: 222 sec
2023.02.22-01:06:49:27:[step-33700/44600: 75.56%]--[loss-3.482149]--[lr-0.000016]--[ETA-2:51:13]
2023.02.22-01:08:25:127:[step-33800/44600: 75.78%]--[loss-3.492924]--[lr-0.000016]--[ETA-2:50:04]
End of epoch 152 / 200: train_loss: 3.494 	 time: 220 sec
2023.02.22-01:10:09:4:[step-33900/44600: 76.01%]--[loss-3.881186]--[lr-0.000016]--[ETA-2:47:15]
2023.02.22-01:11:45:104:[step-34000/44600: 76.23%]--[loss-3.574896]--[lr-0.000016]--[ETA-2:49:36]
2023.02.22-01:13:21:204:[step-34100/44600: 76.46%]--[loss-3.420522]--[lr-0.000016]--[ETA-2:45:35]
End of epoch 153 / 200: train_loss: 3.488 	 time: 222 sec
2023.02.22-01:15:04:81:[step-34200/44600: 76.68%]--[loss-3.681756]--[lr-0.000016]--[ETA-2:50:21]
2023.02.22-01:16:40:181:[step-34300/44600: 76.91%]--[loss-3.361802]--[lr-0.000016]--[ETA-2:45:44]
End of epoch 154 / 200: train_loss: 3.485 	 time: 220 sec
2023.02.22-01:18:23:58:[step-34400/44600: 77.13%]--[loss-3.692500]--[lr-0.000015]--[ETA-2:45:01]
2023.02.22-01:19:59:158:[step-34500/44600: 77.35%]--[loss-3.441776]--[lr-0.000015]--[ETA-2:46:16]
End of epoch 155 / 200: train_loss: 3.481 	 time: 221 sec
Saving the model at the end of epoch 155, iters 34565
2023.02.22-01:21:44:35:[step-34600/44600: 77.58%]--[loss-3.815508]--[lr-0.000015]--[ETA-2:39:41]
2023.02.22-01:23:19:135:[step-34700/44600: 77.80%]--[loss-3.657989]--[lr-0.000015]--[ETA-2:37:00]
End of epoch 156 / 200: train_loss: 3.476 	 time: 222 sec
2023.02.22-01:25:03:12:[step-34800/44600: 78.03%]--[loss-3.418985]--[lr-0.000015]--[ETA-2:35:10]
2023.02.22-01:26:39:112:[step-34900/44600: 78.25%]--[loss-3.558646]--[lr-0.000015]--[ETA-2:36:04]
2023.02.22-01:28:15:212:[step-35000/44600: 78.48%]--[loss-3.475892]--[lr-0.000015]--[ETA-2:30:32]
End of epoch 157 / 200: train_loss: 3.475 	 time: 221 sec
2023.02.22-01:29:59:89:[step-35100/44600: 78.70%]--[loss-3.399169]--[lr-0.000014]--[ETA-2:31:24]
2023.02.22-01:31:35:189:[step-35200/44600: 78.92%]--[loss-3.437406]--[lr-0.000014]--[ETA-2:31:24]
End of epoch 158 / 200: train_loss: 3.472 	 time: 221 sec
2023.02.22-01:33:18:66:[step-35300/44600: 79.15%]--[loss-3.382933]--[lr-0.000014]--[ETA-2:30:38]
2023.02.22-01:34:54:166:[step-35400/44600: 79.37%]--[loss-3.555959]--[lr-0.000014]--[ETA-2:29:26]
End of epoch 159 / 200: train_loss: 3.469 	 time: 221 sec
2023.02.22-01:36:36:43:[step-35500/44600: 79.60%]--[loss-3.828577]--[lr-0.000014]--[ETA-2:25:52]
2023.02.22-01:38:12:143:[step-35600/44600: 79.82%]--[loss-3.307309]--[lr-0.000014]--[ETA-2:23:29]
End of epoch 160 / 200: train_loss: 3.469 	 time: 220 sec
Saving the model at the end of epoch 160, iters 35680
2023.02.22-01:39:55:20:[step-35700/44600: 80.04%]--[loss-3.224602]--[lr-0.000013]--[ETA-2:24:27]
2023.02.22-01:41:31:120:[step-35800/44600: 80.27%]--[loss-3.476844]--[lr-0.000013]--[ETA-2:20:30]
2023.02.22-01:43:07:220:[step-35900/44600: 80.49%]--[loss-3.450769]--[lr-0.000013]--[ETA-2:19:37]
End of epoch 161 / 200: train_loss: 3.461 	 time: 221 sec
2023.02.22-01:44:51:97:[step-36000/44600: 80.72%]--[loss-3.615190]--[lr-0.000013]--[ETA-2:18:39]
2023.02.22-01:46:27:197:[step-36100/44600: 80.94%]--[loss-3.635898]--[lr-0.000013]--[ETA-2:14:11]
End of epoch 162 / 200: train_loss: 3.460 	 time: 221 sec
2023.02.22-01:48:11:74:[step-36200/44600: 81.17%]--[loss-3.386998]--[lr-0.000013]--[ETA-2:13:28]
2023.02.22-01:49:47:174:[step-36300/44600: 81.39%]--[loss-3.362961]--[lr-0.000013]--[ETA-2:12:29]
End of epoch 163 / 200: train_loss: 3.459 	 time: 222 sec
2023.02.22-01:51:30:51:[step-36400/44600: 81.61%]--[loss-3.239279]--[lr-0.000012]--[ETA-2:10:51]
2023.02.22-01:53:06:151:[step-36500/44600: 81.84%]--[loss-3.733022]--[lr-0.000012]--[ETA-2:07:15]
End of epoch 164 / 200: train_loss: 3.457 	 time: 221 sec
2023.02.22-01:54:49:28:[step-36600/44600: 82.06%]--[loss-3.313861]--[lr-0.000012]--[ETA-2:06:55]
2023.02.22-01:56:25:128:[step-36700/44600: 82.29%]--[loss-3.373269]--[lr-0.000012]--[ETA-2:06:53]
End of epoch 165 / 200: train_loss: 3.454 	 time: 220 sec
Saving the model at the end of epoch 165, iters 36795
2023.02.22-01:58:09:5:[step-36800/44600: 82.51%]--[loss-3.422491]--[lr-0.000012]--[ETA-2:06:02]
2023.02.22-01:59:45:105:[step-36900/44600: 82.74%]--[loss-3.224605]--[lr-0.000012]--[ETA-2:02:45]
2023.02.22-02:01:21:205:[step-37000/44600: 82.96%]--[loss-3.244529]--[lr-0.000012]--[ETA-2:00:00]
End of epoch 166 / 200: train_loss: 3.449 	 time: 222 sec
2023.02.22-02:03:04:82:[step-37100/44600: 83.18%]--[loss-3.549681]--[lr-0.000011]--[ETA-1:57:42]
2023.02.22-02:04:39:182:[step-37200/44600: 83.41%]--[loss-3.563003]--[lr-0.000011]--[ETA-2:02:32]
End of epoch 167 / 200: train_loss: 3.448 	 time: 220 sec
2023.02.22-02:06:23:59:[step-37300/44600: 83.63%]--[loss-3.120333]--[lr-0.000011]--[ETA-1:56:04]
2023.02.22-02:07:59:159:[step-37400/44600: 83.86%]--[loss-3.649445]--[lr-0.000011]--[ETA-1:55:27]
End of epoch 168 / 200: train_loss: 3.443 	 time: 221 sec
2023.02.22-02:09:41:36:[step-37500/44600: 84.08%]--[loss-3.464784]--[lr-0.000011]--[ETA-1:55:52]
2023.02.22-02:11:18:136:[step-37600/44600: 84.30%]--[loss-3.041196]--[lr-0.000011]--[ETA-1:51:16]
End of epoch 169 / 200: train_loss: 3.440 	 time: 220 sec
2023.02.22-02:13:02:13:[step-37700/44600: 84.53%]--[loss-3.578984]--[lr-0.000010]--[ETA-1:50:20]
2023.02.22-02:14:38:113:[step-37800/44600: 84.75%]--[loss-3.461624]--[lr-0.000010]--[ETA-1:53:36]
2023.02.22-02:16:14:213:[step-37900/44600: 84.98%]--[loss-3.330277]--[lr-0.000010]--[ETA-1:45:21]
End of epoch 170 / 200: train_loss: 3.440 	 time: 221 sec
Saving the model at the end of epoch 170, iters 37910
2023.02.22-02:17:57:90:[step-38000/44600: 85.20%]--[loss-3.316306]--[lr-0.000010]--[ETA-1:46:29]
2023.02.22-02:19:33:190:[step-38100/44600: 85.43%]--[loss-3.853467]--[lr-0.000010]--[ETA-1:43:40]
End of epoch 171 / 200: train_loss: 3.435 	 time: 221 sec
2023.02.22-02:21:16:67:[step-38200/44600: 85.65%]--[loss-3.376768]--[lr-0.000010]--[ETA-1:42:18]
2023.02.22-02:22:52:167:[step-38300/44600: 85.87%]--[loss-3.336341]--[lr-0.000010]--[ETA-1:39:39]
End of epoch 172 / 200: train_loss: 3.430 	 time: 220 sec
2023.02.22-02:24:36:44:[step-38400/44600: 86.10%]--[loss-3.159375]--[lr-0.000009]--[ETA-1:39:49]
2023.02.22-02:26:12:144:[step-38500/44600: 86.32%]--[loss-3.344145]--[lr-0.000009]--[ETA-1:37:54]
End of epoch 173 / 200: train_loss: 3.428 	 time: 221 sec
2023.02.22-02:27:56:21:[step-38600/44600: 86.55%]--[loss-3.201078]--[lr-0.000009]--[ETA-1:36:35]
2023.02.22-02:29:32:121:[step-38700/44600: 86.77%]--[loss-3.257496]--[lr-0.000009]--[ETA-1:33:51]
2023.02.22-02:31:08:221:[step-38800/44600: 87.00%]--[loss-3.255251]--[lr-0.000009]--[ETA-1:31:30]
End of epoch 174 / 200: train_loss: 3.430 	 time: 221 sec
2023.02.22-02:32:52:98:[step-38900/44600: 87.22%]--[loss-3.265854]--[lr-0.000009]--[ETA-1:33:13]
2023.02.22-02:34:28:198:[step-39000/44600: 87.44%]--[loss-3.495879]--[lr-0.000009]--[ETA-1:28:14]
End of epoch 175 / 200: train_loss: 3.424 	 time: 222 sec
Saving the model at the end of epoch 175, iters 39025
2023.02.22-02:36:12:75:[step-39100/44600: 87.67%]--[loss-3.240176]--[lr-0.000008]--[ETA-1:27:52]
2023.02.22-02:37:48:175:[step-39200/44600: 87.89%]--[loss-3.202161]--[lr-0.000008]--[ETA-1:25:07]
End of epoch 176 / 200: train_loss: 3.422 	 time: 221 sec
2023.02.22-02:39:31:52:[step-39300/44600: 88.12%]--[loss-3.363392]--[lr-0.000008]--[ETA-1:25:34]
2023.02.22-02:41:07:152:[step-39400/44600: 88.34%]--[loss-3.275453]--[lr-0.000008]--[ETA-1:22:23]
End of epoch 177 / 200: train_loss: 3.419 	 time: 221 sec
2023.02.22-02:42:50:29:[step-39500/44600: 88.57%]--[loss-3.569697]--[lr-0.000008]--[ETA-1:21:38]
2023.02.22-02:44:26:129:[step-39600/44600: 88.79%]--[loss-3.458365]--[lr-0.000008]--[ETA-1:19:50]
End of epoch 178 / 200: train_loss: 3.417 	 time: 220 sec
2023.02.22-02:46:08:6:[step-39700/44600: 89.01%]--[loss-3.649040]--[lr-0.000007]--[ETA-1:20:24]
2023.02.22-02:47:44:106:[step-39800/44600: 89.24%]--[loss-3.726290]--[lr-0.000007]--[ETA-1:15:58]
2023.02.22-02:49:20:206:[step-39900/44600: 89.46%]--[loss-3.485667]--[lr-0.000007]--[ETA-1:14:12]
End of epoch 179 / 200: train_loss: 3.414 	 time: 219 sec
2023.02.22-02:51:03:83:[step-40000/44600: 89.69%]--[loss-3.583111]--[lr-0.000007]--[ETA-1:13:25]
2023.02.22-02:52:39:183:[step-40100/44600: 89.91%]--[loss-3.495785]--[lr-0.000007]--[ETA-1:11:49]
End of epoch 180 / 200: train_loss: 3.411 	 time: 221 sec
Saving the model at the end of epoch 180, iters 40140
2023.02.22-02:54:22:60:[step-40200/44600: 90.13%]--[loss-3.315923]--[lr-0.000007]--[ETA-1:09:10]
2023.02.22-02:55:59:160:[step-40300/44600: 90.36%]--[loss-3.181433]--[lr-0.000007]--[ETA-1:07:23]
End of epoch 181 / 200: train_loss: 3.410 	 time: 221 sec
2023.02.22-02:57:42:37:[step-40400/44600: 90.58%]--[loss-3.492563]--[lr-0.000006]--[ETA-1:08:18]
2023.02.22-02:59:18:137:[step-40500/44600: 90.81%]--[loss-3.443906]--[lr-0.000006]--[ETA-1:05:00]
End of epoch 182 / 200: train_loss: 3.410 	 time: 221 sec
2023.02.22-03:01:01:14:[step-40600/44600: 91.03%]--[loss-3.353929]--[lr-0.000006]--[ETA-1:03:50]
2023.02.22-03:02:37:114:[step-40700/44600: 91.26%]--[loss-3.287070]--[lr-0.000006]--[ETA-1:04:12]
2023.02.22-03:04:13:214:[step-40800/44600: 91.48%]--[loss-3.598512]--[lr-0.000006]--[ETA-0:59:24]
End of epoch 183 / 200: train_loss: 3.406 	 time: 221 sec
2023.02.22-03:05:57:91:[step-40900/44600: 91.70%]--[loss-3.399680]--[lr-0.000006]--[ETA-0:58:40]
2023.02.22-03:07:33:191:[step-41000/44600: 91.93%]--[loss-3.164232]--[lr-0.000006]--[ETA-0:57:48]
End of epoch 184 / 200: train_loss: 3.407 	 time: 221 sec
2023.02.22-03:09:16:68:[step-41100/44600: 92.15%]--[loss-3.448579]--[lr-0.000005]--[ETA-0:56:06]
2023.02.22-03:10:52:168:[step-41200/44600: 92.38%]--[loss-3.499893]--[lr-0.000005]--[ETA-0:54:39]
End of epoch 185 / 200: train_loss: 3.401 	 time: 220 sec
Saving the model at the end of epoch 185, iters 41255
2023.02.22-03:12:35:45:[step-41300/44600: 92.60%]--[loss-3.331238]--[lr-0.000005]--[ETA-0:52:54]
2023.02.22-03:14:11:145:[step-41400/44600: 92.83%]--[loss-3.852688]--[lr-0.000005]--[ETA-0:50:49]
End of epoch 186 / 200: train_loss: 3.400 	 time: 220 sec
2023.02.22-03:15:54:22:[step-41500/44600: 93.05%]--[loss-3.386033]--[lr-0.000005]--[ETA-0:49:15]
2023.02.22-03:17:30:122:[step-41600/44600: 93.27%]--[loss-3.619703]--[lr-0.000005]--[ETA-0:47:49]
2023.02.22-03:19:06:222:[step-41700/44600: 93.50%]--[loss-3.436016]--[lr-0.000005]--[ETA-0:45:42]
End of epoch 187 / 200: train_loss: 3.397 	 time: 221 sec
2023.02.22-03:20:49:99:[step-41800/44600: 93.72%]--[loss-3.352828]--[lr-0.000004]--[ETA-0:44:08]
2023.02.22-03:22:25:199:[step-41900/44600: 93.95%]--[loss-3.311182]--[lr-0.000004]--[ETA-0:42:40]
End of epoch 188 / 200: train_loss: 3.395 	 time: 221 sec
2023.02.22-03:24:09:76:[step-42000/44600: 94.17%]--[loss-3.323130]--[lr-0.000004]--[ETA-0:42:09]
2023.02.22-03:25:45:176:[step-42100/44600: 94.39%]--[loss-3.179168]--[lr-0.000004]--[ETA-0:39:51]
End of epoch 189 / 200: train_loss: 3.392 	 time: 221 sec
2023.02.22-03:27:27:53:[step-42200/44600: 94.62%]--[loss-3.348622]--[lr-0.000004]--[ETA-0:37:57]
2023.02.22-03:29:03:153:[step-42300/44600: 94.84%]--[loss-3.135138]--[lr-0.000004]--[ETA-0:36:43]
End of epoch 190 / 200: train_loss: 3.390 	 time: 220 sec
Saving the model at the end of epoch 190, iters 42370
2023.02.22-03:30:47:30:[step-42400/44600: 95.07%]--[loss-3.593528]--[lr-0.000003]--[ETA-0:34:56]
2023.02.22-03:32:23:130:[step-42500/44600: 95.29%]--[loss-3.566771]--[lr-0.000003]--[ETA-0:33:34]
End of epoch 191 / 200: train_loss: 3.389 	 time: 221 sec
2023.02.22-03:34:05:7:[step-42600/44600: 95.52%]--[loss-3.254343]--[lr-0.000003]--[ETA-0:32:26]
2023.02.22-03:35:41:107:[step-42700/44600: 95.74%]--[loss-3.391625]--[lr-0.000003]--[ETA-0:30:52]
2023.02.22-03:37:17:207:[step-42800/44600: 95.96%]--[loss-3.410729]--[lr-0.000003]--[ETA-0:28:15]
End of epoch 192 / 200: train_loss: 3.389 	 time: 220 sec
2023.02.22-03:39:01:84:[step-42900/44600: 96.19%]--[loss-3.404866]--[lr-0.000003]--[ETA-0:27:08]
2023.02.22-03:40:37:184:[step-43000/44600: 96.41%]--[loss-3.140936]--[lr-0.000003]--[ETA-0:25:25]
End of epoch 193 / 200: train_loss: 3.388 	 time: 222 sec
2023.02.22-03:42:21:61:[step-43100/44600: 96.64%]--[loss-3.075063]--[lr-0.000002]--[ETA-0:24:13]
2023.02.22-03:43:57:161:[step-43200/44600: 96.86%]--[loss-3.527444]--[lr-0.000002]--[ETA-0:22:29]
End of epoch 194 / 200: train_loss: 3.385 	 time: 222 sec
2023.02.22-03:45:41:38:[step-43300/44600: 97.09%]--[loss-3.683574]--[lr-0.000002]--[ETA-0:20:55]
2023.02.22-03:47:18:138:[step-43400/44600: 97.31%]--[loss-3.419234]--[lr-0.000002]--[ETA-0:19:00]
End of epoch 195 / 200: train_loss: 3.384 	 time: 221 sec
Saving the model at the end of epoch 195, iters 43485
2023.02.22-03:49:01:15:[step-43500/44600: 97.53%]--[loss-3.367114]--[lr-0.000002]--[ETA-0:17:22]
2023.02.22-03:50:38:115:[step-43600/44600: 97.76%]--[loss-3.430460]--[lr-0.000002]--[ETA-0:15:51]
2023.02.22-03:52:13:215:[step-43700/44600: 97.98%]--[loss-3.469739]--[lr-0.000002]--[ETA-0:14:13]
End of epoch 196 / 200: train_loss: 3.383 	 time: 222 sec
2023.02.22-03:53:57:92:[step-43800/44600: 98.21%]--[loss-3.386734]--[lr-0.000001]--[ETA-0:12:43]
2023.02.22-03:55:33:192:[step-43900/44600: 98.43%]--[loss-3.316025]--[lr-0.000001]--[ETA-0:11:01]
End of epoch 197 / 200: train_loss: 3.380 	 time: 221 sec
2023.02.22-03:57:16:69:[step-44000/44600: 98.65%]--[loss-3.627750]--[lr-0.000001]--[ETA-0:09:25]
2023.02.22-03:58:52:169:[step-44100/44600: 98.88%]--[loss-3.351223]--[lr-0.000001]--[ETA-0:07:57]
End of epoch 198 / 200: train_loss: 3.378 	 time: 221 sec
2023.02.22-04:00:35:46:[step-44200/44600: 99.10%]--[loss-3.275255]--[lr-0.000001]--[ETA-0:06:34]
2023.02.22-04:02:10:146:[step-44300/44600: 99.33%]--[loss-3.245912]--[lr-0.000001]--[ETA-0:04:47]
End of epoch 199 / 200: train_loss: 3.381 	 time: 219 sec
2023.02.22-04:03:54:23:[step-44400/44600: 99.55%]--[loss-3.484336]--[lr-0.000000]--[ETA-0:03:13]
2023.02.22-04:05:30:123:[step-44500/44600: 99.78%]--[loss-3.161547]--[lr-0.000000]--[ETA-0:01:35]
2023.02.22-04:07:05:223:[step-44600/44600: 100.00%]--[loss-3.473348]--[lr-0.000000]--[ETA-0:01:03]
End of epoch 200 / 200: train_loss: 3.378 	 time: 221 sec
Saving the model at the end of epoch 200, iters 44600
