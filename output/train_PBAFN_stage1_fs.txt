------------ Options -------------
PBAFN_gen_checkpoint: None
PBAFN_warp_checkpoint: None
PFAFN_gen_checkpoint: None
PFAFN_warp_checkpoint: None
batchSize: 32
beta1: 0.5
checkpoints_dir: checkpoints_fs
continue_train: False
data_type: 32
dataroot: ../dataset/Flow-Style-VTON/VITON_traindata
debug: False
display_freq: 100
display_winsize: 512
fineSize: 512
gpu_ids: [0]
input_nc: 3
isTrain: True
label_nc: 14
lambda_feat: 10.0
launcher: pytorch
loadSize: 512
load_pretrain: 
local_rank: 0
lr: 5e-05
max_dataset_size: inf
nThreads: 1
n_blocks_global: 4
n_blocks_local: 3
n_downsample_global: 4
n_layers_D: 3
n_local_enhancers: 1
name: PBAFN_stage1_fs
ndf: 64
netG: global
ngf: 64
niter: 50
niter_decay: 150
niter_fix_global: 0
no_flip: False
no_ganFeat_loss: False
no_html: False
no_lsgan: False
no_vgg_loss: False
norm: instance
num_D: 2
num_gpus: 1
output_nc: 3
phase: train
pool_size: 0
print_freq: 100
resize_or_crop: None
save_epoch_freq: 5
save_latest_freq: 1000
serial_batches: False
tf_log: False
tv_weight: 0.1
use_dropout: False
verbose: False
which_epoch: latest
-------------- End ----------------
------------ Options -------------
PBAFN_gen_checkpoint: None
PBAFN_warp_checkpoint: None
PFAFN_gen_checkpoint: None
PFAFN_warp_checkpoint: None
batchSize: 32
beta1: 0.5
checkpoints_dir: checkpoints_fs
continue_train: False
data_type: 32
dataroot: ../dataset/Flow-Style-VTON/VITON_traindata
debug: False
display_freq: 100
display_winsize: 512
fineSize: 512
gpu_ids: [0]
input_nc: 3
isTrain: True
label_nc: 14
lambda_feat: 10.0
launcher: pytorch
loadSize: 512
load_pretrain: 
local_rank: 0
lr: 5e-05
max_dataset_size: inf
nThreads: 1
n_blocks_global: 4
n_blocks_local: 3
n_downsample_global: 4
n_layers_D: 3
n_local_enhancers: 1
name: PBAFN_stage1_fs
ndf: 64
netG: global
ngf: 64
niter: 50
niter_decay: 150
niter_fix_global: 0
no_flip: False
no_ganFeat_loss: False
no_html: False
no_lsgan: False
no_vgg_loss: False
norm: instance
num_D: 2
num_gpus: 1
output_nc: 3
phase: train
pool_size: 0
print_freq: 100
resize_or_crop: None
save_epoch_freq: 5
save_latest_freq: 1000
serial_batches: False
tf_log: False
tv_weight: 0.1
use_dropout: False
verbose: False
which_epoch: latest
-------------- End ----------------
dataset [AlignedDataset] was created
../dataset/Flow-Style-VTON/VITON_traindata/train_label label
../dataset/Flow-Style-VTON/VITON_traindata/train_img img
../dataset/Flow-Style-VTON/VITON_traindata/train_edge edge
../dataset/Flow-Style-VTON/VITON_traindata/train_color color
#training images = 444
AFWM(
  (image_features): FeatureEncoder(
    (encoders): ModuleList(
      (0): Sequential(
        (0): DownSample(
          (block): Sequential(
            (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          )
        )
        (1): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (2): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
      )
      (1): Sequential(
        (0): DownSample(
          (block): Sequential(
            (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          )
        )
        (1): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (2): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
      )
      (2): Sequential(
        (0): DownSample(
          (block): Sequential(
            (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          )
        )
        (1): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (2): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
      )
      (3): Sequential(
        (0): DownSample(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          )
        )
        (1): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (2): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
      )
      (4): Sequential(
        (0): DownSample(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          )
        )
        (1): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (2): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
      )
    )
  )
  (cond_features): FeatureEncoder(
    (encoders): ModuleList(
      (0): Sequential(
        (0): DownSample(
          (block): Sequential(
            (0): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(45, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          )
        )
        (1): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (2): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
      )
      (1): Sequential(
        (0): DownSample(
          (block): Sequential(
            (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          )
        )
        (1): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (2): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
      )
      (2): Sequential(
        (0): DownSample(
          (block): Sequential(
            (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          )
        )
        (1): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (2): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
      )
      (3): Sequential(
        (0): DownSample(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          )
        )
        (1): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (2): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
      )
      (4): Sequential(
        (0): DownSample(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          )
        )
        (1): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (2): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
      )
    )
  )
  (image_FPN): RefinePyramid(
    (adaptive): ModuleList(
      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
      (4): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    )
    (smooth): ModuleList(
      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (cond_FPN): RefinePyramid(
    (adaptive): ModuleList(
      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
      (4): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    )
    (smooth): ModuleList(
      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (aflow_net): AFlowNet(
    (netRefine): ModuleList(
      (0): Sequential(
        (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.1)
        (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): LeakyReLU(negative_slope=0.1)
        (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): LeakyReLU(negative_slope=0.1)
        (6): Conv2d(32, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (1): Sequential(
        (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.1)
        (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): LeakyReLU(negative_slope=0.1)
        (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): LeakyReLU(negative_slope=0.1)
        (6): Conv2d(32, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (2): Sequential(
        (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.1)
        (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): LeakyReLU(negative_slope=0.1)
        (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): LeakyReLU(negative_slope=0.1)
        (6): Conv2d(32, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (3): Sequential(
        (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.1)
        (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): LeakyReLU(negative_slope=0.1)
        (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): LeakyReLU(negative_slope=0.1)
        (6): Conv2d(32, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (4): Sequential(
        (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.1)
        (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): LeakyReLU(negative_slope=0.1)
        (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): LeakyReLU(negative_slope=0.1)
        (6): Conv2d(32, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
    (netStyle): ModuleList(
      (0): StyledConvBlock(
        (conv0): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=256, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn0): LeakyReLU(negative_slope=0.2, inplace=True)
        (conv1): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=49, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn1): LeakyReLU(negative_slope=0.2, inplace=True)
      )
      (1): StyledConvBlock(
        (conv0): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=256, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn0): LeakyReLU(negative_slope=0.2, inplace=True)
        (conv1): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=49, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn1): LeakyReLU(negative_slope=0.2, inplace=True)
      )
      (2): StyledConvBlock(
        (conv0): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=256, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn0): LeakyReLU(negative_slope=0.2, inplace=True)
        (conv1): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=49, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn1): LeakyReLU(negative_slope=0.2, inplace=True)
      )
      (3): StyledConvBlock(
        (conv0): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=256, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn0): LeakyReLU(negative_slope=0.2, inplace=True)
        (conv1): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=49, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn1): LeakyReLU(negative_slope=0.2, inplace=True)
      )
      (4): StyledConvBlock(
        (conv0): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=256, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn0): LeakyReLU(negative_slope=0.2, inplace=True)
        (conv1): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=49, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn1): LeakyReLU(negative_slope=0.2, inplace=True)
      )
    )
    (netF): ModuleList(
      (0): Styled_F_ConvBlock(
        (conv0): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=49, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn0): LeakyReLU(negative_slope=0.2, inplace=True)
        (conv1): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=128, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
      )
      (1): Styled_F_ConvBlock(
        (conv0): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=49, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn0): LeakyReLU(negative_slope=0.2, inplace=True)
        (conv1): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=128, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
      )
      (2): Styled_F_ConvBlock(
        (conv0): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=49, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn0): LeakyReLU(negative_slope=0.2, inplace=True)
        (conv1): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=128, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
      )
      (3): Styled_F_ConvBlock(
        (conv0): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=49, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn0): LeakyReLU(negative_slope=0.2, inplace=True)
        (conv1): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=128, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
      )
      (4): Styled_F_ConvBlock(
        (conv0): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=49, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn0): LeakyReLU(negative_slope=0.2, inplace=True)
        (conv1): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=128, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
      )
    )
    (cond_style): Sequential(
      (0): Conv2d(256, 128, kernel_size=(8, 6), stride=(1, 1))
      (1): LeakyReLU(negative_slope=0.1)
    )
    (image_style): Sequential(
      (0): Conv2d(256, 128, kernel_size=(8, 6), stride=(1, 1))
      (1): LeakyReLU(negative_slope=0.1)
    )
  )
)
train_PBAFN_stage1_fs.py:92: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  t_mask = torch.FloatTensor((data['label'].cpu().numpy()==7).astype(np.float))
train_PBAFN_stage1_fs.py:95: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  pre_clothes_edge = torch.FloatTensor((edge.detach().numpy() > 0.5).astype(np.int))
train_PBAFN_stage1_fs.py:98: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  person_clothes_edge = torch.FloatTensor((data['label'].cpu().numpy()==4).astype(np.int))
train_PBAFN_stage1_fs.py:107: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  face_mask = torch.FloatTensor((data['label'].cpu().numpy()==1).astype(np.int)) + torch.FloatTensor((data['label'].cpu().numpy()==12).astype(np.int))
train_PBAFN_stage1_fs.py:109: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  torch.FloatTensor((data['label'].cpu().numpy()==8).astype(np.int)) + torch.FloatTensor((data['label'].cpu().numpy()==9).astype(np.int)) + \
train_PBAFN_stage1_fs.py:110: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  torch.FloatTensor((data['label'].cpu().numpy()==10).astype(np.int))
/opt/conda/lib/python3.7/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272168290/work/aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:4004: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  "Default grid_sample and affine_grid behavior has changed "
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:3635: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  "See the documentation of nn.Upsample for details.".format(mode)
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
2023.01.20-13:41:43:100:[step-100/88800: 0.11%]--[loss-6.594262]--[lr-0.000050]--[ETA-12:51:26]
2023.01.20-13:42:40:200:[step-200/88800: 0.23%]--[loss-5.825700]--[lr-0.000050]--[ETA-14:25:41]
2023.01.20-13:44:09:300:[step-300/88800: 0.34%]--[loss-6.079247]--[lr-0.000050]--[ETA-21:51:25]
2023.01.20-13:45:38:400:[step-400/88800: 0.45%]--[loss-5.828449]--[lr-0.000050]--[ETA-21:48:44]
End of epoch 1 / 200 	 Time Taken: 334 sec
2023.01.20-13:47:01:56:[step-500/88800: 0.56%]--[loss-5.380235]--[lr-0.000050]--[ETA-16:16:50]
2023.01.20-13:48:07:156:[step-600/88800: 0.68%]--[loss-5.227430]--[lr-0.000050]--[ETA-17:21:32]
2023.01.20-13:49:17:256:[step-700/88800: 0.79%]--[loss-5.294216]--[lr-0.000050]--[ETA-17:21:50]
2023.01.20-13:50:38:356:[step-800/88800: 0.90%]--[loss-5.067358]--[lr-0.000050]--[ETA-23:02:28]
End of epoch 2 / 200 	 Time Taken: 340 sec
2023.01.20-13:52:09:12:[step-900/88800: 1.01%]--[loss-5.512043]--[lr-0.000050]--[ETA-22:41:14]
2023.01.20-13:53:39:112:[step-1000/88800: 1.13%]--[loss-5.249575]--[lr-0.000050]--[ETA-15:56:56]
2023.01.20-13:54:46:212:[step-1100/88800: 1.24%]--[loss-4.789573]--[lr-0.000050]--[ETA-17:05:21]
2023.01.20-13:55:57:312:[step-1200/88800: 1.35%]--[loss-5.432685]--[lr-0.000050]--[ETA-18:32:09]
2023.01.20-13:57:13:412:[step-1300/88800: 1.46%]--[loss-5.301426]--[lr-0.000050]--[ETA-23:21:38]
End of epoch 3 / 200 	 Time Taken: 344 sec
2023.01.20-13:58:43:68:[step-1400/88800: 1.58%]--[loss-4.770694]--[lr-0.000050]--[ETA-22:25:31]
2023.01.20-14:00:13:168:[step-1500/88800: 1.69%]--[loss-5.624459]--[lr-0.000050]--[ETA-21:22:29]
2023.01.20-14:01:28:268:[step-1600/88800: 1.80%]--[loss-4.959870]--[lr-0.000050]--[ETA-17:34:27]
2023.01.20-14:02:38:368:[step-1700/88800: 1.91%]--[loss-4.767312]--[lr-0.000050]--[ETA-18:35:32]
End of epoch 4 / 200 	 Time Taken: 353 sec
2023.01.20-14:03:51:24:[step-1800/88800: 2.03%]--[loss-5.052531]--[lr-0.000050]--[ETA-14:04:57]
2023.01.20-14:05:15:124:[step-1900/88800: 2.14%]--[loss-4.701735]--[lr-0.000050]--[ETA-19:53:11]
2023.01.20-14:06:44:224:[step-2000/88800: 2.25%]--[loss-4.314403]--[lr-0.000050]--[ETA-22:33:56]
2023.01.20-14:08:11:324:[step-2100/88800: 2.36%]--[loss-4.891587]--[lr-0.000050]--[ETA-16:05:17]
2023.01.20-14:09:19:424:[step-2200/88800: 2.48%]--[loss-5.142232]--[lr-0.000050]--[ETA-14:52:40]
End of epoch 5 / 200 	 Time Taken: 358 sec
saving the model at the end of epoch 5, iters 2220
2023.01.20-14:10:33:80:[step-2300/88800: 2.59%]--[loss-5.610834]--[lr-0.000050]--[ETA-17:59:03]
2023.01.20-14:11:44:180:[step-2400/88800: 2.70%]--[loss-5.073980]--[lr-0.000050]--[ETA-21:03:16]
2023.01.20-14:13:14:280:[step-2500/88800: 2.82%]--[loss-5.245331]--[lr-0.000050]--[ETA-21:05:21]
2023.01.20-14:14:44:380:[step-2600/88800: 2.93%]--[loss-4.446443]--[lr-0.000050]--[ETA-20:06:58]
End of epoch 6 / 200 	 Time Taken: 363 sec
2023.01.20-14:16:04:36:[step-2700/88800: 3.04%]--[loss-4.876789]--[lr-0.000050]--[ETA-16:35:41]
2023.01.20-14:17:13:136:[step-2800/88800: 3.15%]--[loss-4.480012]--[lr-0.000050]--[ETA-16:52:21]
2023.01.20-14:18:25:236:[step-2900/88800: 3.27%]--[loss-4.833817]--[lr-0.000050]--[ETA-14:01:09]
2023.01.20-14:19:44:336:[step-3000/88800: 3.38%]--[loss-4.978928]--[lr-0.000050]--[ETA-20:58:49]
2023.01.20-14:21:14:436:[step-3100/88800: 3.49%]--[loss-5.021778]--[lr-0.000050]--[ETA-20:06:57]
End of epoch 7 / 200 	 Time Taken: 343 sec
2023.01.20-14:22:45:92:[step-3200/88800: 3.60%]--[loss-5.645493]--[lr-0.000050]--[ETA-21:13:22]
2023.01.20-14:23:52:192:[step-3300/88800: 3.72%]--[loss-4.811998]--[lr-0.000050]--[ETA-15:19:32]
2023.01.20-14:25:00:292:[step-3400/88800: 3.83%]--[loss-4.091188]--[lr-0.000050]--[ETA-17:52:03]
2023.01.20-14:26:10:392:[step-3500/88800: 3.94%]--[loss-4.496259]--[lr-0.000050]--[ETA-13:46:52]
End of epoch 8 / 200 	 Time Taken: 329 sec
2023.01.20-14:27:35:48:[step-3600/88800: 4.05%]--[loss-4.672681]--[lr-0.000050]--[ETA-21:34:22]
2023.01.20-14:29:06:148:[step-3700/88800: 4.17%]--[loss-4.428864]--[lr-0.000050]--[ETA-20:58:24]
2023.01.20-14:30:32:248:[step-3800/88800: 4.28%]--[loss-4.311711]--[lr-0.000050]--[ETA-15:33:24]
2023.01.20-14:31:40:348:[step-3900/88800: 4.39%]--[loss-4.645279]--[lr-0.000050]--[ETA-14:54:45]
End of epoch 9 / 200 	 Time Taken: 357 sec
2023.01.20-14:32:53:4:[step-4000/88800: 4.50%]--[loss-5.089171]--[lr-0.000050]--[ETA-17:56:53]
2023.01.20-14:34:04:104:[step-4100/88800: 4.62%]--[loss-5.219682]--[lr-0.000050]--[ETA-21:34:16]
2023.01.20-14:35:34:204:[step-4200/88800: 4.73%]--[loss-4.533601]--[lr-0.000050]--[ETA-17:41:35]
2023.01.20-14:37:04:304:[step-4300/88800: 4.84%]--[loss-4.762022]--[lr-0.000050]--[ETA-21:42:42]
2023.01.20-14:38:21:404:[step-4400/88800: 4.95%]--[loss-4.881476]--[lr-0.000050]--[ETA-15:50:46]
End of epoch 10 / 200 	 Time Taken: 360 sec
saving the model at the end of epoch 10, iters 4440
2023.01.20-14:39:31:60:[step-4500/88800: 5.07%]--[loss-4.703533]--[lr-0.000050]--[ETA-16:33:00]
2023.01.20-14:40:41:160:[step-4600/88800: 5.18%]--[loss-4.321488]--[lr-0.000050]--[ETA-17:43:46]
2023.01.20-14:41:52:260:[step-4700/88800: 5.29%]--[loss-4.313041]--[lr-0.000050]--[ETA-21:55:17]
2023.01.20-14:43:23:360:[step-4800/88800: 5.41%]--[loss-4.389486]--[lr-0.000050]--[ETA-21:41:49]
End of epoch 11 / 200 	 Time Taken: 350 sec
2023.01.20-14:44:55:16:[step-4900/88800: 5.52%]--[loss-4.612717]--[lr-0.000050]--[ETA-21:12:40]
2023.01.20-14:46:08:116:[step-5000/88800: 5.63%]--[loss-4.178934]--[lr-0.000050]--[ETA-16:47:09]
2023.01.20-14:47:15:216:[step-5100/88800: 5.74%]--[loss-4.609535]--[lr-0.000050]--[ETA-16:42:52]
2023.01.20-14:48:26:316:[step-5200/88800: 5.86%]--[loss-4.383712]--[lr-0.000050]--[ETA-17:25:02]
2023.01.20-14:49:39:416:[step-5300/88800: 5.97%]--[loss-4.552650]--[lr-0.000050]--[ETA-20:39:31]
End of epoch 12 / 200 	 Time Taken: 325 sec
2023.01.20-14:51:12:72:[step-5400/88800: 6.08%]--[loss-3.709110]--[lr-0.000050]--[ETA-22:05:04]
2023.01.20-14:52:41:172:[step-5500/88800: 6.19%]--[loss-4.431332]--[lr-0.000050]--[ETA-21:48:16]
2023.01.20-14:53:52:272:[step-5600/88800: 6.31%]--[loss-4.133595]--[lr-0.000050]--[ETA-17:20:53]
2023.01.20-14:55:00:372:[step-5700/88800: 6.42%]--[loss-4.113092]--[lr-0.000050]--[ETA-15:55:15]
End of epoch 13 / 200 	 Time Taken: 347 sec
2023.01.20-14:56:14:28:[step-5800/88800: 6.53%]--[loss-4.051983]--[lr-0.000050]--[ETA-14:23:53]
2023.01.20-14:57:33:128:[step-5900/88800: 6.64%]--[loss-4.098565]--[lr-0.000050]--[ETA-19:06:53]
2023.01.20-14:59:03:228:[step-6000/88800: 6.76%]--[loss-4.182322]--[lr-0.000050]--[ETA-21:22:33]
2023.01.20-15:00:33:328:[step-6100/88800: 6.87%]--[loss-4.334728]--[lr-0.000050]--[ETA-14:53:47]
2023.01.20-15:01:39:428:[step-6200/88800: 6.98%]--[loss-4.008859]--[lr-0.000050]--[ETA-14:55:44]
End of epoch 14 / 200 	 Time Taken: 357 sec
2023.01.20-15:02:49:84:[step-6300/88800: 7.09%]--[loss-4.125462]--[lr-0.000050]--[ETA-16:06:20]
2023.01.20-15:03:56:184:[step-6400/88800: 7.21%]--[loss-4.319785]--[lr-0.000050]--[ETA-13:00:03]
2023.01.20-15:05:23:284:[step-6500/88800: 7.32%]--[loss-4.297348]--[lr-0.000050]--[ETA-20:31:49]
2023.01.20-15:06:53:384:[step-6600/88800: 7.43%]--[loss-4.938449]--[lr-0.000050]--[ETA-18:21:37]
End of epoch 15 / 200 	 Time Taken: 357 sec
saving the model at the end of epoch 15, iters 6660
2023.01.20-15:08:18:40:[step-6700/88800: 7.55%]--[loss-4.258766]--[lr-0.000050]--[ETA-14:33:01]
2023.01.20-15:09:25:140:[step-6800/88800: 7.66%]--[loss-4.952953]--[lr-0.000050]--[ETA-14:11:51]
2023.01.20-15:10:35:240:[step-6900/88800: 7.77%]--[loss-3.951758]--[lr-0.000050]--[ETA-17:53:17]
2023.01.20-15:11:43:340:[step-7000/88800: 7.88%]--[loss-4.160392]--[lr-0.000050]--[ETA-16:25:47]
2023.01.20-15:13:12:440:[step-7100/88800: 8.00%]--[loss-4.214054]--[lr-0.000050]--[ETA-17:18:02]
End of epoch 16 / 200 	 Time Taken: 328 sec
2023.01.20-15:14:44:96:[step-7200/88800: 8.11%]--[loss-3.975713]--[lr-0.000050]--[ETA-16:48:25]
2023.01.20-15:16:03:196:[step-7300/88800: 8.22%]--[loss-4.008260]--[lr-0.000050]--[ETA-15:22:44]
2023.01.20-15:17:11:296:[step-7400/88800: 8.33%]--[loss-4.348296]--[lr-0.000050]--[ETA-16:14:59]
2023.01.20-15:18:21:396:[step-7500/88800: 8.45%]--[loss-4.072874]--[lr-0.000050]--[ETA-14:13:01]
End of epoch 17 / 200 	 Time Taken: 337 sec
2023.01.20-15:19:34:52:[step-7600/88800: 8.56%]--[loss-4.394630]--[lr-0.000050]--[ETA-19:06:48]
2023.01.20-15:21:04:152:[step-7700/88800: 8.67%]--[loss-4.283295]--[lr-0.000050]--[ETA-15:58:08]
2023.01.20-15:22:34:252:[step-7800/88800: 8.78%]--[loss-4.143738]--[lr-0.000050]--[ETA-20:14:36]
2023.01.20-15:23:50:352:[step-7900/88800: 8.90%]--[loss-4.082250]--[lr-0.000050]--[ETA-13:43:55]
End of epoch 18 / 200 	 Time Taken: 358 sec
2023.01.20-15:25:00:8:[step-8000/88800: 9.01%]--[loss-4.137808]--[lr-0.000050]--[ETA-14:54:38]
2023.01.20-15:26:10:108:[step-8100/88800: 9.12%]--[loss-4.150642]--[lr-0.000050]--[ETA-18:00:10]
2023.01.20-15:27:23:208:[step-8200/88800: 9.23%]--[loss-4.338232]--[lr-0.000050]--[ETA-20:28:16]
2023.01.20-15:28:52:308:[step-8300/88800: 9.35%]--[loss-4.267825]--[lr-0.000050]--[ETA-20:47:28]
2023.01.20-15:30:23:408:[step-8400/88800: 9.46%]--[loss-4.037641]--[lr-0.000050]--[ETA-21:52:40]
End of epoch 19 / 200 	 Time Taken: 361 sec
2023.01.20-15:31:38:64:[step-8500/88800: 9.57%]--[loss-4.443254]--[lr-0.000050]--[ETA-14:25:39]
2023.01.20-15:32:46:164:[step-8600/88800: 9.68%]--[loss-4.146831]--[lr-0.000050]--[ETA-16:11:58]
2023.01.20-15:33:57:264:[step-8700/88800: 9.80%]--[loss-4.294039]--[lr-0.000050]--[ETA-13:27:43]
2023.01.20-15:35:14:364:[step-8800/88800: 9.91%]--[loss-4.200448]--[lr-0.000050]--[ETA-19:59:51]
End of epoch 20 / 200 	 Time Taken: 332 sec
saving the model at the end of epoch 20, iters 8880
2023.01.20-15:36:46:20:[step-8900/88800: 10.02%]--[loss-4.745058]--[lr-0.000050]--[ETA-19:32:34]
2023.01.20-15:38:16:120:[step-9000/88800: 10.14%]--[loss-4.943604]--[lr-0.000050]--[ETA-20:08:06]
2023.01.20-15:39:23:220:[step-9100/88800: 10.25%]--[loss-4.015445]--[lr-0.000050]--[ETA-14:56:53]
2023.01.20-15:40:31:320:[step-9200/88800: 10.36%]--[loss-5.361292]--[lr-0.000050]--[ETA-15:50:57]
2023.01.20-15:41:41:420:[step-9300/88800: 10.47%]--[loss-4.392045]--[lr-0.000050]--[ETA-13:03:20]
End of epoch 21 / 200 	 Time Taken: 328 sec
2023.01.20-15:43:04:76:[step-9400/88800: 10.59%]--[loss-4.289721]--[lr-0.000050]--[ETA-17:36:52]
2023.01.20-15:44:34:176:[step-9500/88800: 10.70%]--[loss-4.307303]--[lr-0.000050]--[ETA-19:08:18]
2023.01.20-15:46:03:276:[step-9600/88800: 10.81%]--[loss-4.417643]--[lr-0.000050]--[ETA-13:58:02]
2023.01.20-15:47:10:376:[step-9700/88800: 10.92%]--[loss-4.569031]--[lr-0.000050]--[ETA-15:21:42]
End of epoch 22 / 200 	 Time Taken: 360 sec
2023.01.20-15:48:20:32:[step-9800/88800: 11.04%]--[loss-4.381288]--[lr-0.000050]--[ETA-13:57:38]
2023.01.20-15:49:26:132:[step-9900/88800: 11.15%]--[loss-4.336288]--[lr-0.000050]--[ETA-13:07:23]
2023.01.20-15:50:54:232:[step-10000/88800: 11.26%]--[loss-4.487967]--[lr-0.000050]--[ETA-19:16:14]
2023.01.20-15:52:24:332:[step-10100/88800: 11.37%]--[loss-4.078405]--[lr-0.000050]--[ETA-19:16:59]
2023.01.20-15:53:48:432:[step-10200/88800: 11.49%]--[loss-5.038257]--[lr-0.000050]--[ETA-14:46:54]
End of epoch 23 / 200 	 Time Taken: 360 sec
2023.01.20-15:54:57:88:[step-10300/88800: 11.60%]--[loss-4.247042]--[lr-0.000050]--[ETA-14:43:00]
2023.01.20-15:56:06:188:[step-10400/88800: 11.71%]--[loss-4.706536]--[lr-0.000050]--[ETA-15:55:15]
2023.01.20-15:57:14:288:[step-10500/88800: 11.82%]--[loss-4.218443]--[lr-0.000050]--[ETA-17:20:45]
2023.01.20-15:58:44:388:[step-10600/88800: 11.94%]--[loss-4.191498]--[lr-0.000050]--[ETA-20:08:53]
End of epoch 24 / 200 	 Time Taken: 337 sec
2023.01.20-16:00:14:44:[step-10700/88800: 12.05%]--[loss-3.967794]--[lr-0.000050]--[ETA-18:47:46]
2023.01.20-16:01:32:144:[step-10800/88800: 12.16%]--[loss-4.886868]--[lr-0.000050]--[ETA-14:50:07]
2023.01.20-16:02:39:244:[step-10900/88800: 12.27%]--[loss-4.165499]--[lr-0.000050]--[ETA-15:16:41]
2023.01.20-16:03:49:344:[step-11000/88800: 12.39%]--[loss-3.965886]--[lr-0.000050]--[ETA-16:29:18]
2023.01.20-16:05:06:444:[step-11100/88800: 12.50%]--[loss-4.102722]--[lr-0.000050]--[ETA-15:26:01]
End of epoch 25 / 200 	 Time Taken: 332 sec
saving the model at the end of epoch 25, iters 11100
2023.01.20-16:06:37:100:[step-11200/88800: 12.61%]--[loss-3.998271]--[lr-0.000050]--[ETA-20:12:14]
2023.01.20-16:08:08:200:[step-11300/88800: 12.73%]--[loss-4.187273]--[lr-0.000050]--[ETA-20:22:20]
2023.01.20-16:09:18:300:[step-11400/88800: 12.84%]--[loss-4.404459]--[lr-0.000050]--[ETA-15:10:02]
2023.01.20-16:10:26:400:[step-11500/88800: 12.95%]--[loss-4.108977]--[lr-0.000050]--[ETA-15:29:11]
End of epoch 26 / 200 	 Time Taken: 350 sec
2023.01.20-16:11:38:56:[step-11600/88800: 13.06%]--[loss-4.225599]--[lr-0.000050]--[ETA-11:42:57]
2023.01.20-16:13:03:156:[step-11700/88800: 13.18%]--[loss-4.202981]--[lr-0.000050]--[ETA-19:53:41]
2023.01.20-16:14:32:256:[step-11800/88800: 13.29%]--[loss-4.218000]--[lr-0.000050]--[ETA-19:14:11]
2023.01.20-16:15:58:356:[step-11900/88800: 13.40%]--[loss-4.166802]--[lr-0.000050]--[ETA-14:14:24]
End of epoch 27 / 200 	 Time Taken: 360 sec
2023.01.20-16:17:07:12:[step-12000/88800: 13.51%]--[loss-4.209027]--[lr-0.000050]--[ETA-14:56:23]
2023.01.20-16:18:15:112:[step-12100/88800: 13.63%]--[loss-4.027294]--[lr-0.000050]--[ETA-14:33:05]
2023.01.20-16:19:20:212:[step-12200/88800: 13.74%]--[loss-4.151775]--[lr-0.000050]--[ETA-18:52:55]
2023.01.20-16:20:50:312:[step-12300/88800: 13.85%]--[loss-3.982195]--[lr-0.000050]--[ETA-19:38:31]
2023.01.20-16:22:20:412:[step-12400/88800: 13.96%]--[loss-3.665658]--[lr-0.000050]--[ETA-19:06:22]
End of epoch 28 / 200 	 Time Taken: 353 sec
2023.01.20-16:23:42:68:[step-12500/88800: 14.08%]--[loss-3.639221]--[lr-0.000050]--[ETA-14:50:06]
2023.01.20-16:24:49:168:[step-12600/88800: 14.19%]--[loss-3.972051]--[lr-0.000050]--[ETA-15:15:39]
2023.01.20-16:25:58:268:[step-12700/88800: 14.30%]--[loss-4.250237]--[lr-0.000050]--[ETA-15:33:48]
2023.01.20-16:27:14:368:[step-12800/88800: 14.41%]--[loss-3.726663]--[lr-0.000050]--[ETA-17:36:28]
End of epoch 29 / 200 	 Time Taken: 332 sec
2023.01.20-16:28:45:24:[step-12900/88800: 14.53%]--[loss-4.209617]--[lr-0.000050]--[ETA-19:37:18]
2023.01.20-16:30:15:124:[step-13000/88800: 14.64%]--[loss-4.537440]--[lr-0.000050]--[ETA-19:48:36]
2023.01.20-16:31:27:224:[step-13100/88800: 14.75%]--[loss-4.263195]--[lr-0.000050]--[ETA-14:37:46]
2023.01.20-16:32:34:324:[step-13200/88800: 14.86%]--[loss-4.119163]--[lr-0.000050]--[ETA-14:47:04]
2023.01.20-16:33:44:424:[step-13300/88800: 14.98%]--[loss-3.797696]--[lr-0.000050]--[ETA-16:25:37]
End of epoch 30 / 200 	 Time Taken: 334 sec
saving the model at the end of epoch 30, iters 13320
2023.01.20-16:35:09:80:[step-13400/88800: 15.09%]--[loss-4.197367]--[lr-0.000050]--[ETA-20:26:29]
2023.01.20-16:36:38:180:[step-13500/88800: 15.20%]--[loss-4.094435]--[lr-0.000050]--[ETA-19:35:58]
2023.01.20-16:38:06:280:[step-13600/88800: 15.32%]--[loss-4.358538]--[lr-0.000050]--[ETA-13:40:04]
2023.01.20-16:39:12:380:[step-13700/88800: 15.43%]--[loss-3.943349]--[lr-0.000050]--[ETA-14:06:59]
End of epoch 31 / 200 	 Time Taken: 358 sec
2023.01.20-16:40:23:36:[step-13800/88800: 15.54%]--[loss-4.158852]--[lr-0.000050]--[ETA-15:30:43]
2023.01.20-16:41:32:136:[step-13900/88800: 15.65%]--[loss-4.711716]--[lr-0.000050]--[ETA-19:18:33]
2023.01.20-16:43:02:236:[step-14000/88800: 15.77%]--[loss-4.032496]--[lr-0.000050]--[ETA-20:38:09]
2023.01.20-16:44:32:336:[step-14100/88800: 15.88%]--[loss-5.120108]--[lr-0.000050]--[ETA-17:54:38]
2023.01.20-16:45:52:436:[step-14200/88800: 15.99%]--[loss-4.186106]--[lr-0.000050]--[ETA-12:56:42]
End of epoch 32 / 200 	 Time Taken: 361 sec
2023.01.20-16:47:02:92:[step-14300/88800: 16.10%]--[loss-5.062630]--[lr-0.000050]--[ETA-13:52:15]
2023.01.20-16:48:11:192:[step-14400/88800: 16.22%]--[loss-4.370215]--[lr-0.000050]--[ETA-15:12:33]
2023.01.20-16:49:22:292:[step-14500/88800: 16.33%]--[loss-3.819190]--[lr-0.000050]--[ETA-18:43:33]
2023.01.20-16:50:52:392:[step-14600/88800: 16.44%]--[loss-4.097113]--[lr-0.000050]--[ETA-18:45:08]
End of epoch 33 / 200 	 Time Taken: 341 sec
2023.01.20-16:52:23:48:[step-14700/88800: 16.55%]--[loss-4.318032]--[lr-0.000050]--[ETA-17:23:51]
2023.01.20-16:53:38:148:[step-14800/88800: 16.67%]--[loss-3.837372]--[lr-0.000050]--[ETA-14:06:34]
2023.01.20-16:54:45:248:[step-14900/88800: 16.78%]--[loss-4.444337]--[lr-0.000050]--[ETA-13:25:17]
2023.01.20-16:55:55:348:[step-15000/88800: 16.89%]--[loss-3.829887]--[lr-0.000050]--[ETA-14:55:08]
End of epoch 34 / 200 	 Time Taken: 333 sec
2023.01.20-16:57:17:4:[step-15100/88800: 17.00%]--[loss-3.940095]--[lr-0.000050]--[ETA-19:25:21]
2023.01.20-16:58:47:104:[step-15200/88800: 17.12%]--[loss-3.638878]--[lr-0.000050]--[ETA-19:11:27]
2023.01.20-17:00:16:204:[step-15300/88800: 17.23%]--[loss-4.206388]--[lr-0.000050]--[ETA-13:33:47]
2023.01.20-17:01:22:304:[step-15400/88800: 17.34%]--[loss-3.694910]--[lr-0.000050]--[ETA-14:20:35]
2023.01.20-17:02:30:404:[step-15500/88800: 17.45%]--[loss-3.930804]--[lr-0.000050]--[ETA-12:24:28]
End of epoch 35 / 200 	 Time Taken: 346 sec
saving the model at the end of epoch 35, iters 15540
2023.01.20-17:03:38:60:[step-15600/88800: 17.57%]--[loss-4.057124]--[lr-0.000050]--[ETA-18:49:22]
2023.01.20-17:05:07:160:[step-15700/88800: 17.68%]--[loss-3.899915]--[lr-0.000050]--[ETA-19:23:38]
2023.01.20-17:06:38:260:[step-15800/88800: 17.79%]--[loss-4.087695]--[lr-0.000050]--[ETA-17:28:16]
2023.01.20-17:07:58:360:[step-15900/88800: 17.91%]--[loss-4.085337]--[lr-0.000050]--[ETA-12:41:26]
End of epoch 36 / 200 	 Time Taken: 354 sec
2023.01.20-17:09:07:16:[step-16000/88800: 18.02%]--[loss-4.827845]--[lr-0.000050]--[ETA-13:08:29]
2023.01.20-17:10:16:116:[step-16100/88800: 18.13%]--[loss-4.019163]--[lr-0.000050]--[ETA-17:00:59]
2023.01.20-17:11:28:216:[step-16200/88800: 18.24%]--[loss-4.035828]--[lr-0.000050]--[ETA-18:16:45]
2023.01.20-17:12:58:316:[step-16300/88800: 18.36%]--[loss-3.913177]--[lr-0.000050]--[ETA-18:14:26]
2023.01.20-17:14:29:416:[step-16400/88800: 18.47%]--[loss-4.330976]--[lr-0.000050]--[ETA-18:12:13]
End of epoch 37 / 200 	 Time Taken: 359 sec
2023.01.20-17:15:42:72:[step-16500/88800: 18.58%]--[loss-4.062033]--[lr-0.000050]--[ETA-12:27:08]
2023.01.20-17:16:49:172:[step-16600/88800: 18.69%]--[loss-3.915139]--[lr-0.000050]--[ETA-14:07:50]
2023.01.20-17:17:59:272:[step-16700/88800: 18.81%]--[loss-4.014339]--[lr-0.000050]--[ETA-10:40:17]
2023.01.20-17:19:19:372:[step-16800/88800: 18.92%]--[loss-3.982313]--[lr-0.000050]--[ETA-18:47:38]
End of epoch 38 / 200 	 Time Taken: 330 sec
2023.01.20-17:20:51:28:[step-16900/88800: 19.03%]--[loss-4.610540]--[lr-0.000050]--[ETA-17:40:51]
2023.01.20-17:22:19:128:[step-17000/88800: 19.14%]--[loss-3.973339]--[lr-0.000050]--[ETA-12:08:45]
2023.01.20-17:23:25:228:[step-17100/88800: 19.26%]--[loss-3.897039]--[lr-0.000050]--[ETA-13:57:06]
2023.01.20-17:24:33:328:[step-17200/88800: 19.37%]--[loss-3.837157]--[lr-0.000050]--[ETA-15:22:31]
2023.01.20-17:25:43:428:[step-17300/88800: 19.48%]--[loss-4.069541]--[lr-0.000050]--[ETA-17:06:24]
End of epoch 39 / 200 	 Time Taken: 333 sec
2023.01.20-17:27:15:84:[step-17400/88800: 19.59%]--[loss-3.758071]--[lr-0.000050]--[ETA-17:23:19]
2023.01.20-17:28:44:184:[step-17500/88800: 19.71%]--[loss-3.872577]--[lr-0.000050]--[ETA-17:43:19]
2023.01.20-17:30:02:284:[step-17600/88800: 19.82%]--[loss-4.352243]--[lr-0.000050]--[ETA-13:44:41]
2023.01.20-17:31:09:384:[step-17700/88800: 19.93%]--[loss-4.169051]--[lr-0.000050]--[ETA-13:08:37]
End of epoch 40 / 200 	 Time Taken: 353 sec
saving the model at the end of epoch 40, iters 17760
2023.01.20-17:32:22:40:[step-17800/88800: 20.05%]--[loss-4.336741]--[lr-0.000050]--[ETA-15:55:50]
2023.01.20-17:33:41:140:[step-17900/88800: 20.16%]--[loss-3.853533]--[lr-0.000050]--[ETA-18:22:35]
2023.01.20-17:35:11:240:[step-18000/88800: 20.27%]--[loss-4.286687]--[lr-0.000050]--[ETA-17:12:28]
2023.01.20-17:36:40:340:[step-18100/88800: 20.38%]--[loss-4.094203]--[lr-0.000050]--[ETA-16:38:41]
2023.01.20-17:37:47:440:[step-18200/88800: 20.50%]--[loss-3.780689]--[lr-0.000050]--[ETA-13:19:32]
End of epoch 41 / 200 	 Time Taken: 359 sec
2023.01.20-17:38:57:96:[step-18300/88800: 20.61%]--[loss-4.145774]--[lr-0.000050]--[ETA-14:39:07]
2023.01.20-17:40:05:196:[step-18400/88800: 20.72%]--[loss-4.013963]--[lr-0.000050]--[ETA-11:33:44]
2023.01.20-17:41:35:296:[step-18500/88800: 20.83%]--[loss-4.088814]--[lr-0.000050]--[ETA-17:49:23]
2023.01.20-17:43:04:396:[step-18600/88800: 20.95%]--[loss-4.151403]--[lr-0.000050]--[ETA-16:44:39]
End of epoch 42 / 200 	 Time Taken: 357 sec
2023.01.20-17:44:27:52:[step-18700/88800: 21.06%]--[loss-4.310988]--[lr-0.000050]--[ETA-12:13:03]
2023.01.20-17:45:33:152:[step-18800/88800: 21.17%]--[loss-3.955914]--[lr-0.000050]--[ETA-13:01:10]
2023.01.20-17:46:43:252:[step-18900/88800: 21.28%]--[loss-3.703227]--[lr-0.000050]--[ETA-11:10:53]
2023.01.20-17:47:58:352:[step-19000/88800: 21.40%]--[loss-4.104470]--[lr-0.000050]--[ETA-18:17:42]
End of epoch 43 / 200 	 Time Taken: 333 sec
2023.01.20-17:49:30:8:[step-19100/88800: 21.51%]--[loss-3.917404]--[lr-0.000050]--[ETA-17:32:54]
2023.01.20-17:51:00:108:[step-19200/88800: 21.62%]--[loss-5.013909]--[lr-0.000050]--[ETA-17:46:12]
2023.01.20-17:52:11:208:[step-19300/88800: 21.73%]--[loss-3.746432]--[lr-0.000050]--[ETA-13:14:29]
2023.01.20-17:53:18:308:[step-19400/88800: 21.85%]--[loss-3.703343]--[lr-0.000050]--[ETA-12:13:25]
2023.01.20-17:54:28:408:[step-19500/88800: 21.96%]--[loss-4.781959]--[lr-0.000050]--[ETA-10:18:23]
End of epoch 44 / 200 	 Time Taken: 327 sec
2023.01.20-17:55:48:64:[step-19600/88800: 22.07%]--[loss-3.778646]--[lr-0.000050]--[ETA-18:34:12]
2023.01.20-17:57:18:164:[step-19700/88800: 22.18%]--[loss-3.908560]--[lr-0.000050]--[ETA-17:41:45]
2023.01.20-17:58:48:264:[step-19800/88800: 22.30%]--[loss-3.781996]--[lr-0.000050]--[ETA-18:12:30]
2023.01.20-17:59:53:364:[step-19900/88800: 22.41%]--[loss-3.728809]--[lr-0.000050]--[ETA-13:24:17]
End of epoch 45 / 200 	 Time Taken: 358 sec
saving the model at the end of epoch 45, iters 19980
2023.01.20-18:01:03:20:[step-20000/88800: 22.52%]--[loss-3.983736]--[lr-0.000050]--[ETA-11:52:04]
2023.01.20-18:02:09:120:[step-20100/88800: 22.64%]--[loss-3.898188]--[lr-0.000050]--[ETA-17:52:56]
2023.01.20-18:03:39:220:[step-20200/88800: 22.75%]--[loss-3.999164]--[lr-0.000050]--[ETA-16:31:33]
2023.01.20-18:05:09:320:[step-20300/88800: 22.86%]--[loss-3.419019]--[lr-0.000050]--[ETA-14:43:34]
2023.01.20-18:06:29:420:[step-20400/88800: 22.97%]--[loss-4.197283]--[lr-0.000050]--[ETA-11:49:10]
End of epoch 46 / 200 	 Time Taken: 357 sec
2023.01.20-18:07:37:76:[step-20500/88800: 23.09%]--[loss-3.788945]--[lr-0.000050]--[ETA-11:59:06]
2023.01.20-18:08:45:176:[step-20600/88800: 23.20%]--[loss-3.834682]--[lr-0.000050]--[ETA-14:07:28]
2023.01.20-18:09:56:276:[step-20700/88800: 23.31%]--[loss-3.790064]--[lr-0.000050]--[ETA-16:31:02]
2023.01.20-18:11:26:376:[step-20800/88800: 23.42%]--[loss-4.468555]--[lr-0.000050]--[ETA-13:48:52]
End of epoch 47 / 200 	 Time Taken: 342 sec
2023.01.20-18:12:58:32:[step-20900/88800: 23.54%]--[loss-3.579283]--[lr-0.000050]--[ETA-16:24:30]
2023.01.20-18:14:11:132:[step-21000/88800: 23.65%]--[loss-3.469949]--[lr-0.000050]--[ETA-12:33:46]
2023.01.20-18:15:18:232:[step-21100/88800: 23.76%]--[loss-3.863562]--[lr-0.000050]--[ETA-11:47:47]
2023.01.20-18:16:28:332:[step-21200/88800: 23.87%]--[loss-4.071957]--[lr-0.000050]--[ETA-10:12:28]
2023.01.20-18:17:43:432:[step-21300/88800: 23.99%]--[loss-4.133823]--[lr-0.000050]--[ETA-16:43:35]
End of epoch 48 / 200 	 Time Taken: 326 sec
2023.01.20-18:19:16:88:[step-21400/88800: 24.10%]--[loss-4.253611]--[lr-0.000050]--[ETA-16:18:10]
2023.01.20-18:20:45:188:[step-21500/88800: 24.21%]--[loss-3.960004]--[lr-0.000050]--[ETA-17:20:56]
2023.01.20-18:21:51:288:[step-21600/88800: 24.32%]--[loss-3.800947]--[lr-0.000050]--[ETA-12:34:38]
2023.01.20-18:22:58:388:[step-21700/88800: 24.44%]--[loss-4.076082]--[lr-0.000050]--[ETA-11:42:29]
End of epoch 49 / 200 	 Time Taken: 343 sec
2023.01.20-18:24:06:44:[step-21800/88800: 24.55%]--[loss-4.217093]--[lr-0.000050]--[ETA-10:51:19]
2023.01.20-18:25:33:144:[step-21900/88800: 24.66%]--[loss-3.490005]--[lr-0.000050]--[ETA-13:48:29]
2023.01.20-18:27:03:244:[step-22000/88800: 24.77%]--[loss-4.398783]--[lr-0.000050]--[ETA-16:58:32]
2023.01.20-18:28:27:344:[step-22100/88800: 24.89%]--[loss-4.002713]--[lr-0.000050]--[ETA-12:15:13]
2023.01.20-18:29:32:444:[step-22200/88800: 25.00%]--[loss-4.418921]--[lr-0.000050]--[ETA-12:08:58]
End of epoch 50 / 200 	 Time Taken: 355 sec
saving the model at the end of epoch 50, iters 22200
2023.01.20-18:30:45:100:[step-22300/88800: 25.11%]--[loss-3.900863]--[lr-0.000050]--[ETA-14:37:08]
2023.01.20-18:32:01:200:[step-22400/88800: 25.23%]--[loss-3.687517]--[lr-0.000050]--[ETA-16:08:30]
2023.01.20-18:33:31:300:[step-22500/88800: 25.34%]--[loss-3.688300]--[lr-0.000050]--[ETA-16:56:30]
2023.01.20-18:35:02:400:[step-22600/88800: 25.45%]--[loss-4.171135]--[lr-0.000050]--[ETA-17:44:30]
End of epoch 51 / 200 	 Time Taken: 362 sec
2023.01.20-18:36:14:56:[step-22700/88800: 25.56%]--[loss-4.210759]--[lr-0.000050]--[ETA-11:46:02]
2023.01.20-18:37:22:156:[step-22800/88800: 25.68%]--[loss-4.488082]--[lr-0.000050]--[ETA-12:30:05]
2023.01.20-18:38:31:256:[step-22900/88800: 25.79%]--[loss-3.861028]--[lr-0.000050]--[ETA-16:40:21]
2023.01.20-18:39:59:356:[step-23000/88800: 25.90%]--[loss-4.032207]--[lr-0.000050]--[ETA-14:27:06]
End of epoch 52 / 200 	 Time Taken: 343 sec
2023.01.20-18:41:31:12:[step-23100/88800: 26.01%]--[loss-3.781299]--[lr-0.000049]--[ETA-15:38:46]
2023.01.20-18:42:50:112:[step-23200/88800: 26.13%]--[loss-4.630785]--[lr-0.000049]--[ETA-12:09:38]
2023.01.20-18:43:56:212:[step-23300/88800: 26.24%]--[loss-3.771008]--[lr-0.000049]--[ETA-11:43:52]
2023.01.20-18:45:05:312:[step-23400/88800: 26.35%]--[loss-3.646017]--[lr-0.000049]--[ETA-13:00:53]
2023.01.20-18:46:22:412:[step-23500/88800: 26.46%]--[loss-4.455185]--[lr-0.000049]--[ETA-16:07:19]
End of epoch 53 / 200 	 Time Taken: 332 sec
2023.01.20-18:47:54:68:[step-23600/88800: 26.58%]--[loss-3.697537]--[lr-0.000049]--[ETA-16:51:02]
2023.01.20-18:49:23:168:[step-23700/88800: 26.69%]--[loss-3.963247]--[lr-0.000049]--[ETA-16:31:28]
2023.01.20-18:50:33:268:[step-23800/88800: 26.80%]--[loss-4.136786]--[lr-0.000049]--[ETA-11:50:07]
2023.01.20-18:51:41:368:[step-23900/88800: 26.91%]--[loss-3.833771]--[lr-0.000049]--[ETA-14:03:11]
End of epoch 54 / 200 	 Time Taken: 346 sec
2023.01.20-18:52:53:24:[step-24000/88800: 27.03%]--[loss-3.431409]--[lr-0.000049]--[ETA-10:36:06]
2023.01.20-18:54:21:124:[step-24100/88800: 27.14%]--[loss-3.780783]--[lr-0.000049]--[ETA-16:03:15]
2023.01.20-18:55:50:224:[step-24200/88800: 27.25%]--[loss-3.795832]--[lr-0.000049]--[ETA-15:39:07]
2023.01.20-18:57:14:324:[step-24300/88800: 27.36%]--[loss-3.788031]--[lr-0.000049]--[ETA-11:50:22]
2023.01.20-18:58:21:424:[step-24400/88800: 27.48%]--[loss-4.299020]--[lr-0.000049]--[ETA-12:26:56]
End of epoch 55 / 200 	 Time Taken: 357 sec
saving the model at the end of epoch 55, iters 24420
2023.01.20-18:59:32:80:[step-24500/88800: 27.59%]--[loss-4.126355]--[lr-0.000048]--[ETA-13:20:09]
2023.01.20-19:00:47:180:[step-24600/88800: 27.70%]--[loss-3.931595]--[lr-0.000048]--[ETA-16:43:52]
2023.01.20-19:02:16:280:[step-24700/88800: 27.82%]--[loss-3.712154]--[lr-0.000048]--[ETA-15:05:36]
2023.01.20-19:03:46:380:[step-24800/88800: 27.93%]--[loss-3.556213]--[lr-0.000048]--[ETA-17:13:51]
End of epoch 56 / 200 	 Time Taken: 361 sec
2023.01.20-19:05:03:36:[step-24900/88800: 28.04%]--[loss-3.919840]--[lr-0.000048]--[ETA-12:30:06]
2023.01.20-19:06:11:136:[step-25000/88800: 28.15%]--[loss-3.629248]--[lr-0.000048]--[ETA-12:53:02]
2023.01.20-19:07:22:236:[step-25100/88800: 28.27%]--[loss-3.990688]--[lr-0.000048]--[ETA-9:55:23]
2023.01.20-19:08:38:336:[step-25200/88800: 28.38%]--[loss-3.836492]--[lr-0.000048]--[ETA-15:51:09]
2023.01.20-19:10:08:436:[step-25300/88800: 28.49%]--[loss-3.537001]--[lr-0.000048]--[ETA-14:36:14]
End of epoch 57 / 200 	 Time Taken: 338 sec
2023.01.20-19:11:39:92:[step-25400/88800: 28.60%]--[loss-3.675164]--[lr-0.000048]--[ETA-16:30:35]
2023.01.20-19:12:47:192:[step-25500/88800: 28.72%]--[loss-3.532612]--[lr-0.000048]--[ETA-12:06:24]
2023.01.20-19:13:55:292:[step-25600/88800: 28.83%]--[loss-3.335028]--[lr-0.000048]--[ETA-13:17:11]
2023.01.20-19:15:03:392:[step-25700/88800: 28.94%]--[loss-3.766474]--[lr-0.000048]--[ETA-9:50:58]
End of epoch 58 / 200 	 Time Taken: 325 sec
2023.01.20-19:16:26:48:[step-25800/88800: 29.05%]--[loss-3.731790]--[lr-0.000047]--[ETA-15:58:11]
2023.01.20-19:17:56:148:[step-25900/88800: 29.17%]--[loss-3.552244]--[lr-0.000047]--[ETA-15:13:30]
2023.01.20-19:19:23:248:[step-26000/88800: 29.28%]--[loss-3.687191]--[lr-0.000047]--[ETA-12:32:24]
2023.01.20-19:20:30:348:[step-26100/88800: 29.39%]--[loss-4.360390]--[lr-0.000047]--[ETA-12:14:31]
End of epoch 59 / 200 	 Time Taken: 354 sec
2023.01.20-19:21:40:4:[step-26200/88800: 29.50%]--[loss-4.091985]--[lr-0.000047]--[ETA-12:41:39]
2023.01.20-19:22:45:104:[step-26300/88800: 29.62%]--[loss-3.774590]--[lr-0.000047]--[ETA-9:19:29]
2023.01.20-19:24:15:204:[step-26400/88800: 29.73%]--[loss-3.528407]--[lr-0.000047]--[ETA-15:38:04]
2023.01.20-19:25:44:304:[step-26500/88800: 29.84%]--[loss-3.806060]--[lr-0.000047]--[ETA-14:39:24]
2023.01.20-19:27:07:404:[step-26600/88800: 29.95%]--[loss-4.030628]--[lr-0.000047]--[ETA-11:37:19]
End of epoch 60 / 200 	 Time Taken: 358 sec
saving the model at the end of epoch 60, iters 26640
2023.01.20-19:28:17:60:[step-26700/88800: 30.07%]--[loss-3.455694]--[lr-0.000047]--[ETA-12:19:05]
2023.01.20-19:29:26:160:[step-26800/88800: 30.18%]--[loss-3.737192]--[lr-0.000047]--[ETA-11:57:53]
2023.01.20-19:30:36:260:[step-26900/88800: 30.29%]--[loss-3.806671]--[lr-0.000047]--[ETA-13:16:11]
2023.01.20-19:32:06:360:[step-27000/88800: 30.41%]--[loss-3.556615]--[lr-0.000047]--[ETA-15:00:49]
End of epoch 61 / 200 	 Time Taken: 347 sec
2023.01.20-19:33:38:16:[step-27100/88800: 30.52%]--[loss-4.001745]--[lr-0.000046]--[ETA-16:20:07]
2023.01.20-19:34:53:116:[step-27200/88800: 30.63%]--[loss-3.587204]--[lr-0.000046]--[ETA-10:36:20]
2023.01.20-19:36:01:216:[step-27300/88800: 30.74%]--[loss-4.095513]--[lr-0.000046]--[ETA-11:20:37]
2023.01.20-19:37:11:316:[step-27400/88800: 30.86%]--[loss-4.276679]--[lr-0.000046]--[ETA-13:11:18]
2023.01.20-19:38:23:416:[step-27500/88800: 30.97%]--[loss-4.030122]--[lr-0.000046]--[ETA-15:55:21]
End of epoch 62 / 200 	 Time Taken: 326 sec
2023.01.20-19:39:55:72:[step-27600/88800: 31.08%]--[loss-4.251615]--[lr-0.000046]--[ETA-15:17:09]
2023.01.20-19:41:24:172:[step-27700/88800: 31.19%]--[loss-3.572285]--[lr-0.000046]--[ETA-15:48:39]
2023.01.20-19:42:35:272:[step-27800/88800: 31.31%]--[loss-4.563087]--[lr-0.000046]--[ETA-10:30:38]
2023.01.20-19:43:43:372:[step-27900/88800: 31.42%]--[loss-3.515343]--[lr-0.000046]--[ETA-11:53:21]
End of epoch 63 / 200 	 Time Taken: 344 sec
2023.01.20-19:44:53:28:[step-28000/88800: 31.53%]--[loss-3.710907]--[lr-0.000046]--[ETA-9:01:40]
2023.01.20-19:46:14:128:[step-28100/88800: 31.64%]--[loss-3.899693]--[lr-0.000046]--[ETA-14:05:47]
2023.01.20-19:47:43:228:[step-28200/88800: 31.76%]--[loss-3.757746]--[lr-0.000046]--[ETA-14:31:11]
2023.01.20-19:49:12:328:[step-28300/88800: 31.87%]--[loss-3.572274]--[lr-0.000046]--[ETA-10:52:41]
2023.01.20-19:50:19:428:[step-28400/88800: 31.98%]--[loss-3.656822]--[lr-0.000046]--[ETA-10:31:33]
End of epoch 64 / 200 	 Time Taken: 356 sec
2023.01.20-19:51:30:84:[step-28500/88800: 32.09%]--[loss-4.113450]--[lr-0.000045]--[ETA-13:08:59]
2023.01.20-19:52:35:184:[step-28600/88800: 32.21%]--[loss-4.099886]--[lr-0.000045]--[ETA-9:29:22]
2023.01.20-19:53:32:284:[step-28700/88800: 32.32%]--[loss-3.745787]--[lr-0.000045]--[ETA-8:42:14]
2023.01.20-19:54:28:384:[step-28800/88800: 32.43%]--[loss-3.605662]--[lr-0.000045]--[ETA-9:41:24]
End of epoch 65 / 200 	 Time Taken: 272 sec
saving the model at the end of epoch 65, iters 28860
2023.01.20-19:55:27:40:[step-28900/88800: 32.55%]--[loss-3.984616]--[lr-0.000045]--[ETA-9:48:52]
2023.01.20-19:56:24:140:[step-29000/88800: 32.66%]--[loss-3.739556]--[lr-0.000045]--[ETA-9:53:42]
2023.01.20-19:57:21:240:[step-29100/88800: 32.77%]--[loss-4.002522]--[lr-0.000045]--[ETA-9:20:29]
2023.01.20-19:58:17:340:[step-29200/88800: 32.88%]--[loss-3.811453]--[lr-0.000045]--[ETA-9:03:44]
2023.01.20-19:59:14:440:[step-29300/88800: 33.00%]--[loss-4.554738]--[lr-0.000045]--[ETA-8:46:46]
End of epoch 66 / 200 	 Time Taken: 253 sec
2023.01.20-20:00:13:96:[step-29400/88800: 33.11%]--[loss-3.878286]--[lr-0.000045]--[ETA-9:27:33]
2023.01.20-20:01:09:196:[step-29500/88800: 33.22%]--[loss-3.534239]--[lr-0.000045]--[ETA-10:03:55]
2023.01.20-20:02:05:296:[step-29600/88800: 33.33%]--[loss-3.856847]--[lr-0.000045]--[ETA-10:22:01]
2023.01.20-20:03:02:396:[step-29700/88800: 33.45%]--[loss-4.362826]--[lr-0.000045]--[ETA-9:44:57]
End of epoch 67 / 200 	 Time Taken: 253 sec
2023.01.20-20:04:01:52:[step-29800/88800: 33.56%]--[loss-3.772708]--[lr-0.000044]--[ETA-8:32:41]
2023.01.20-20:04:57:152:[step-29900/88800: 33.67%]--[loss-3.717037]--[lr-0.000044]--[ETA-9:19:39]
2023.01.20-20:05:54:252:[step-30000/88800: 33.78%]--[loss-3.819150]--[lr-0.000044]--[ETA-9:40:00]
2023.01.20-20:06:51:352:[step-30100/88800: 33.90%]--[loss-4.101768]--[lr-0.000044]--[ETA-8:47:30]
End of epoch 68 / 200 	 Time Taken: 253 sec
2023.01.20-20:07:49:8:[step-30200/88800: 34.01%]--[loss-3.734126]--[lr-0.000044]--[ETA-9:02:20]
2023.01.20-20:08:46:108:[step-30300/88800: 34.12%]--[loss-3.563190]--[lr-0.000044]--[ETA-9:00:06]
2023.01.20-20:09:43:208:[step-30400/88800: 34.23%]--[loss-3.889832]--[lr-0.000044]--[ETA-8:52:27]
2023.01.20-20:10:40:308:[step-30500/88800: 34.35%]--[loss-3.847461]--[lr-0.000044]--[ETA-9:08:06]
2023.01.20-20:11:37:408:[step-30600/88800: 34.46%]--[loss-3.850466]--[lr-0.000044]--[ETA-9:17:36]
End of epoch 69 / 200 	 Time Taken: 254 sec
2023.01.20-20:12:35:64:[step-30700/88800: 34.57%]--[loss-3.951039]--[lr-0.000044]--[ETA-10:41:55]
2023.01.20-20:13:32:164:[step-30800/88800: 34.68%]--[loss-4.126227]--[lr-0.000044]--[ETA-8:44:13]
2023.01.20-20:14:29:264:[step-30900/88800: 34.80%]--[loss-3.288467]--[lr-0.000044]--[ETA-9:21:46]
2023.01.20-20:15:25:364:[step-31000/88800: 34.91%]--[loss-3.683315]--[lr-0.000044]--[ETA-9:13:08]
End of epoch 70 / 200 	 Time Taken: 253 sec
saving the model at the end of epoch 70, iters 31080
2023.01.20-20:16:24:20:[step-31100/88800: 35.02%]--[loss-4.049629]--[lr-0.000043]--[ETA-9:44:18]
2023.01.20-20:17:21:120:[step-31200/88800: 35.14%]--[loss-3.902550]--[lr-0.000043]--[ETA-9:10:52]
2023.01.20-20:18:17:220:[step-31300/88800: 35.25%]--[loss-3.517578]--[lr-0.000043]--[ETA-8:35:42]
2023.01.20-20:19:14:320:[step-31400/88800: 35.36%]--[loss-3.916648]--[lr-0.000043]--[ETA-9:15:35]
2023.01.20-20:20:11:420:[step-31500/88800: 35.47%]--[loss-3.713515]--[lr-0.000043]--[ETA-9:07:43]
End of epoch 71 / 200 	 Time Taken: 253 sec
2023.01.20-20:21:10:76:[step-31600/88800: 35.59%]--[loss-3.698893]--[lr-0.000043]--[ETA-9:08:48]
2023.01.20-20:22:06:176:[step-31700/88800: 35.70%]--[loss-3.588286]--[lr-0.000043]--[ETA-8:59:35]
2023.01.20-20:23:03:276:[step-31800/88800: 35.81%]--[loss-3.397069]--[lr-0.000043]--[ETA-9:24:27]
2023.01.20-20:23:59:376:[step-31900/88800: 35.92%]--[loss-4.008440]--[lr-0.000043]--[ETA-8:36:55]
End of epoch 72 / 200 	 Time Taken: 253 sec
2023.01.20-20:24:58:32:[step-32000/88800: 36.04%]--[loss-3.838046]--[lr-0.000043]--[ETA-8:38:02]
2023.01.20-20:25:55:132:[step-32100/88800: 36.15%]--[loss-3.616318]--[lr-0.000043]--[ETA-8:47:29]
2023.01.20-20:26:51:232:[step-32200/88800: 36.26%]--[loss-3.734306]--[lr-0.000043]--[ETA-9:01:47]
2023.01.20-20:27:48:332:[step-32300/88800: 36.37%]--[loss-4.090976]--[lr-0.000043]--[ETA-8:37:56]
2023.01.20-20:28:44:432:[step-32400/88800: 36.49%]--[loss-3.469239]--[lr-0.000043]--[ETA-9:32:06]
End of epoch 73 / 200 	 Time Taken: 253 sec
2023.01.20-20:29:43:88:[step-32500/88800: 36.60%]--[loss-3.806291]--[lr-0.000042]--[ETA-8:17:12]
2023.01.20-20:30:40:188:[step-32600/88800: 36.71%]--[loss-3.883853]--[lr-0.000042]--[ETA-8:36:45]
2023.01.20-20:31:37:288:[step-32700/88800: 36.82%]--[loss-3.722230]--[lr-0.000042]--[ETA-8:43:12]
2023.01.20-20:32:34:388:[step-32800/88800: 36.94%]--[loss-3.677247]--[lr-0.000042]--[ETA-9:05:25]
End of epoch 74 / 200 	 Time Taken: 254 sec
2023.01.20-20:33:32:44:[step-32900/88800: 37.05%]--[loss-3.940965]--[lr-0.000042]--[ETA-8:24:42]
2023.01.20-20:34:29:144:[step-33000/88800: 37.16%]--[loss-3.427993]--[lr-0.000042]--[ETA-9:30:52]
2023.01.20-20:35:27:244:[step-33100/88800: 37.27%]--[loss-3.331345]--[lr-0.000042]--[ETA-8:25:03]
2023.01.20-20:36:24:344:[step-33200/88800: 37.39%]--[loss-3.753401]--[lr-0.000042]--[ETA-8:33:34]
2023.01.20-20:37:21:444:[step-33300/88800: 37.50%]--[loss-3.477267]--[lr-0.000042]--[ETA-7:57:09]
End of epoch 75 / 200 	 Time Taken: 255 sec
saving the model at the end of epoch 75, iters 33300
2023.01.20-20:38:20:100:[step-33400/88800: 37.61%]--[loss-3.656947]--[lr-0.000042]--[ETA-8:03:11]
2023.01.20-20:39:16:200:[step-33500/88800: 37.73%]--[loss-3.840201]--[lr-0.000042]--[ETA-9:52:25]
2023.01.20-20:40:13:300:[step-33600/88800: 37.84%]--[loss-3.721483]--[lr-0.000042]--[ETA-8:05:45]
2023.01.20-20:41:10:400:[step-33700/88800: 37.95%]--[loss-3.597130]--[lr-0.000042]--[ETA-8:53:01]
End of epoch 76 / 200 	 Time Taken: 254 sec
2023.01.20-20:42:10:56:[step-33800/88800: 38.06%]--[loss-3.707833]--[lr-0.000041]--[ETA-9:27:36]
2023.01.20-20:43:08:156:[step-33900/88800: 38.18%]--[loss-3.902402]--[lr-0.000041]--[ETA-8:05:35]
2023.01.20-20:44:06:256:[step-34000/88800: 38.29%]--[loss-3.574540]--[lr-0.000041]--[ETA-8:09:10]
2023.01.20-20:45:04:356:[step-34100/88800: 38.40%]--[loss-3.837038]--[lr-0.000041]--[ETA-9:14:01]
End of epoch 77 / 200 	 Time Taken: 259 sec
2023.01.20-20:46:04:12:[step-34200/88800: 38.51%]--[loss-3.911195]--[lr-0.000041]--[ETA-8:41:18]
2023.01.20-20:47:01:112:[step-34300/88800: 38.63%]--[loss-4.427042]--[lr-0.000041]--[ETA-8:43:43]
2023.01.20-20:47:58:212:[step-34400/88800: 38.74%]--[loss-3.932747]--[lr-0.000041]--[ETA-8:46:26]
2023.01.20-20:48:54:312:[step-34500/88800: 38.85%]--[loss-3.995165]--[lr-0.000041]--[ETA-8:45:22]
2023.01.20-20:49:51:412:[step-34600/88800: 38.96%]--[loss-3.798817]--[lr-0.000041]--[ETA-8:00:07]
End of epoch 78 / 200 	 Time Taken: 254 sec
2023.01.20-20:50:50:68:[step-34700/88800: 39.08%]--[loss-4.034642]--[lr-0.000041]--[ETA-8:38:40]
2023.01.20-20:51:46:168:[step-34800/88800: 39.19%]--[loss-3.745663]--[lr-0.000041]--[ETA-9:04:22]
2023.01.20-20:52:43:268:[step-34900/88800: 39.30%]--[loss-3.838010]--[lr-0.000041]--[ETA-8:23:13]
2023.01.20-20:53:40:368:[step-35000/88800: 39.41%]--[loss-3.960935]--[lr-0.000041]--[ETA-7:55:51]
End of epoch 79 / 200 	 Time Taken: 254 sec
2023.01.20-20:54:39:24:[step-35100/88800: 39.53%]--[loss-3.754605]--[lr-0.000040]--[ETA-7:51:53]
2023.01.20-20:55:36:124:[step-35200/88800: 39.64%]--[loss-3.778793]--[lr-0.000040]--[ETA-8:31:15]
2023.01.20-20:56:33:224:[step-35300/88800: 39.75%]--[loss-3.492092]--[lr-0.000040]--[ETA-8:25:29]
2023.01.20-20:57:30:324:[step-35400/88800: 39.86%]--[loss-3.630934]--[lr-0.000040]--[ETA-8:45:41]
2023.01.20-20:58:27:424:[step-35500/88800: 39.98%]--[loss-3.727992]--[lr-0.000040]--[ETA-8:16:05]
End of epoch 80 / 200 	 Time Taken: 254 sec
saving the model at the end of epoch 80, iters 35520
2023.01.20-20:59:25:80:[step-35600/88800: 40.09%]--[loss-4.228083]--[lr-0.000040]--[ETA-7:43:04]
2023.01.20-21:00:22:180:[step-35700/88800: 40.20%]--[loss-3.467314]--[lr-0.000040]--[ETA-8:23:33]
2023.01.20-21:01:18:280:[step-35800/88800: 40.32%]--[loss-3.685742]--[lr-0.000040]--[ETA-8:30:43]
2023.01.20-21:02:15:380:[step-35900/88800: 40.43%]--[loss-3.568313]--[lr-0.000040]--[ETA-8:06:38]
End of epoch 81 / 200 	 Time Taken: 253 sec
2023.01.20-21:03:14:36:[step-36000/88800: 40.54%]--[loss-3.890184]--[lr-0.000040]--[ETA-7:46:10]
2023.01.20-21:04:11:136:[step-36100/88800: 40.65%]--[loss-3.659888]--[lr-0.000040]--[ETA-7:54:19]
2023.01.20-21:05:07:236:[step-36200/88800: 40.77%]--[loss-3.625117]--[lr-0.000040]--[ETA-8:01:48]
2023.01.20-21:06:04:336:[step-36300/88800: 40.88%]--[loss-3.532780]--[lr-0.000040]--[ETA-8:31:22]
2023.01.20-21:07:01:436:[step-36400/88800: 40.99%]--[loss-4.234180]--[lr-0.000040]--[ETA-7:44:20]
End of epoch 82 / 200 	 Time Taken: 253 sec
2023.01.20-21:07:59:92:[step-36500/88800: 41.10%]--[loss-3.651829]--[lr-0.000039]--[ETA-8:20:25]
2023.01.20-21:08:56:192:[step-36600/88800: 41.22%]--[loss-3.712594]--[lr-0.000039]--[ETA-7:52:54]
2023.01.20-21:09:52:292:[step-36700/88800: 41.33%]--[loss-3.695740]--[lr-0.000039]--[ETA-7:35:47]
2023.01.20-21:10:49:392:[step-36800/88800: 41.44%]--[loss-3.617041]--[lr-0.000039]--[ETA-8:07:31]
End of epoch 83 / 200 	 Time Taken: 253 sec
2023.01.20-21:11:48:48:[step-36900/88800: 41.55%]--[loss-3.387322]--[lr-0.000039]--[ETA-7:30:34]
2023.01.20-21:12:45:148:[step-37000/88800: 41.67%]--[loss-3.420869]--[lr-0.000039]--[ETA-7:46:39]
2023.01.20-21:13:42:248:[step-37100/88800: 41.78%]--[loss-4.070951]--[lr-0.000039]--[ETA-8:04:48]
2023.01.20-21:14:38:348:[step-37200/88800: 41.89%]--[loss-3.596322]--[lr-0.000039]--[ETA-8:07:38]
End of epoch 84 / 200 	 Time Taken: 253 sec
2023.01.20-21:15:37:4:[step-37300/88800: 42.00%]--[loss-3.360010]--[lr-0.000039]--[ETA-9:09:26]
2023.01.20-21:16:34:104:[step-37400/88800: 42.12%]--[loss-3.538356]--[lr-0.000039]--[ETA-7:34:30]
2023.01.20-21:17:30:204:[step-37500/88800: 42.23%]--[loss-3.416403]--[lr-0.000039]--[ETA-7:49:23]
2023.01.20-21:18:27:304:[step-37600/88800: 42.34%]--[loss-3.503776]--[lr-0.000039]--[ETA-7:33:54]
2023.01.20-21:19:23:404:[step-37700/88800: 42.45%]--[loss-3.676050]--[lr-0.000039]--[ETA-7:56:41]
End of epoch 85 / 200 	 Time Taken: 253 sec
saving the model at the end of epoch 85, iters 37740
2023.01.20-21:20:22:60:[step-37800/88800: 42.57%]--[loss-3.865198]--[lr-0.000038]--[ETA-7:34:28]
2023.01.20-21:21:19:160:[step-37900/88800: 42.68%]--[loss-4.264541]--[lr-0.000038]--[ETA-7:56:18]
2023.01.20-21:22:16:260:[step-38000/88800: 42.79%]--[loss-3.597883]--[lr-0.000038]--[ETA-8:17:20]
2023.01.20-21:23:12:360:[step-38100/88800: 42.91%]--[loss-3.884756]--[lr-0.000038]--[ETA-7:45:19]
End of epoch 86 / 200 	 Time Taken: 253 sec
2023.01.20-21:24:11:16:[step-38200/88800: 43.02%]--[loss-3.510252]--[lr-0.000038]--[ETA-8:40:23]
2023.01.20-21:25:08:116:[step-38300/88800: 43.13%]--[loss-3.573536]--[lr-0.000038]--[ETA-7:38:51]
2023.01.20-21:26:05:216:[step-38400/88800: 43.24%]--[loss-3.777725]--[lr-0.000038]--[ETA-7:49:34]
2023.01.20-21:27:02:316:[step-38500/88800: 43.36%]--[loss-3.764575]--[lr-0.000038]--[ETA-7:32:22]
2023.01.20-21:27:59:416:[step-38600/88800: 43.47%]--[loss-3.883685]--[lr-0.000038]--[ETA-7:49:10]
End of epoch 87 / 200 	 Time Taken: 254 sec
2023.01.20-21:28:58:72:[step-38700/88800: 43.58%]--[loss-4.553454]--[lr-0.000038]--[ETA-8:07:32]
2023.01.20-21:29:54:172:[step-38800/88800: 43.69%]--[loss-3.955420]--[lr-0.000038]--[ETA-8:07:09]
2023.01.20-21:30:51:272:[step-38900/88800: 43.81%]--[loss-3.685854]--[lr-0.000038]--[ETA-7:18:49]
2023.01.20-21:31:48:372:[step-39000/88800: 43.92%]--[loss-3.439836]--[lr-0.000038]--[ETA-8:26:58]
End of epoch 88 / 200 	 Time Taken: 253 sec
2023.01.20-21:32:47:28:[step-39100/88800: 44.03%]--[loss-3.418237]--[lr-0.000037]--[ETA-8:13:34]
2023.01.20-21:33:43:128:[step-39200/88800: 44.14%]--[loss-3.750950]--[lr-0.000037]--[ETA-7:38:45]
2023.01.20-21:34:40:228:[step-39300/88800: 44.26%]--[loss-3.559609]--[lr-0.000037]--[ETA-7:20:44]
2023.01.20-21:35:37:328:[step-39400/88800: 44.37%]--[loss-3.887913]--[lr-0.000037]--[ETA-8:05:14]
2023.01.20-21:36:34:428:[step-39500/88800: 44.48%]--[loss-3.977137]--[lr-0.000037]--[ETA-7:06:26]
End of epoch 89 / 200 	 Time Taken: 254 sec
2023.01.20-21:37:33:84:[step-39600/88800: 44.59%]--[loss-3.553995]--[lr-0.000037]--[ETA-7:59:29]
2023.01.20-21:38:30:184:[step-39700/88800: 44.71%]--[loss-3.267132]--[lr-0.000037]--[ETA-7:42:35]
2023.01.20-21:39:27:284:[step-39800/88800: 44.82%]--[loss-3.388273]--[lr-0.000037]--[ETA-7:25:27]
2023.01.20-21:40:23:384:[step-39900/88800: 44.93%]--[loss-3.602940]--[lr-0.000037]--[ETA-7:12:06]
End of epoch 90 / 200 	 Time Taken: 254 sec
saving the model at the end of epoch 90, iters 39960
2023.01.20-21:41:22:40:[step-40000/88800: 45.05%]--[loss-3.169740]--[lr-0.000037]--[ETA-7:29:40]
2023.01.20-21:42:19:140:[step-40100/88800: 45.16%]--[loss-3.462655]--[lr-0.000037]--[ETA-8:15:07]
2023.01.20-21:43:15:240:[step-40200/88800: 45.27%]--[loss-3.257232]--[lr-0.000037]--[ETA-8:01:18]
2023.01.20-21:44:12:340:[step-40300/88800: 45.38%]--[loss-3.990134]--[lr-0.000037]--[ETA-8:12:30]
2023.01.20-21:45:09:440:[step-40400/88800: 45.50%]--[loss-3.511857]--[lr-0.000037]--[ETA-7:00:56]
End of epoch 91 / 200 	 Time Taken: 253 sec
2023.01.20-21:46:08:96:[step-40500/88800: 45.61%]--[loss-3.368699]--[lr-0.000036]--[ETA-7:47:00]
2023.01.20-21:47:05:196:[step-40600/88800: 45.72%]--[loss-3.863471]--[lr-0.000036]--[ETA-7:05:49]
2023.01.20-21:48:02:296:[step-40700/88800: 45.83%]--[loss-3.425187]--[lr-0.000036]--[ETA-8:03:27]
2023.01.20-21:48:58:396:[step-40800/88800: 45.95%]--[loss-3.337734]--[lr-0.000036]--[ETA-7:04:04]
End of epoch 92 / 200 	 Time Taken: 254 sec
2023.01.20-21:49:57:52:[step-40900/88800: 46.06%]--[loss-3.333318]--[lr-0.000036]--[ETA-7:12:58]
2023.01.20-21:50:54:152:[step-41000/88800: 46.17%]--[loss-3.779973]--[lr-0.000036]--[ETA-6:58:58]
2023.01.20-21:51:51:252:[step-41100/88800: 46.28%]--[loss-3.789088]--[lr-0.000036]--[ETA-7:29:17]
2023.01.20-21:52:47:352:[step-41200/88800: 46.40%]--[loss-3.458034]--[lr-0.000036]--[ETA-7:02:21]
End of epoch 93 / 200 	 Time Taken: 254 sec
2023.01.20-21:53:46:8:[step-41300/88800: 46.51%]--[loss-3.875012]--[lr-0.000036]--[ETA-7:42:22]
2023.01.20-21:54:43:108:[step-41400/88800: 46.62%]--[loss-3.341953]--[lr-0.000036]--[ETA-6:56:19]
2023.01.20-21:55:40:208:[step-41500/88800: 46.73%]--[loss-3.419173]--[lr-0.000036]--[ETA-7:16:25]
2023.01.20-21:56:37:308:[step-41600/88800: 46.85%]--[loss-3.644338]--[lr-0.000036]--[ETA-7:32:44]
2023.01.20-21:57:33:408:[step-41700/88800: 46.96%]--[loss-4.103848]--[lr-0.000036]--[ETA-6:56:32]
End of epoch 94 / 200 	 Time Taken: 254 sec
2023.01.20-21:58:32:64:[step-41800/88800: 47.07%]--[loss-3.594892]--[lr-0.000035]--[ETA-7:29:58]
2023.01.20-21:59:29:164:[step-41900/88800: 47.18%]--[loss-3.835171]--[lr-0.000035]--[ETA-8:28:44]
2023.01.20-22:00:26:264:[step-42000/88800: 47.30%]--[loss-3.594050]--[lr-0.000035]--[ETA-7:12:35]
2023.01.20-22:01:23:364:[step-42100/88800: 47.41%]--[loss-3.504327]--[lr-0.000035]--[ETA-7:29:43]
End of epoch 95 / 200 	 Time Taken: 254 sec
saving the model at the end of epoch 95, iters 42180
2023.01.20-22:02:22:20:[step-42200/88800: 47.52%]--[loss-3.573552]--[lr-0.000035]--[ETA-7:49:34]
2023.01.20-22:03:19:120:[step-42300/88800: 47.64%]--[loss-3.388385]--[lr-0.000035]--[ETA-7:09:48]
2023.01.20-22:04:15:220:[step-42400/88800: 47.75%]--[loss-3.389768]--[lr-0.000035]--[ETA-7:09:50]
2023.01.20-22:05:12:320:[step-42500/88800: 47.86%]--[loss-3.482274]--[lr-0.000035]--[ETA-6:44:42]
2023.01.20-22:06:09:420:[step-42600/88800: 47.97%]--[loss-4.060513]--[lr-0.000035]--[ETA-7:35:15]
End of epoch 96 / 200 	 Time Taken: 253 sec
2023.01.20-22:07:08:76:[step-42700/88800: 48.09%]--[loss-3.398136]--[lr-0.000035]--[ETA-6:41:25]
2023.01.20-22:08:05:176:[step-42800/88800: 48.20%]--[loss-3.626210]--[lr-0.000035]--[ETA-7:18:39]
2023.01.20-22:09:02:276:[step-42900/88800: 48.31%]--[loss-3.736049]--[lr-0.000035]--[ETA-7:35:31]
2023.01.20-22:09:58:376:[step-43000/88800: 48.42%]--[loss-3.743813]--[lr-0.000035]--[ETA-7:01:07]
End of epoch 97 / 200 	 Time Taken: 254 sec
2023.01.20-22:10:57:32:[step-43100/88800: 48.54%]--[loss-3.803780]--[lr-0.000034]--[ETA-6:44:10]
2023.01.20-22:11:54:132:[step-43200/88800: 48.65%]--[loss-3.502798]--[lr-0.000034]--[ETA-7:10:55]
2023.01.20-22:12:50:232:[step-43300/88800: 48.76%]--[loss-3.417140]--[lr-0.000034]--[ETA-6:54:52]
2023.01.20-22:13:47:332:[step-43400/88800: 48.87%]--[loss-3.340329]--[lr-0.000034]--[ETA-6:39:04]
2023.01.20-22:14:44:432:[step-43500/88800: 48.99%]--[loss-3.725161]--[lr-0.000034]--[ETA-7:10:22]
End of epoch 98 / 200 	 Time Taken: 253 sec
2023.01.20-22:15:42:88:[step-43600/88800: 49.10%]--[loss-3.898979]--[lr-0.000034]--[ETA-7:22:15]
2023.01.20-22:16:39:188:[step-43700/88800: 49.21%]--[loss-3.814436]--[lr-0.000034]--[ETA-6:52:16]
2023.01.20-22:17:36:288:[step-43800/88800: 49.32%]--[loss-3.316712]--[lr-0.000034]--[ETA-8:09:31]
2023.01.20-22:18:32:388:[step-43900/88800: 49.44%]--[loss-3.619239]--[lr-0.000034]--[ETA-7:07:28]
End of epoch 99 / 200 	 Time Taken: 253 sec
2023.01.20-22:19:31:44:[step-44000/88800: 49.55%]--[loss-3.140270]--[lr-0.000034]--[ETA-7:02:42]
2023.01.20-22:20:28:144:[step-44100/88800: 49.66%]--[loss-3.578779]--[lr-0.000034]--[ETA-6:44:54]
2023.01.20-22:21:25:244:[step-44200/88800: 49.77%]--[loss-3.728157]--[lr-0.000034]--[ETA-8:00:51]
2023.01.20-22:22:21:344:[step-44300/88800: 49.89%]--[loss-3.930959]--[lr-0.000034]--[ETA-6:34:05]
2023.01.20-22:23:18:444:[step-44400/88800: 50.00%]--[loss-4.037081]--[lr-0.000034]--[ETA-6:35:12]
End of epoch 100 / 200 	 Time Taken: 254 sec
saving the model at the end of epoch 100, iters 44400
2023.01.20-22:24:17:100:[step-44500/88800: 50.11%]--[loss-3.358164]--[lr-0.000033]--[ETA-6:39:09]
2023.01.20-22:25:14:200:[step-44600/88800: 50.23%]--[loss-4.535074]--[lr-0.000033]--[ETA-7:07:20]
2023.01.20-22:26:11:300:[step-44700/88800: 50.34%]--[loss-3.744758]--[lr-0.000033]--[ETA-7:09:01]
2023.01.20-22:27:07:400:[step-44800/88800: 50.45%]--[loss-3.452189]--[lr-0.000033]--[ETA-6:49:42]
End of epoch 101 / 200 	 Time Taken: 253 sec
2023.01.20-22:28:06:56:[step-44900/88800: 50.56%]--[loss-3.836178]--[lr-0.000033]--[ETA-6:35:29]
2023.01.20-22:29:03:156:[step-45000/88800: 50.68%]--[loss-3.406390]--[lr-0.000033]--[ETA-7:21:24]
2023.01.20-22:29:59:256:[step-45100/88800: 50.79%]--[loss-4.135820]--[lr-0.000033]--[ETA-7:28:02]
2023.01.20-22:30:56:356:[step-45200/88800: 50.90%]--[loss-3.606388]--[lr-0.000033]--[ETA-7:12:28]
End of epoch 102 / 200 	 Time Taken: 253 sec
2023.01.20-22:31:55:12:[step-45300/88800: 51.01%]--[loss-4.214383]--[lr-0.000033]--[ETA-6:22:56]
2023.01.20-22:32:51:112:[step-45400/88800: 51.13%]--[loss-3.927144]--[lr-0.000033]--[ETA-6:27:07]
2023.01.20-22:33:48:212:[step-45500/88800: 51.24%]--[loss-4.018246]--[lr-0.000033]--[ETA-7:00:21]
2023.01.20-22:34:45:312:[step-45600/88800: 51.35%]--[loss-3.342662]--[lr-0.000033]--[ETA-6:19:16]
2023.01.20-22:35:42:412:[step-45700/88800: 51.46%]--[loss-3.674399]--[lr-0.000033]--[ETA-6:51:01]
End of epoch 103 / 200 	 Time Taken: 254 sec
2023.01.20-22:36:41:68:[step-45800/88800: 51.58%]--[loss-3.585356]--[lr-0.000032]--[ETA-6:52:41]
2023.01.20-22:37:38:168:[step-45900/88800: 51.69%]--[loss-3.579114]--[lr-0.000032]--[ETA-6:50:36]
2023.01.20-22:38:34:268:[step-46000/88800: 51.80%]--[loss-3.467522]--[lr-0.000032]--[ETA-6:30:02]
2023.01.20-22:39:32:368:[step-46100/88800: 51.91%]--[loss-3.181296]--[lr-0.000032]--[ETA-7:39:12]
End of epoch 104 / 200 	 Time Taken: 257 sec
2023.01.20-22:40:36:24:[step-46200/88800: 52.03%]--[loss-3.215780]--[lr-0.000032]--[ETA-7:32:34]
2023.01.20-22:41:40:124:[step-46300/88800: 52.14%]--[loss-3.495143]--[lr-0.000032]--[ETA-7:46:26]
2023.01.20-22:42:43:224:[step-46400/88800: 52.25%]--[loss-3.418482]--[lr-0.000032]--[ETA-7:22:47]
2023.01.20-22:43:47:324:[step-46500/88800: 52.36%]--[loss-3.590547]--[lr-0.000032]--[ETA-7:20:28]
2023.01.20-22:44:50:424:[step-46600/88800: 52.48%]--[loss-3.397010]--[lr-0.000032]--[ETA-7:37:46]
End of epoch 105 / 200 	 Time Taken: 284 sec
saving the model at the end of epoch 105, iters 46620
2023.01.20-22:45:57:80:[step-46700/88800: 52.59%]--[loss-3.404614]--[lr-0.000032]--[ETA-7:02:47]
2023.01.20-22:46:58:180:[step-46800/88800: 52.70%]--[loss-3.707076]--[lr-0.000032]--[ETA-7:06:31]
2023.01.20-22:48:00:280:[step-46900/88800: 52.82%]--[loss-3.559285]--[lr-0.000032]--[ETA-7:27:43]
2023.01.20-22:49:02:380:[step-47000/88800: 52.93%]--[loss-3.639889]--[lr-0.000032]--[ETA-7:16:05]
End of epoch 106 / 200 	 Time Taken: 279 sec
2023.01.20-22:50:07:36:[step-47100/88800: 53.04%]--[loss-3.577424]--[lr-0.000031]--[ETA-7:18:20]
2023.01.20-22:51:09:136:[step-47200/88800: 53.15%]--[loss-3.601277]--[lr-0.000031]--[ETA-7:49:13]
2023.01.20-22:52:12:236:[step-47300/88800: 53.27%]--[loss-3.349948]--[lr-0.000031]--[ETA-7:01:44]
2023.01.20-22:53:15:336:[step-47400/88800: 53.38%]--[loss-3.555145]--[lr-0.000031]--[ETA-7:20:42]
2023.01.20-22:54:18:436:[step-47500/88800: 53.49%]--[loss-4.247100]--[lr-0.000031]--[ETA-7:04:12]
End of epoch 107 / 200 	 Time Taken: 281 sec
2023.01.20-22:55:24:92:[step-47600/88800: 53.60%]--[loss-3.703472]--[lr-0.000031]--[ETA-6:54:23]
2023.01.20-22:56:27:192:[step-47700/88800: 53.72%]--[loss-3.553696]--[lr-0.000031]--[ETA-7:15:16]
2023.01.20-22:57:30:292:[step-47800/88800: 53.83%]--[loss-3.830263]--[lr-0.000031]--[ETA-7:02:50]
2023.01.20-22:58:33:392:[step-47900/88800: 53.94%]--[loss-3.531927]--[lr-0.000031]--[ETA-6:57:31]
End of epoch 108 / 200 	 Time Taken: 281 sec
2023.01.20-22:59:38:48:[step-48000/88800: 54.05%]--[loss-3.345936]--[lr-0.000031]--[ETA-7:06:40]
2023.01.20-23:00:41:148:[step-48100/88800: 54.17%]--[loss-3.986989]--[lr-0.000031]--[ETA-7:08:52]
2023.01.20-23:01:44:248:[step-48200/88800: 54.28%]--[loss-3.639343]--[lr-0.000031]--[ETA-6:56:13]
2023.01.20-23:02:47:348:[step-48300/88800: 54.39%]--[loss-4.149205]--[lr-0.000031]--[ETA-7:01:30]
End of epoch 109 / 200 	 Time Taken: 282 sec
2023.01.20-23:03:52:4:[step-48400/88800: 54.50%]--[loss-3.546661]--[lr-0.000030]--[ETA-7:02:58]
2023.01.20-23:04:54:104:[step-48500/88800: 54.62%]--[loss-3.703747]--[lr-0.000030]--[ETA-6:43:43]
2023.01.20-23:05:56:204:[step-48600/88800: 54.73%]--[loss-3.503555]--[lr-0.000030]--[ETA-6:54:24]
2023.01.20-23:06:59:304:[step-48700/88800: 54.84%]--[loss-3.853740]--[lr-0.000030]--[ETA-7:15:10]
2023.01.20-23:08:02:404:[step-48800/88800: 54.95%]--[loss-3.914668]--[lr-0.000030]--[ETA-6:41:15]
End of epoch 110 / 200 	 Time Taken: 279 sec
saving the model at the end of epoch 110, iters 48840
2023.01.20-23:09:07:60:[step-48900/88800: 55.07%]--[loss-3.400924]--[lr-0.000030]--[ETA-6:39:08]
2023.01.20-23:10:09:160:[step-49000/88800: 55.18%]--[loss-3.448747]--[lr-0.000030]--[ETA-6:47:10]
2023.01.20-23:11:13:260:[step-49100/88800: 55.29%]--[loss-3.327129]--[lr-0.000030]--[ETA-7:14:54]
2023.01.20-23:12:16:360:[step-49200/88800: 55.41%]--[loss-3.184799]--[lr-0.000030]--[ETA-7:07:31]
End of epoch 111 / 200 	 Time Taken: 282 sec
2023.01.20-23:13:22:16:[step-49300/88800: 55.52%]--[loss-3.409909]--[lr-0.000030]--[ETA-6:56:31]
2023.01.20-23:14:25:116:[step-49400/88800: 55.63%]--[loss-3.784060]--[lr-0.000030]--[ETA-7:27:29]
2023.01.20-23:15:28:216:[step-49500/88800: 55.74%]--[loss-3.196810]--[lr-0.000030]--[ETA-6:45:35]
2023.01.20-23:16:31:316:[step-49600/88800: 55.86%]--[loss-3.597273]--[lr-0.000030]--[ETA-6:55:19]
2023.01.20-23:17:34:416:[step-49700/88800: 55.97%]--[loss-3.737200]--[lr-0.000030]--[ETA-7:11:59]
End of epoch 112 / 200 	 Time Taken: 281 sec
2023.01.20-23:18:39:72:[step-49800/88800: 56.08%]--[loss-3.414378]--[lr-0.000029]--[ETA-6:40:55]
2023.01.20-23:19:42:172:[step-49900/88800: 56.19%]--[loss-3.656979]--[lr-0.000029]--[ETA-6:37:49]
2023.01.20-23:20:45:272:[step-50000/88800: 56.31%]--[loss-3.604799]--[lr-0.000029]--[ETA-6:41:18]
2023.01.20-23:21:48:372:[step-50100/88800: 56.42%]--[loss-3.523206]--[lr-0.000029]--[ETA-6:36:34]
End of epoch 113 / 200 	 Time Taken: 282 sec
2023.01.20-23:22:53:28:[step-50200/88800: 56.53%]--[loss-3.941912]--[lr-0.000029]--[ETA-6:34:52]
2023.01.20-23:23:56:128:[step-50300/88800: 56.64%]--[loss-3.556608]--[lr-0.000029]--[ETA-6:57:33]
2023.01.20-23:24:59:228:[step-50400/88800: 56.76%]--[loss-3.283498]--[lr-0.000029]--[ETA-6:43:14]
2023.01.20-23:26:02:328:[step-50500/88800: 56.87%]--[loss-3.475531]--[lr-0.000029]--[ETA-6:31:21]
2023.01.20-23:27:05:428:[step-50600/88800: 56.98%]--[loss-3.407403]--[lr-0.000029]--[ETA-6:32:27]
End of epoch 114 / 200 	 Time Taken: 281 sec
2023.01.20-23:28:11:84:[step-50700/88800: 57.09%]--[loss-3.598150]--[lr-0.000029]--[ETA-6:31:51]
2023.01.20-23:29:14:184:[step-50800/88800: 57.21%]--[loss-3.421569]--[lr-0.000029]--[ETA-6:34:18]
2023.01.20-23:30:17:284:[step-50900/88800: 57.32%]--[loss-3.593566]--[lr-0.000029]--[ETA-7:09:51]
2023.01.20-23:31:20:384:[step-51000/88800: 57.43%]--[loss-3.716471]--[lr-0.000029]--[ETA-6:46:01]
End of epoch 115 / 200 	 Time Taken: 282 sec
saving the model at the end of epoch 115, iters 51060
2023.01.20-23:32:25:40:[step-51100/88800: 57.55%]--[loss-3.545506]--[lr-0.000028]--[ETA-6:33:33]
2023.01.20-23:33:28:140:[step-51200/88800: 57.66%]--[loss-3.883981]--[lr-0.000028]--[ETA-6:39:28]
2023.01.20-23:34:31:240:[step-51300/88800: 57.77%]--[loss-3.468677]--[lr-0.000028]--[ETA-6:20:00]
2023.01.20-23:35:34:340:[step-51400/88800: 57.88%]--[loss-3.486732]--[lr-0.000028]--[ETA-6:27:38]
2023.01.20-23:36:37:440:[step-51500/88800: 58.00%]--[loss-3.510952]--[lr-0.000028]--[ETA-6:26:53]
End of epoch 116 / 200 	 Time Taken: 282 sec
2023.01.20-23:37:43:96:[step-51600/88800: 58.11%]--[loss-3.519989]--[lr-0.000028]--[ETA-6:26:52]
2023.01.20-23:38:45:196:[step-51700/88800: 58.22%]--[loss-3.635134]--[lr-0.000028]--[ETA-6:18:38]
2023.01.20-23:39:48:296:[step-51800/88800: 58.33%]--[loss-3.552847]--[lr-0.000028]--[ETA-6:15:06]
2023.01.20-23:40:51:396:[step-51900/88800: 58.45%]--[loss-3.366794]--[lr-0.000028]--[ETA-6:44:31]
End of epoch 117 / 200 	 Time Taken: 281 sec
2023.01.20-23:41:57:52:[step-52000/88800: 58.56%]--[loss-3.857640]--[lr-0.000028]--[ETA-6:20:17]
2023.01.20-23:43:00:152:[step-52100/88800: 58.67%]--[loss-3.446226]--[lr-0.000028]--[ETA-6:45:28]
2023.01.20-23:44:03:252:[step-52200/88800: 58.78%]--[loss-4.377007]--[lr-0.000028]--[ETA-6:49:19]
2023.01.20-23:45:06:352:[step-52300/88800: 58.90%]--[loss-3.733437]--[lr-0.000028]--[ETA-6:08:34]
End of epoch 118 / 200 	 Time Taken: 282 sec
2023.01.20-23:46:12:8:[step-52400/88800: 59.01%]--[loss-3.138248]--[lr-0.000027]--[ETA-6:27:03]
2023.01.20-23:47:15:108:[step-52500/88800: 59.12%]--[loss-3.177555]--[lr-0.000027]--[ETA-6:27:28]
2023.01.20-23:48:17:208:[step-52600/88800: 59.23%]--[loss-3.355281]--[lr-0.000027]--[ETA-6:19:18]
2023.01.20-23:49:20:308:[step-52700/88800: 59.35%]--[loss-3.433609]--[lr-0.000027]--[ETA-6:17:00]
2023.01.20-23:50:23:408:[step-52800/88800: 59.46%]--[loss-3.668223]--[lr-0.000027]--[ETA-6:15:34]
End of epoch 119 / 200 	 Time Taken: 281 sec
2023.01.20-23:51:28:64:[step-52900/88800: 59.57%]--[loss-3.561440]--[lr-0.000027]--[ETA-6:06:01]
2023.01.20-23:52:32:164:[step-53000/88800: 59.68%]--[loss-3.393777]--[lr-0.000027]--[ETA-6:18:30]
2023.01.20-23:53:35:264:[step-53100/88800: 59.80%]--[loss-3.396525]--[lr-0.000027]--[ETA-6:23:30]
2023.01.20-23:54:38:364:[step-53200/88800: 59.91%]--[loss-3.876195]--[lr-0.000027]--[ETA-6:14:20]
End of epoch 120 / 200 	 Time Taken: 281 sec
saving the model at the end of epoch 120, iters 53280
2023.01.20-23:55:43:20:[step-53300/88800: 60.02%]--[loss-3.811516]--[lr-0.000027]--[ETA-6:00:15]
2023.01.20-23:56:47:120:[step-53400/88800: 60.14%]--[loss-4.079973]--[lr-0.000027]--[ETA-5:58:01]
2023.01.20-23:57:50:220:[step-53500/88800: 60.25%]--[loss-3.870204]--[lr-0.000027]--[ETA-6:01:02]
2023.01.20-23:58:53:320:[step-53600/88800: 60.36%]--[loss-3.417681]--[lr-0.000027]--[ETA-6:09:25]
2023.01.20-23:59:56:420:[step-53700/88800: 60.47%]--[loss-3.349767]--[lr-0.000027]--[ETA-6:02:11]
End of epoch 121 / 200 	 Time Taken: 283 sec
2023.01.21-00:01:02:76:[step-53800/88800: 60.59%]--[loss-3.561168]--[lr-0.000026]--[ETA-6:07:58]
2023.01.21-00:02:05:176:[step-53900/88800: 60.70%]--[loss-3.266452]--[lr-0.000026]--[ETA-6:02:47]
2023.01.21-00:03:08:276:[step-54000/88800: 60.81%]--[loss-4.072150]--[lr-0.000026]--[ETA-6:07:42]
2023.01.21-00:04:10:376:[step-54100/88800: 60.92%]--[loss-3.264993]--[lr-0.000026]--[ETA-5:59:23]
End of epoch 122 / 200 	 Time Taken: 282 sec
2023.01.21-00:05:16:32:[step-54200/88800: 61.04%]--[loss-4.931035]--[lr-0.000026]--[ETA-6:11:57]
2023.01.21-00:06:19:132:[step-54300/88800: 61.15%]--[loss-4.072590]--[lr-0.000026]--[ETA-6:03:03]
2023.01.21-00:07:22:232:[step-54400/88800: 61.26%]--[loss-3.986278]--[lr-0.000026]--[ETA-6:42:11]
2023.01.21-00:08:25:332:[step-54500/88800: 61.37%]--[loss-3.492686]--[lr-0.000026]--[ETA-6:13:40]
2023.01.21-00:09:28:432:[step-54600/88800: 61.49%]--[loss-3.245025]--[lr-0.000026]--[ETA-5:59:52]
End of epoch 123 / 200 	 Time Taken: 282 sec
2023.01.21-00:10:34:88:[step-54700/88800: 61.60%]--[loss-3.605540]--[lr-0.000026]--[ETA-6:14:24]
2023.01.21-00:11:37:188:[step-54800/88800: 61.71%]--[loss-3.571074]--[lr-0.000026]--[ETA-5:56:14]
2023.01.21-00:12:40:288:[step-54900/88800: 61.82%]--[loss-3.938672]--[lr-0.000026]--[ETA-5:49:39]
2023.01.21-00:13:43:388:[step-55000/88800: 61.94%]--[loss-3.507480]--[lr-0.000026]--[ETA-6:01:26]
End of epoch 124 / 200 	 Time Taken: 282 sec
2023.01.21-00:14:48:44:[step-55100/88800: 62.05%]--[loss-3.500157]--[lr-0.000025]--[ETA-5:43:02]
2023.01.21-00:15:51:144:[step-55200/88800: 62.16%]--[loss-4.132502]--[lr-0.000025]--[ETA-5:41:38]
2023.01.21-00:16:54:244:[step-55300/88800: 62.27%]--[loss-3.176373]--[lr-0.000025]--[ETA-5:48:34]
2023.01.21-00:17:57:344:[step-55400/88800: 62.39%]--[loss-3.316086]--[lr-0.000025]--[ETA-5:40:19]
2023.01.21-00:19:00:444:[step-55500/88800: 62.50%]--[loss-3.070610]--[lr-0.000025]--[ETA-5:42:58]
End of epoch 125 / 200 	 Time Taken: 282 sec
saving the model at the end of epoch 125, iters 55500
2023.01.21-00:20:06:100:[step-55600/88800: 62.61%]--[loss-3.303175]--[lr-0.000025]--[ETA-5:44:35]
2023.01.21-00:21:10:200:[step-55700/88800: 62.73%]--[loss-3.745577]--[lr-0.000025]--[ETA-5:58:19]
2023.01.21-00:22:13:300:[step-55800/88800: 62.84%]--[loss-4.215701]--[lr-0.000025]--[ETA-5:44:29]
2023.01.21-00:23:16:400:[step-55900/88800: 62.95%]--[loss-3.489681]--[lr-0.000025]--[ETA-5:32:43]
End of epoch 126 / 200 	 Time Taken: 283 sec
2023.01.21-00:24:21:56:[step-56000/88800: 63.06%]--[loss-4.106330]--[lr-0.000025]--[ETA-5:35:23]
2023.01.21-00:25:25:156:[step-56100/88800: 63.18%]--[loss-3.150475]--[lr-0.000025]--[ETA-5:39:59]
2023.01.21-00:26:28:256:[step-56200/88800: 63.29%]--[loss-3.303360]--[lr-0.000025]--[ETA-5:43:08]
2023.01.21-00:27:31:356:[step-56300/88800: 63.40%]--[loss-3.394647]--[lr-0.000025]--[ETA-5:50:20]
End of epoch 127 / 200 	 Time Taken: 282 sec
2023.01.21-00:28:37:12:[step-56400/88800: 63.51%]--[loss-3.495586]--[lr-0.000024]--[ETA-5:39:32]
2023.01.21-00:29:40:112:[step-56500/88800: 63.63%]--[loss-3.477045]--[lr-0.000024]--[ETA-5:29:24]
2023.01.21-00:30:43:212:[step-56600/88800: 63.74%]--[loss-3.381261]--[lr-0.000024]--[ETA-5:31:24]
2023.01.21-00:31:46:312:[step-56700/88800: 63.85%]--[loss-3.600980]--[lr-0.000024]--[ETA-5:38:42]
2023.01.21-00:32:49:412:[step-56800/88800: 63.96%]--[loss-3.249830]--[lr-0.000024]--[ETA-5:39:17]
End of epoch 128 / 200 	 Time Taken: 281 sec
2023.01.21-00:33:54:68:[step-56900/88800: 64.08%]--[loss-3.230698]--[lr-0.000024]--[ETA-5:17:09]
2023.01.21-00:34:57:168:[step-57000/88800: 64.19%]--[loss-3.529803]--[lr-0.000024]--[ETA-5:40:03]
2023.01.21-00:36:00:268:[step-57100/88800: 64.30%]--[loss-3.278363]--[lr-0.000024]--[ETA-5:24:31]
2023.01.21-00:37:03:368:[step-57200/88800: 64.41%]--[loss-3.475757]--[lr-0.000024]--[ETA-5:28:09]
End of epoch 129 / 200 	 Time Taken: 282 sec
2023.01.21-00:38:09:24:[step-57300/88800: 64.53%]--[loss-3.182241]--[lr-0.000024]--[ETA-5:45:12]
2023.01.21-00:39:12:124:[step-57400/88800: 64.64%]--[loss-3.565432]--[lr-0.000024]--[ETA-5:39:03]
2023.01.21-00:40:15:224:[step-57500/88800: 64.75%]--[loss-3.198748]--[lr-0.000024]--[ETA-5:21:20]
2023.01.21-00:41:18:324:[step-57600/88800: 64.86%]--[loss-3.439039]--[lr-0.000024]--[ETA-5:34:09]
2023.01.21-00:42:21:424:[step-57700/88800: 64.98%]--[loss-3.070804]--[lr-0.000024]--[ETA-5:22:01]
End of epoch 130 / 200 	 Time Taken: 281 sec
saving the model at the end of epoch 130, iters 57720
2023.01.21-00:43:26:80:[step-57800/88800: 65.09%]--[loss-4.137908]--[lr-0.000023]--[ETA-6:01:29]
2023.01.21-00:44:29:180:[step-57900/88800: 65.20%]--[loss-3.485570]--[lr-0.000023]--[ETA-5:10:58]
2023.01.21-00:45:33:280:[step-58000/88800: 65.32%]--[loss-3.380697]--[lr-0.000023]--[ETA-5:15:33]
2023.01.21-00:46:36:380:[step-58100/88800: 65.43%]--[loss-3.266005]--[lr-0.000023]--[ETA-5:16:44]
End of epoch 131 / 200 	 Time Taken: 282 sec
2023.01.21-00:47:41:36:[step-58200/88800: 65.54%]--[loss-3.930044]--[lr-0.000023]--[ETA-5:14:18]
2023.01.21-00:48:44:136:[step-58300/88800: 65.65%]--[loss-3.390757]--[lr-0.000023]--[ETA-5:13:43]
2023.01.21-00:49:46:236:[step-58400/88800: 65.77%]--[loss-3.224393]--[lr-0.000023]--[ETA-5:14:56]
2023.01.21-00:50:49:336:[step-58500/88800: 65.88%]--[loss-3.092887]--[lr-0.000023]--[ETA-5:08:57]
2023.01.21-00:51:51:436:[step-58600/88800: 65.99%]--[loss-3.956701]--[lr-0.000023]--[ETA-5:20:13]
End of epoch 132 / 200 	 Time Taken: 280 sec
2023.01.21-00:52:56:92:[step-58700/88800: 66.10%]--[loss-3.581775]--[lr-0.000023]--[ETA-5:07:18]
2023.01.21-00:53:59:192:[step-58800/88800: 66.22%]--[loss-3.726121]--[lr-0.000023]--[ETA-5:29:51]
2023.01.21-00:55:02:292:[step-58900/88800: 66.33%]--[loss-3.273188]--[lr-0.000023]--[ETA-5:12:40]
2023.01.21-00:56:05:392:[step-59000/88800: 66.44%]--[loss-3.700072]--[lr-0.000023]--[ETA-5:09:58]
End of epoch 133 / 200 	 Time Taken: 281 sec
2023.01.21-00:57:11:48:[step-59100/88800: 66.55%]--[loss-3.597512]--[lr-0.000022]--[ETA-5:08:17]
2023.01.21-00:58:15:148:[step-59200/88800: 66.67%]--[loss-3.444038]--[lr-0.000022]--[ETA-5:03:38]
2023.01.21-00:59:18:248:[step-59300/88800: 66.78%]--[loss-3.723875]--[lr-0.000022]--[ETA-5:14:22]
2023.01.21-01:00:22:348:[step-59400/88800: 66.89%]--[loss-3.256274]--[lr-0.000022]--[ETA-5:53:56]
End of epoch 134 / 200 	 Time Taken: 282 sec
2023.01.21-01:01:26:4:[step-59500/88800: 67.00%]--[loss-3.912169]--[lr-0.000022]--[ETA-5:12:00]
2023.01.21-01:02:28:104:[step-59600/88800: 67.12%]--[loss-3.387408]--[lr-0.000022]--[ETA-4:57:06]
2023.01.21-01:03:30:204:[step-59700/88800: 67.23%]--[loss-3.356937]--[lr-0.000022]--[ETA-5:26:26]
2023.01.21-01:04:33:304:[step-59800/88800: 67.34%]--[loss-3.366533]--[lr-0.000022]--[ETA-5:03:56]
2023.01.21-01:05:36:404:[step-59900/88800: 67.45%]--[loss-3.628538]--[lr-0.000022]--[ETA-4:54:26]
End of epoch 135 / 200 	 Time Taken: 280 sec
saving the model at the end of epoch 135, iters 59940
2023.01.21-01:06:41:60:[step-60000/88800: 67.57%]--[loss-4.364737]--[lr-0.000022]--[ETA-4:54:46]
2023.01.21-01:07:45:160:[step-60100/88800: 67.68%]--[loss-3.965414]--[lr-0.000022]--[ETA-5:00:38]
2023.01.21-01:08:47:260:[step-60200/88800: 67.79%]--[loss-3.536732]--[lr-0.000022]--[ETA-5:01:46]
2023.01.21-01:09:50:360:[step-60300/88800: 67.91%]--[loss-3.679918]--[lr-0.000022]--[ETA-4:58:04]
End of epoch 136 / 200 	 Time Taken: 282 sec
2023.01.21-01:10:56:16:[step-60400/88800: 68.02%]--[loss-3.448305]--[lr-0.000021]--[ETA-4:48:34]
2023.01.21-01:11:59:116:[step-60500/88800: 68.13%]--[loss-3.518551]--[lr-0.000021]--[ETA-5:04:18]
2023.01.21-01:13:02:216:[step-60600/88800: 68.24%]--[loss-3.550835]--[lr-0.000021]--[ETA-4:54:54]
2023.01.21-01:14:05:316:[step-60700/88800: 68.36%]--[loss-3.086365]--[lr-0.000021]--[ETA-5:00:11]
2023.01.21-01:15:08:416:[step-60800/88800: 68.47%]--[loss-3.773075]--[lr-0.000021]--[ETA-4:55:29]
End of epoch 137 / 200 	 Time Taken: 282 sec
2023.01.21-01:16:13:72:[step-60900/88800: 68.58%]--[loss-3.372303]--[lr-0.000021]--[ETA-4:48:53]
2023.01.21-01:17:16:172:[step-61000/88800: 68.69%]--[loss-3.380234]--[lr-0.000021]--[ETA-4:43:29]
2023.01.21-01:18:19:272:[step-61100/88800: 68.81%]--[loss-3.568679]--[lr-0.000021]--[ETA-4:42:35]
2023.01.21-01:19:22:372:[step-61200/88800: 68.92%]--[loss-3.912916]--[lr-0.000021]--[ETA-4:44:55]
End of epoch 138 / 200 	 Time Taken: 281 sec
2023.01.21-01:20:27:28:[step-61300/88800: 69.03%]--[loss-3.533996]--[lr-0.000021]--[ETA-4:55:09]
2023.01.21-01:21:30:128:[step-61400/88800: 69.14%]--[loss-3.696307]--[lr-0.000021]--[ETA-4:44:43]
2023.01.21-01:22:33:228:[step-61500/88800: 69.26%]--[loss-4.193263]--[lr-0.000021]--[ETA-5:15:25]
2023.01.21-01:23:37:328:[step-61600/88800: 69.37%]--[loss-3.344246]--[lr-0.000021]--[ETA-4:36:07]
2023.01.21-01:24:40:428:[step-61700/88800: 69.48%]--[loss-3.401326]--[lr-0.000021]--[ETA-4:31:06]
End of epoch 139 / 200 	 Time Taken: 282 sec
2023.01.21-01:25:45:84:[step-61800/88800: 69.59%]--[loss-3.094956]--[lr-0.000020]--[ETA-4:41:25]
2023.01.21-01:26:48:184:[step-61900/88800: 69.71%]--[loss-3.721075]--[lr-0.000020]--[ETA-4:47:53]
2023.01.21-01:27:51:284:[step-62000/88800: 69.82%]--[loss-3.479144]--[lr-0.000020]--[ETA-4:38:44]
2023.01.21-01:28:54:384:[step-62100/88800: 69.93%]--[loss-3.416234]--[lr-0.000020]--[ETA-4:31:46]
End of epoch 140 / 200 	 Time Taken: 282 sec
saving the model at the end of epoch 140, iters 62160
2023.01.21-01:30:00:40:[step-62200/88800: 70.05%]--[loss-3.458618]--[lr-0.000020]--[ETA-4:33:40]
2023.01.21-01:31:03:140:[step-62300/88800: 70.16%]--[loss-3.692585]--[lr-0.000020]--[ETA-4:40:50]
2023.01.21-01:32:06:240:[step-62400/88800: 70.27%]--[loss-3.329541]--[lr-0.000020]--[ETA-4:40:34]
2023.01.21-01:33:09:340:[step-62500/88800: 70.38%]--[loss-3.719092]--[lr-0.000020]--[ETA-4:29:39]
2023.01.21-01:34:12:440:[step-62600/88800: 70.50%]--[loss-3.187346]--[lr-0.000020]--[ETA-4:27:02]
End of epoch 141 / 200 	 Time Taken: 282 sec
2023.01.21-01:35:17:96:[step-62700/88800: 70.61%]--[loss-3.396393]--[lr-0.000020]--[ETA-4:31:17]
2023.01.21-01:36:20:196:[step-62800/88800: 70.72%]--[loss-3.232724]--[lr-0.000020]--[ETA-4:32:07]
2023.01.21-01:37:24:296:[step-62900/88800: 70.83%]--[loss-3.622525]--[lr-0.000020]--[ETA-4:52:22]
2023.01.21-01:38:27:396:[step-63000/88800: 70.95%]--[loss-3.636642]--[lr-0.000020]--[ETA-4:22:55]
End of epoch 142 / 200 	 Time Taken: 282 sec
2023.01.21-01:39:32:52:[step-63100/88800: 71.06%]--[loss-3.620711]--[lr-0.000019]--[ETA-4:33:18]
2023.01.21-01:40:34:152:[step-63200/88800: 71.17%]--[loss-3.577847]--[lr-0.000019]--[ETA-4:22:15]
2023.01.21-01:41:37:252:[step-63300/88800: 71.28%]--[loss-3.671721]--[lr-0.000019]--[ETA-4:30:14]
2023.01.21-01:42:40:352:[step-63400/88800: 71.40%]--[loss-3.988839]--[lr-0.000019]--[ETA-4:38:39]
End of epoch 143 / 200 	 Time Taken: 281 sec
2023.01.21-01:43:46:8:[step-63500/88800: 71.51%]--[loss-3.284486]--[lr-0.000019]--[ETA-4:25:20]
2023.01.21-01:44:49:108:[step-63600/88800: 71.62%]--[loss-3.318599]--[lr-0.000019]--[ETA-4:14:04]
2023.01.21-01:45:52:208:[step-63700/88800: 71.73%]--[loss-3.106743]--[lr-0.000019]--[ETA-4:37:06]
2023.01.21-01:46:55:308:[step-63800/88800: 71.85%]--[loss-3.499992]--[lr-0.000019]--[ETA-4:27:15]
2023.01.21-01:47:58:408:[step-63900/88800: 71.96%]--[loss-3.739518]--[lr-0.000019]--[ETA-4:16:06]
End of epoch 144 / 200 	 Time Taken: 282 sec
2023.01.21-01:49:04:64:[step-64000/88800: 72.07%]--[loss-3.417227]--[lr-0.000019]--[ETA-4:21:23]
2023.01.21-01:50:07:164:[step-64100/88800: 72.18%]--[loss-3.405978]--[lr-0.000019]--[ETA-4:23:38]
2023.01.21-01:51:10:264:[step-64200/88800: 72.30%]--[loss-3.351741]--[lr-0.000019]--[ETA-4:35:42]
2023.01.21-01:52:13:364:[step-64300/88800: 72.41%]--[loss-3.512803]--[lr-0.000019]--[ETA-4:13:33]
End of epoch 145 / 200 	 Time Taken: 282 sec
saving the model at the end of epoch 145, iters 64380
2023.01.21-01:53:19:20:[step-64400/88800: 72.52%]--[loss-3.010773]--[lr-0.000018]--[ETA-4:18:59]
2023.01.21-01:54:22:120:[step-64500/88800: 72.64%]--[loss-3.400591]--[lr-0.000018]--[ETA-4:17:23]
2023.01.21-01:55:25:220:[step-64600/88800: 72.75%]--[loss-3.275589]--[lr-0.000018]--[ETA-4:16:14]
2023.01.21-01:56:28:320:[step-64700/88800: 72.86%]--[loss-3.197911]--[lr-0.000018]--[ETA-4:22:34]
2023.01.21-01:57:31:420:[step-64800/88800: 72.97%]--[loss-3.772286]--[lr-0.000018]--[ETA-4:03:35]
End of epoch 146 / 200 	 Time Taken: 282 sec
2023.01.21-01:58:37:76:[step-64900/88800: 73.09%]--[loss-3.623196]--[lr-0.000018]--[ETA-4:05:40]
2023.01.21-01:59:40:176:[step-65000/88800: 73.20%]--[loss-3.206406]--[lr-0.000018]--[ETA-4:04:29]
2023.01.21-02:00:43:276:[step-65100/88800: 73.31%]--[loss-3.379815]--[lr-0.000018]--[ETA-4:04:25]
2023.01.21-02:01:46:376:[step-65200/88800: 73.42%]--[loss-3.245572]--[lr-0.000018]--[ETA-4:00:43]
End of epoch 147 / 200 	 Time Taken: 283 sec
2023.01.21-02:02:52:32:[step-65300/88800: 73.54%]--[loss-3.713769]--[lr-0.000018]--[ETA-4:23:42]
2023.01.21-02:03:55:132:[step-65400/88800: 73.65%]--[loss-3.636443]--[lr-0.000018]--[ETA-3:57:50]
2023.01.21-02:04:57:232:[step-65500/88800: 73.76%]--[loss-3.258277]--[lr-0.000018]--[ETA-4:02:16]
2023.01.21-02:06:01:332:[step-65600/88800: 73.87%]--[loss-3.489111]--[lr-0.000018]--[ETA-3:57:41]
2023.01.21-02:07:04:432:[step-65700/88800: 73.99%]--[loss-3.483472]--[lr-0.000018]--[ETA-3:55:27]
End of epoch 148 / 200 	 Time Taken: 282 sec
2023.01.21-02:08:09:88:[step-65800/88800: 74.10%]--[loss-3.856939]--[lr-0.000017]--[ETA-3:59:32]
2023.01.21-02:09:13:188:[step-65900/88800: 74.21%]--[loss-3.193935]--[lr-0.000017]--[ETA-3:51:59]
2023.01.21-02:10:16:288:[step-66000/88800: 74.32%]--[loss-3.116126]--[lr-0.000017]--[ETA-3:59:27]
2023.01.21-02:11:19:388:[step-66100/88800: 74.44%]--[loss-3.310065]--[lr-0.000017]--[ETA-4:28:14]
End of epoch 149 / 200 	 Time Taken: 283 sec
2023.01.21-02:12:25:44:[step-66200/88800: 74.55%]--[loss-3.540486]--[lr-0.000017]--[ETA-4:07:36]
2023.01.21-02:13:28:144:[step-66300/88800: 74.66%]--[loss-3.416561]--[lr-0.000017]--[ETA-3:50:42]
2023.01.21-02:14:31:244:[step-66400/88800: 74.77%]--[loss-3.321991]--[lr-0.000017]--[ETA-3:51:37]
2023.01.21-02:15:34:344:[step-66500/88800: 74.89%]--[loss-3.898276]--[lr-0.000017]--[ETA-3:50:52]
2023.01.21-02:16:37:444:[step-66600/88800: 75.00%]--[loss-3.721943]--[lr-0.000017]--[ETA-3:50:57]
End of epoch 150 / 200 	 Time Taken: 282 sec
saving the model at the end of epoch 150, iters 66600
2023.01.21-02:17:43:100:[step-66700/88800: 75.11%]--[loss-3.359262]--[lr-0.000017]--[ETA-3:52:22]
2023.01.21-02:18:46:200:[step-66800/88800: 75.23%]--[loss-3.186420]--[lr-0.000017]--[ETA-3:52:18]
2023.01.21-02:19:49:300:[step-66900/88800: 75.34%]--[loss-3.386568]--[lr-0.000017]--[ETA-3:43:43]
2023.01.21-02:20:52:400:[step-67000/88800: 75.45%]--[loss-3.205426]--[lr-0.000017]--[ETA-3:46:12]
End of epoch 151 / 200 	 Time Taken: 282 sec
2023.01.21-02:21:58:56:[step-67100/88800: 75.56%]--[loss-3.885116]--[lr-0.000016]--[ETA-3:51:51]
2023.01.21-02:23:00:156:[step-67200/88800: 75.68%]--[loss-3.895905]--[lr-0.000016]--[ETA-3:47:59]
2023.01.21-02:24:03:256:[step-67300/88800: 75.79%]--[loss-3.457566]--[lr-0.000016]--[ETA-3:55:01]
2023.01.21-02:25:06:356:[step-67400/88800: 75.90%]--[loss-3.792005]--[lr-0.000016]--[ETA-3:37:47]
End of epoch 152 / 200 	 Time Taken: 282 sec
2023.01.21-02:26:12:12:[step-67500/88800: 76.01%]--[loss-3.479373]--[lr-0.000016]--[ETA-3:41:48]
2023.01.21-02:27:14:112:[step-67600/88800: 76.13%]--[loss-3.327247]--[lr-0.000016]--[ETA-3:38:12]
2023.01.21-02:28:17:212:[step-67700/88800: 76.24%]--[loss-3.541116]--[lr-0.000016]--[ETA-3:39:52]
2023.01.21-02:29:20:312:[step-67800/88800: 76.35%]--[loss-3.245988]--[lr-0.000016]--[ETA-3:38:51]
2023.01.21-02:30:23:412:[step-67900/88800: 76.46%]--[loss-3.276754]--[lr-0.000016]--[ETA-3:32:50]
End of epoch 153 / 200 	 Time Taken: 281 sec
2023.01.21-02:31:29:68:[step-68000/88800: 76.58%]--[loss-3.358616]--[lr-0.000016]--[ETA-3:36:30]
2023.01.21-02:32:32:168:[step-68100/88800: 76.69%]--[loss-3.778550]--[lr-0.000016]--[ETA-3:30:32]
2023.01.21-02:33:35:268:[step-68200/88800: 76.80%]--[loss-3.526755]--[lr-0.000016]--[ETA-3:56:59]
2023.01.21-02:34:38:368:[step-68300/88800: 76.91%]--[loss-3.239204]--[lr-0.000016]--[ETA-3:32:55]
End of epoch 154 / 200 	 Time Taken: 282 sec
2023.01.21-02:35:44:24:[step-68400/88800: 77.03%]--[loss-3.424345]--[lr-0.000015]--[ETA-3:26:42]
2023.01.21-02:36:47:124:[step-68500/88800: 77.14%]--[loss-3.712349]--[lr-0.000015]--[ETA-3:26:05]
2023.01.21-02:37:50:224:[step-68600/88800: 77.25%]--[loss-3.144893]--[lr-0.000015]--[ETA-3:21:11]
2023.01.21-02:38:53:324:[step-68700/88800: 77.36%]--[loss-3.204540]--[lr-0.000015]--[ETA-3:35:06]
2023.01.21-02:39:56:424:[step-68800/88800: 77.48%]--[loss-3.736495]--[lr-0.000015]--[ETA-3:30:03]
End of epoch 155 / 200 	 Time Taken: 282 sec
saving the model at the end of epoch 155, iters 68820
2023.01.21-02:41:01:80:[step-68900/88800: 77.59%]--[loss-3.491366]--[lr-0.000015]--[ETA-3:23:30]
2023.01.21-02:42:04:180:[step-69000/88800: 77.70%]--[loss-3.656102]--[lr-0.000015]--[ETA-3:45:27]
2023.01.21-02:43:07:280:[step-69100/88800: 77.82%]--[loss-3.414070]--[lr-0.000015]--[ETA-3:33:46]
2023.01.21-02:44:10:380:[step-69200/88800: 77.93%]--[loss-3.250594]--[lr-0.000015]--[ETA-3:41:29]
End of epoch 156 / 200 	 Time Taken: 282 sec
2023.01.21-02:45:16:36:[step-69300/88800: 78.04%]--[loss-3.246652]--[lr-0.000015]--[ETA-3:20:07]
2023.01.21-02:46:18:136:[step-69400/88800: 78.15%]--[loss-3.363703]--[lr-0.000015]--[ETA-3:21:29]
2023.01.21-02:47:22:236:[step-69500/88800: 78.27%]--[loss-3.305242]--[lr-0.000015]--[ETA-3:15:41]
2023.01.21-02:48:23:336:[step-69600/88800: 78.38%]--[loss-3.423826]--[lr-0.000015]--[ETA-3:12:33]
2023.01.21-02:49:26:436:[step-69700/88800: 78.49%]--[loss-3.651722]--[lr-0.000015]--[ETA-3:10:34]
End of epoch 157 / 200 	 Time Taken: 279 sec
2023.01.21-02:50:31:92:[step-69800/88800: 78.60%]--[loss-3.643925]--[lr-0.000014]--[ETA-3:24:30]
2023.01.21-02:51:34:192:[step-69900/88800: 78.72%]--[loss-3.383271]--[lr-0.000014]--[ETA-3:18:32]
2023.01.21-02:52:36:292:[step-70000/88800: 78.83%]--[loss-3.208488]--[lr-0.000014]--[ETA-3:17:25]
2023.01.21-02:53:40:392:[step-70100/88800: 78.94%]--[loss-3.324320]--[lr-0.000014]--[ETA-3:16:36]
End of epoch 158 / 200 	 Time Taken: 282 sec
2023.01.21-02:54:46:48:[step-70200/88800: 79.05%]--[loss-3.313121]--[lr-0.000014]--[ETA-3:11:33]
2023.01.21-02:55:49:148:[step-70300/88800: 79.17%]--[loss-3.580515]--[lr-0.000014]--[ETA-3:09:39]
2023.01.21-02:56:53:248:[step-70400/88800: 79.28%]--[loss-3.998847]--[lr-0.000014]--[ETA-3:06:54]
2023.01.21-02:57:57:348:[step-70500/88800: 79.39%]--[loss-3.303065]--[lr-0.000014]--[ETA-3:10:03]
End of epoch 159 / 200 	 Time Taken: 284 sec
2023.01.21-02:59:02:4:[step-70600/88800: 79.50%]--[loss-3.580383]--[lr-0.000014]--[ETA-3:13:19]
2023.01.21-03:00:04:104:[step-70700/88800: 79.62%]--[loss-3.534693]--[lr-0.000014]--[ETA-3:01:24]
2023.01.21-03:01:06:204:[step-70800/88800: 79.73%]--[loss-3.806969]--[lr-0.000014]--[ETA-3:09:11]
2023.01.21-03:02:08:304:[step-70900/88800: 79.84%]--[loss-3.130548]--[lr-0.000014]--[ETA-3:07:34]
2023.01.21-03:03:12:404:[step-71000/88800: 79.95%]--[loss-3.459327]--[lr-0.000014]--[ETA-3:05:29]
End of epoch 160 / 200 	 Time Taken: 279 sec
saving the model at the end of epoch 160, iters 71040
2023.01.21-03:04:17:60:[step-71100/88800: 80.07%]--[loss-3.277622]--[lr-0.000013]--[ETA-3:20:22]
2023.01.21-03:05:21:160:[step-71200/88800: 80.18%]--[loss-3.230297]--[lr-0.000013]--[ETA-3:10:36]
2023.01.21-03:06:23:260:[step-71300/88800: 80.29%]--[loss-3.226697]--[lr-0.000013]--[ETA-3:01:48]
2023.01.21-03:07:27:360:[step-71400/88800: 80.41%]--[loss-3.031866]--[lr-0.000013]--[ETA-3:02:33]
End of epoch 161 / 200 	 Time Taken: 282 sec
2023.01.21-03:08:32:16:[step-71500/88800: 80.52%]--[loss-3.899603]--[lr-0.000013]--[ETA-3:02:11]
2023.01.21-03:09:35:116:[step-71600/88800: 80.63%]--[loss-3.392396]--[lr-0.000013]--[ETA-3:17:27]
2023.01.21-03:10:38:216:[step-71700/88800: 80.74%]--[loss-3.319370]--[lr-0.000013]--[ETA-2:58:08]
2023.01.21-03:11:41:316:[step-71800/88800: 80.86%]--[loss-3.389416]--[lr-0.000013]--[ETA-2:57:12]
2023.01.21-03:12:44:416:[step-71900/88800: 80.97%]--[loss-3.464784]--[lr-0.000013]--[ETA-3:11:22]
End of epoch 162 / 200 	 Time Taken: 282 sec
2023.01.21-03:13:49:72:[step-72000/88800: 81.08%]--[loss-3.602464]--[lr-0.000013]--[ETA-2:50:32]
2023.01.21-03:14:52:172:[step-72100/88800: 81.19%]--[loss-2.974640]--[lr-0.000013]--[ETA-2:57:35]
2023.01.21-03:15:55:272:[step-72200/88800: 81.31%]--[loss-3.495922]--[lr-0.000013]--[ETA-2:52:13]
2023.01.21-03:16:59:372:[step-72300/88800: 81.42%]--[loss-3.178744]--[lr-0.000013]--[ETA-2:45:16]
End of epoch 163 / 200 	 Time Taken: 282 sec
2023.01.21-03:18:04:28:[step-72400/88800: 81.53%]--[loss-3.758846]--[lr-0.000012]--[ETA-2:46:11]
2023.01.21-03:19:07:128:[step-72500/88800: 81.64%]--[loss-3.383640]--[lr-0.000012]--[ETA-2:54:01]
2023.01.21-03:20:09:228:[step-72600/88800: 81.76%]--[loss-3.580912]--[lr-0.000012]--[ETA-2:45:41]
2023.01.21-03:21:13:328:[step-72700/88800: 81.87%]--[loss-4.075983]--[lr-0.000012]--[ETA-2:50:10]
2023.01.21-03:22:16:428:[step-72800/88800: 81.98%]--[loss-3.223309]--[lr-0.000012]--[ETA-2:44:06]
End of epoch 164 / 200 	 Time Taken: 282 sec
2023.01.21-03:23:21:84:[step-72900/88800: 82.09%]--[loss-3.775247]--[lr-0.000012]--[ETA-2:55:03]
2023.01.21-03:24:25:184:[step-73000/88800: 82.21%]--[loss-3.344401]--[lr-0.000012]--[ETA-2:47:29]
2023.01.21-03:25:28:284:[step-73100/88800: 82.32%]--[loss-3.629686]--[lr-0.000012]--[ETA-2:51:10]
2023.01.21-03:26:30:384:[step-73200/88800: 82.43%]--[loss-3.313393]--[lr-0.000012]--[ETA-2:39:32]
End of epoch 165 / 200 	 Time Taken: 281 sec
saving the model at the end of epoch 165, iters 73260
2023.01.21-03:27:36:40:[step-73300/88800: 82.55%]--[loss-3.178160]--[lr-0.000012]--[ETA-2:37:55]
2023.01.21-03:28:39:140:[step-73400/88800: 82.66%]--[loss-3.630937]--[lr-0.000012]--[ETA-2:49:48]
2023.01.21-03:29:42:240:[step-73500/88800: 82.77%]--[loss-3.015726]--[lr-0.000012]--[ETA-2:42:18]
2023.01.21-03:30:45:340:[step-73600/88800: 82.88%]--[loss-3.043692]--[lr-0.000012]--[ETA-2:32:54]
2023.01.21-03:31:48:440:[step-73700/88800: 83.00%]--[loss-3.317952]--[lr-0.000012]--[ETA-2:41:26]
End of epoch 166 / 200 	 Time Taken: 282 sec
2023.01.21-03:32:53:96:[step-73800/88800: 83.11%]--[loss-3.237246]--[lr-0.000011]--[ETA-2:38:11]
2023.01.21-03:33:56:196:[step-73900/88800: 83.22%]--[loss-3.494622]--[lr-0.000011]--[ETA-2:32:13]
2023.01.21-03:34:59:296:[step-74000/88800: 83.33%]--[loss-3.371463]--[lr-0.000011]--[ETA-2:38:44]
2023.01.21-03:36:03:396:[step-74100/88800: 83.45%]--[loss-4.811735]--[lr-0.000011]--[ETA-2:29:06]
End of epoch 167 / 200 	 Time Taken: 282 sec
2023.01.21-03:37:08:52:[step-74200/88800: 83.56%]--[loss-3.006971]--[lr-0.000011]--[ETA-2:31:52]
2023.01.21-03:38:11:152:[step-74300/88800: 83.67%]--[loss-3.286055]--[lr-0.000011]--[ETA-2:27:30]
2023.01.21-03:39:13:252:[step-74400/88800: 83.78%]--[loss-3.227480]--[lr-0.000011]--[ETA-2:29:42]
2023.01.21-03:40:16:352:[step-74500/88800: 83.90%]--[loss-3.271158]--[lr-0.000011]--[ETA-2:27:25]
End of epoch 168 / 200 	 Time Taken: 281 sec
2023.01.21-03:41:21:8:[step-74600/88800: 84.01%]--[loss-3.326111]--[lr-0.000011]--[ETA-2:31:36]
2023.01.21-03:42:25:108:[step-74700/88800: 84.12%]--[loss-3.787346]--[lr-0.000011]--[ETA-2:21:38]
2023.01.21-03:43:28:208:[step-74800/88800: 84.23%]--[loss-3.474308]--[lr-0.000011]--[ETA-2:25:36]
2023.01.21-03:44:31:308:[step-74900/88800: 84.35%]--[loss-3.501139]--[lr-0.000011]--[ETA-2:29:04]
2023.01.21-03:45:34:408:[step-75000/88800: 84.46%]--[loss-3.101499]--[lr-0.000011]--[ETA-2:27:51]
End of epoch 169 / 200 	 Time Taken: 282 sec
2023.01.21-03:46:39:64:[step-75100/88800: 84.57%]--[loss-3.695580]--[lr-0.000010]--[ETA-2:19:58]
2023.01.21-03:47:42:164:[step-75200/88800: 84.68%]--[loss-4.003492]--[lr-0.000010]--[ETA-2:21:25]
2023.01.21-03:48:45:264:[step-75300/88800: 84.80%]--[loss-3.604325]--[lr-0.000010]--[ETA-2:17:05]
2023.01.21-03:49:48:364:[step-75400/88800: 84.91%]--[loss-3.398778]--[lr-0.000010]--[ETA-2:16:37]
End of epoch 170 / 200 	 Time Taken: 281 sec
saving the model at the end of epoch 170, iters 75480
2023.01.21-03:50:53:20:[step-75500/88800: 85.02%]--[loss-3.579439]--[lr-0.000010]--[ETA-2:17:38]
2023.01.21-03:51:56:120:[step-75600/88800: 85.14%]--[loss-3.225176]--[lr-0.000010]--[ETA-2:16:43]
2023.01.21-03:52:59:220:[step-75700/88800: 85.25%]--[loss-3.838210]--[lr-0.000010]--[ETA-2:28:53]
2023.01.21-03:54:01:320:[step-75800/88800: 85.36%]--[loss-3.641065]--[lr-0.000010]--[ETA-2:16:20]
2023.01.21-03:55:04:420:[step-75900/88800: 85.47%]--[loss-3.233382]--[lr-0.000010]--[ETA-2:10:38]
End of epoch 171 / 200 	 Time Taken: 280 sec
2023.01.21-03:56:09:76:[step-76000/88800: 85.59%]--[loss-3.188465]--[lr-0.000010]--[ETA-2:15:32]
2023.01.21-03:57:12:176:[step-76100/88800: 85.70%]--[loss-3.198758]--[lr-0.000010]--[ETA-2:11:04]
2023.01.21-03:58:15:276:[step-76200/88800: 85.81%]--[loss-3.385046]--[lr-0.000010]--[ETA-2:17:04]
2023.01.21-03:59:19:376:[step-76300/88800: 85.92%]--[loss-3.475449]--[lr-0.000010]--[ETA-2:10:34]
End of epoch 172 / 200 	 Time Taken: 282 sec
2023.01.21-04:00:24:32:[step-76400/88800: 86.04%]--[loss-3.558469]--[lr-0.000009]--[ETA-2:13:57]
2023.01.21-04:01:27:132:[step-76500/88800: 86.15%]--[loss-3.451368]--[lr-0.000009]--[ETA-2:11:05]
2023.01.21-04:02:30:232:[step-76600/88800: 86.26%]--[loss-3.309237]--[lr-0.000009]--[ETA-2:09:34]
2023.01.21-04:03:33:332:[step-76700/88800: 86.37%]--[loss-3.528376]--[lr-0.000009]--[ETA-2:10:57]
2023.01.21-04:04:36:432:[step-76800/88800: 86.49%]--[loss-3.613667]--[lr-0.000009]--[ETA-2:04:20]
End of epoch 173 / 200 	 Time Taken: 281 sec
2023.01.21-04:05:41:88:[step-76900/88800: 86.60%]--[loss-3.148150]--[lr-0.000009]--[ETA-2:01:56]
2023.01.21-04:06:44:188:[step-77000/88800: 86.71%]--[loss-3.268590]--[lr-0.000009]--[ETA-2:00:23]
2023.01.21-04:07:47:288:[step-77100/88800: 86.82%]--[loss-3.515776]--[lr-0.000009]--[ETA-2:01:45]
2023.01.21-04:08:50:388:[step-77200/88800: 86.94%]--[loss-3.690239]--[lr-0.000009]--[ETA-1:58:52]
End of epoch 174 / 200 	 Time Taken: 281 sec
2023.01.21-04:09:55:44:[step-77300/88800: 87.05%]--[loss-3.781502]--[lr-0.000009]--[ETA-2:01:20]
2023.01.21-04:10:58:144:[step-77400/88800: 87.16%]--[loss-3.371892]--[lr-0.000009]--[ETA-1:54:20]
2023.01.21-04:12:01:244:[step-77500/88800: 87.27%]--[loss-3.567423]--[lr-0.000009]--[ETA-2:02:27]
2023.01.21-04:13:04:344:[step-77600/88800: 87.39%]--[loss-3.080087]--[lr-0.000009]--[ETA-1:54:15]
2023.01.21-04:14:06:444:[step-77700/88800: 87.50%]--[loss-3.181737]--[lr-0.000009]--[ETA-1:57:02]
End of epoch 175 / 200 	 Time Taken: 281 sec
saving the model at the end of epoch 175, iters 77700
2023.01.21-04:15:12:100:[step-77800/88800: 87.61%]--[loss-3.286796]--[lr-0.000008]--[ETA-1:54:41]
2023.01.21-04:16:15:200:[step-77900/88800: 87.73%]--[loss-3.342035]--[lr-0.000008]--[ETA-1:56:49]
2023.01.21-04:17:18:300:[step-78000/88800: 87.84%]--[loss-3.246596]--[lr-0.000008]--[ETA-1:56:50]
2023.01.21-04:18:22:400:[step-78100/88800: 87.95%]--[loss-3.177913]--[lr-0.000008]--[ETA-1:49:12]
End of epoch 176 / 200 	 Time Taken: 282 sec
2023.01.21-04:19:27:56:[step-78200/88800: 88.06%]--[loss-3.968530]--[lr-0.000008]--[ETA-1:53:13]
2023.01.21-04:20:30:156:[step-78300/88800: 88.18%]--[loss-3.508147]--[lr-0.000008]--[ETA-2:00:49]
2023.01.21-04:21:33:256:[step-78400/88800: 88.29%]--[loss-3.458061]--[lr-0.000008]--[ETA-1:47:48]
2023.01.21-04:22:36:356:[step-78500/88800: 88.40%]--[loss-3.399918]--[lr-0.000008]--[ETA-1:49:40]
End of epoch 177 / 200 	 Time Taken: 281 sec
2023.01.21-04:23:41:12:[step-78600/88800: 88.51%]--[loss-3.008837]--[lr-0.000008]--[ETA-1:46:44]
2023.01.21-04:24:43:112:[step-78700/88800: 88.63%]--[loss-3.446842]--[lr-0.000008]--[ETA-1:43:03]
2023.01.21-04:25:46:212:[step-78800/88800: 88.74%]--[loss-3.438932]--[lr-0.000008]--[ETA-1:47:16]
2023.01.21-04:26:49:312:[step-78900/88800: 88.85%]--[loss-3.037814]--[lr-0.000008]--[ETA-1:43:59]
2023.01.21-04:27:52:412:[step-79000/88800: 88.96%]--[loss-3.832132]--[lr-0.000008]--[ETA-1:52:00]
End of epoch 178 / 200 	 Time Taken: 281 sec
2023.01.21-04:28:57:68:[step-79100/88800: 89.08%]--[loss-3.173496]--[lr-0.000007]--[ETA-1:39:12]
2023.01.21-04:30:01:168:[step-79200/88800: 89.19%]--[loss-3.287275]--[lr-0.000007]--[ETA-1:36:45]
2023.01.21-04:31:04:268:[step-79300/88800: 89.30%]--[loss-3.649008]--[lr-0.000007]--[ETA-1:37:01]
2023.01.21-04:32:06:368:[step-79400/88800: 89.41%]--[loss-3.932441]--[lr-0.000007]--[ETA-1:36:28]
End of epoch 179 / 200 	 Time Taken: 281 sec
2023.01.21-04:33:11:24:[step-79500/88800: 89.53%]--[loss-3.530739]--[lr-0.000007]--[ETA-1:36:26]
2023.01.21-04:34:15:124:[step-79600/88800: 89.64%]--[loss-3.078420]--[lr-0.000007]--[ETA-1:37:50]
2023.01.21-04:35:18:224:[step-79700/88800: 89.75%]--[loss-3.207082]--[lr-0.000007]--[ETA-1:33:00]
2023.01.21-04:36:21:324:[step-79800/88800: 89.86%]--[loss-3.777917]--[lr-0.000007]--[ETA-1:30:43]
2023.01.21-04:37:23:424:[step-79900/88800: 89.98%]--[loss-3.642943]--[lr-0.000007]--[ETA-1:34:33]
End of epoch 180 / 200 	 Time Taken: 281 sec
saving the model at the end of epoch 180, iters 79920
2023.01.21-04:38:29:80:[step-80000/88800: 90.09%]--[loss-3.506674]--[lr-0.000007]--[ETA-1:38:30]
2023.01.21-04:39:32:180:[step-80100/88800: 90.20%]--[loss-3.270844]--[lr-0.000007]--[ETA-1:33:41]
2023.01.21-04:40:35:280:[step-80200/88800: 90.32%]--[loss-3.510834]--[lr-0.000007]--[ETA-1:35:21]
2023.01.21-04:41:38:380:[step-80300/88800: 90.43%]--[loss-3.207588]--[lr-0.000007]--[ETA-1:28:58]
End of epoch 181 / 200 	 Time Taken: 281 sec
2023.01.21-04:42:43:36:[step-80400/88800: 90.54%]--[loss-3.733280]--[lr-0.000006]--[ETA-1:31:13]
2023.01.21-04:43:46:136:[step-80500/88800: 90.65%]--[loss-3.697295]--[lr-0.000006]--[ETA-1:26:21]
2023.01.21-04:44:49:236:[step-80600/88800: 90.77%]--[loss-3.148547]--[lr-0.000006]--[ETA-1:24:06]
2023.01.21-04:45:50:336:[step-80700/88800: 90.88%]--[loss-3.490244]--[lr-0.000006]--[ETA-1:25:08]
2023.01.21-04:46:53:436:[step-80800/88800: 90.99%]--[loss-3.320527]--[lr-0.000006]--[ETA-1:28:58]
End of epoch 182 / 200 	 Time Taken: 280 sec
2023.01.21-04:47:58:92:[step-80900/88800: 91.10%]--[loss-3.571635]--[lr-0.000006]--[ETA-1:24:36]
2023.01.21-04:49:01:192:[step-81000/88800: 91.22%]--[loss-3.401244]--[lr-0.000006]--[ETA-1:19:33]
2023.01.21-04:50:04:292:[step-81100/88800: 91.33%]--[loss-2.944632]--[lr-0.000006]--[ETA-1:20:52]
2023.01.21-04:51:07:392:[step-81200/88800: 91.44%]--[loss-3.380012]--[lr-0.000006]--[ETA-1:18:56]
End of epoch 183 / 200 	 Time Taken: 282 sec
2023.01.21-04:52:13:48:[step-81300/88800: 91.55%]--[loss-3.177523]--[lr-0.000006]--[ETA-1:17:16]
2023.01.21-04:53:17:148:[step-81400/88800: 91.67%]--[loss-3.277117]--[lr-0.000006]--[ETA-1:29:13]
2023.01.21-04:54:20:248:[step-81500/88800: 91.78%]--[loss-3.580443]--[lr-0.000006]--[ETA-1:15:13]
2023.01.21-04:55:24:348:[step-81600/88800: 91.89%]--[loss-3.141785]--[lr-0.000006]--[ETA-1:16:05]
End of epoch 184 / 200 	 Time Taken: 285 sec
2023.01.21-04:56:30:4:[step-81700/88800: 92.00%]--[loss-3.156708]--[lr-0.000005]--[ETA-1:11:37]
2023.01.21-04:57:31:104:[step-81800/88800: 92.12%]--[loss-3.131716]--[lr-0.000005]--[ETA-1:10:48]
2023.01.21-04:58:32:204:[step-81900/88800: 92.23%]--[loss-3.350128]--[lr-0.000005]--[ETA-1:10:23]
2023.01.21-04:59:35:304:[step-82000/88800: 92.34%]--[loss-3.584848]--[lr-0.000005]--[ETA-1:06:27]
2023.01.21-05:00:38:404:[step-82100/88800: 92.45%]--[loss-3.271244]--[lr-0.000005]--[ETA-1:08:57]
End of epoch 185 / 200 	 Time Taken: 277 sec
saving the model at the end of epoch 185, iters 82140
2023.01.21-05:01:43:60:[step-82200/88800: 92.57%]--[loss-3.238636]--[lr-0.000005]--[ETA-1:07:08]
2023.01.21-05:02:46:160:[step-82300/88800: 92.68%]--[loss-3.490937]--[lr-0.000005]--[ETA-1:06:10]
2023.01.21-05:03:49:260:[step-82400/88800: 92.79%]--[loss-2.991233]--[lr-0.000005]--[ETA-1:06:17]
2023.01.21-05:04:51:360:[step-82500/88800: 92.91%]--[loss-3.474932]--[lr-0.000005]--[ETA-1:08:05]
End of epoch 186 / 200 	 Time Taken: 281 sec
2023.01.21-05:05:57:16:[step-82600/88800: 93.02%]--[loss-3.094268]--[lr-0.000005]--[ETA-1:06:59]
2023.01.21-05:07:00:116:[step-82700/88800: 93.13%]--[loss-3.386869]--[lr-0.000005]--[ETA-1:01:19]
2023.01.21-05:08:02:216:[step-82800/88800: 93.24%]--[loss-3.201874]--[lr-0.000005]--[ETA-1:00:02]
2023.01.21-05:09:05:316:[step-82900/88800: 93.36%]--[loss-3.435522]--[lr-0.000005]--[ETA-1:00:32]
2023.01.21-05:10:08:416:[step-83000/88800: 93.47%]--[loss-3.204051]--[lr-0.000005]--[ETA-0:59:01]
End of epoch 187 / 200 	 Time Taken: 281 sec
2023.01.21-05:11:13:72:[step-83100/88800: 93.58%]--[loss-3.534767]--[lr-0.000004]--[ETA-1:00:40]
2023.01.21-05:12:16:172:[step-83200/88800: 93.69%]--[loss-3.440984]--[lr-0.000004]--[ETA-0:58:19]
2023.01.21-05:13:19:272:[step-83300/88800: 93.81%]--[loss-3.528126]--[lr-0.000004]--[ETA-0:56:24]
2023.01.21-05:14:22:372:[step-83400/88800: 93.92%]--[loss-3.490421]--[lr-0.000004]--[ETA-0:56:37]
End of epoch 188 / 200 	 Time Taken: 281 sec
2023.01.21-05:15:27:28:[step-83500/88800: 94.03%]--[loss-3.273259]--[lr-0.000004]--[ETA-0:54:42]
2023.01.21-05:16:30:128:[step-83600/88800: 94.14%]--[loss-3.088040]--[lr-0.000004]--[ETA-0:52:54]
2023.01.21-05:17:33:228:[step-83700/88800: 94.26%]--[loss-3.565843]--[lr-0.000004]--[ETA-0:53:57]
2023.01.21-05:18:36:328:[step-83800/88800: 94.37%]--[loss-3.723627]--[lr-0.000004]--[ETA-0:53:02]
2023.01.21-05:19:39:428:[step-83900/88800: 94.48%]--[loss-3.369155]--[lr-0.000004]--[ETA-0:50:03]
End of epoch 189 / 200 	 Time Taken: 281 sec
2023.01.21-05:20:44:84:[step-84000/88800: 94.59%]--[loss-3.286721]--[lr-0.000004]--[ETA-0:48:04]
2023.01.21-05:21:47:184:[step-84100/88800: 94.71%]--[loss-3.239365]--[lr-0.000004]--[ETA-0:47:54]
2023.01.21-05:22:50:284:[step-84200/88800: 94.82%]--[loss-3.459620]--[lr-0.000004]--[ETA-0:48:44]
2023.01.21-05:23:53:384:[step-84300/88800: 94.93%]--[loss-3.928153]--[lr-0.000004]--[ETA-0:46:25]
End of epoch 190 / 200 	 Time Taken: 281 sec
saving the model at the end of epoch 190, iters 84360
2023.01.21-05:24:58:40:[step-84400/88800: 95.05%]--[loss-3.677693]--[lr-0.000003]--[ETA-0:47:36]
2023.01.21-05:26:01:140:[step-84500/88800: 95.16%]--[loss-3.150411]--[lr-0.000003]--[ETA-0:45:13]
2023.01.21-05:27:04:240:[step-84600/88800: 95.27%]--[loss-3.769073]--[lr-0.000003]--[ETA-0:43:57]
2023.01.21-05:28:06:340:[step-84700/88800: 95.38%]--[loss-3.335405]--[lr-0.000003]--[ETA-0:41:42]
2023.01.21-05:29:10:440:[step-84800/88800: 95.50%]--[loss-3.567658]--[lr-0.000003]--[ETA-0:41:27]
End of epoch 191 / 200 	 Time Taken: 281 sec
2023.01.21-05:30:15:96:[step-84900/88800: 95.61%]--[loss-3.115656]--[lr-0.000003]--[ETA-0:39:50]
2023.01.21-05:31:18:196:[step-85000/88800: 95.72%]--[loss-3.376378]--[lr-0.000003]--[ETA-0:40:10]
2023.01.21-05:32:22:296:[step-85100/88800: 95.83%]--[loss-3.280838]--[lr-0.000003]--[ETA-0:38:01]
2023.01.21-05:33:25:396:[step-85200/88800: 95.95%]--[loss-3.274320]--[lr-0.000003]--[ETA-0:36:22]
End of epoch 192 / 200 	 Time Taken: 282 sec
2023.01.21-05:34:30:52:[step-85300/88800: 96.06%]--[loss-3.086143]--[lr-0.000003]--[ETA-0:36:53]
2023.01.21-05:35:33:152:[step-85400/88800: 96.17%]--[loss-3.060583]--[lr-0.000003]--[ETA-0:34:45]
2023.01.21-05:36:35:252:[step-85500/88800: 96.28%]--[loss-3.273250]--[lr-0.000003]--[ETA-0:34:17]
2023.01.21-05:37:39:352:[step-85600/88800: 96.40%]--[loss-3.187913]--[lr-0.000003]--[ETA-0:33:04]
End of epoch 193 / 200 	 Time Taken: 281 sec
2023.01.21-05:38:44:8:[step-85700/88800: 96.51%]--[loss-3.452086]--[lr-0.000002]--[ETA-0:31:50]
2023.01.21-05:39:47:108:[step-85800/88800: 96.62%]--[loss-3.233472]--[lr-0.000002]--[ETA-0:31:24]
2023.01.21-05:40:50:208:[step-85900/88800: 96.73%]--[loss-3.131401]--[lr-0.000002]--[ETA-0:31:03]
2023.01.21-05:41:53:308:[step-86000/88800: 96.85%]--[loss-3.288597]--[lr-0.000002]--[ETA-0:29:25]
2023.01.21-05:42:56:408:[step-86100/88800: 96.96%]--[loss-3.153488]--[lr-0.000002]--[ETA-0:28:04]
End of epoch 194 / 200 	 Time Taken: 282 sec
2023.01.21-05:44:02:64:[step-86200/88800: 97.07%]--[loss-3.204424]--[lr-0.000002]--[ETA-0:27:45]
2023.01.21-05:45:04:164:[step-86300/88800: 97.18%]--[loss-3.058115]--[lr-0.000002]--[ETA-0:26:14]
2023.01.21-05:46:07:264:[step-86400/88800: 97.30%]--[loss-3.197863]--[lr-0.000002]--[ETA-0:24:22]
2023.01.21-05:47:10:364:[step-86500/88800: 97.41%]--[loss-3.970398]--[lr-0.000002]--[ETA-0:23:31]
End of epoch 195 / 200 	 Time Taken: 281 sec
saving the model at the end of epoch 195, iters 86580
2023.01.21-05:48:16:20:[step-86600/88800: 97.52%]--[loss-3.018667]--[lr-0.000002]--[ETA-0:23:17]
2023.01.21-05:49:19:120:[step-86700/88800: 97.64%]--[loss-3.320799]--[lr-0.000002]--[ETA-0:24:50]
2023.01.21-05:50:22:220:[step-86800/88800: 97.75%]--[loss-3.478659]--[lr-0.000002]--[ETA-0:20:43]
2023.01.21-05:51:25:320:[step-86900/88800: 97.86%]--[loss-3.170279]--[lr-0.000002]--[ETA-0:20:15]
2023.01.21-05:52:28:420:[step-87000/88800: 97.97%]--[loss-3.938184]--[lr-0.000002]--[ETA-0:20:31]
End of epoch 196 / 200 	 Time Taken: 282 sec
2023.01.21-05:53:34:76:[step-87100/88800: 98.09%]--[loss-3.419474]--[lr-0.000001]--[ETA-0:17:59]
2023.01.21-05:54:37:176:[step-87200/88800: 98.20%]--[loss-3.237527]--[lr-0.000001]--[ETA-0:16:45]
2023.01.21-05:55:40:276:[step-87300/88800: 98.31%]--[loss-3.682918]--[lr-0.000001]--[ETA-0:15:53]
2023.01.21-05:56:42:376:[step-87400/88800: 98.42%]--[loss-3.191250]--[lr-0.000001]--[ETA-0:15:28]
End of epoch 197 / 200 	 Time Taken: 280 sec
2023.01.21-05:57:47:32:[step-87500/88800: 98.54%]--[loss-3.279477]--[lr-0.000001]--[ETA-0:13:24]
2023.01.21-05:58:50:132:[step-87600/88800: 98.65%]--[loss-3.530662]--[lr-0.000001]--[ETA-0:12:35]
2023.01.21-05:59:54:232:[step-87700/88800: 98.76%]--[loss-3.447075]--[lr-0.000001]--[ETA-0:11:27]
2023.01.21-06:00:57:332:[step-87800/88800: 98.87%]--[loss-3.383325]--[lr-0.000001]--[ETA-0:10:41]
2023.01.21-06:02:00:432:[step-87900/88800: 98.99%]--[loss-3.390939]--[lr-0.000001]--[ETA-0:09:08]
End of epoch 198 / 200 	 Time Taken: 282 sec
2023.01.21-06:03:05:88:[step-88000/88800: 99.10%]--[loss-3.239315]--[lr-0.000001]--[ETA-0:08:06]
2023.01.21-06:04:08:188:[step-88100/88800: 99.21%]--[loss-3.404954]--[lr-0.000001]--[ETA-0:07:22]
2023.01.21-06:05:11:288:[step-88200/88800: 99.32%]--[loss-3.226439]--[lr-0.000001]--[ETA-0:06:19]
2023.01.21-06:06:14:388:[step-88300/88800: 99.44%]--[loss-3.388345]--[lr-0.000001]--[ETA-0:05:18]
End of epoch 199 / 200 	 Time Taken: 281 sec
2023.01.21-06:07:19:44:[step-88400/88800: 99.55%]--[loss-3.600226]--[lr-0.000000]--[ETA-0:04:12]
2023.01.21-06:08:22:144:[step-88500/88800: 99.66%]--[loss-3.329998]--[lr-0.000000]--[ETA-0:03:10]
2023.01.21-06:09:25:244:[step-88600/88800: 99.77%]--[loss-3.088754]--[lr-0.000000]--[ETA-0:02:17]
2023.01.21-06:10:28:344:[step-88700/88800: 99.89%]--[loss-3.392632]--[lr-0.000000]--[ETA-0:01:03]
2023.01.21-06:11:30:444:[step-88800/88800: 100.00%]--[loss-3.540642]--[lr-0.000000]--[ETA-0:04:31]
End of epoch 200 / 200 	 Time Taken: 281 sec
saving the model at the end of epoch 200, iters 88800
/opt/conda/lib/python3.7/site-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
