/opt/conda/envs/srmgn/lib/python3.10/site-packages/torch/distributed/launch.py:180: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
/opt/conda/envs/srmgn/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525552843/work/aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
2023.04.10-00:52:31:100:[step-100/34000: 0.29%]--[loss-1.390412: wl-2.046251, gl-0.367287]--[lr-0.000050]--[ETA-9:06:13]
2023.04.10-00:54:08:200:[step-200/34000: 0.59%]--[loss-1.342395: wl-1.972698, gl-0.356046]--[lr-0.000050]--[ETA-9:03:42]
2023.04.10-00:55:46:300:[step-300/34000: 0.88%]--[loss-1.534915: wl-2.273056, gl-0.398387]--[lr-0.000050]--[ETA-9:13:55]
End of epoch 1 / 100: train_loss: 1.423 	 time: 344 sec
2023.04.10-00:57:37:60:[step-400/34000: 1.18%]--[loss-1.408702: wl-2.072176, gl-0.372615]--[lr-0.000050]--[ETA-9:20:25]
2023.04.10-00:59:14:160:[step-500/34000: 1.47%]--[loss-1.501736: wl-2.152576, gl-0.425448]--[lr-0.000050]--[ETA-8:58:06]
2023.04.10-01:00:52:260:[step-600/34000: 1.76%]--[loss-1.480569: wl-2.090058, gl-0.435541]--[lr-0.000050]--[ETA-8:56:03]
End of epoch 2 / 100: train_loss: 1.449 	 time: 347 sec
2023.04.10-01:02:58:20:[step-700/34000: 2.06%]--[loss-1.607003: wl-2.246807, gl-0.483599]--[lr-0.000050]--[ETA-9:25:24]
2023.04.10-01:04:38:120:[step-800/34000: 2.35%]--[loss-1.429888: wl-2.026350, gl-0.416713]--[lr-0.000050]--[ETA-8:54:52]
2023.04.10-01:06:17:220:[step-900/34000: 2.65%]--[loss-1.421227: wl-2.114043, gl-0.364205]--[lr-0.000050]--[ETA-9:04:03]
2023.04.10-01:07:56:320:[step-1000/34000: 2.94%]--[loss-1.411728: wl-2.061381, gl-0.381038]--[lr-0.000050]--[ETA-8:35:46]
End of epoch 3 / 100: train_loss: 1.460 	 time: 365 sec
2023.04.10-01:10:05:80:[step-1100/34000: 3.24%]--[loss-1.407784: wl-2.067820, gl-0.373874]--[lr-0.000050]--[ETA-9:15:29]
2023.04.10-01:11:47:180:[step-1200/34000: 3.53%]--[loss-1.326660: wl-1.977962, gl-0.337679]--[lr-0.000050]--[ETA-9:05:39]
2023.04.10-01:13:26:280:[step-1300/34000: 3.82%]--[loss-1.356245: wl-1.996857, gl-0.357816]--[lr-0.000050]--[ETA-9:23:46]
End of epoch 4 / 100: train_loss: 1.432 	 time: 369 sec
2023.04.10-01:15:39:40:[step-1400/34000: 4.12%]--[loss-1.551265: wl-2.314235, gl-0.394147]--[lr-0.000050]--[ETA-9:01:10]
2023.04.10-01:17:19:140:[step-1500/34000: 4.41%]--[loss-1.329122: wl-1.986394, gl-0.335925]--[lr-0.000050]--[ETA-8:48:25]
2023.04.10-01:19:00:240:[step-1600/34000: 4.71%]--[loss-1.425187: wl-2.069958, gl-0.390208]--[lr-0.000050]--[ETA-8:48:49]
2023.04.10-01:20:37:340:[step-1700/34000: 5.00%]--[loss-1.564338: wl-2.081806, gl-0.523435]--[lr-0.000050]--[ETA-2:29:17]
End of epoch 5 / 100: train_loss: 1.430 	 time: 373 sec
Saving the model at the end of epoch 5, iters 1700
2023.04.10-01:22:59:100:[step-1800/34000: 5.29%]--[loss-1.494527: wl-2.168284, gl-0.410386]--[lr-0.000050]--[ETA-9:06:15]
2023.04.10-01:24:40:200:[step-1900/34000: 5.59%]--[loss-1.499521: wl-2.267332, gl-0.365855]--[lr-0.000050]--[ETA-9:13:29]
2023.04.10-01:26:22:300:[step-2000/34000: 5.88%]--[loss-1.468499: wl-2.131357, gl-0.402821]--[lr-0.000050]--[ETA-9:04:06]
End of epoch 6 / 100: train_loss: 1.429 	 time: 384 sec
2023.04.10-01:28:45:60:[step-2100/34000: 6.18%]--[loss-1.405787: wl-2.038881, gl-0.386347]--[lr-0.000050]--[ETA-8:46:40]
2023.04.10-01:30:25:160:[step-2200/34000: 6.47%]--[loss-1.564915: wl-2.319216, gl-0.405307]--[lr-0.000050]--[ETA-8:49:17]
2023.04.10-01:32:05:260:[step-2300/34000: 6.76%]--[loss-1.421334: wl-2.117570, gl-0.362549]--[lr-0.000050]--[ETA-8:38:22]
End of epoch 7 / 100: train_loss: 1.431 	 time: 380 sec
2023.04.10-01:34:25:20:[step-2400/34000: 7.06%]--[loss-1.406137: wl-2.070012, gl-0.371131]--[lr-0.000050]--[ETA-9:22:28]
2023.04.10-01:36:07:120:[step-2500/34000: 7.35%]--[loss-1.443663: wl-2.103260, gl-0.392033]--[lr-0.000050]--[ETA-8:44:05]
2023.04.10-01:37:49:220:[step-2600/34000: 7.65%]--[loss-1.550714: wl-2.283336, gl-0.409045]--[lr-0.000050]--[ETA-8:50:45]
2023.04.10-01:39:30:320:[step-2700/34000: 7.94%]--[loss-1.386554: wl-2.040861, gl-0.366124]--[lr-0.000050]--[ETA-8:25:01]
End of epoch 8 / 100: train_loss: 1.433 	 time: 388 sec
2023.04.10-01:41:57:80:[step-2800/34000: 8.24%]--[loss-1.504373: wl-2.230779, gl-0.388983]--[lr-0.000050]--[ETA-8:43:51]
2023.04.10-01:43:38:180:[step-2900/34000: 8.53%]--[loss-1.421849: wl-2.136105, gl-0.353797]--[lr-0.000050]--[ETA-8:42:46]
2023.04.10-01:45:18:280:[step-3000/34000: 8.82%]--[loss-1.431762: wl-2.111109, gl-0.376207]--[lr-0.000050]--[ETA-8:36:37]
End of epoch 9 / 100: train_loss: 1.427 	 time: 385 sec
2023.04.10-01:47:42:40:[step-3100/34000: 9.12%]--[loss-1.417207: wl-2.134654, gl-0.349881]--[lr-0.000050]--[ETA-9:12:05]
2023.04.10-01:49:25:140:[step-3200/34000: 9.41%]--[loss-1.463225: wl-2.132496, gl-0.396977]--[lr-0.000050]--[ETA-8:43:34]
2023.04.10-01:51:07:240:[step-3300/34000: 9.71%]--[loss-1.495724: wl-2.164164, gl-0.413642]--[lr-0.000050]--[ETA-8:41:15]
2023.04.10-01:52:47:340:[step-3400/34000: 10.00%]--[loss-1.328624: wl-1.972793, gl-0.342227]--[lr-0.000050]--[ETA-2:29:26]
End of epoch 10 / 100: train_loss: 1.433 	 time: 391 sec
Saving the model at the end of epoch 10, iters 3400
2023.04.10-01:55:15:100:[step-3500/34000: 10.29%]--[loss-1.547127: wl-2.219698, gl-0.437278]--[lr-0.000050]--[ETA-8:47:00]
2023.04.10-01:56:56:200:[step-3600/34000: 10.59%]--[loss-1.360620: wl-1.976700, gl-0.372270]--[lr-0.000050]--[ETA-8:18:18]
2023.04.10-01:58:36:300:[step-3700/34000: 10.88%]--[loss-1.384104: wl-2.059998, gl-0.354105]--[lr-0.000050]--[ETA-8:42:14]
End of epoch 11 / 100: train_loss: 1.425 	 time: 385 sec
2023.04.10-02:00:59:60:[step-3800/34000: 11.18%]--[loss-1.528410: wl-2.163085, gl-0.446867]--[lr-0.000050]--[ETA-8:37:43]
2023.04.10-02:02:43:160:[step-3900/34000: 11.47%]--[loss-1.357679: wl-1.989542, gl-0.362908]--[lr-0.000050]--[ETA-8:54:08]
2023.04.10-02:04:24:260:[step-4000/34000: 11.76%]--[loss-1.442264: wl-2.128088, gl-0.378220]--[lr-0.000050]--[ETA-8:20:17]
End of epoch 12 / 100: train_loss: 1.441 	 time: 389 sec
2023.04.10-02:06:50:20:[step-4100/34000: 12.06%]--[loss-1.402306: wl-2.097831, gl-0.353390]--[lr-0.000050]--[ETA-8:30:23]
2023.04.10-02:08:31:120:[step-4200/34000: 12.35%]--[loss-1.458223: wl-2.116910, gl-0.399768]--[lr-0.000050]--[ETA-8:19:39]
2023.04.10-02:10:12:220:[step-4300/34000: 12.65%]--[loss-1.400533: wl-2.069799, gl-0.365633]--[lr-0.000050]--[ETA-8:11:27]
2023.04.10-02:11:51:320:[step-4400/34000: 12.94%]--[loss-1.485363: wl-2.216341, gl-0.377193]--[lr-0.000050]--[ETA-7:58:44]
End of epoch 13 / 100: train_loss: 1.423 	 time: 385 sec
2023.04.10-02:14:14:80:[step-4500/34000: 13.24%]--[loss-1.475921: wl-2.147043, gl-0.402400]--[lr-0.000050]--[ETA-8:23:45]
2023.04.10-02:15:57:180:[step-4600/34000: 13.53%]--[loss-1.394425: wl-2.050920, gl-0.368965]--[lr-0.000050]--[ETA-8:26:46]
2023.04.10-02:17:39:280:[step-4700/34000: 13.82%]--[loss-1.502701: wl-2.267838, gl-0.368782]--[lr-0.000050]--[ETA-8:15:18]
End of epoch 14 / 100: train_loss: 1.422 	 time: 389 sec
2023.04.10-02:20:05:40:[step-4800/34000: 14.12%]--[loss-1.411753: wl-2.076657, gl-0.373425]--[lr-0.000050]--[ETA-8:01:14]
2023.04.10-02:21:45:140:[step-4900/34000: 14.41%]--[loss-1.409157: wl-2.089027, gl-0.364643]--[lr-0.000050]--[ETA-8:00:10]
2023.04.10-02:23:25:240:[step-5000/34000: 14.71%]--[loss-1.329452: wl-1.940156, gl-0.359374]--[lr-0.000050]--[ETA-7:55:13]
2023.04.10-02:25:03:340:[step-5100/34000: 15.00%]--[loss-1.456966: wl-2.101130, gl-0.406401]--[lr-0.000050]--[ETA-2:28:50]
End of epoch 15 / 100: train_loss: 1.420 	 time: 383 sec
Saving the model at the end of epoch 15, iters 5100
2023.04.10-02:27:28:100:[step-5200/34000: 15.29%]--[loss-1.368880: wl-2.011501, gl-0.363129]--[lr-0.000050]--[ETA-8:38:47]
2023.04.10-02:29:11:200:[step-5300/34000: 15.59%]--[loss-1.351565: wl-2.006726, gl-0.348202]--[lr-0.000050]--[ETA-7:40:59]
2023.04.10-02:30:51:300:[step-5400/34000: 15.88%]--[loss-1.502280: wl-2.176565, gl-0.413998]--[lr-0.000050]--[ETA-7:56:55]
End of epoch 16 / 100: train_loss: 1.416 	 time: 387 sec
2023.04.10-02:33:14:60:[step-5500/34000: 16.18%]--[loss-1.385266: wl-2.061518, gl-0.354507]--[lr-0.000050]--[ETA-7:52:21]
2023.04.10-02:34:54:160:[step-5600/34000: 16.47%]--[loss-1.359480: wl-2.052527, gl-0.333216]--[lr-0.000050]--[ETA-8:01:15]
2023.04.10-02:36:34:260:[step-5700/34000: 16.76%]--[loss-1.410543: wl-2.048491, gl-0.386298]--[lr-0.000050]--[ETA-7:30:35]
End of epoch 17 / 100: train_loss: 1.417 	 time: 380 sec
2023.04.10-02:38:55:20:[step-5800/34000: 17.06%]--[loss-1.479662: wl-2.072876, gl-0.443224]--[lr-0.000050]--[ETA-8:20:09]
2023.04.10-02:40:37:120:[step-5900/34000: 17.35%]--[loss-1.383430: wl-2.066501, gl-0.350180]--[lr-0.000050]--[ETA-8:01:34]
2023.04.10-02:42:20:220:[step-6000/34000: 17.65%]--[loss-1.354993: wl-2.045285, gl-0.332350]--[lr-0.000050]--[ETA-7:42:03]
2023.04.10-02:44:01:320:[step-6100/34000: 17.94%]--[loss-1.379377: wl-2.071167, gl-0.343794]--[lr-0.000050]--[ETA-7:25:18]
End of epoch 18 / 100: train_loss: 1.433 	 time: 388 sec
2023.04.10-02:46:28:80:[step-6200/34000: 18.24%]--[loss-1.458761: wl-2.139570, gl-0.388976]--[lr-0.000050]--[ETA-7:41:30]
2023.04.10-02:48:08:180:[step-6300/34000: 18.53%]--[loss-1.354014: wl-2.002731, gl-0.352649]--[lr-0.000050]--[ETA-7:42:57]
2023.04.10-02:49:48:280:[step-6400/34000: 18.82%]--[loss-1.429984: wl-2.160790, gl-0.349588]--[lr-0.000050]--[ETA-7:30:32]
End of epoch 19 / 100: train_loss: 1.413 	 time: 386 sec
2023.04.10-02:52:10:40:[step-6500/34000: 19.12%]--[loss-1.446493: wl-2.156342, gl-0.368322]--[lr-0.000050]--[ETA-8:37:18]
2023.04.10-02:53:54:140:[step-6600/34000: 19.41%]--[loss-1.379321: wl-2.051618, gl-0.353512]--[lr-0.000050]--[ETA-7:43:58]
2023.04.10-02:55:36:240:[step-6700/34000: 19.71%]--[loss-1.473520: wl-2.182643, gl-0.382199]--[lr-0.000050]--[ETA-7:23:58]
2023.04.10-02:57:15:340:[step-6800/34000: 20.00%]--[loss-1.371498: wl-1.929843, gl-0.406576]--[lr-0.000050]--[ETA-2:05:20]
End of epoch 20 / 100: train_loss: 1.423 	 time: 389 sec
Saving the model at the end of epoch 20, iters 6800
2023.04.10-02:59:37:100:[step-6900/34000: 20.29%]--[loss-1.447675: wl-2.085844, gl-0.404753]--[lr-0.000050]--[ETA-7:48:22]
2023.04.10-03:01:17:200:[step-7000/34000: 20.59%]--[loss-1.434923: wl-2.107988, gl-0.380928]--[lr-0.000050]--[ETA-7:35:53]
2023.04.10-03:02:56:300:[step-7100/34000: 20.88%]--[loss-1.488729: wl-2.186338, gl-0.395560]--[lr-0.000050]--[ETA-7:11:49]
End of epoch 21 / 100: train_loss: 1.412 	 time: 378 sec
2023.04.10-03:05:18:60:[step-7200/34000: 21.18%]--[loss-1.435179: wl-2.098287, gl-0.386036]--[lr-0.000050]--[ETA-7:32:12]
2023.04.10-03:07:01:160:[step-7300/34000: 21.47%]--[loss-1.388034: wl-2.045890, gl-0.365089]--[lr-0.000050]--[ETA-7:34:28]
2023.04.10-03:08:43:260:[step-7400/34000: 21.76%]--[loss-1.408161: wl-2.119862, gl-0.348230]--[lr-0.000050]--[ETA-7:37:03]
End of epoch 22 / 100: train_loss: 1.412 	 time: 387 sec
2023.04.10-03:11:06:20:[step-7500/34000: 22.06%]--[loss-1.444068: wl-2.112915, gl-0.387610]--[lr-0.000050]--[ETA-7:27:57]
2023.04.10-03:12:46:120:[step-7600/34000: 22.35%]--[loss-1.414476: wl-2.106142, gl-0.361405]--[lr-0.000050]--[ETA-7:27:32]
2023.04.10-03:14:26:220:[step-7700/34000: 22.65%]--[loss-1.421470: wl-2.118712, gl-0.362114]--[lr-0.000050]--[ETA-7:25:06]
2023.04.10-03:16:04:320:[step-7800/34000: 22.94%]--[loss-1.419490: wl-2.094557, gl-0.372211]--[lr-0.000050]--[ETA-6:51:48]
End of epoch 23 / 100: train_loss: 1.408 	 time: 379 sec
2023.04.10-03:18:28:80:[step-7900/34000: 23.24%]--[loss-1.477268: wl-2.176185, gl-0.389175]--[lr-0.000050]--[ETA-7:19:36]
2023.04.10-03:20:10:180:[step-8000/34000: 23.53%]--[loss-1.355595: wl-2.028312, gl-0.341438]--[lr-0.000050]--[ETA-7:18:20]
2023.04.10-03:21:50:280:[step-8100/34000: 23.82%]--[loss-1.321502: wl-1.970974, gl-0.336015]--[lr-0.000050]--[ETA-7:05:16]
End of epoch 24 / 100: train_loss: 1.406 	 time: 387 sec
2023.04.10-03:24:10:40:[step-8200/34000: 24.12%]--[loss-1.373423: wl-2.033612, gl-0.356617]--[lr-0.000050]--[ETA-7:04:15]
2023.04.10-03:25:49:140:[step-8300/34000: 24.41%]--[loss-1.383483: wl-2.064837, gl-0.351065]--[lr-0.000050]--[ETA-6:57:21]
2023.04.10-03:27:27:240:[step-8400/34000: 24.71%]--[loss-1.410024: wl-2.006238, gl-0.406905]--[lr-0.000050]--[ETA-7:07:45]
2023.04.10-03:29:04:340:[step-8500/34000: 25.00%]--[loss-1.769840: wl-1.867587, gl-0.836047]--[lr-0.000050]--[ETA-2:03:35]
End of epoch 25 / 100: train_loss: 1.423 	 time: 373 sec
Saving the model at the end of epoch 25, iters 8500
2023.04.10-03:31:25:100:[step-8600/34000: 25.29%]--[loss-1.451512: wl-2.175238, gl-0.363893]--[lr-0.000050]--[ETA-7:08:56]
2023.04.10-03:33:06:200:[step-8700/34000: 25.59%]--[loss-1.410855: wl-2.175985, gl-0.322863]--[lr-0.000050]--[ETA-7:05:46]
2023.04.10-03:34:46:300:[step-8800/34000: 25.88%]--[loss-1.424392: wl-2.055256, gl-0.396764]--[lr-0.000050]--[ETA-6:59:49]
End of epoch 26 / 100: train_loss: 1.411 	 time: 381 sec
2023.04.10-03:37:07:60:[step-8900/34000: 26.18%]--[loss-1.407926: wl-2.106620, gl-0.354615]--[lr-0.000050]--[ETA-7:01:09]
2023.04.10-03:38:46:160:[step-9000/34000: 26.47%]--[loss-1.493073: wl-2.229203, gl-0.378471]--[lr-0.000050]--[ETA-6:45:37]
2023.04.10-03:40:24:260:[step-9100/34000: 26.76%]--[loss-1.374825: wl-2.027956, gl-0.360847]--[lr-0.000050]--[ETA-7:16:53]
End of epoch 27 / 100: train_loss: 1.404 	 time: 376 sec
2023.04.10-03:42:42:20:[step-9200/34000: 27.06%]--[loss-1.430175: wl-2.064836, gl-0.397757]--[lr-0.000050]--[ETA-6:58:21]
2023.04.10-03:44:24:120:[step-9300/34000: 27.35%]--[loss-1.410916: wl-2.069197, gl-0.376318]--[lr-0.000050]--[ETA-6:56:21]
2023.04.10-03:46:05:220:[step-9400/34000: 27.65%]--[loss-1.337529: wl-1.994947, gl-0.340055]--[lr-0.000050]--[ETA-6:53:06]
2023.04.10-03:47:43:320:[step-9500/34000: 27.94%]--[loss-1.425684: wl-2.142668, gl-0.354350]--[lr-0.000050]--[ETA-6:26:19]
End of epoch 28 / 100: train_loss: 1.404 	 time: 381 sec
2023.04.10-03:50:03:80:[step-9600/34000: 28.24%]--[loss-1.394727: wl-2.068824, gl-0.360315]--[lr-0.000050]--[ETA-6:31:30]
2023.04.10-03:51:41:180:[step-9700/34000: 28.53%]--[loss-1.328236: wl-2.012314, gl-0.322079]--[lr-0.000050]--[ETA-6:41:45]
2023.04.10-03:53:20:280:[step-9800/34000: 28.82%]--[loss-1.342356: wl-2.023727, gl-0.330493]--[lr-0.000050]--[ETA-6:40:57]
End of epoch 29 / 100: train_loss: 1.399 	 time: 374 sec
2023.04.10-03:55:40:40:[step-9900/34000: 29.12%]--[loss-1.410551: wl-2.026962, gl-0.397071]--[lr-0.000050]--[ETA-6:54:22]
2023.04.10-03:57:21:140:[step-10000/34000: 29.41%]--[loss-1.481001: wl-2.211807, gl-0.375098]--[lr-0.000050]--[ETA-6:37:12]
2023.04.10-03:59:01:240:[step-10100/34000: 29.71%]--[loss-1.538460: wl-2.298976, gl-0.388972]--[lr-0.000050]--[ETA-6:33:02]
2023.04.10-04:00:39:340:[step-10200/34000: 30.00%]--[loss-1.464307: wl-2.148684, gl-0.389966]--[lr-0.000050]--[ETA-2:22:20]
End of epoch 30 / 100: train_loss: 1.400 	 time: 382 sec
Saving the model at the end of epoch 30, iters 10200
2023.04.10-04:03:02:100:[step-10300/34000: 30.29%]--[loss-1.378735: wl-2.072619, gl-0.342426]--[lr-0.000050]--[ETA-6:35:03]
2023.04.10-04:04:41:200:[step-10400/34000: 30.59%]--[loss-1.305819: wl-1.954400, gl-0.328619]--[lr-0.000050]--[ETA-6:18:36]
2023.04.10-04:06:19:300:[step-10500/34000: 30.88%]--[loss-1.388610: wl-2.070814, gl-0.353203]--[lr-0.000050]--[ETA-6:38:24]
End of epoch 31 / 100: train_loss: 1.400 	 time: 376 sec
2023.04.10-04:08:39:60:[step-10600/34000: 31.18%]--[loss-1.433799: wl-2.092102, gl-0.387748]--[lr-0.000050]--[ETA-6:22:15]
2023.04.10-04:10:20:160:[step-10700/34000: 31.47%]--[loss-1.379311: wl-2.086051, gl-0.336286]--[lr-0.000050]--[ETA-6:34:07]
2023.04.10-04:12:00:260:[step-10800/34000: 31.76%]--[loss-1.368379: wl-2.008519, gl-0.364119]--[lr-0.000050]--[ETA-6:11:01]
End of epoch 32 / 100: train_loss: 1.397 	 time: 381 sec
2023.04.10-04:14:20:20:[step-10900/34000: 32.06%]--[loss-1.439528: wl-2.166398, gl-0.356329]--[lr-0.000050]--[ETA-6:21:08]
2023.04.10-04:15:59:120:[step-11000/34000: 32.35%]--[loss-1.379691: wl-2.037037, gl-0.361172]--[lr-0.000050]--[ETA-6:19:03]
2023.04.10-04:17:38:220:[step-11100/34000: 32.65%]--[loss-1.449796: wl-2.203307, gl-0.348143]--[lr-0.000050]--[ETA-6:13:07]
2023.04.10-04:19:16:320:[step-11200/34000: 32.94%]--[loss-1.323085: wl-1.988008, gl-0.329081]--[lr-0.000050]--[ETA-6:05:39]
End of epoch 33 / 100: train_loss: 1.399 	 time: 375 sec
2023.04.10-04:21:36:80:[step-11300/34000: 33.24%]--[loss-1.348508: wl-2.000393, gl-0.348312]--[lr-0.000050]--[ETA-6:28:55]
2023.04.10-04:23:17:180:[step-11400/34000: 33.53%]--[loss-1.369359: wl-2.070377, gl-0.334171]--[lr-0.000050]--[ETA-6:15:24]
2023.04.10-04:24:57:280:[step-11500/34000: 33.82%]--[loss-1.326288: wl-2.022582, gl-0.314997]--[lr-0.000050]--[ETA-6:18:08]
End of epoch 34 / 100: train_loss: 1.397 	 time: 381 sec
2023.04.10-04:27:16:40:[step-11600/34000: 34.12%]--[loss-1.414283: wl-2.125127, gl-0.351719]--[lr-0.000050]--[ETA-6:11:17]
2023.04.10-04:28:56:140:[step-11700/34000: 34.41%]--[loss-1.466657: wl-2.102799, gl-0.415257]--[lr-0.000050]--[ETA-6:03:31]
2023.04.10-04:30:34:240:[step-11800/34000: 34.71%]--[loss-1.400638: wl-2.141868, gl-0.329704]--[lr-0.000050]--[ETA-5:52:32]
2023.04.10-04:32:10:340:[step-11900/34000: 35.00%]--[loss-1.293772: wl-1.793872, gl-0.396837]--[lr-0.000050]--[ETA-1:48:16]
End of epoch 35 / 100: train_loss: 1.399 	 time: 374 sec
Saving the model at the end of epoch 35, iters 11900
2023.04.10-04:34:33:100:[step-12000/34000: 35.29%]--[loss-1.422709: wl-2.174310, gl-0.335554]--[lr-0.000050]--[ETA-6:11:23]
2023.04.10-04:36:14:200:[step-12100/34000: 35.59%]--[loss-1.370875: wl-2.079163, gl-0.331293]--[lr-0.000050]--[ETA-6:04:24]
2023.04.10-04:37:53:300:[step-12200/34000: 35.88%]--[loss-1.380862: wl-2.069484, gl-0.346120]--[lr-0.000050]--[ETA-6:10:00]
End of epoch 36 / 100: train_loss: 1.395 	 time: 381 sec
2023.04.10-04:40:14:60:[step-12300/34000: 36.18%]--[loss-1.451818: wl-2.113702, gl-0.394967]--[lr-0.000050]--[ETA-5:53:38]
2023.04.10-04:41:53:160:[step-12400/34000: 36.47%]--[loss-1.373152: wl-2.058735, gl-0.343785]--[lr-0.000050]--[ETA-6:02:10]
2023.04.10-04:43:32:260:[step-12500/34000: 36.76%]--[loss-1.340569: wl-1.942947, gl-0.369095]--[lr-0.000050]--[ETA-5:46:18]
End of epoch 37 / 100: train_loss: 1.407 	 time: 376 sec
2023.04.10-04:45:50:20:[step-12600/34000: 37.06%]--[loss-1.384858: wl-2.102572, gl-0.333572]--[lr-0.000050]--[ETA-5:49:14]
2023.04.10-04:47:31:120:[step-12700/34000: 37.35%]--[loss-1.362619: wl-2.053962, gl-0.335638]--[lr-0.000050]--[ETA-5:47:50]
2023.04.10-04:49:12:220:[step-12800/34000: 37.65%]--[loss-1.492289: wl-2.262759, gl-0.360910]--[lr-0.000050]--[ETA-6:00:37]
2023.04.10-04:50:51:320:[step-12900/34000: 37.94%]--[loss-1.391210: wl-2.100345, gl-0.341037]--[lr-0.000050]--[ETA-5:29:45]
End of epoch 38 / 100: train_loss: 1.389 	 time: 381 sec
2023.04.10-04:53:11:80:[step-13000/34000: 38.24%]--[loss-1.337432: wl-1.995492, gl-0.339686]--[lr-0.000050]--[ETA-5:49:58]
2023.04.10-04:54:50:180:[step-13100/34000: 38.53%]--[loss-1.356442: wl-2.025633, gl-0.343626]--[lr-0.000050]--[ETA-5:57:55]
2023.04.10-04:56:28:280:[step-13200/34000: 38.82%]--[loss-1.402823: wl-2.073200, gl-0.366223]--[lr-0.000050]--[ETA-5:36:11]
End of epoch 39 / 100: train_loss: 1.389 	 time: 374 sec
2023.04.10-04:58:47:40:[step-13300/34000: 39.12%]--[loss-1.391112: wl-2.051528, gl-0.365348]--[lr-0.000050]--[ETA-6:17:21]
2023.04.10-05:00:28:140:[step-13400/34000: 39.41%]--[loss-1.311388: wl-1.965862, gl-0.328457]--[lr-0.000050]--[ETA-5:53:35]
2023.04.10-05:02:08:240:[step-13500/34000: 39.71%]--[loss-1.389320: wl-2.094335, gl-0.342152]--[lr-0.000050]--[ETA-5:34:12]
2023.04.10-05:03:46:340:[step-13600/34000: 40.00%]--[loss-1.300863: wl-2.021282, gl-0.290222]--[lr-0.000050]--[ETA-1:37:19]
End of epoch 40 / 100: train_loss: 1.396 	 time: 381 sec
Saving the model at the end of epoch 40, iters 13600
2023.04.10-05:06:08:100:[step-13700/34000: 40.29%]--[loss-1.346093: wl-2.049832, gl-0.321177]--[lr-0.000050]--[ETA-5:29:24]
2023.04.10-05:07:47:200:[step-13800/34000: 40.59%]--[loss-1.358123: wl-2.074639, gl-0.320804]--[lr-0.000050]--[ETA-5:30:40]
2023.04.10-05:09:24:300:[step-13900/34000: 40.88%]--[loss-1.361466: wl-2.036874, gl-0.343029]--[lr-0.000050]--[ETA-5:29:14]
End of epoch 41 / 100: train_loss: 1.384 	 time: 375 sec
2023.04.10-05:11:44:60:[step-14000/34000: 41.18%]--[loss-1.472717: wl-2.247416, gl-0.349009]--[lr-0.000050]--[ETA-5:32:16]
2023.04.10-05:13:25:160:[step-14100/34000: 41.47%]--[loss-1.341213: wl-2.009832, gl-0.336297]--[lr-0.000050]--[ETA-5:26:44]
2023.04.10-05:15:05:260:[step-14200/34000: 41.76%]--[loss-1.426943: wl-2.146067, gl-0.353910]--[lr-0.000050]--[ETA-5:32:05]
End of epoch 42 / 100: train_loss: 1.395 	 time: 380 sec
2023.04.10-05:17:24:20:[step-14300/34000: 42.06%]--[loss-1.389390: wl-2.058177, gl-0.360301]--[lr-0.000050]--[ETA-5:23:24]
2023.04.10-05:19:04:120:[step-14400/34000: 42.35%]--[loss-1.350018: wl-2.053468, gl-0.323284]--[lr-0.000050]--[ETA-5:18:45]
2023.04.10-05:20:42:220:[step-14500/34000: 42.65%]--[loss-1.364395: wl-2.064345, gl-0.332223]--[lr-0.000050]--[ETA-5:15:56]
2023.04.10-05:22:21:320:[step-14600/34000: 42.94%]--[loss-1.329813: wl-1.979270, gl-0.340178]--[lr-0.000050]--[ETA-5:02:53]
End of epoch 43 / 100: train_loss: 1.385 	 time: 376 sec
2023.04.10-05:24:41:80:[step-14700/34000: 43.24%]--[loss-1.332097: wl-2.003751, gl-0.330221]--[lr-0.000050]--[ETA-5:14:23]
2023.04.10-05:26:21:180:[step-14800/34000: 43.53%]--[loss-1.362135: wl-2.087580, gl-0.318345]--[lr-0.000050]--[ETA-5:23:08]
2023.04.10-05:28:01:280:[step-14900/34000: 43.82%]--[loss-1.373388: wl-2.084346, gl-0.331215]--[lr-0.000050]--[ETA-5:08:45]
End of epoch 44 / 100: train_loss: 1.382 	 time: 380 sec
2023.04.10-05:30:21:40:[step-15000/34000: 44.12%]--[loss-1.427081: wl-2.184133, gl-0.335015]--[lr-0.000050]--[ETA-5:07:45]
2023.04.10-05:32:00:140:[step-15100/34000: 44.41%]--[loss-1.355658: wl-2.070152, gl-0.320582]--[lr-0.000050]--[ETA-5:03:00]
2023.04.10-05:33:39:240:[step-15200/34000: 44.71%]--[loss-1.345327: wl-2.019869, gl-0.335392]--[lr-0.000050]--[ETA-5:18:07]
2023.04.10-05:35:15:340:[step-15300/34000: 45.00%]--[loss-1.277225: wl-1.953249, gl-0.300600]--[lr-0.000050]--[ETA-1:27:00]
End of epoch 45 / 100: train_loss: 1.381 	 time: 375 sec
Saving the model at the end of epoch 45, iters 15300
2023.04.10-05:37:37:100:[step-15400/34000: 45.29%]--[loss-1.409731: wl-2.087347, gl-0.366058]--[lr-0.000050]--[ETA-5:08:05]
2023.04.10-05:39:16:200:[step-15500/34000: 45.59%]--[loss-1.303337: wl-2.032827, gl-0.286923]--[lr-0.000050]--[ETA-5:08:52]
2023.04.10-05:40:56:300:[step-15600/34000: 45.88%]--[loss-1.348546: wl-2.064317, gl-0.316388]--[lr-0.000050]--[ETA-5:09:30]
End of epoch 46 / 100: train_loss: 1.381 	 time: 378 sec
2023.04.10-05:43:15:60:[step-15700/34000: 46.18%]--[loss-1.485946: wl-2.279706, gl-0.346093]--[lr-0.000050]--[ETA-5:00:27]
2023.04.10-05:44:54:160:[step-15800/34000: 46.47%]--[loss-1.471673: wl-2.220176, gl-0.361585]--[lr-0.000050]--[ETA-5:15:48]
2023.04.10-05:46:32:260:[step-15900/34000: 46.76%]--[loss-1.341952: wl-2.041318, gl-0.321293]--[lr-0.000050]--[ETA-4:56:36]
End of epoch 47 / 100: train_loss: 1.380 	 time: 374 sec
2023.04.10-05:48:51:20:[step-16000/34000: 47.06%]--[loss-1.455283: wl-2.161475, gl-0.374546]--[lr-0.000050]--[ETA-5:11:05]
2023.04.10-05:50:32:120:[step-16100/34000: 47.35%]--[loss-1.333322: wl-1.990401, gl-0.338121]--[lr-0.000050]--[ETA-5:01:51]
2023.04.10-05:52:12:220:[step-16200/34000: 47.65%]--[loss-1.308963: wl-1.994045, gl-0.311940]--[lr-0.000050]--[ETA-4:55:32]
2023.04.10-05:53:50:320:[step-16300/34000: 47.94%]--[loss-1.364300: wl-2.081856, gl-0.323372]--[lr-0.000050]--[ETA-4:39:43]
End of epoch 48 / 100: train_loss: 1.378 	 time: 380 sec
2023.04.10-05:56:11:80:[step-16400/34000: 48.24%]--[loss-1.341459: wl-2.059953, gl-0.311483]--[lr-0.000050]--[ETA-4:50:12]
2023.04.10-05:57:50:180:[step-16500/34000: 48.53%]--[loss-1.329926: wl-2.042715, gl-0.308569]--[lr-0.000050]--[ETA-4:44:03]
2023.04.10-05:59:28:280:[step-16600/34000: 48.82%]--[loss-1.440563: wl-2.146995, gl-0.367066]--[lr-0.000050]--[ETA-4:47:56]
End of epoch 49 / 100: train_loss: 1.379 	 time: 375 sec
2023.04.10-06:01:47:40:[step-16700/34000: 49.12%]--[loss-1.342163: wl-2.013470, gl-0.335428]--[lr-0.000050]--[ETA-4:47:21]
2023.04.10-06:03:27:140:[step-16800/34000: 49.41%]--[loss-1.335032: wl-2.036425, gl-0.316819]--[lr-0.000050]--[ETA-4:44:44]
2023.04.10-06:05:07:240:[step-16900/34000: 49.71%]--[loss-1.282079: wl-1.913181, gl-0.325489]--[lr-0.000050]--[ETA-4:33:41]
2023.04.10-06:06:44:340:[step-17000/34000: 50.00%]--[loss-1.702878: wl-2.063774, gl-0.670992]--[lr-0.000050]--[ETA-1:12:07]
End of epoch 50 / 100: train_loss: 1.380 	 time: 379 sec
Saving the model at the end of epoch 50, iters 17000
2023.04.10-06:09:07:100:[step-17100/34000: 50.29%]--[loss-1.480563: wl-2.196655, gl-0.382235]--[lr-0.000050]--[ETA-4:40:44]
2023.04.10-06:10:45:200:[step-17200/34000: 50.59%]--[loss-1.395619: wl-2.091133, gl-0.350052]--[lr-0.000050]--[ETA-4:28:05]
2023.04.10-06:12:23:300:[step-17300/34000: 50.88%]--[loss-1.319711: wl-1.982689, gl-0.328367]--[lr-0.000050]--[ETA-4:32:02]
End of epoch 51 / 100: train_loss: 1.390 	 time: 375 sec
2023.04.10-06:14:44:60:[step-17400/34000: 51.18%]--[loss-1.364949: wl-2.089954, gl-0.319972]--[lr-0.000049]--[ETA-4:50:15]
2023.04.10-06:16:25:160:[step-17500/34000: 51.47%]--[loss-1.379415: wl-2.057201, gl-0.350814]--[lr-0.000049]--[ETA-4:37:25]
2023.04.10-06:18:05:260:[step-17600/34000: 51.76%]--[loss-1.360561: wl-2.058254, gl-0.331434]--[lr-0.000049]--[ETA-4:36:06]
End of epoch 52 / 100: train_loss: 1.374 	 time: 381 sec
2023.04.10-06:20:28:20:[step-17700/34000: 52.06%]--[loss-1.413470: wl-2.114075, gl-0.356432]--[lr-0.000048]--[ETA-4:43:43]
2023.04.10-06:22:08:120:[step-17800/34000: 52.35%]--[loss-1.497411: wl-2.298039, gl-0.348391]--[lr-0.000048]--[ETA-4:24:39]
2023.04.10-06:23:46:220:[step-17900/34000: 52.65%]--[loss-1.464201: wl-2.249025, gl-0.339689]--[lr-0.000048]--[ETA-4:17:08]
2023.04.10-06:25:24:320:[step-18000/34000: 52.94%]--[loss-1.332957: wl-2.058676, gl-0.303619]--[lr-0.000048]--[ETA-4:17:12]
End of epoch 53 / 100: train_loss: 1.374 	 time: 378 sec
2023.04.10-06:27:44:80:[step-18100/34000: 53.24%]--[loss-1.382616: wl-2.083369, gl-0.340932]--[lr-0.000047]--[ETA-4:19:21]
2023.04.10-06:29:25:180:[step-18200/34000: 53.53%]--[loss-1.437148: wl-2.179833, gl-0.347231]--[lr-0.000047]--[ETA-4:20:14]
2023.04.10-06:31:04:280:[step-18300/34000: 53.82%]--[loss-1.384787: wl-2.110843, gl-0.329365]--[lr-0.000047]--[ETA-4:21:28]
End of epoch 54 / 100: train_loss: 1.379 	 time: 380 sec
2023.04.10-06:33:27:40:[step-18400/34000: 54.12%]--[loss-1.430748: wl-2.238567, gl-0.311465]--[lr-0.000046]--[ETA-4:17:31]
2023.04.10-06:35:06:140:[step-18500/34000: 54.41%]--[loss-1.441754: wl-2.184289, gl-0.349610]--[lr-0.000046]--[ETA-4:08:12]
2023.04.10-06:36:45:240:[step-18600/34000: 54.71%]--[loss-1.281625: wl-1.983572, gl-0.289839]--[lr-0.000046]--[ETA-4:07:46]
2023.04.10-06:38:21:340:[step-18700/34000: 55.00%]--[loss-1.600173: wl-2.215064, gl-0.492641]--[lr-0.000046]--[ETA-1:12:28]
End of epoch 55 / 100: train_loss: 1.368 	 time: 378 sec
Saving the model at the end of epoch 55, iters 18700
2023.04.10-06:40:43:100:[step-18800/34000: 55.29%]--[loss-1.297779: wl-2.021505, gl-0.287027]--[lr-0.000045]--[ETA-4:12:10]
2023.04.10-06:42:24:200:[step-18900/34000: 55.59%]--[loss-1.283855: wl-1.946751, gl-0.310479]--[lr-0.000045]--[ETA-4:12:34]
2023.04.10-06:44:03:300:[step-19000/34000: 55.88%]--[loss-1.409493: wl-2.178220, gl-0.320383]--[lr-0.000045]--[ETA-4:06:06]
End of epoch 56 / 100: train_loss: 1.365 	 time: 380 sec
2023.04.10-06:46:22:60:[step-19100/34000: 56.18%]--[loss-1.335873: wl-2.016870, gl-0.327439]--[lr-0.000044]--[ETA-4:18:42]
2023.04.10-06:48:01:160:[step-19200/34000: 56.47%]--[loss-1.429981: wl-2.143223, gl-0.358370]--[lr-0.000044]--[ETA-3:55:49]
2023.04.10-06:49:40:260:[step-19300/34000: 56.76%]--[loss-1.349085: wl-2.057695, gl-0.320238]--[lr-0.000044]--[ETA-4:07:17]
End of epoch 57 / 100: train_loss: 1.362 	 time: 374 sec
2023.04.10-06:51:58:20:[step-19400/34000: 57.06%]--[loss-1.379043: wl-2.068210, gl-0.344939]--[lr-0.000043]--[ETA-3:54:32]
2023.04.10-06:53:38:120:[step-19500/34000: 57.35%]--[loss-1.411027: wl-2.175945, gl-0.323054]--[lr-0.000043]--[ETA-4:01:28]
2023.04.10-06:55:19:220:[step-19600/34000: 57.65%]--[loss-1.406258: wl-2.160213, gl-0.326152]--[lr-0.000043]--[ETA-4:03:16]
2023.04.10-06:56:58:320:[step-19700/34000: 57.94%]--[loss-1.406069: wl-2.162961, gl-0.324588]--[lr-0.000043]--[ETA-3:45:53]
End of epoch 58 / 100: train_loss: 1.363 	 time: 380 sec
2023.04.10-06:59:21:80:[step-19800/34000: 58.24%]--[loss-1.397232: wl-2.112523, gl-0.340970]--[lr-0.000042]--[ETA-3:50:21]
2023.04.10-07:01:00:180:[step-19900/34000: 58.53%]--[loss-1.294591: wl-2.014816, gl-0.287183]--[lr-0.000042]--[ETA-3:44:51]
2023.04.10-07:02:38:280:[step-20000/34000: 58.82%]--[loss-1.290315: wl-1.965832, gl-0.307399]--[lr-0.000042]--[ETA-3:47:59]
End of epoch 59 / 100: train_loss: 1.359 	 time: 378 sec
2023.04.10-07:04:55:40:[step-20100/34000: 59.12%]--[loss-1.369147: wl-2.041256, gl-0.348519]--[lr-0.000041]--[ETA-3:55:40]
2023.04.10-07:06:37:140:[step-20200/34000: 59.41%]--[loss-1.390561: wl-2.158122, gl-0.311500]--[lr-0.000041]--[ETA-3:47:43]
2023.04.10-07:08:17:240:[step-20300/34000: 59.71%]--[loss-1.417140: wl-2.154896, gl-0.339692]--[lr-0.000041]--[ETA-3:58:58]
2023.04.10-07:09:54:340:[step-20400/34000: 60.00%]--[loss-1.222611: wl-1.901713, gl-0.271754]--[lr-0.000041]--[ETA-1:09:17]
End of epoch 60 / 100: train_loss: 1.356 	 time: 379 sec
Saving the model at the end of epoch 60, iters 20400
2023.04.10-07:12:19:100:[step-20500/34000: 60.29%]--[loss-1.260527: wl-1.967807, gl-0.276623]--[lr-0.000040]--[ETA-4:05:35]
2023.04.10-07:13:58:200:[step-20600/34000: 60.59%]--[loss-1.389641: wl-2.158111, gl-0.310585]--[lr-0.000040]--[ETA-3:36:29]
2023.04.10-07:15:37:300:[step-20700/34000: 60.88%]--[loss-1.333911: wl-2.025815, gl-0.321003]--[lr-0.000040]--[ETA-3:33:57]
End of epoch 61 / 100: train_loss: 1.354 	 time: 378 sec
2023.04.10-07:17:55:60:[step-20800/34000: 61.18%]--[loss-1.398501: wl-2.143509, gl-0.326746]--[lr-0.000039]--[ETA-3:37:35]
2023.04.10-07:19:37:160:[step-20900/34000: 61.47%]--[loss-1.381049: wl-2.105865, gl-0.328116]--[lr-0.000039]--[ETA-3:36:34]
2023.04.10-07:21:18:260:[step-21000/34000: 61.76%]--[loss-1.335633: wl-2.023213, gl-0.324026]--[lr-0.000039]--[ETA-3:26:08]
End of epoch 62 / 100: train_loss: 1.355 	 time: 381 sec
2023.04.10-07:23:40:20:[step-21100/34000: 62.06%]--[loss-1.282797: wl-1.966878, gl-0.299358]--[lr-0.000038]--[ETA-3:38:44]
2023.04.10-07:25:19:120:[step-21200/34000: 62.35%]--[loss-1.373101: wl-2.083564, gl-0.331319]--[lr-0.000038]--[ETA-3:29:46]
2023.04.10-07:26:58:220:[step-21300/34000: 62.65%]--[loss-1.392821: wl-2.146574, gl-0.319534]--[lr-0.000038]--[ETA-3:25:26]
2023.04.10-07:28:36:320:[step-21400/34000: 62.94%]--[loss-1.379536: wl-2.166489, gl-0.296291]--[lr-0.000038]--[ETA-3:20:03]
End of epoch 63 / 100: train_loss: 1.353 	 time: 378 sec
2023.04.10-07:30:54:80:[step-21500/34000: 63.24%]--[loss-1.286523: wl-1.944225, gl-0.314411]--[lr-0.000037]--[ETA-3:31:23]
2023.04.10-07:32:34:180:[step-21600/34000: 63.53%]--[loss-1.380051: wl-2.108052, gl-0.326025]--[lr-0.000037]--[ETA-3:27:40]
2023.04.10-07:34:14:280:[step-21700/34000: 63.82%]--[loss-1.341743: wl-2.094250, gl-0.294618]--[lr-0.000037]--[ETA-3:32:05]
End of epoch 64 / 100: train_loss: 1.350 	 time: 378 sec
2023.04.10-07:36:37:40:[step-21800/34000: 64.12%]--[loss-1.305976: wl-1.992239, gl-0.309857]--[lr-0.000036]--[ETA-3:22:28]
2023.04.10-07:38:17:140:[step-21900/34000: 64.41%]--[loss-1.410844: wl-2.096128, gl-0.362780]--[lr-0.000036]--[ETA-3:12:14]
2023.04.10-07:39:55:240:[step-22000/34000: 64.71%]--[loss-1.338364: wl-2.057361, gl-0.309684]--[lr-0.000036]--[ETA-3:11:33]
2023.04.10-07:41:32:340:[step-22100/34000: 65.00%]--[loss-1.229738: wl-1.972828, gl-0.243325]--[lr-0.000036]--[ETA-0:55:25]
End of epoch 65 / 100: train_loss: 1.347 	 time: 378 sec
Saving the model at the end of epoch 65, iters 22100
2023.04.10-07:43:53:100:[step-22200/34000: 65.29%]--[loss-1.288732: wl-1.989582, gl-0.293941]--[lr-0.000035]--[ETA-3:27:31]
2023.04.10-07:45:35:200:[step-22300/34000: 65.59%]--[loss-1.260834: wl-1.954989, gl-0.283339]--[lr-0.000035]--[ETA-3:24:02]
2023.04.10-07:47:14:300:[step-22400/34000: 65.88%]--[loss-1.353921: wl-2.061129, gl-0.323357]--[lr-0.000035]--[ETA-3:05:17]
End of epoch 66 / 100: train_loss: 1.345 	 time: 380 sec
2023.04.10-07:49:32:60:[step-22500/34000: 66.18%]--[loss-1.369565: wl-2.086840, gl-0.326145]--[lr-0.000034]--[ETA-3:04:43]
2023.04.10-07:51:12:160:[step-22600/34000: 66.47%]--[loss-1.408668: wl-2.160894, gl-0.328221]--[lr-0.000034]--[ETA-3:11:10]
2023.04.10-07:52:50:260:[step-22700/34000: 66.76%]--[loss-1.314084: wl-2.046934, gl-0.290617]--[lr-0.000034]--[ETA-3:07:12]
End of epoch 67 / 100: train_loss: 1.345 	 time: 374 sec
2023.04.10-07:55:08:20:[step-22800/34000: 67.06%]--[loss-1.284654: wl-1.975374, gl-0.296967]--[lr-0.000033]--[ETA-3:02:57]
2023.04.10-07:56:48:120:[step-22900/34000: 67.35%]--[loss-1.320957: wl-2.056851, gl-0.292532]--[lr-0.000033]--[ETA-3:08:02]
2023.04.10-07:58:29:220:[step-23000/34000: 67.65%]--[loss-1.374383: wl-2.119084, gl-0.314841]--[lr-0.000033]--[ETA-3:02:15]
2023.04.10-08:00:08:320:[step-23100/34000: 67.94%]--[loss-1.346714: wl-2.052447, gl-0.320490]--[lr-0.000033]--[ETA-2:53:47]
End of epoch 68 / 100: train_loss: 1.344 	 time: 380 sec
2023.04.10-08:02:32:80:[step-23200/34000: 68.24%]--[loss-1.334076: wl-2.073280, gl-0.297436]--[lr-0.000032]--[ETA-2:55:52]
2023.04.10-08:04:11:180:[step-23300/34000: 68.53%]--[loss-1.341185: wl-2.028266, gl-0.327052]--[lr-0.000032]--[ETA-2:57:53]
2023.04.10-08:05:50:280:[step-23400/34000: 68.82%]--[loss-1.319193: wl-2.036127, gl-0.301129]--[lr-0.000032]--[ETA-2:52:30]
End of epoch 69 / 100: train_loss: 1.339 	 time: 379 sec
2023.04.10-08:08:08:40:[step-23500/34000: 69.12%]--[loss-1.409721: wl-2.224339, gl-0.297551]--[lr-0.000031]--[ETA-2:56:37]
2023.04.10-08:09:49:140:[step-23600/34000: 69.41%]--[loss-1.293309: wl-1.989393, gl-0.298612]--[lr-0.000031]--[ETA-2:55:45]
2023.04.10-08:11:29:240:[step-23700/34000: 69.71%]--[loss-1.309406: wl-2.005371, gl-0.306721]--[lr-0.000031]--[ETA-2:47:06]
2023.04.10-08:13:07:340:[step-23800/34000: 70.00%]--[loss-1.152049: wl-1.813126, gl-0.245486]--[lr-0.000031]--[ETA-0:45:23]
End of epoch 70 / 100: train_loss: 1.337 	 time: 380 sec
Saving the model at the end of epoch 70, iters 23800
2023.04.10-08:15:33:100:[step-23900/34000: 70.29%]--[loss-1.451178: wl-2.247114, gl-0.327621]--[lr-0.000030]--[ETA-2:47:24]
2023.04.10-08:17:11:200:[step-24000/34000: 70.59%]--[loss-1.387398: wl-2.135096, gl-0.319850]--[lr-0.000030]--[ETA-2:37:52]
2023.04.10-08:18:50:300:[step-24100/34000: 70.88%]--[loss-1.327696: wl-2.050642, gl-0.302375]--[lr-0.000030]--[ETA-2:42:00]
End of epoch 71 / 100: train_loss: 1.335 	 time: 378 sec
2023.04.10-08:21:08:60:[step-24200/34000: 71.18%]--[loss-1.436492: wl-2.192846, gl-0.340069]--[lr-0.000029]--[ETA-2:49:53]
2023.04.10-08:22:49:160:[step-24300/34000: 71.47%]--[loss-1.396295: wl-2.171507, gl-0.310542]--[lr-0.000029]--[ETA-2:39:07]
2023.04.10-08:24:28:260:[step-24400/34000: 71.76%]--[loss-1.366375: wl-2.151940, gl-0.290406]--[lr-0.000029]--[ETA-2:38:54]
End of epoch 72 / 100: train_loss: 1.334 	 time: 379 sec
2023.04.10-08:26:50:20:[step-24500/34000: 72.06%]--[loss-1.387104: wl-2.151675, gl-0.311266]--[lr-0.000028]--[ETA-2:39:48]
2023.04.10-08:28:29:120:[step-24600/34000: 72.35%]--[loss-1.281876: wl-2.016104, gl-0.273824]--[lr-0.000028]--[ETA-2:43:24]
2023.04.10-08:30:08:220:[step-24700/34000: 72.65%]--[loss-1.348432: wl-2.094160, gl-0.301352]--[lr-0.000028]--[ETA-2:45:10]
2023.04.10-08:31:46:320:[step-24800/34000: 72.94%]--[loss-1.326002: wl-2.049545, gl-0.301229]--[lr-0.000028]--[ETA-2:25:32]
End of epoch 73 / 100: train_loss: 1.333 	 time: 377 sec
2023.04.10-08:34:04:80:[step-24900/34000: 73.24%]--[loss-1.423918: wl-2.194278, gl-0.326779]--[lr-0.000027]--[ETA-2:29:25]
2023.04.10-08:35:45:180:[step-25000/34000: 73.53%]--[loss-1.375969: wl-2.150240, gl-0.300849]--[lr-0.000027]--[ETA-2:34:11]
2023.04.10-08:37:25:280:[step-25100/34000: 73.82%]--[loss-1.344896: wl-2.065728, gl-0.312032]--[lr-0.000027]--[ETA-2:21:51]
End of epoch 74 / 100: train_loss: 1.331 	 time: 379 sec
2023.04.10-08:39:48:40:[step-25200/34000: 74.12%]--[loss-1.347238: wl-2.064509, gl-0.314984]--[lr-0.000026]--[ETA-2:26:44]
2023.04.10-08:41:27:140:[step-25300/34000: 74.41%]--[loss-1.373794: wl-2.161504, gl-0.293042]--[lr-0.000026]--[ETA-2:21:26]
2023.04.10-08:43:05:240:[step-25400/34000: 74.71%]--[loss-1.275932: wl-1.924414, gl-0.313726]--[lr-0.000026]--[ETA-2:24:46]
2023.04.10-08:44:41:340:[step-25500/34000: 75.00%]--[loss-1.567698: wl-2.300110, gl-0.417643]--[lr-0.000026]--[ETA-0:40:43]
End of epoch 75 / 100: train_loss: 1.331 	 time: 376 sec
Saving the model at the end of epoch 75, iters 25500
2023.04.10-08:47:01:100:[step-25600/34000: 75.29%]--[loss-1.392567: wl-2.173230, gl-0.305952]--[lr-0.000025]--[ETA-2:18:17]
2023.04.10-08:48:41:200:[step-25700/34000: 75.59%]--[loss-1.331998: wl-2.061730, gl-0.301133]--[lr-0.000025]--[ETA-2:13:49]
2023.04.10-08:50:21:300:[step-25800/34000: 75.88%]--[loss-1.294438: wl-2.005413, gl-0.291732]--[lr-0.000025]--[ETA-2:16:02]
End of epoch 76 / 100: train_loss: 1.328 	 time: 378 sec
2023.04.10-08:52:41:60:[step-25900/34000: 76.18%]--[loss-1.366013: wl-2.111867, gl-0.310080]--[lr-0.000024]--[ETA-2:13:50]
2023.04.10-08:54:20:160:[step-26000/34000: 76.47%]--[loss-1.267308: wl-1.983733, gl-0.275441]--[lr-0.000024]--[ETA-2:04:51]
2023.04.10-08:55:59:260:[step-26100/34000: 76.76%]--[loss-1.308792: wl-2.024553, gl-0.296515]--[lr-0.000024]--[ETA-2:10:29]
End of epoch 77 / 100: train_loss: 1.328 	 time: 376 sec
2023.04.10-08:58:17:20:[step-26200/34000: 77.06%]--[loss-1.333892: wl-2.051342, gl-0.308221]--[lr-0.000023]--[ETA-2:09:32]
2023.04.10-08:59:57:120:[step-26300/34000: 77.35%]--[loss-1.372560: wl-2.126445, gl-0.309338]--[lr-0.000023]--[ETA-2:05:25]
2023.04.10-09:01:38:220:[step-26400/34000: 77.65%]--[loss-1.275722: wl-2.000620, gl-0.275412]--[lr-0.000023]--[ETA-2:10:45]
2023.04.10-09:03:18:320:[step-26500/34000: 77.94%]--[loss-1.279218: wl-1.988436, gl-0.284999]--[lr-0.000023]--[ETA-1:58:10]
End of epoch 78 / 100: train_loss: 1.324 	 time: 381 sec
2023.04.10-09:05:39:80:[step-26600/34000: 78.24%]--[loss-1.298869: wl-2.029930, gl-0.283904]--[lr-0.000022]--[ETA-2:05:44]
2023.04.10-09:07:19:180:[step-26700/34000: 78.53%]--[loss-1.316994: wl-2.093674, gl-0.270157]--[lr-0.000022]--[ETA-2:01:38]
2023.04.10-09:08:58:280:[step-26800/34000: 78.82%]--[loss-1.216384: wl-1.914113, gl-0.259327]--[lr-0.000022]--[ETA-1:55:24]
End of epoch 79 / 100: train_loss: 1.322 	 time: 378 sec
2023.04.10-09:11:21:40:[step-26900/34000: 79.12%]--[loss-1.280411: wl-2.004786, gl-0.278017]--[lr-0.000021]--[ETA-1:58:47]
2023.04.10-09:13:02:140:[step-27000/34000: 79.41%]--[loss-1.391485: wl-2.182609, gl-0.300180]--[lr-0.000021]--[ETA-1:54:36]
2023.04.10-09:14:42:240:[step-27100/34000: 79.71%]--[loss-1.358719: wl-2.103916, gl-0.306761]--[lr-0.000021]--[ETA-1:58:07]
2023.04.10-09:16:20:340:[step-27200/34000: 80.00%]--[loss-1.897056: wl-2.070994, gl-0.861559]--[lr-0.000021]--[ETA-0:36:38]
End of epoch 80 / 100: train_loss: 1.323 	 time: 386 sec
Saving the model at the end of epoch 80, iters 27200
2023.04.10-09:18:43:100:[step-27300/34000: 80.29%]--[loss-1.318451: wl-2.068870, gl-0.284016]--[lr-0.000020]--[ETA-1:54:30]
2023.04.10-09:20:25:200:[step-27400/34000: 80.59%]--[loss-1.410099: wl-2.231375, gl-0.294411]--[lr-0.000020]--[ETA-1:50:19]
2023.04.10-09:22:04:300:[step-27500/34000: 80.88%]--[loss-1.342198: wl-2.090790, gl-0.296803]--[lr-0.000020]--[ETA-1:46:05]
End of epoch 81 / 100: train_loss: 1.320 	 time: 378 sec
2023.04.10-09:24:25:60:[step-27600/34000: 81.18%]--[loss-1.245133: wl-1.949903, gl-0.270181]--[lr-0.000019]--[ETA-1:50:50]
2023.04.10-09:26:06:160:[step-27700/34000: 81.47%]--[loss-1.336893: wl-2.090426, gl-0.291680]--[lr-0.000019]--[ETA-1:43:53]
2023.04.10-09:27:47:260:[step-27800/34000: 81.76%]--[loss-1.228652: wl-1.925534, gl-0.265885]--[lr-0.000019]--[ETA-1:55:35]
End of epoch 82 / 100: train_loss: 1.318 	 time: 385 sec
2023.04.10-09:30:08:20:[step-27900/34000: 82.06%]--[loss-1.294958: wl-2.014577, gl-0.287669]--[lr-0.000018]--[ETA-1:42:00]
2023.04.10-09:31:48:120:[step-28000/34000: 82.35%]--[loss-1.366632: wl-2.139140, gl-0.297062]--[lr-0.000018]--[ETA-1:52:14]
2023.04.10-09:33:28:220:[step-28100/34000: 82.65%]--[loss-1.350263: wl-2.075508, gl-0.312509]--[lr-0.000018]--[ETA-1:38:55]
2023.04.10-09:35:06:320:[step-28200/34000: 82.94%]--[loss-1.274791: wl-1.991046, gl-0.279268]--[lr-0.000018]--[ETA-1:29:56]
End of epoch 83 / 100: train_loss: 1.319 	 time: 378 sec
2023.04.10-09:37:28:80:[step-28300/34000: 83.24%]--[loss-1.320330: wl-2.093430, gl-0.273615]--[lr-0.000017]--[ETA-1:33:42]
2023.04.10-09:39:08:180:[step-28400/34000: 83.53%]--[loss-1.279984: wl-1.983391, gl-0.288289]--[lr-0.000017]--[ETA-1:35:07]
2023.04.10-09:40:47:280:[step-28500/34000: 83.82%]--[loss-1.389785: wl-2.127789, gl-0.325891]--[lr-0.000017]--[ETA-1:30:39]
End of epoch 84 / 100: train_loss: 1.314 	 time: 382 sec
2023.04.10-09:43:11:40:[step-28600/34000: 84.12%]--[loss-1.365956: wl-2.172315, gl-0.279798]--[lr-0.000016]--[ETA-1:33:02]
2023.04.10-09:44:50:140:[step-28700/34000: 84.41%]--[loss-1.276233: wl-1.979373, gl-0.286547]--[lr-0.000016]--[ETA-1:25:36]
2023.04.10-09:46:29:240:[step-28800/34000: 84.71%]--[loss-1.253964: wl-2.003211, gl-0.252359]--[lr-0.000016]--[ETA-1:25:27]
2023.04.10-09:48:05:340:[step-28900/34000: 85.00%]--[loss-1.224144: wl-1.847766, gl-0.300261]--[lr-0.000016]--[ETA-0:26:29]
End of epoch 85 / 100: train_loss: 1.313 	 time: 378 sec
Saving the model at the end of epoch 85, iters 28900
2023.04.10-09:50:27:100:[step-29000/34000: 85.29%]--[loss-1.485436: wl-2.348445, gl-0.311213]--[lr-0.000015]--[ETA-1:21:28]
2023.04.10-09:52:09:200:[step-29100/34000: 85.59%]--[loss-1.307354: wl-2.049888, gl-0.282410]--[lr-0.000015]--[ETA-1:26:30]
2023.04.10-09:53:50:300:[step-29200/34000: 85.88%]--[loss-1.351278: wl-2.084904, gl-0.308826]--[lr-0.000015]--[ETA-1:19:28]
End of epoch 86 / 100: train_loss: 1.311 	 time: 382 sec
2023.04.10-09:56:10:60:[step-29300/34000: 86.18%]--[loss-1.332202: wl-2.057309, gl-0.303548]--[lr-0.000014]--[ETA-1:19:34]
2023.04.10-09:57:50:160:[step-29400/34000: 86.47%]--[loss-1.289361: wl-2.044200, gl-0.267261]--[lr-0.000014]--[ETA-1:14:59]
2023.04.10-09:59:29:260:[step-29500/34000: 86.76%]--[loss-1.276185: wl-1.987815, gl-0.282277]--[lr-0.000014]--[ETA-1:14:26]
End of epoch 87 / 100: train_loss: 1.310 	 time: 377 sec
2023.04.10-10:01:48:20:[step-29600/34000: 87.06%]--[loss-1.272248: wl-2.001298, gl-0.271599]--[lr-0.000013]--[ETA-1:12:32]
2023.04.10-10:03:28:120:[step-29700/34000: 87.35%]--[loss-1.237712: wl-1.956634, gl-0.259395]--[lr-0.000013]--[ETA-1:12:53]
2023.04.10-10:05:10:220:[step-29800/34000: 87.65%]--[loss-1.318816: wl-2.104720, gl-0.266456]--[lr-0.000013]--[ETA-1:11:56]
2023.04.10-10:06:50:320:[step-29900/34000: 87.94%]--[loss-1.276227: wl-2.022315, gl-0.265069]--[lr-0.000013]--[ETA-1:11:24]
End of epoch 88 / 100: train_loss: 1.309 	 time: 383 sec
2023.04.10-10:09:14:80:[step-30000/34000: 88.24%]--[loss-1.241658: wl-1.972157, gl-0.255580]--[lr-0.000012]--[ETA-1:02:55]
2023.04.10-10:10:53:180:[step-30100/34000: 88.53%]--[loss-1.325174: wl-2.097940, gl-0.276204]--[lr-0.000012]--[ETA-1:04:17]
2023.04.10-10:12:31:280:[step-30200/34000: 88.82%]--[loss-1.253180: wl-2.010752, gl-0.247804]--[lr-0.000012]--[ETA-1:00:53]
End of epoch 89 / 100: train_loss: 1.307 	 time: 378 sec
2023.04.10-10:14:49:40:[step-30300/34000: 89.12%]--[loss-1.212026: wl-1.927242, gl-0.248405]--[lr-0.000011]--[ETA-1:02:35]
2023.04.10-10:16:30:140:[step-30400/34000: 89.41%]--[loss-1.299659: wl-2.031835, gl-0.283742]--[lr-0.000011]--[ETA-0:59:48]
2023.04.10-10:18:10:240:[step-30500/34000: 89.71%]--[loss-1.325646: wl-2.063946, gl-0.293673]--[lr-0.000011]--[ETA-1:00:16]
2023.04.10-10:19:48:340:[step-30600/34000: 90.00%]--[loss-1.193664: wl-1.976869, gl-0.205229]--[lr-0.000011]--[ETA-0:18:37]
End of epoch 90 / 100: train_loss: 1.306 	 time: 380 sec
Saving the model at the end of epoch 90, iters 30600
2023.04.10-10:22:15:100:[step-30700/34000: 90.29%]--[loss-1.343667: wl-2.074851, gl-0.306241]--[lr-0.000010]--[ETA-0:56:11]
2023.04.10-10:23:55:200:[step-30800/34000: 90.59%]--[loss-1.299184: wl-2.006287, gl-0.296041]--[lr-0.000010]--[ETA-0:53:08]
2023.04.10-10:25:34:300:[step-30900/34000: 90.88%]--[loss-1.293373: wl-2.020491, gl-0.283128]--[lr-0.000010]--[ETA-0:50:44]
End of epoch 91 / 100: train_loss: 1.305 	 time: 382 sec
2023.04.10-10:27:53:60:[step-31000/34000: 91.18%]--[loss-1.338542: wl-2.120704, gl-0.278190]--[lr-0.000009]--[ETA-0:50:26]
2023.04.10-10:29:34:160:[step-31100/34000: 91.47%]--[loss-1.283241: wl-2.053604, gl-0.256439]--[lr-0.000009]--[ETA-0:51:20]
2023.04.10-10:31:14:260:[step-31200/34000: 91.76%]--[loss-1.293161: wl-1.975714, gl-0.305304]--[lr-0.000009]--[ETA-0:46:45]
End of epoch 92 / 100: train_loss: 1.305 	 time: 381 sec
2023.04.10-10:33:39:20:[step-31300/34000: 92.06%]--[loss-1.268187: wl-1.999613, gl-0.268381]--[lr-0.000008]--[ETA-0:45:28]
2023.04.10-10:35:19:120:[step-31400/34000: 92.35%]--[loss-1.300141: wl-2.039101, gl-0.280591]--[lr-0.000008]--[ETA-0:43:06]
2023.04.10-10:36:58:220:[step-31500/34000: 92.65%]--[loss-1.317843: wl-2.048223, gl-0.293732]--[lr-0.000008]--[ETA-0:42:04]
2023.04.10-10:38:35:320:[step-31600/34000: 92.94%]--[loss-1.311021: wl-2.070146, gl-0.275949]--[lr-0.000008]--[ETA-0:37:42]
End of epoch 93 / 100: train_loss: 1.302 	 time: 380 sec
2023.04.10-10:40:55:80:[step-31700/34000: 93.24%]--[loss-1.354682: wl-2.176967, gl-0.266198]--[lr-0.000007]--[ETA-0:37:54]
2023.04.10-10:42:36:180:[step-31800/34000: 93.53%]--[loss-1.328868: wl-2.098168, gl-0.279784]--[lr-0.000007]--[ETA-0:35:48]
2023.04.10-10:44:16:280:[step-31900/34000: 93.82%]--[loss-1.264689: wl-1.998015, gl-0.265682]--[lr-0.000007]--[ETA-0:34:43]
End of epoch 94 / 100: train_loss: 1.302 	 time: 381 sec
2023.04.10-10:46:40:40:[step-32000/34000: 94.12%]--[loss-1.369385: wl-2.161065, gl-0.288853]--[lr-0.000006]--[ETA-0:32:48]
2023.04.10-10:48:20:140:[step-32100/34000: 94.41%]--[loss-1.321853: wl-2.092509, gl-0.275598]--[lr-0.000006]--[ETA-0:31:54]
2023.04.10-10:49:59:240:[step-32200/34000: 94.71%]--[loss-1.359345: wl-2.128143, gl-0.295274]--[lr-0.000006]--[ETA-0:30:07]
2023.04.10-10:51:36:340:[step-32300/34000: 95.00%]--[loss-1.788885: wl-2.300531, gl-0.638620]--[lr-0.000006]--[ETA-0:09:50]
End of epoch 95 / 100: train_loss: 1.302 	 time: 380 sec
Saving the model at the end of epoch 95, iters 32300
2023.04.10-10:53:58:100:[step-32400/34000: 95.29%]--[loss-1.272768: wl-2.014391, gl-0.265572]--[lr-0.000005]--[ETA-0:26:05]
2023.04.10-10:55:39:200:[step-32500/34000: 95.59%]--[loss-1.317372: wl-2.131206, gl-0.251769]--[lr-0.000005]--[ETA-0:26:38]
2023.04.10-10:57:20:300:[step-32600/34000: 95.88%]--[loss-1.285866: wl-2.016711, gl-0.277511]--[lr-0.000005]--[ETA-0:23:56]
End of epoch 96 / 100: train_loss: 1.300 	 time: 383 sec
2023.04.10-10:59:41:60:[step-32700/34000: 96.18%]--[loss-1.292942: wl-2.082323, gl-0.251780]--[lr-0.000004]--[ETA-0:20:17]
2023.04.10-11:01:20:160:[step-32800/34000: 96.47%]--[loss-1.293038: wl-2.064594, gl-0.260741]--[lr-0.000004]--[ETA-0:19:26]
2023.04.10-11:02:59:260:[step-32900/34000: 96.76%]--[loss-1.282431: wl-1.991320, gl-0.286771]--[lr-0.000004]--[ETA-0:17:40]
End of epoch 97 / 100: train_loss: 1.299 	 time: 376 sec
2023.04.10-11:05:19:20:[step-33000/34000: 97.06%]--[loss-1.350883: wl-2.178812, gl-0.261478]--[lr-0.000003]--[ETA-0:16:21]
2023.04.10-11:06:59:120:[step-33100/34000: 97.35%]--[loss-1.308696: wl-2.026938, gl-0.295227]--[lr-0.000003]--[ETA-0:14:24]
2023.04.10-11:08:40:220:[step-33200/34000: 97.65%]--[loss-1.261746: wl-2.028142, gl-0.247675]--[lr-0.000003]--[ETA-0:13:07]
2023.04.10-11:10:20:320:[step-33300/34000: 97.94%]--[loss-1.311542: wl-2.049612, gl-0.286736]--[lr-0.000003]--[ETA-0:11:20]
End of epoch 98 / 100: train_loss: 1.298 	 time: 383 sec
2023.04.10-11:12:41:80:[step-33400/34000: 98.24%]--[loss-1.368553: wl-2.148285, gl-0.294411]--[lr-0.000002]--[ETA-0:09:52]
2023.04.10-11:14:20:180:[step-33500/34000: 98.53%]--[loss-1.267828: wl-2.014703, gl-0.260477]--[lr-0.000002]--[ETA-0:07:52]
2023.04.10-11:16:01:280:[step-33600/34000: 98.82%]--[loss-1.277311: wl-2.041685, gl-0.256468]--[lr-0.000002]--[ETA-0:06:44]
End of epoch 99 / 100: train_loss: 1.297 	 time: 381 sec
2023.04.10-11:18:29:40:[step-33700/34000: 99.12%]--[loss-1.236151: wl-1.982194, gl-0.245053]--[lr-0.000001]--[ETA-0:04:52]
2023.04.10-11:20:14:140:[step-33800/34000: 99.41%]--[loss-1.346911: wl-2.104233, gl-0.294795]--[lr-0.000001]--[ETA-0:03:40]
2023.04.10-11:21:59:240:[step-33900/34000: 99.71%]--[loss-1.260888: wl-1.989139, gl-0.266319]--[lr-0.000001]--[ETA-0:01:55]
2023.04.10-11:23:42:340:[step-34000/34000: 100.00%]--[loss-1.385385: wl-2.119745, gl-0.325512]--[lr-0.000001]--[ETA-0:01:56]
End of epoch 100 / 100: train_loss: 1.297 	 time: 402 sec
Saving the model at the end of epoch 100, iters 34000
