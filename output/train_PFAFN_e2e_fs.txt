------------ Options -------------
PBAFN_gen_checkpoint: checkpoints_fs/PBAFN_e2e_fs/PBAFN_gen_epoch_201.pth
PBAFN_warp_checkpoint: checkpoints_fs/PBAFN_e2e_fs/PBAFN_warp_epoch_201.pth
PFAFN_gen_checkpoint: None
PFAFN_warp_checkpoint: checkpoints_fs/PFAFN_stage1_fs/PFAFN_warp_epoch_201.pth
batchSize: 16
beta1: 0.5
checkpoints_dir: checkpoints_fs
continue_train: False
data_type: 32
dataroot: ../dataset/Flow-Style-VTON/VITON_traindata
debug: False
display_freq: 100
display_winsize: 512
fineSize: 512
gpu_ids: [0]
input_nc: 3
isTrain: True
label_nc: 14
lambda_feat: 10.0
launcher: pytorch
loadSize: 512
load_pretrain: 
local_rank: 0
lr: 5e-05
max_dataset_size: inf
nThreads: 1
n_blocks_global: 4
n_blocks_local: 3
n_downsample_global: 4
n_layers_D: 3
n_local_enhancers: 1
name: PFAFN_e2e
ndf: 64
netG: global
ngf: 64
niter: 50
niter_decay: 150
niter_fix_global: 0
no_flip: False
no_ganFeat_loss: False
no_html: False
no_lsgan: False
no_vgg_loss: False
norm: instance
num_D: 2
num_gpus: 1
output_nc: 3
phase: train
pool_size: 0
print_freq: 100
resize_or_crop: None
save_epoch_freq: 5
save_latest_freq: 1000
serial_batches: False
tf_log: False
tv_weight: 0.1
use_dropout: False
verbose: False
which_epoch: latest
-------------- End ----------------
------------ Options -------------
PBAFN_gen_checkpoint: checkpoints_fs/PBAFN_e2e_fs/PBAFN_gen_epoch_201.pth
PBAFN_warp_checkpoint: checkpoints_fs/PBAFN_e2e_fs/PBAFN_warp_epoch_201.pth
PFAFN_gen_checkpoint: None
PFAFN_warp_checkpoint: checkpoints_fs/PFAFN_stage1_fs/PFAFN_warp_epoch_201.pth
batchSize: 16
beta1: 0.5
checkpoints_dir: checkpoints_fs
continue_train: False
data_type: 32
dataroot: ../dataset/Flow-Style-VTON/VITON_traindata
debug: False
display_freq: 100
display_winsize: 512
fineSize: 512
gpu_ids: [0]
input_nc: 3
isTrain: True
label_nc: 14
lambda_feat: 10.0
launcher: pytorch
loadSize: 512
load_pretrain: 
local_rank: 0
lr: 5e-05
max_dataset_size: inf
nThreads: 1
n_blocks_global: 4
n_blocks_local: 3
n_downsample_global: 4
n_layers_D: 3
n_local_enhancers: 1
name: PFAFN_e2e
ndf: 64
netG: global
ngf: 64
niter: 50
niter_decay: 150
niter_fix_global: 0
no_flip: False
no_ganFeat_loss: False
no_html: False
no_lsgan: False
no_vgg_loss: False
norm: instance
num_D: 2
num_gpus: 1
output_nc: 3
phase: train
pool_size: 0
print_freq: 100
resize_or_crop: None
save_epoch_freq: 5
save_latest_freq: 1000
serial_batches: False
tf_log: False
tv_weight: 0.1
use_dropout: False
verbose: False
which_epoch: latest
-------------- End ----------------
------------ Options -------------
PBAFN_gen_checkpoint: checkpoints_fs/PBAFN_e2e_fs/PBAFN_gen_epoch_201.pth
PBAFN_warp_checkpoint: checkpoints_fs/PBAFN_e2e_fs/PBAFN_warp_epoch_201.pth
PFAFN_gen_checkpoint: None
PFAFN_warp_checkpoint: checkpoints_fs/PFAFN_stage1_fs/PFAFN_warp_epoch_201.pth
batchSize: 16
beta1: 0.5
checkpoints_dir: checkpoints_fs
continue_train: False
data_type: 32
dataroot: ../dataset/Flow-Style-VTON/VITON_traindata
debug: False
display_freq: 100
display_winsize: 512
fineSize: 512
gpu_ids: [0]
input_nc: 3
isTrain: True
label_nc: 14
lambda_feat: 10.0
launcher: pytorch
loadSize: 512
load_pretrain: 
local_rank: 0
lr: 5e-05
max_dataset_size: inf
nThreads: 1
n_blocks_global: 4
n_blocks_local: 3
n_downsample_global: 4
n_layers_D: 3
n_local_enhancers: 1
name: PFAFN_e2e
ndf: 64
netG: global
ngf: 64
niter: 50
niter_decay: 150
niter_fix_global: 0
no_flip: False
no_ganFeat_loss: False
no_html: False
no_lsgan: False
no_vgg_loss: False
norm: instance
num_D: 2
num_gpus: 1
output_nc: 3
phase: train
pool_size: 0
print_freq: 100
resize_or_crop: None
save_epoch_freq: 5
save_latest_freq: 1000
serial_batches: False
tf_log: False
tv_weight: 0.1
use_dropout: False
verbose: False
which_epoch: latest
-------------- End ----------------
dataset [AlignedDataset] was created
../dataset/Flow-Style-VTON/VITON_traindata/train_label label
../dataset/Flow-Style-VTON/VITON_traindata/train_img img
../dataset/Flow-Style-VTON/VITON_traindata/train_edge edge
../dataset/Flow-Style-VTON/VITON_traindata/train_color color
AFWM(
  (image_features): FeatureEncoder(
    (encoders): ModuleList(
      (0): Sequential(
        (0): DownSample(
          (block): Sequential(
            (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          )
        )
        (1): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (2): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
      )
      (1): Sequential(
        (0): DownSample(
          (block): Sequential(
            (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          )
        )
        (1): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (2): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
      )
      (2): Sequential(
        (0): DownSample(
          (block): Sequential(
            (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          )
        )
        (1): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (2): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
      )
      (3): Sequential(
        (0): DownSample(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          )
        )
        (1): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (2): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
      )
      (4): Sequential(
        (0): DownSample(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          )
        )
        (1): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (2): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
      )
    )
  )
  (cond_features): FeatureEncoder(
    (encoders): ModuleList(
      (0): Sequential(
        (0): DownSample(
          (block): Sequential(
            (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          )
        )
        (1): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (2): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
      )
      (1): Sequential(
        (0): DownSample(
          (block): Sequential(
            (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          )
        )
        (1): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (2): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
      )
      (2): Sequential(
        (0): DownSample(
          (block): Sequential(
            (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          )
        )
        (1): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (2): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
      )
      (3): Sequential(
        (0): DownSample(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          )
        )
        (1): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (2): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
      )
      (4): Sequential(
        (0): DownSample(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          )
        )
        (1): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (2): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
      )
    )
  )
  (image_FPN): RefinePyramid(
    (adaptive): ModuleList(
      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
      (4): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    )
    (smooth): ModuleList(
      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (cond_FPN): RefinePyramid(
    (adaptive): ModuleList(
      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
      (4): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    )
    (smooth): ModuleList(
      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (aflow_net): AFlowNet(
    (netRefine): ModuleList(
      (0): Sequential(
        (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.1)
        (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): LeakyReLU(negative_slope=0.1)
        (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): LeakyReLU(negative_slope=0.1)
        (6): Conv2d(32, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (1): Sequential(
        (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.1)
        (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): LeakyReLU(negative_slope=0.1)
        (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): LeakyReLU(negative_slope=0.1)
        (6): Conv2d(32, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (2): Sequential(
        (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.1)
        (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): LeakyReLU(negative_slope=0.1)
        (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): LeakyReLU(negative_slope=0.1)
        (6): Conv2d(32, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (3): Sequential(
        (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.1)
        (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): LeakyReLU(negative_slope=0.1)
        (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): LeakyReLU(negative_slope=0.1)
        (6): Conv2d(32, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (4): Sequential(
        (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.1)
        (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): LeakyReLU(negative_slope=0.1)
        (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): LeakyReLU(negative_slope=0.1)
        (6): Conv2d(32, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
    (netStyle): ModuleList(
      (0): StyledConvBlock(
        (conv0): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=256, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn0): LeakyReLU(negative_slope=0.2, inplace=True)
        (conv1): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=49, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn1): LeakyReLU(negative_slope=0.2, inplace=True)
      )
      (1): StyledConvBlock(
        (conv0): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=256, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn0): LeakyReLU(negative_slope=0.2, inplace=True)
        (conv1): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=49, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn1): LeakyReLU(negative_slope=0.2, inplace=True)
      )
      (2): StyledConvBlock(
        (conv0): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=256, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn0): LeakyReLU(negative_slope=0.2, inplace=True)
        (conv1): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=49, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn1): LeakyReLU(negative_slope=0.2, inplace=True)
      )
      (3): StyledConvBlock(
        (conv0): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=256, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn0): LeakyReLU(negative_slope=0.2, inplace=True)
        (conv1): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=49, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn1): LeakyReLU(negative_slope=0.2, inplace=True)
      )
      (4): StyledConvBlock(
        (conv0): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=256, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn0): LeakyReLU(negative_slope=0.2, inplace=True)
        (conv1): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=49, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn1): LeakyReLU(negative_slope=0.2, inplace=True)
      )
    )
    (netF): ModuleList(
      (0): Styled_F_ConvBlock(
        (conv0): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=49, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn0): LeakyReLU(negative_slope=0.2, inplace=True)
        (conv1): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=128, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
      )
      (1): Styled_F_ConvBlock(
        (conv0): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=49, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn0): LeakyReLU(negative_slope=0.2, inplace=True)
        (conv1): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=128, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
      )
      (2): Styled_F_ConvBlock(
        (conv0): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=49, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn0): LeakyReLU(negative_slope=0.2, inplace=True)
        (conv1): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=128, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
      )
      (3): Styled_F_ConvBlock(
        (conv0): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=49, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn0): LeakyReLU(negative_slope=0.2, inplace=True)
        (conv1): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=128, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
      )
      (4): Styled_F_ConvBlock(
        (conv0): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=49, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn0): LeakyReLU(negative_slope=0.2, inplace=True)
        (conv1): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=128, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
      )
    )
    (cond_style): Sequential(
      (0): Conv2d(256, 128, kernel_size=(8, 6), stride=(1, 1))
      (1): LeakyReLU(negative_slope=0.1)
    )
    (image_style): Sequential(
      (0): Conv2d(256, 128, kernel_size=(8, 6), stride=(1, 1))
      (1): LeakyReLU(negative_slope=0.1)
    )
  )
)
RMGNGenerator(
  (inp_encoder): AttrEncoder(
    (conv1): Sequential(
      (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
      (1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (2): LeakyReLU(negative_slope=0.1, inplace=True)
    )
    (conv2): Sequential(
      (0): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
      (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (2): LeakyReLU(negative_slope=0.1, inplace=True)
    )
    (conv3): Sequential(
      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
      (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (2): LeakyReLU(negative_slope=0.1, inplace=True)
    )
    (conv4): Sequential(
      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
      (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (2): LeakyReLU(negative_slope=0.1, inplace=True)
    )
    (conv5): Sequential(
      (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
      (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (2): LeakyReLU(negative_slope=0.1, inplace=True)
    )
    (deconv1): deconv4x4(
      (deconv): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (lrelu): LeakyReLU(negative_slope=0.1, inplace=True)
    )
    (deconv2): deconv4x4(
      (deconv): ConvTranspose2d(512, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (lrelu): LeakyReLU(negative_slope=0.1, inplace=True)
    )
    (deconv3): deconv4x4(
      (deconv): ConvTranspose2d(256, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (lrelu): LeakyReLU(negative_slope=0.1, inplace=True)
    )
    (deconv4): deconv4x4(
      (deconv): ConvTranspose2d(128, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (lrelu): LeakyReLU(negative_slope=0.1, inplace=True)
    )
  )
  (ref_encoder): AttrEncoder(
    (conv1): Sequential(
      (0): Conv2d(4, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
      (1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (2): LeakyReLU(negative_slope=0.1, inplace=True)
    )
    (conv2): Sequential(
      (0): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
      (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (2): LeakyReLU(negative_slope=0.1, inplace=True)
    )
    (conv3): Sequential(
      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
      (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (2): LeakyReLU(negative_slope=0.1, inplace=True)
    )
    (conv4): Sequential(
      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
      (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (2): LeakyReLU(negative_slope=0.1, inplace=True)
    )
    (conv5): Sequential(
      (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
      (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (2): LeakyReLU(negative_slope=0.1, inplace=True)
    )
    (deconv1): deconv4x4(
      (deconv): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (lrelu): LeakyReLU(negative_slope=0.1, inplace=True)
    )
    (deconv2): deconv4x4(
      (deconv): ConvTranspose2d(512, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (lrelu): LeakyReLU(negative_slope=0.1, inplace=True)
    )
    (deconv3): deconv4x4(
      (deconv): ConvTranspose2d(256, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (lrelu): LeakyReLU(negative_slope=0.1, inplace=True)
    )
    (deconv4): deconv4x4(
      (deconv): ConvTranspose2d(128, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (lrelu): LeakyReLU(negative_slope=0.1, inplace=True)
    )
  )
  (generator): AADGenerator(
    (conv_head): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (AADResBlk_mid_0): AADResnetBlock(
      (AAD_1): AAD(
        (norm): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv_h): Conv2d(1024, 1, kernel_size=(1, 1), stride=(1, 1))
        (conv_a1_gamma): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
        (conv_a1_beta): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
        (conv_a2_gamma): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
        (conv_a2_beta): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
      )
      (relu_1): ReLU(inplace=True)
      (conv_1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (AAD_2): AAD(
        (norm): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv_h): Conv2d(1024, 1, kernel_size=(1, 1), stride=(1, 1))
        (conv_a1_gamma): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
        (conv_a1_beta): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
        (conv_a2_gamma): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
        (conv_a2_beta): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
      )
      (relu_2): ReLU(inplace=True)
      (conv_2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (AADResBlk_mid_1): AADResnetBlock(
      (AAD_1): AAD(
        (norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv_h): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))
        (conv_a1_gamma): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
        (conv_a1_beta): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
        (conv_a2_gamma): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
        (conv_a2_beta): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (relu_1): ReLU(inplace=True)
      (conv_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (AAD_2): AAD(
        (norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv_h): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))
        (conv_a1_gamma): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
        (conv_a1_beta): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
        (conv_a2_gamma): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
        (conv_a2_beta): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (relu_2): ReLU(inplace=True)
      (conv_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (AADResBlk_mid_2): AADResnetBlock(
      (AAD_1): AAD(
        (norm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv_h): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))
        (conv_a1_gamma): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
        (conv_a1_beta): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
        (conv_a2_gamma): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
        (conv_a2_beta): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (relu_1): ReLU(inplace=True)
      (conv_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (AAD_2): AAD(
        (norm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv_h): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))
        (conv_a1_gamma): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
        (conv_a1_beta): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
        (conv_a2_gamma): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
        (conv_a2_beta): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (relu_2): ReLU(inplace=True)
      (conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (AADResBlk_up_0): AADResnetBlock(
      (AAD_1): AAD(
        (norm): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv_h): Conv2d(1024, 1, kernel_size=(1, 1), stride=(1, 1))
        (conv_a1_gamma): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
        (conv_a1_beta): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
        (conv_a2_gamma): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
        (conv_a2_beta): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
      )
      (relu_1): ReLU(inplace=True)
      (conv_1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (AAD_2): AAD(
        (norm): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv_h): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))
        (conv_a1_gamma): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
        (conv_a1_beta): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
        (conv_a2_gamma): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
        (conv_a2_beta): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (relu_2): ReLU(inplace=True)
      (conv_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (AAD_s): AAD(
        (norm): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv_h): Conv2d(1024, 1, kernel_size=(1, 1), stride=(1, 1))
        (conv_a1_gamma): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
        (conv_a1_beta): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
        (conv_a2_gamma): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
        (conv_a2_beta): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
      )
      (relu_s): ReLU(inplace=True)
      (conv_s): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (AADResBlk_up_1): AADResnetBlock(
      (AAD_1): AAD(
        (norm): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv_h): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))
        (conv_a1_gamma): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
        (conv_a1_beta): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
        (conv_a2_gamma): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
        (conv_a2_beta): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (relu_1): ReLU(inplace=True)
      (conv_1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (AAD_2): AAD(
        (norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv_h): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))
        (conv_a1_gamma): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (conv_a1_beta): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (conv_a2_gamma): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (conv_a2_beta): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (relu_2): ReLU(inplace=True)
      (conv_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (AAD_s): AAD(
        (norm): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv_h): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))
        (conv_a1_gamma): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
        (conv_a1_beta): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
        (conv_a2_gamma): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
        (conv_a2_beta): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
      )
      (relu_s): ReLU(inplace=True)
      (conv_s): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (AADResBlk_up_2): AADResnetBlock(
      (AAD_1): AAD(
        (norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv_h): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))
        (conv_a1_gamma): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
        (conv_a1_beta): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
        (conv_a2_gamma): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
        (conv_a2_beta): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (relu_1): ReLU(inplace=True)
      (conv_1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (AAD_2): AAD(
        (norm): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv_h): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))
        (conv_a1_gamma): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
        (conv_a1_beta): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
        (conv_a2_gamma): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
        (conv_a2_beta): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
      )
      (relu_2): ReLU(inplace=True)
      (conv_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (AAD_s): AAD(
        (norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv_h): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))
        (conv_a1_gamma): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
        (conv_a1_beta): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
        (conv_a2_gamma): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
        (conv_a2_beta): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
      )
      (relu_s): ReLU(inplace=True)
      (conv_s): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (AADResBlk_up_3): AADResnetBlock(
      (AAD_1): AAD(
        (norm): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv_h): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))
        (conv_a1_gamma): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
        (conv_a1_beta): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
        (conv_a2_gamma): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
        (conv_a2_beta): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
      )
      (relu_1): ReLU(inplace=True)
      (conv_1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (AAD_2): AAD(
        (norm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv_h): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))
        (conv_a1_gamma): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
        (conv_a1_beta): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
        (conv_a2_gamma): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
        (conv_a2_beta): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (relu_2): ReLU(inplace=True)
      (conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (AAD_s): AAD(
        (norm): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv_h): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))
        (conv_a1_gamma): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
        (conv_a1_beta): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
        (conv_a2_gamma): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
        (conv_a2_beta): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
      )
      (relu_s): ReLU(inplace=True)
      (conv_s): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (conv_final): Conv2d(64, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (final): Identity()
  )
)
AFWM(
  (image_features): FeatureEncoder(
    (encoders): ModuleList(
      (0): Sequential(
        (0): DownSample(
          (block): Sequential(
            (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          )
        )
        (1): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (2): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
      )
      (1): Sequential(
        (0): DownSample(
          (block): Sequential(
            (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          )
        )
        (1): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (2): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
      )
      (2): Sequential(
        (0): DownSample(
          (block): Sequential(
            (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          )
        )
        (1): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (2): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
      )
      (3): Sequential(
        (0): DownSample(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          )
        )
        (1): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (2): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
      )
      (4): Sequential(
        (0): DownSample(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          )
        )
        (1): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (2): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
      )
    )
  )
  (cond_features): FeatureEncoder(
    (encoders): ModuleList(
      (0): Sequential(
        (0): DownSample(
          (block): Sequential(
            (0): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(45, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          )
        )
        (1): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (2): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
      )
      (1): Sequential(
        (0): DownSample(
          (block): Sequential(
            (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          )
        )
        (1): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (2): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
      )
      (2): Sequential(
        (0): DownSample(
          (block): Sequential(
            (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          )
        )
        (1): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (2): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
      )
      (3): Sequential(
        (0): DownSample(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          )
        )
        (1): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (2): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
      )
      (4): Sequential(
        (0): DownSample(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          )
        )
        (1): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (2): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
      )
    )
  )
  (image_FPN): RefinePyramid(
    (adaptive): ModuleList(
      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
      (4): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    )
    (smooth): ModuleList(
      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (cond_FPN): RefinePyramid(
    (adaptive): ModuleList(
      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
      (4): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    )
    (smooth): ModuleList(
      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (aflow_net): AFlowNet(
    (netRefine): ModuleList(
      (0): Sequential(
        (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.1)
        (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): LeakyReLU(negative_slope=0.1)
        (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): LeakyReLU(negative_slope=0.1)
        (6): Conv2d(32, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (1): Sequential(
        (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.1)
        (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): LeakyReLU(negative_slope=0.1)
        (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): LeakyReLU(negative_slope=0.1)
        (6): Conv2d(32, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (2): Sequential(
        (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.1)
        (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): LeakyReLU(negative_slope=0.1)
        (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): LeakyReLU(negative_slope=0.1)
        (6): Conv2d(32, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (3): Sequential(
        (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.1)
        (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): LeakyReLU(negative_slope=0.1)
        (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): LeakyReLU(negative_slope=0.1)
        (6): Conv2d(32, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (4): Sequential(
        (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.1)
        (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): LeakyReLU(negative_slope=0.1)
        (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): LeakyReLU(negative_slope=0.1)
        (6): Conv2d(32, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
    (netStyle): ModuleList(
      (0): StyledConvBlock(
        (conv0): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=256, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn0): LeakyReLU(negative_slope=0.2, inplace=True)
        (conv1): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=49, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn1): LeakyReLU(negative_slope=0.2, inplace=True)
      )
      (1): StyledConvBlock(
        (conv0): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=256, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn0): LeakyReLU(negative_slope=0.2, inplace=True)
        (conv1): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=49, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn1): LeakyReLU(negative_slope=0.2, inplace=True)
      )
      (2): StyledConvBlock(
        (conv0): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=256, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn0): LeakyReLU(negative_slope=0.2, inplace=True)
        (conv1): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=49, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn1): LeakyReLU(negative_slope=0.2, inplace=True)
      )
      (3): StyledConvBlock(
        (conv0): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=256, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn0): LeakyReLU(negative_slope=0.2, inplace=True)
        (conv1): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=49, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn1): LeakyReLU(negative_slope=0.2, inplace=True)
      )
      (4): StyledConvBlock(
        (conv0): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=256, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn0): LeakyReLU(negative_slope=0.2, inplace=True)
        (conv1): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=49, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn1): LeakyReLU(negative_slope=0.2, inplace=True)
      )
    )
    (netF): ModuleList(
      (0): Styled_F_ConvBlock(
        (conv0): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=49, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn0): LeakyReLU(negative_slope=0.2, inplace=True)
        (conv1): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=128, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
      )
      (1): Styled_F_ConvBlock(
        (conv0): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=49, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn0): LeakyReLU(negative_slope=0.2, inplace=True)
        (conv1): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=128, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
      )
      (2): Styled_F_ConvBlock(
        (conv0): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=49, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn0): LeakyReLU(negative_slope=0.2, inplace=True)
        (conv1): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=128, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
      )
      (3): Styled_F_ConvBlock(
        (conv0): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=49, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn0): LeakyReLU(negative_slope=0.2, inplace=True)
        (conv1): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=128, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
      )
      (4): Styled_F_ConvBlock(
        (conv0): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=49, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn0): LeakyReLU(negative_slope=0.2, inplace=True)
        (conv1): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=128, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
      )
    )
    (cond_style): Sequential(
      (0): Conv2d(256, 128, kernel_size=(8, 6), stride=(1, 1))
      (1): LeakyReLU(negative_slope=0.1)
    )
    (image_style): Sequential(
      (0): Conv2d(256, 128, kernel_size=(8, 6), stride=(1, 1))
      (1): LeakyReLU(negative_slope=0.1)
    )
  )
)
ResUnetGenerator(
  (model): ResUnetSkipConnectionBlock(
    (model): Sequential(
      (0): Conv2d(8, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): ReLU(inplace=True)
      (2): ResidualBlock(
        (relu): ReLU(inplace=True)
        (block): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): ResidualBlock(
        (relu): ReLU(inplace=True)
        (block): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): ResUnetSkipConnectionBlock(
        (model): Sequential(
          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): ResidualBlock(
            (relu): ReLU(inplace=True)
            (block): Sequential(
              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (4): ResidualBlock(
            (relu): ReLU(inplace=True)
            (block): Sequential(
              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (5): ResUnetSkipConnectionBlock(
            (model): Sequential(
              (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
              (3): ResidualBlock(
                (relu): ReLU(inplace=True)
                (block): Sequential(
                  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (2): ReLU(inplace=True)
                  (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                )
              )
              (4): ResidualBlock(
                (relu): ReLU(inplace=True)
                (block): Sequential(
                  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (2): ReLU(inplace=True)
                  (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                )
              )
              (5): ResUnetSkipConnectionBlock(
                (model): Sequential(
                  (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (2): ReLU(inplace=True)
                  (3): ResidualBlock(
                    (relu): ReLU(inplace=True)
                    (block): Sequential(
                      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (2): ReLU(inplace=True)
                      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    )
                  )
                  (4): ResidualBlock(
                    (relu): ReLU(inplace=True)
                    (block): Sequential(
                      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (2): ReLU(inplace=True)
                      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    )
                  )
                  (5): ResUnetSkipConnectionBlock(
                    (model): Sequential(
                      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                      (1): ReLU(inplace=True)
                      (2): ResidualBlock(
                        (relu): ReLU(inplace=True)
                        (block): Sequential(
                          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                          (2): ReLU(inplace=True)
                          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                        )
                      )
                      (3): ResidualBlock(
                        (relu): ReLU(inplace=True)
                        (block): Sequential(
                          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                          (2): ReLU(inplace=True)
                          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                        )
                      )
                      (4): Upsample(scale_factor=2.0, mode=nearest)
                      (5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (7): ReLU(inplace=True)
                      (8): ResidualBlock(
                        (relu): ReLU(inplace=True)
                        (block): Sequential(
                          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                          (2): ReLU(inplace=True)
                          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                        )
                      )
                      (9): ResidualBlock(
                        (relu): ReLU(inplace=True)
                        (block): Sequential(
                          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                          (2): ReLU(inplace=True)
                          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                        )
                      )
                    )
                  )
                  (6): Upsample(scale_factor=2.0, mode=nearest)
                  (7): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (9): ReLU(inplace=True)
                  (10): ResidualBlock(
                    (relu): ReLU(inplace=True)
                    (block): Sequential(
                      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (2): ReLU(inplace=True)
                      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    )
                  )
                  (11): ResidualBlock(
                    (relu): ReLU(inplace=True)
                    (block): Sequential(
                      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (2): ReLU(inplace=True)
                      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    )
                  )
                )
              )
              (6): Upsample(scale_factor=2.0, mode=nearest)
              (7): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (9): ReLU(inplace=True)
              (10): ResidualBlock(
                (relu): ReLU(inplace=True)
                (block): Sequential(
                  (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (2): ReLU(inplace=True)
                  (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                )
              )
              (11): ResidualBlock(
                (relu): ReLU(inplace=True)
                (block): Sequential(
                  (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (2): ReLU(inplace=True)
                  (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                )
              )
            )
          )
          (6): Upsample(scale_factor=2.0, mode=nearest)
          (7): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (9): ReLU(inplace=True)
          (10): ResidualBlock(
            (relu): ReLU(inplace=True)
            (block): Sequential(
              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (11): ResidualBlock(
            (relu): ReLU(inplace=True)
            (block): Sequential(
              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (5): Upsample(scale_factor=2.0, mode=nearest)
      (6): Conv2d(128, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
  )
)
/opt/conda/lib/python3.7/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.
  warnings.warn(warning.format(ret))
train_PFAFN_e2e_fs.py:123: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  t_mask = torch.FloatTensor((data['label'].cpu().numpy() == 7).astype(np.float))
train_PFAFN_e2e_fs.py:126: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  pre_clothes_edge = torch.FloatTensor((edge.detach().numpy() > 0.5).astype(np.int))
train_PFAFN_e2e_fs.py:130: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  pre_clothes_edge_un = torch.FloatTensor((edge_un.detach().numpy() > 0.5).astype(np.int))
train_PFAFN_e2e_fs.py:133: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  person_clothes_edge = torch.FloatTensor((data['label'].cpu().numpy() == 4).astype(np.int))
train_PFAFN_e2e_fs.py:142: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  face_mask = torch.FloatTensor((data['label'].cpu().numpy() == 1).astype(np.int)) + torch.FloatTensor((data['label'].cpu().numpy() == 12).astype(np.int))
train_PFAFN_e2e_fs.py:145: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  + torch.FloatTensor((data['label'].cpu().numpy() == 10).astype(np.int))
/opt/conda/lib/python3.7/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272168290/work/aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:4004: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  "Default grid_sample and affine_grid behavior has changed "
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:3635: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  "See the documentation of nn.Upsample for details.".format(mode)
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
train_PFAFN_e2e_fs.py:159: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  arm_mask = torch.FloatTensor((data['label'].cpu().numpy() == 11).astype(np.float)) + torch.FloatTensor((data['label'].cpu().numpy() == 13).astype(np.float))
train_PFAFN_e2e_fs.py:160: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  hand_mask = torch.FloatTensor((data['densepose'].cpu().numpy() == 3).astype(np.int)) + torch.FloatTensor((data['densepose'].cpu().numpy() == 4).astype(np.int))
train_PFAFN_e2e_fs.py:164: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  + torch.FloatTensor((data['densepose'].cpu().numpy() == 21).astype(np.int)) + torch.FloatTensor((data['densepose'].cpu().numpy() == 22))
2023.01.25-22:42:39:100:[step-100/177600: 0.06%]--[loss-5.049003: wl-4.330410, gl-3.966401]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:51:35]
2023.01.25-22:44:08:200:[step-200/177600: 0.11%]--[loss-4.610248: wl-3.802969, gl-3.659505]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:16:53]
2023.01.25-22:45:38:300:[step-300/177600: 0.17%]--[loss-4.457263: wl-3.716469, gl-3.528146]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 20:12:56]
2023.01.25-22:47:08:400:[step-400/177600: 0.23%]--[loss-4.682737: wl-3.963927, gl-3.691756]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:49:26]
2023.01.25-22:48:38:500:[step-500/177600: 0.28%]--[loss-4.331957: wl-4.155194, gl-3.293158]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 21:24:55]
2023.01.25-22:50:08:600:[step-600/177600: 0.34%]--[loss-3.959916: wl-3.597641, gl-3.060506]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 20:09:12]
2023.01.25-22:51:39:700:[step-700/177600: 0.39%]--[loss-4.018164: wl-3.426782, gl-3.161469]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:36:30]
2023.01.25-22:53:09:800:[step-800/177600: 0.45%]--[loss-3.902427: wl-3.866261, gl-2.935862]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:32:44]
End of epoch 1 / 200 	 Time Taken: 801 sec
2023.01.25-22:54:41:12:[step-900/177600: 0.51%]--[loss-3.638355: wl-3.512886, gl-2.760133]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 20:02:03]
2023.01.25-22:56:11:112:[step-1000/177600: 0.56%]--[loss-3.927359: wl-3.663964, gl-3.011368]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 21:13:14]
2023.01.25-22:57:42:212:[step-1100/177600: 0.62%]--[loss-4.287483: wl-4.285240, gl-3.216173]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:57:38]
2023.01.25-22:59:13:312:[step-1200/177600: 0.68%]--[loss-3.774590: wl-3.680836, gl-2.854381]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:40:22]
2023.01.25-23:00:43:412:[step-1300/177600: 0.73%]--[loss-3.778347: wl-3.835340, gl-2.819512]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:26:21]
2023.01.25-23:02:14:512:[step-1400/177600: 0.79%]--[loss-3.860551: wl-3.963926, gl-2.869570]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:31:14]
2023.01.25-23:03:45:612:[step-1500/177600: 0.84%]--[loss-3.670616: wl-4.243454, gl-2.609752]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:17:09]
2023.01.25-23:05:15:712:[step-1600/177600: 0.90%]--[loss-3.781011: wl-3.812990, gl-2.827763]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 21:05:28]
2023.01.25-23:06:46:812:[step-1700/177600: 0.96%]--[loss-3.787491: wl-3.973557, gl-2.794102]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 21:26:22]
End of epoch 2 / 200 	 Time Taken: 806 sec
2023.01.25-23:08:18:24:[step-1800/177600: 1.01%]--[loss-3.461472: wl-3.755154, gl-2.522683]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:37:15]
2023.01.25-23:09:50:124:[step-1900/177600: 1.07%]--[loss-3.773192: wl-3.706963, gl-2.846451]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 23:00:18]
2023.01.25-23:11:21:224:[step-2000/177600: 1.13%]--[loss-3.808347: wl-4.277714, gl-2.738919]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 21:43:17]
2023.01.25-23:12:52:324:[step-2100/177600: 1.18%]--[loss-3.766575: wl-4.059284, gl-2.751754]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 20:56:48]
2023.01.25-23:14:23:424:[step-2200/177600: 1.24%]--[loss-3.406938: wl-3.545730, gl-2.520505]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:44:05]
2023.01.25-23:15:54:524:[step-2300/177600: 1.30%]--[loss-3.557814: wl-3.522646, gl-2.677153]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:25:55]
2023.01.25-23:17:26:624:[step-2400/177600: 1.35%]--[loss-4.115148: wl-4.462933, gl-2.999414]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 20:19:14]
2023.01.25-23:18:57:724:[step-2500/177600: 1.41%]--[loss-3.240319: wl-3.387230, gl-2.393512]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 20:08:38]
2023.01.25-23:20:28:824:[step-2600/177600: 1.46%]--[loss-3.415617: wl-4.066208, gl-2.399064]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:56:09]
End of epoch 3 / 200 	 Time Taken: 810 sec
2023.01.25-23:22:00:36:[step-2700/177600: 1.52%]--[loss-3.889305: wl-4.287061, gl-2.817540]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 21:54:36]
2023.01.25-23:23:32:136:[step-2800/177600: 1.58%]--[loss-3.812416: wl-4.120450, gl-2.782303]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:51:59]
2023.01.25-23:25:03:236:[step-2900/177600: 1.63%]--[loss-3.627064: wl-4.052757, gl-2.613875]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 20:51:13]
2023.01.25-23:26:34:336:[step-3000/177600: 1.69%]--[loss-4.080445: wl-4.484820, gl-2.959240]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 20:07:42]
2023.01.25-23:28:05:436:[step-3100/177600: 1.75%]--[loss-3.678817: wl-4.001767, gl-2.678375]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:55:15]
2023.01.25-23:29:37:536:[step-3200/177600: 1.80%]--[loss-3.569466: wl-3.993541, gl-2.571081]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 21:12:09]
2023.01.25-23:31:08:636:[step-3300/177600: 1.86%]--[loss-3.690766: wl-4.270241, gl-2.623206]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:42:09]
2023.01.25-23:32:39:736:[step-3400/177600: 1.91%]--[loss-3.635608: wl-3.958335, gl-2.646024]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:24:49]
2023.01.25-23:34:10:836:[step-3500/177600: 1.97%]--[loss-3.663082: wl-4.254278, gl-2.599513]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 20:02:52]
End of epoch 4 / 200 	 Time Taken: 811 sec
2023.01.25-23:35:43:48:[step-3600/177600: 2.03%]--[loss-3.385025: wl-3.944805, gl-2.398824]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 20:33:47]
2023.01.25-23:37:14:148:[step-3700/177600: 2.08%]--[loss-3.933751: wl-4.294666, gl-2.860084]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 20:23:55]
2023.01.25-23:38:46:248:[step-3800/177600: 2.14%]--[loss-3.512277: wl-3.750586, gl-2.574630]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 22:46:31]
2023.01.25-23:40:16:348:[step-3900/177600: 2.20%]--[loss-3.706857: wl-4.403219, gl-2.606052]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 20:45:19]
2023.01.25-23:41:47:448:[step-4000/177600: 2.25%]--[loss-3.445285: wl-3.716464, gl-2.516169]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 21:46:34]
2023.01.25-23:43:18:548:[step-4100/177600: 2.31%]--[loss-3.497699: wl-4.103342, gl-2.471863]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:47:47]
2023.01.25-23:44:49:648:[step-4200/177600: 2.36%]--[loss-3.699057: wl-4.136693, gl-2.664883]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:43:30]
2023.01.25-23:46:21:748:[step-4300/177600: 2.42%]--[loss-3.618804: wl-3.989889, gl-2.621332]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:09:52]
2023.01.25-23:47:52:848:[step-4400/177600: 2.48%]--[loss-3.432740: wl-4.083267, gl-2.411923]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 18:34:42]
End of epoch 5 / 200 	 Time Taken: 810 sec
saving the model at the end of epoch 5, iters 4440
2023.01.25-23:49:25:60:[step-4500/177600: 2.53%]--[loss-3.298369: wl-3.828117, gl-2.341340]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:26:35]
2023.01.25-23:50:56:160:[step-4600/177600: 2.59%]--[loss-3.655872: wl-4.547678, gl-2.518953]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 18:38:31]
2023.01.25-23:52:28:260:[step-4700/177600: 2.65%]--[loss-3.245970: wl-3.569351, gl-2.353632]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:30:29]
2023.01.25-23:53:59:360:[step-4800/177600: 2.70%]--[loss-3.538389: wl-4.087378, gl-2.516545]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 18:30:21]
2023.01.25-23:55:30:460:[step-4900/177600: 2.76%]--[loss-3.462627: wl-3.765052, gl-2.521364]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 18:20:54]
2023.01.25-23:57:03:560:[step-5000/177600: 2.82%]--[loss-3.964202: wl-4.716702, gl-2.785027]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:17:37]
2023.01.25-23:58:36:660:[step-5100/177600: 2.87%]--[loss-3.774308: wl-3.919773, gl-2.794365]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:27:34]
2023.01.26-00:00:08:760:[step-5200/177600: 2.93%]--[loss-3.465793: wl-3.938370, gl-2.481200]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 18:44:54]
2023.01.26-00:01:39:860:[step-5300/177600: 2.98%]--[loss-3.390383: wl-3.711414, gl-2.462530]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 18:11:20]
End of epoch 6 / 200 	 Time Taken: 815 sec
2023.01.26-00:03:11:72:[step-5400/177600: 3.04%]--[loss-3.537035: wl-4.308242, gl-2.459975]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:21:16]
2023.01.26-00:04:43:172:[step-5500/177600: 3.10%]--[loss-3.464734: wl-4.086017, gl-2.443230]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:50:35]
2023.01.26-00:06:14:272:[step-5600/177600: 3.15%]--[loss-3.534520: wl-4.213473, gl-2.481152]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:43:22]
2023.01.26-00:07:45:372:[step-5700/177600: 3.21%]--[loss-3.466814: wl-3.768088, gl-2.524792]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:16:37]
2023.01.26-00:09:17:472:[step-5800/177600: 3.27%]--[loss-3.194076: wl-3.791136, gl-2.246292]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 21:38:41]
2023.01.26-00:10:48:572:[step-5900/177600: 3.32%]--[loss-3.634635: wl-4.398091, gl-2.535112]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:45:52]
2023.01.26-00:12:19:672:[step-6000/177600: 3.38%]--[loss-3.496284: wl-4.028659, gl-2.489119]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 20:08:43]
2023.01.26-00:13:50:772:[step-6100/177600: 3.43%]--[loss-3.320366: wl-3.844787, gl-2.359170]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 18:13:16]
2023.01.26-00:15:22:872:[step-6200/177600: 3.49%]--[loss-3.154053: wl-3.488206, gl-2.282002]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 18:17:17]
End of epoch 7 / 200 	 Time Taken: 811 sec
2023.01.26-00:16:54:84:[step-6300/177600: 3.55%]--[loss-3.259641: wl-3.649889, gl-2.347169]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:05:46]
2023.01.26-00:18:25:184:[step-6400/177600: 3.60%]--[loss-3.877982: wl-4.931632, gl-2.645074]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:28:00]
2023.01.26-00:19:56:284:[step-6500/177600: 3.66%]--[loss-3.764905: wl-4.408533, gl-2.662772]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:31:56]
2023.01.26-00:21:27:384:[step-6600/177600: 3.72%]--[loss-3.289243: wl-4.068641, gl-2.272083]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 18:13:52]
2023.01.26-00:22:58:484:[step-6700/177600: 3.77%]--[loss-3.749230: wl-4.259647, gl-2.684319]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 18:51:37]
2023.01.26-00:24:29:584:[step-6800/177600: 3.83%]--[loss-3.057564: wl-3.373858, gl-2.214100]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 18:43:11]
2023.01.26-00:26:00:684:[step-6900/177600: 3.89%]--[loss-3.656036: wl-4.078989, gl-2.636289]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 18:11:47]
2023.01.26-00:27:32:784:[step-7000/177600: 3.94%]--[loss-3.357603: wl-3.755897, gl-2.418629]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:59:17]
2023.01.26-00:29:04:884:[step-7100/177600: 4.00%]--[loss-3.315877: wl-3.558474, gl-2.426259]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:13:18]
End of epoch 8 / 200 	 Time Taken: 811 sec
2023.01.26-00:30:37:96:[step-7200/177600: 4.05%]--[loss-3.347862: wl-4.279772, gl-2.277919]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 18:35:06]
2023.01.26-00:32:08:196:[step-7300/177600: 4.11%]--[loss-3.422329: wl-4.083443, gl-2.401468]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 18:13:36]
2023.01.26-00:33:39:296:[step-7400/177600: 4.17%]--[loss-3.539535: wl-4.014888, gl-2.535813]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 17:54:37]
2023.01.26-00:35:10:396:[step-7500/177600: 4.22%]--[loss-3.604444: wl-4.102417, gl-2.578840]--[lr: pb-0.000050, pf-0.000050]--[ETA-2 days, 0:14:04]
2023.01.26-00:36:41:496:[step-7600/177600: 4.28%]--[loss-3.247118: wl-3.812884, gl-2.293897]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:15:53]
2023.01.26-00:38:13:596:[step-7700/177600: 4.34%]--[loss-3.805916: wl-5.299884, gl-2.480945]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 18:01:23]
2023.01.26-00:39:44:696:[step-7800/177600: 4.39%]--[loss-3.371099: wl-4.161431, gl-2.330741]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 17:52:16]
2023.01.26-00:41:14:796:[step-7900/177600: 4.45%]--[loss-3.831713: wl-4.725070, gl-2.650446]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 18:54:17]
End of epoch 9 / 200 	 Time Taken: 810 sec
2023.01.26-00:42:47:8:[step-8000/177600: 4.50%]--[loss-3.574342: wl-4.730471, gl-2.391724]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 20:46:24]
2023.01.26-00:44:18:108:[step-8100/177600: 4.56%]--[loss-3.368297: wl-3.993850, gl-2.369834]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 18:31:28]
2023.01.26-00:45:49:208:[step-8200/177600: 4.62%]--[loss-4.004973: wl-5.407389, gl-2.653126]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:48:04]
2023.01.26-00:47:21:308:[step-8300/177600: 4.67%]--[loss-3.335959: wl-3.773650, gl-2.392546]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 18:47:24]
2023.01.26-00:48:52:408:[step-8400/177600: 4.73%]--[loss-3.186678: wl-3.681827, gl-2.266221]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 18:23:05]
2023.01.26-00:50:24:508:[step-8500/177600: 4.79%]--[loss-3.319697: wl-3.839310, gl-2.359869]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 22:07:15]
2023.01.26-00:51:55:608:[step-8600/177600: 4.84%]--[loss-3.336187: wl-3.904307, gl-2.360110]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 18:59:46]
2023.01.26-00:53:25:708:[step-8700/177600: 4.90%]--[loss-3.438591: wl-4.348172, gl-2.351548]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:32:12]
2023.01.26-00:54:57:808:[step-8800/177600: 4.95%]--[loss-3.380409: wl-4.110027, gl-2.352902]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 18:33:39]
End of epoch 10 / 200 	 Time Taken: 811 sec
saving the model at the end of epoch 10, iters 8880
2023.01.26-00:56:30:20:[step-8900/177600: 5.01%]--[loss-3.321611: wl-4.126544, gl-2.289975]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 18:20:29]
2023.01.26-00:58:01:120:[step-9000/177600: 5.07%]--[loss-3.334383: wl-4.077825, gl-2.314927]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 17:26:21]
2023.01.26-00:59:32:220:[step-9100/177600: 5.12%]--[loss-3.498894: wl-4.055121, gl-2.485114]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:09:29]
2023.01.26-01:01:03:320:[step-9200/177600: 5.18%]--[loss-3.168288: wl-3.694775, gl-2.244595]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 18:23:15]
2023.01.26-01:02:35:420:[step-9300/177600: 5.24%]--[loss-3.738536: wl-4.402833, gl-2.637827]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:21:48]
2023.01.26-01:04:06:520:[step-9400/177600: 5.29%]--[loss-3.154739: wl-3.508880, gl-2.277519]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 20:04:54]
2023.01.26-01:05:36:620:[step-9500/177600: 5.35%]--[loss-3.299695: wl-3.691211, gl-2.376893]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 17:22:44]
2023.01.26-01:07:08:720:[step-9600/177600: 5.41%]--[loss-3.387997: wl-3.956822, gl-2.398792]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:16:35]
2023.01.26-01:08:39:820:[step-9700/177600: 5.46%]--[loss-3.258784: wl-3.719266, gl-2.328968]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:10:16]
End of epoch 11 / 200 	 Time Taken: 810 sec
2023.01.26-01:10:11:32:[step-9800/177600: 5.52%]--[loss-3.523428: wl-4.555153, gl-2.384640]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:54:41]
2023.01.26-01:11:42:132:[step-9900/177600: 5.57%]--[loss-3.662863: wl-4.681640, gl-2.492453]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 17:27:12]
2023.01.26-01:13:13:232:[step-10000/177600: 5.63%]--[loss-3.210420: wl-4.109828, gl-2.182963]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 18:11:06]
2023.01.26-01:14:45:332:[step-10100/177600: 5.69%]--[loss-3.351890: wl-3.889341, gl-2.379555]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 18:26:32]
2023.01.26-01:16:15:432:[step-10200/177600: 5.74%]--[loss-3.634429: wl-4.153272, gl-2.596111]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 18:12:36]
2023.01.26-01:17:46:532:[step-10300/177600: 5.80%]--[loss-3.384682: wl-3.900074, gl-2.409663]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 17:15:46]
2023.01.26-01:19:18:632:[step-10400/177600: 5.86%]--[loss-3.295985: wl-3.661454, gl-2.380621]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 17:30:37]
2023.01.26-01:20:49:732:[step-10500/177600: 5.91%]--[loss-3.516871: wl-4.269468, gl-2.449504]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 18:13:16]
2023.01.26-01:22:20:832:[step-10600/177600: 5.97%]--[loss-3.399624: wl-4.316877, gl-2.320405]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 17:48:39]
End of epoch 12 / 200 	 Time Taken: 811 sec
2023.01.26-01:23:53:44:[step-10700/177600: 6.02%]--[loss-3.535701: wl-4.729751, gl-2.353263]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 16:56:58]
2023.01.26-01:25:24:144:[step-10800/177600: 6.08%]--[loss-3.036225: wl-3.472482, gl-2.168105]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 17:27:32]
2023.01.26-01:26:56:244:[step-10900/177600: 6.14%]--[loss-3.473258: wl-3.916737, gl-2.494073]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 18:04:41]
2023.01.26-01:28:27:344:[step-11000/177600: 6.19%]--[loss-3.392213: wl-3.979270, gl-2.397395]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 18:25:16]
2023.01.26-01:29:59:444:[step-11100/177600: 6.25%]--[loss-3.045834: wl-3.885646, gl-2.074422]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 18:01:13]
2023.01.26-01:31:30:544:[step-11200/177600: 6.31%]--[loss-3.362876: wl-4.087090, gl-2.341104]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 18:45:06]
2023.01.26-01:33:01:644:[step-11300/177600: 6.36%]--[loss-3.291339: wl-3.963715, gl-2.300410]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:10:35]
2023.01.26-01:34:32:744:[step-11400/177600: 6.42%]--[loss-3.117294: wl-3.589864, gl-2.219828]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 18:35:30]
2023.01.26-01:36:02:844:[step-11500/177600: 6.48%]--[loss-3.498636: wl-4.189390, gl-2.451288]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 20:51:53]
End of epoch 13 / 200 	 Time Taken: 811 sec
2023.01.26-01:37:35:56:[step-11600/177600: 6.53%]--[loss-3.275887: wl-3.680486, gl-2.355766]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 18:51:15]
2023.01.26-01:39:06:156:[step-11700/177600: 6.59%]--[loss-3.410381: wl-4.725089, gl-2.229109]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:55:56]
2023.01.26-01:40:37:256:[step-11800/177600: 6.64%]--[loss-3.080659: wl-3.621815, gl-2.175205]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 18:51:24]
2023.01.26-01:42:08:356:[step-11900/177600: 6.70%]--[loss-3.426102: wl-4.650310, gl-2.263525]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 17:45:56]
2023.01.26-01:43:39:456:[step-12000/177600: 6.76%]--[loss-2.997899: wl-3.622346, gl-2.092313]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 16:45:22]
2023.01.26-01:45:10:556:[step-12100/177600: 6.81%]--[loss-3.216524: wl-4.194028, gl-2.168017]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 17:48:57]
2023.01.26-01:46:41:656:[step-12200/177600: 6.87%]--[loss-3.452114: wl-4.294530, gl-2.378481]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 17:13:29]
2023.01.26-01:48:12:756:[step-12300/177600: 6.93%]--[loss-3.711344: wl-4.721178, gl-2.531050]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 17:56:10]
2023.01.26-01:49:44:856:[step-12400/177600: 6.98%]--[loss-3.025311: wl-3.606213, gl-2.123757]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 17:26:57]
End of epoch 14 / 200 	 Time Taken: 810 sec
2023.01.26-01:51:16:68:[step-12500/177600: 7.04%]--[loss-3.364353: wl-4.164196, gl-2.323304]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 17:22:39]
2023.01.26-01:52:47:168:[step-12600/177600: 7.09%]--[loss-3.157226: wl-3.787312, gl-2.210398]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 16:46:07]
2023.01.26-01:54:18:268:[step-12700/177600: 7.15%]--[loss-3.114242: wl-3.562831, gl-2.223535]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 18:06:33]
2023.01.26-01:55:49:368:[step-12800/177600: 7.21%]--[loss-3.247293: wl-3.933065, gl-2.264026]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 16:47:37]
2023.01.26-01:57:21:468:[step-12900/177600: 7.26%]--[loss-3.508215: wl-4.056693, gl-2.494042]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 18:18:09]
2023.01.26-01:58:53:568:[step-13000/177600: 7.32%]--[loss-3.188544: wl-3.850437, gl-2.225935]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 16:45:56]
2023.01.26-02:00:23:668:[step-13100/177600: 7.38%]--[loss-3.375020: wl-4.237244, gl-2.315708]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 20:22:31]
2023.01.26-02:01:57:768:[step-13200/177600: 7.43%]--[loss-3.548404: wl-4.583510, gl-2.402526]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:00:23]
2023.01.26-02:03:31:868:[step-13300/177600: 7.49%]--[loss-3.193253: wl-4.103121, gl-2.167473]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 16:54:47]
End of epoch 15 / 200 	 Time Taken: 816 sec
saving the model at the end of epoch 15, iters 13320
2023.01.26-02:05:06:80:[step-13400/177600: 7.55%]--[loss-3.144733: wl-3.671063, gl-2.226967]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:42:04]
2023.01.26-02:06:41:180:[step-13500/177600: 7.60%]--[loss-3.219243: wl-3.703062, gl-2.293478]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 16:41:20]
2023.01.26-02:08:15:280:[step-13600/177600: 7.66%]--[loss-3.257612: wl-4.626380, gl-2.101017]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 23:14:50]
2023.01.26-02:09:52:380:[step-13700/177600: 7.71%]--[loss-3.414181: wl-4.330421, gl-2.331575]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 17:50:02]
2023.01.26-02:11:28:480:[step-13800/177600: 7.77%]--[loss-3.192710: wl-3.959494, gl-2.202837]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 17:24:53]
2023.01.26-02:13:04:580:[step-13900/177600: 7.83%]--[loss-2.948995: wl-3.418799, gl-2.094295]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:21:20]
2023.01.26-02:14:38:680:[step-14000/177600: 7.88%]--[loss-3.098447: wl-3.963086, gl-2.107675]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 17:52:46]
2023.01.26-02:16:14:780:[step-14100/177600: 7.94%]--[loss-3.670786: wl-4.415256, gl-2.566972]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 19:18:23]
2023.01.26-02:17:47:880:[step-14200/177600: 8.00%]--[loss-2.912371: wl-3.478896, gl-2.042647]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 17:30:05]
End of epoch 16 / 200 	 Time Taken: 844 sec
2023.01.26-02:19:22:92:[step-14300/177600: 8.05%]--[loss-3.595629: wl-4.729739, gl-2.413194]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 16:03:41]
2023.01.26-02:20:52:192:[step-14400/177600: 8.11%]--[loss-3.255852: wl-3.830749, gl-2.298165]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 16:57:59]
2023.01.26-02:22:20:292:[step-14500/177600: 8.16%]--[loss-3.360099: wl-4.231799, gl-2.302150]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 16:02:17]
2023.01.26-02:23:50:392:[step-14600/177600: 8.22%]--[loss-3.306110: wl-3.808862, gl-2.353894]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 16:03:47]
2023.01.26-02:25:20:492:[step-14700/177600: 8.28%]--[loss-3.302001: wl-4.059688, gl-2.287079]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 16:03:14]
2023.01.26-02:26:50:592:[step-14800/177600: 8.33%]--[loss-3.108205: wl-3.907251, gl-2.131392]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 16:08:07]
2023.01.26-02:28:20:692:[step-14900/177600: 8.39%]--[loss-3.270015: wl-3.940378, gl-2.284921]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 17:45:16]
2023.01.26-02:29:50:792:[step-15000/177600: 8.45%]--[loss-3.012683: wl-3.678597, gl-2.093033]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 16:08:05]
End of epoch 17 / 200 	 Time Taken: 800 sec
2023.01.26-02:31:20:4:[step-15100/177600: 8.50%]--[loss-3.262042: wl-3.845137, gl-2.300758]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 15:49:05]
2023.01.26-02:32:51:104:[step-15200/177600: 8.56%]--[loss-3.407062: wl-4.540240, gl-2.272002]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 17:46:24]
2023.01.26-02:34:21:204:[step-15300/177600: 8.61%]--[loss-3.514467: wl-4.313447, gl-2.436105]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 16:49:37]
2023.01.26-02:35:50:304:[step-15400/177600: 8.67%]--[loss-3.140086: wl-3.891799, gl-2.167136]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 15:43:49]
2023.01.26-02:37:20:404:[step-15500/177600: 8.73%]--[loss-3.266075: wl-3.926532, gl-2.284442]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 16:20:54]
2023.01.26-02:38:49:504:[step-15600/177600: 8.78%]--[loss-3.113938: wl-3.913743, gl-2.135502]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 16:59:11]
2023.01.26-02:40:19:604:[step-15700/177600: 8.84%]--[loss-3.296346: wl-4.141581, gl-2.260951]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 15:49:19]
2023.01.26-02:41:48:704:[step-15800/177600: 8.90%]--[loss-3.109948: wl-3.919399, gl-2.130099]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 15:43:51]
2023.01.26-02:43:18:804:[step-15900/177600: 8.95%]--[loss-3.128306: wl-3.680411, gl-2.208204]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 15:44:12]
End of epoch 18 / 200 	 Time Taken: 798 sec
2023.01.26-02:44:50:16:[step-16000/177600: 9.01%]--[loss-3.201389: wl-3.879238, gl-2.231580]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 15:44:39]
2023.01.26-02:46:20:116:[step-16100/177600: 9.07%]--[loss-3.218289: wl-4.007329, gl-2.216457]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 15:37:57]
2023.01.26-02:47:51:216:[step-16200/177600: 9.12%]--[loss-3.215853: wl-3.960671, gl-2.225686]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 15:34:38]
2023.01.26-02:49:21:316:[step-16300/177600: 9.18%]--[loss-3.148898: wl-3.726065, gl-2.217381]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 15:41:52]
2023.01.26-02:50:51:416:[step-16400/177600: 9.23%]--[loss-3.366206: wl-4.106293, gl-2.339633]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 15:30:04]
2023.01.26-02:52:20:516:[step-16500/177600: 9.29%]--[loss-3.025842: wl-3.846168, gl-2.064301]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 15:55:06]
2023.01.26-02:53:50:616:[step-16600/177600: 9.35%]--[loss-3.358705: wl-4.250270, gl-2.296137]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 15:42:18]
2023.01.26-02:55:20:716:[step-16700/177600: 9.40%]--[loss-3.191098: wl-3.942691, gl-2.205425]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 15:35:39]
2023.01.26-02:56:50:816:[step-16800/177600: 9.46%]--[loss-3.144381: wl-3.862189, gl-2.178833]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 15:38:04]
End of epoch 19 / 200 	 Time Taken: 801 sec
2023.01.26-02:58:21:28:[step-16900/177600: 9.52%]--[loss-3.256320: wl-3.937594, gl-2.271921]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 17:11:40]
2023.01.26-02:59:51:128:[step-17000/177600: 9.57%]--[loss-3.346786: wl-4.250756, gl-2.284097]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 15:30:42]
2023.01.26-03:01:21:228:[step-17100/177600: 9.63%]--[loss-3.071114: wl-4.004219, gl-2.070059]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 15:33:17]
2023.01.26-03:02:50:328:[step-17200/177600: 9.68%]--[loss-3.102769: wl-3.745435, gl-2.166410]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 15:25:26]
2023.01.26-03:04:20:428:[step-17300/177600: 9.74%]--[loss-3.241303: wl-3.918773, gl-2.261610]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 15:43:34]
2023.01.26-03:05:50:528:[step-17400/177600: 9.80%]--[loss-3.039436: wl-3.765238, gl-2.098127]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 17:05:56]
2023.01.26-03:07:21:628:[step-17500/177600: 9.85%]--[loss-3.178013: wl-4.083391, gl-2.157165]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 15:44:12]
2023.01.26-03:08:52:728:[step-17600/177600: 9.91%]--[loss-3.383005: wl-4.106488, gl-2.356383]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 16:32:47]
2023.01.26-03:10:22:828:[step-17700/177600: 9.97%]--[loss-3.157401: wl-3.761162, gl-2.217110]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 15:53:29]
End of epoch 20 / 200 	 Time Taken: 800 sec
saving the model at the end of epoch 20, iters 17760
2023.01.26-03:11:53:40:[step-17800/177600: 10.02%]--[loss-3.255090: wl-4.330378, gl-2.172495]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 15:04:45]
2023.01.26-03:13:24:140:[step-17900/177600: 10.08%]--[loss-3.530950: wl-4.092342, gl-2.507864]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 16:06:20]
2023.01.26-03:14:54:240:[step-18000/177600: 10.14%]--[loss-3.495309: wl-4.640646, gl-2.335147]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 16:38:36]
2023.01.26-03:16:25:340:[step-18100/177600: 10.19%]--[loss-3.009451: wl-3.472677, gl-2.141282]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 16:24:31]
2023.01.26-03:17:56:440:[step-18200/177600: 10.25%]--[loss-3.149199: wl-4.034424, gl-2.140594]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 16:56:52]
2023.01.26-03:19:27:540:[step-18300/177600: 10.30%]--[loss-3.279057: wl-4.001201, gl-2.278757]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 15:19:23]
2023.01.26-03:20:56:640:[step-18400/177600: 10.36%]--[loss-3.509192: wl-5.500037, gl-2.134183]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 15:23:13]
2023.01.26-03:22:27:740:[step-18500/177600: 10.42%]--[loss-3.057617: wl-3.941481, gl-2.072247]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 18:15:07]
2023.01.26-03:23:56:840:[step-18600/177600: 10.47%]--[loss-3.490377: wl-4.533921, gl-2.356897]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 16:37:01]
End of epoch 21 / 200 	 Time Taken: 802 sec
2023.01.26-03:25:27:52:[step-18700/177600: 10.53%]--[loss-3.060955: wl-3.603912, gl-2.159976]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 15:07:58]
2023.01.26-03:26:57:152:[step-18800/177600: 10.59%]--[loss-3.314112: wl-4.016283, gl-2.310041]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 18:40:01]
2023.01.26-03:28:27:252:[step-18900/177600: 10.64%]--[loss-3.234867: wl-4.009540, gl-2.232482]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 15:07:49]
2023.01.26-03:29:56:352:[step-19000/177600: 10.70%]--[loss-3.234747: wl-3.885872, gl-2.263279]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 15:08:57]
2023.01.26-03:31:26:452:[step-19100/177600: 10.75%]--[loss-3.137337: wl-3.686056, gl-2.215823]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 15:02:41]
2023.01.26-03:32:56:552:[step-19200/177600: 10.81%]--[loss-3.200795: wl-4.421906, gl-2.095319]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 15:02:04]
2023.01.26-03:34:26:652:[step-19300/177600: 10.87%]--[loss-3.168560: wl-3.966119, gl-2.177031]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 15:13:41]
2023.01.26-03:35:56:752:[step-19400/177600: 10.92%]--[loss-3.324666: wl-4.342642, gl-2.239005]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 16:30:52]
2023.01.26-03:37:27:852:[step-19500/177600: 10.98%]--[loss-3.220590: wl-4.324855, gl-2.139376]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 15:14:17]
End of epoch 22 / 200 	 Time Taken: 800 sec
2023.01.26-03:38:58:64:[step-19600/177600: 11.04%]--[loss-3.614213: wl-4.729431, gl-2.431856]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 14:50:29]
2023.01.26-03:40:28:164:[step-19700/177600: 11.09%]--[loss-3.205904: wl-4.039047, gl-2.196142]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 14:53:37]
2023.01.26-03:41:58:264:[step-19800/177600: 11.15%]--[loss-3.321349: wl-4.393991, gl-2.222851]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 15:13:59]
2023.01.26-03:43:28:364:[step-19900/177600: 11.20%]--[loss-3.048877: wl-4.068389, gl-2.031779]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 15:57:34]
2023.01.26-03:44:57:464:[step-20000/177600: 11.26%]--[loss-3.409325: wl-4.419449, gl-2.304462]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 14:55:57]
2023.01.26-03:46:28:564:[step-20100/177600: 11.32%]--[loss-3.341829: wl-4.357587, gl-2.252432]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 15:06:24]
2023.01.26-03:47:57:664:[step-20200/177600: 11.37%]--[loss-2.980462: wl-3.530611, gl-2.097810]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 14:49:06]
2023.01.26-03:49:27:764:[step-20300/177600: 11.43%]--[loss-3.256019: wl-4.528131, gl-2.123986]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 14:53:16]
2023.01.26-03:50:57:864:[step-20400/177600: 11.49%]--[loss-3.210452: wl-3.815306, gl-2.256625]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 16:22:23]
End of epoch 23 / 200 	 Time Taken: 799 sec
2023.01.26-03:52:28:76:[step-20500/177600: 11.54%]--[loss-3.109263: wl-3.977526, gl-2.114882]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 14:37:24]
2023.01.26-03:53:57:176:[step-20600/177600: 11.60%]--[loss-3.430829: wl-4.403619, gl-2.329924]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 14:35:05]
2023.01.26-03:55:27:276:[step-20700/177600: 11.66%]--[loss-3.485828: wl-4.727228, gl-2.304021]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 16:17:31]
2023.01.26-03:56:58:376:[step-20800/177600: 11.71%]--[loss-3.091365: wl-4.145663, gl-2.054950]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 14:37:25]
2023.01.26-03:58:28:476:[step-20900/177600: 11.77%]--[loss-3.071785: wl-3.927431, gl-2.089927]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 14:45:07]
2023.01.26-03:59:58:576:[step-21000/177600: 11.82%]--[loss-2.892832: wl-3.494622, gl-2.019176]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 15:46:59]
2023.01.26-04:01:28:676:[step-21100/177600: 11.88%]--[loss-3.087648: wl-3.812291, gl-2.134576]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 14:53:54]
2023.01.26-04:02:58:776:[step-21200/177600: 11.94%]--[loss-3.274313: wl-4.581599, gl-2.128913]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 14:46:46]
2023.01.26-04:04:28:876:[step-21300/177600: 11.99%]--[loss-3.137744: wl-4.002491, gl-2.137122]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 14:32:16]
End of epoch 24 / 200 	 Time Taken: 800 sec
2023.01.26-04:05:59:88:[step-21400/177600: 12.05%]--[loss-3.253714: wl-3.918421, gl-2.274109]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 14:12:18]
2023.01.26-04:07:29:188:[step-21500/177600: 12.11%]--[loss-3.002464: wl-3.982078, gl-2.006944]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 15:48:31]
2023.01.26-04:08:58:288:[step-21600/177600: 12.16%]--[loss-3.496089: wl-5.168085, gl-2.204067]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 14:16:38]
2023.01.26-04:10:28:388:[step-21700/177600: 12.22%]--[loss-3.499448: wl-4.508281, gl-2.372378]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 14:28:47]
2023.01.26-04:11:58:488:[step-21800/177600: 12.27%]--[loss-3.218484: wl-4.288484, gl-2.146363]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 14:27:54]
2023.01.26-04:13:27:588:[step-21900/177600: 12.33%]--[loss-3.252163: wl-4.186776, gl-2.205469]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 14:21:02]
2023.01.26-04:14:57:688:[step-22000/177600: 12.39%]--[loss-3.017316: wl-3.737433, gl-2.082957]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 16:04:51]
2023.01.26-04:16:26:788:[step-22100/177600: 12.44%]--[loss-3.024587: wl-3.549796, gl-2.137138]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 14:10:45]
2023.01.26-04:17:56:888:[step-22200/177600: 12.50%]--[loss-3.250457: wl-4.186465, gl-2.203841]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 14:07:26]
End of epoch 25 / 200 	 Time Taken: 796 sec
saving the model at the end of epoch 25, iters 22200
2023.01.26-04:19:27:100:[step-22300/177600: 12.56%]--[loss-3.543128: wl-4.940440, gl-2.308018]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 16:24:09]
2023.01.26-04:20:56:200:[step-22400/177600: 12.61%]--[loss-2.971900: wl-3.845953, gl-2.010411]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 14:53:30]
2023.01.26-04:22:26:300:[step-22500/177600: 12.67%]--[loss-2.944314: wl-3.934383, gl-1.960719]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 14:27:04]
2023.01.26-04:23:55:400:[step-22600/177600: 12.73%]--[loss-3.066528: wl-3.862238, gl-2.100968]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 14:45:28]
2023.01.26-04:25:25:500:[step-22700/177600: 12.78%]--[loss-3.036750: wl-3.868506, gl-2.069623]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 14:06:02]
2023.01.26-04:26:55:600:[step-22800/177600: 12.84%]--[loss-3.143543: wl-3.862176, gl-2.177999]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 14:31:27]
2023.01.26-04:28:25:700:[step-22900/177600: 12.89%]--[loss-3.247349: wl-3.965606, gl-2.255947]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 13:57:41]
2023.01.26-04:29:55:800:[step-23000/177600: 12.95%]--[loss-3.205879: wl-4.116074, gl-2.176860]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 14:50:11]
End of epoch 26 / 200 	 Time Taken: 797 sec
2023.01.26-04:31:26:12:[step-23100/177600: 13.01%]--[loss-3.450417: wl-5.038361, gl-2.190826]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 14:36:52]
2023.01.26-04:32:56:112:[step-23200/177600: 13.06%]--[loss-3.248772: wl-4.240628, gl-2.188615]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 14:02:46]
2023.01.26-04:34:27:212:[step-23300/177600: 13.12%]--[loss-3.008676: wl-3.880877, gl-2.038457]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 14:51:45]
2023.01.26-04:35:56:312:[step-23400/177600: 13.18%]--[loss-3.190530: wl-4.213258, gl-2.137215]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 14:12:37]
2023.01.26-04:37:26:412:[step-23500/177600: 13.23%]--[loss-3.357822: wl-4.339007, gl-2.273071]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 14:17:42]
2023.01.26-04:38:56:512:[step-23600/177600: 13.29%]--[loss-3.259596: wl-4.185118, gl-2.213317]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 16:02:47]
2023.01.26-04:40:26:612:[step-23700/177600: 13.34%]--[loss-3.218198: wl-4.234872, gl-2.159480]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 13:47:10]
2023.01.26-04:41:56:712:[step-23800/177600: 13.40%]--[loss-3.096052: wl-3.812635, gl-2.142893]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 15:06:54]
2023.01.26-04:43:26:812:[step-23900/177600: 13.46%]--[loss-3.045392: wl-3.812306, gl-2.092315]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 15:38:25]
End of epoch 27 / 200 	 Time Taken: 800 sec
2023.01.26-04:44:57:24:[step-24000/177600: 13.51%]--[loss-3.030646: wl-3.934778, gl-2.046952]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 13:47:14]
2023.01.26-04:46:27:124:[step-24100/177600: 13.57%]--[loss-3.600094: wl-5.058807, gl-2.335392]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 14:23:29]
2023.01.26-04:47:58:224:[step-24200/177600: 13.63%]--[loss-3.087142: wl-3.866715, gl-2.120463]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 13:40:48]
2023.01.26-04:49:28:324:[step-24300/177600: 13.68%]--[loss-3.312854: wl-3.997906, gl-2.313377]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 13:43:29]
2023.01.26-04:50:57:424:[step-24400/177600: 13.74%]--[loss-3.263381: wl-4.406415, gl-2.161777]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 13:45:12]
2023.01.26-04:52:27:524:[step-24500/177600: 13.80%]--[loss-3.210264: wl-4.782583, gl-2.014618]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 13:38:07]
2023.01.26-04:53:58:624:[step-24600/177600: 13.85%]--[loss-3.245472: wl-4.065029, gl-2.229215]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 13:35:03]
2023.01.26-04:55:28:724:[step-24700/177600: 13.91%]--[loss-3.448650: wl-4.402463, gl-2.348034]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 13:32:09]
2023.01.26-04:56:58:824:[step-24800/177600: 13.96%]--[loss-2.879169: wl-3.737754, gl-1.944731]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 13:35:45]
End of epoch 28 / 200 	 Time Taken: 800 sec
2023.01.26-04:58:29:36:[step-24900/177600: 14.02%]--[loss-3.091411: wl-3.821319, gl-2.136081]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 14:31:39]
2023.01.26-05:00:00:136:[step-25000/177600: 14.08%]--[loss-2.768042: wl-3.460791, gl-1.902844]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 15:19:49]
2023.01.26-05:01:30:236:[step-25100/177600: 14.13%]--[loss-2.971920: wl-3.880428, gl-2.001813]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 14:44:55]
2023.01.26-05:02:59:336:[step-25200/177600: 14.19%]--[loss-3.330033: wl-4.086774, gl-2.308340]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 14:59:20]
2023.01.26-05:04:29:436:[step-25300/177600: 14.25%]--[loss-3.145948: wl-3.948771, gl-2.158755]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 13:37:02]
2023.01.26-05:05:58:536:[step-25400/177600: 14.30%]--[loss-3.137368: wl-4.138353, gl-2.102779]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 13:42:22]
2023.01.26-05:07:27:636:[step-25500/177600: 14.36%]--[loss-3.012870: wl-3.720702, gl-2.082694]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 15:15:17]
2023.01.26-05:08:57:736:[step-25600/177600: 14.41%]--[loss-3.000649: wl-3.774824, gl-2.056943]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 13:36:40]
2023.01.26-05:10:27:836:[step-25700/177600: 14.47%]--[loss-3.136056: wl-4.357368, gl-2.046714]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 13:20:44]
End of epoch 29 / 200 	 Time Taken: 797 sec
2023.01.26-05:11:58:48:[step-25800/177600: 14.53%]--[loss-3.139114: wl-3.827368, gl-2.182272]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 13:18:30]
2023.01.26-05:13:28:148:[step-25900/177600: 14.58%]--[loss-3.086458: wl-4.135433, gl-2.052600]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 13:31:52]
2023.01.26-05:14:58:248:[step-26000/177600: 14.64%]--[loss-3.230425: wl-4.048608, gl-2.218273]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 13:14:54]
2023.01.26-05:16:29:348:[step-26100/177600: 14.70%]--[loss-3.105910: wl-4.155085, gl-2.067139]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 13:18:09]
2023.01.26-05:17:58:448:[step-26200/177600: 14.75%]--[loss-3.072663: wl-4.032732, gl-2.064480]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 13:52:24]
2023.01.26-05:19:28:548:[step-26300/177600: 14.81%]--[loss-2.938827: wl-3.591185, gl-2.041030]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 13:21:45]
2023.01.26-05:20:58:648:[step-26400/177600: 14.86%]--[loss-3.158001: wl-4.199297, gl-2.108177]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 13:27:53]
2023.01.26-05:22:28:748:[step-26500/177600: 14.92%]--[loss-3.076007: wl-4.425829, gl-1.969549]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 14:45:26]
2023.01.26-05:23:58:848:[step-26600/177600: 14.98%]--[loss-2.905879: wl-3.484829, gl-2.034671]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 13:06:26]
End of epoch 30 / 200 	 Time Taken: 800 sec
saving the model at the end of epoch 30, iters 26640
2023.01.26-05:25:29:60:[step-26700/177600: 15.03%]--[loss-2.997360: wl-3.886039, gl-2.025850]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 13:05:44]
2023.01.26-05:26:59:160:[step-26800/177600: 15.09%]--[loss-2.729865: wl-3.521650, gl-1.849452]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 14:48:37]
2023.01.26-05:28:29:260:[step-26900/177600: 15.15%]--[loss-3.133804: wl-3.882210, gl-2.163251]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 13:11:14]
2023.01.26-05:29:59:360:[step-27000/177600: 15.20%]--[loss-3.263146: wl-4.400801, gl-2.162946]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 14:48:18]
2023.01.26-05:31:29:460:[step-27100/177600: 15.26%]--[loss-3.242424: wl-4.157286, gl-2.203102]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 12:55:38]
2023.01.26-05:32:59:560:[step-27200/177600: 15.32%]--[loss-3.250520: wl-4.599223, gl-2.100715]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 13:10:41]
2023.01.26-05:34:28:660:[step-27300/177600: 15.37%]--[loss-3.099677: wl-3.962330, gl-2.109094]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 13:04:25]
2023.01.26-05:35:57:760:[step-27400/177600: 15.43%]--[loss-3.022996: wl-3.929809, gl-2.040544]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 13:01:00]
2023.01.26-05:37:28:860:[step-27500/177600: 15.48%]--[loss-3.196212: wl-4.256698, gl-2.132038]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 15:00:17]
End of epoch 31 / 200 	 Time Taken: 798 sec
2023.01.26-05:38:59:72:[step-27600/177600: 15.54%]--[loss-3.326789: wl-4.199596, gl-2.276890]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 12:53:10]
2023.01.26-05:40:29:172:[step-27700/177600: 15.60%]--[loss-3.040885: wl-4.154514, gl-2.002256]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 13:25:48]
2023.01.26-05:41:58:272:[step-27800/177600: 15.65%]--[loss-3.265657: wl-4.572337, gl-2.122573]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 14:31:53]
2023.01.26-05:43:28:372:[step-27900/177600: 15.71%]--[loss-2.949957: wl-3.554996, gl-2.061208]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 12:47:52]
2023.01.26-05:44:58:472:[step-28000/177600: 15.77%]--[loss-2.926517: wl-3.847930, gl-1.964535]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 12:52:17]
2023.01.26-05:46:28:572:[step-28100/177600: 15.82%]--[loss-3.150382: wl-4.049335, gl-2.138048]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 12:44:55]
2023.01.26-05:47:58:672:[step-28200/177600: 15.88%]--[loss-3.072411: wl-4.045876, gl-2.060942]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 12:50:51]
2023.01.26-05:49:28:772:[step-28300/177600: 15.93%]--[loss-2.829845: wl-3.675363, gl-1.911005]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 13:20:01]
2023.01.26-05:50:58:872:[step-28400/177600: 15.99%]--[loss-2.955801: wl-3.876040, gl-1.986791]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 12:40:22]
End of epoch 32 / 200 	 Time Taken: 799 sec
2023.01.26-05:52:29:84:[step-28500/177600: 16.05%]--[loss-3.247101: wl-4.909842, gl-2.019641]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 12:36:48]
2023.01.26-05:53:58:184:[step-28600/177600: 16.10%]--[loss-3.192519: wl-4.040435, gl-2.182410]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 14:11:42]
2023.01.26-05:55:28:284:[step-28700/177600: 16.16%]--[loss-2.831644: wl-3.672260, gl-1.913579]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 12:28:52]
2023.01.26-05:56:58:384:[step-28800/177600: 16.22%]--[loss-3.179509: wl-4.736717, gl-1.995330]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 13:16:18]
2023.01.26-05:58:30:484:[step-28900/177600: 16.27%]--[loss-2.746229: wl-3.374346, gl-1.902643]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 12:43:03]
2023.01.26-05:59:59:584:[step-29000/177600: 16.33%]--[loss-2.898115: wl-3.775674, gl-1.954197]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 12:49:59]
2023.01.26-06:01:29:684:[step-29100/177600: 16.39%]--[loss-3.330813: wl-4.701778, gl-2.155368]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 14:07:42]
2023.01.26-06:02:59:784:[step-29200/177600: 16.44%]--[loss-3.182733: wl-4.126315, gl-2.151155]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 12:33:50]
2023.01.26-06:04:29:884:[step-29300/177600: 16.50%]--[loss-2.980578: wl-3.759964, gl-2.040587]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 12:12:58]
End of epoch 33 / 200 	 Time Taken: 799 sec
2023.01.26-06:06:00:96:[step-29400/177600: 16.55%]--[loss-3.374265: wl-4.618278, gl-2.219696]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 13:51:55]
2023.01.26-06:07:30:196:[step-29500/177600: 16.61%]--[loss-2.775726: wl-3.702020, gl-1.850221]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 13:14:46]
2023.01.26-06:09:00:296:[step-29600/177600: 16.67%]--[loss-3.003533: wl-3.705040, gl-2.077273]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 12:24:27]
2023.01.26-06:10:30:396:[step-29700/177600: 16.72%]--[loss-2.848548: wl-3.521020, gl-1.968293]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 12:23:36]
2023.01.26-06:12:00:496:[step-29800/177600: 16.78%]--[loss-2.977324: wl-3.967881, gl-1.985353]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 12:59:25]
2023.01.26-06:13:30:596:[step-29900/177600: 16.84%]--[loss-3.174550: wl-4.319592, gl-2.094652]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 12:55:02]
2023.01.26-06:15:00:696:[step-30000/177600: 16.89%]--[loss-2.955055: wl-3.704793, gl-2.028856]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 12:21:15]
2023.01.26-06:16:30:796:[step-30100/177600: 16.95%]--[loss-2.908313: wl-3.738808, gl-1.973611]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 12:17:43]
End of epoch 34 / 200 	 Time Taken: 800 sec
2023.01.26-06:18:02:8:[step-30200/177600: 17.00%]--[loss-2.954286: wl-3.641065, gl-2.044019]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 12:09:26]
2023.01.26-06:19:32:108:[step-30300/177600: 17.06%]--[loss-2.851146: wl-3.747863, gl-1.914180]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 12:46:36]
2023.01.26-06:21:02:208:[step-30400/177600: 17.12%]--[loss-2.888027: wl-3.727766, gl-1.956085]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 12:15:05]
2023.01.26-06:22:34:308:[step-30500/177600: 17.17%]--[loss-2.849059: wl-3.736887, gl-1.914838]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 12:26:32]
2023.01.26-06:24:04:408:[step-30600/177600: 17.23%]--[loss-2.990139: wl-3.976229, gl-1.996082]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 12:24:59]
2023.01.26-06:25:35:508:[step-30700/177600: 17.29%]--[loss-2.944176: wl-3.884589, gl-1.973029]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 12:32:17]
2023.01.26-06:27:06:608:[step-30800/177600: 17.34%]--[loss-2.943006: wl-3.664039, gl-2.026997]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 13:16:30]
2023.01.26-06:28:37:708:[step-30900/177600: 17.40%]--[loss-2.998542: wl-4.000587, gl-1.998395]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 12:18:25]
2023.01.26-06:30:08:808:[step-31000/177600: 17.45%]--[loss-3.056342: wl-3.780853, gl-2.111129]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 13:20:12]
End of epoch 35 / 200 	 Time Taken: 808 sec
saving the model at the end of epoch 35, iters 31080
2023.01.26-06:31:42:20:[step-31100/177600: 17.51%]--[loss-3.313665: wl-4.828218, gl-2.106611]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 12:00:44]
2023.01.26-06:33:12:120:[step-31200/177600: 17.57%]--[loss-2.945102: wl-4.000240, gl-1.945042]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 12:09:25]
2023.01.26-06:34:42:220:[step-31300/177600: 17.62%]--[loss-2.990847: wl-4.043274, gl-1.980029]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 12:22:53]
2023.01.26-06:36:12:320:[step-31400/177600: 17.68%]--[loss-2.873118: wl-3.845763, gl-1.911677]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 11:52:26]
2023.01.26-06:37:41:420:[step-31500/177600: 17.74%]--[loss-2.941950: wl-3.708411, gl-2.014847]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 12:11:25]
2023.01.26-06:39:11:520:[step-31600/177600: 17.79%]--[loss-2.934557: wl-3.822567, gl-1.978916]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 11:57:14]
2023.01.26-06:40:41:620:[step-31700/177600: 17.85%]--[loss-2.884788: wl-3.619165, gl-1.979997]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 13:17:09]
2023.01.26-06:42:11:720:[step-31800/177600: 17.91%]--[loss-2.978639: wl-3.911614, gl-2.000736]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 11:45:22]
2023.01.26-06:43:41:820:[step-31900/177600: 17.96%]--[loss-3.286221: wl-4.548558, gl-2.149081]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 13:26:47]
End of epoch 36 / 200 	 Time Taken: 799 sec
2023.01.26-06:45:12:32:[step-32000/177600: 18.02%]--[loss-3.301537: wl-4.356673, gl-2.212368]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 11:42:28]
2023.01.26-06:46:42:132:[step-32100/177600: 18.07%]--[loss-3.197879: wl-4.668886, gl-2.030657]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 11:48:52]
2023.01.26-06:48:12:232:[step-32200/177600: 18.13%]--[loss-3.029366: wl-4.180785, gl-1.984170]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 12:44:11]
2023.01.26-06:49:42:332:[step-32300/177600: 18.19%]--[loss-3.053986: wl-4.003787, gl-2.053040]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 12:10:17]
2023.01.26-06:51:12:432:[step-32400/177600: 18.24%]--[loss-2.806262: wl-3.981787, gl-1.810815]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 11:46:27]
2023.01.26-06:52:42:532:[step-32500/177600: 18.30%]--[loss-3.181987: wl-4.937871, gl-1.947519]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 12:05:26]
2023.01.26-06:54:13:632:[step-32600/177600: 18.36%]--[loss-2.848543: wl-3.780139, gl-1.903508]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 11:47:11]
2023.01.26-06:55:43:732:[step-32700/177600: 18.41%]--[loss-2.935835: wl-3.717935, gl-2.006351]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 11:34:30]
2023.01.26-06:57:13:832:[step-32800/177600: 18.47%]--[loss-3.085057: wl-4.283842, gl-2.014096]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 11:58:34]
End of epoch 37 / 200 	 Time Taken: 802 sec
2023.01.26-06:58:45:44:[step-32900/177600: 18.52%]--[loss-2.889836: wl-3.587089, gl-1.993063]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 12:27:17]
2023.01.26-07:00:14:144:[step-33000/177600: 18.58%]--[loss-3.110657: wl-3.948285, gl-2.123586]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 11:50:58]
2023.01.26-07:01:44:244:[step-33100/177600: 18.64%]--[loss-3.128023: wl-4.195755, gl-2.079084]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 11:52:50]
2023.01.26-07:03:13:344:[step-33200/177600: 18.69%]--[loss-2.870508: wl-3.729046, gl-1.938246]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 13:06:06]
2023.01.26-07:04:43:444:[step-33300/177600: 18.75%]--[loss-3.196887: wl-4.209772, gl-2.144444]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 11:26:24]
2023.01.26-07:06:12:544:[step-33400/177600: 18.81%]--[loss-2.896880: wl-3.882807, gl-1.926179]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 11:50:36]
2023.01.26-07:07:42:644:[step-33500/177600: 18.86%]--[loss-3.149873: wl-4.114067, gl-2.121356]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 12:29:54]
2023.01.26-07:09:12:744:[step-33600/177600: 18.92%]--[loss-2.867006: wl-3.635667, gl-1.958090]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 11:48:43]
2023.01.26-07:10:42:844:[step-33700/177600: 18.98%]--[loss-3.136694: wl-4.615600, gl-1.982794]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 11:26:06]
End of epoch 38 / 200 	 Time Taken: 797 sec
2023.01.26-07:12:13:56:[step-33800/177600: 19.03%]--[loss-3.144158: wl-4.384248, gl-2.048096]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 11:12:38]
2023.01.26-07:13:43:156:[step-33900/177600: 19.09%]--[loss-2.986205: wl-3.884196, gl-2.015156]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 11:08:26]
2023.01.26-07:15:13:256:[step-34000/177600: 19.14%]--[loss-3.131578: wl-4.352923, gl-2.043347]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 11:09:58]
2023.01.26-07:16:43:356:[step-34100/177600: 19.20%]--[loss-3.056816: wl-4.246325, gl-1.995234]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 11:18:30]
2023.01.26-07:18:12:456:[step-34200/177600: 19.26%]--[loss-3.075181: wl-4.255606, gl-2.011279]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 11:16:42]
2023.01.26-07:19:42:556:[step-34300/177600: 19.31%]--[loss-3.017009: wl-3.855355, gl-2.053170]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 13:41:28]
2023.01.26-07:21:12:656:[step-34400/177600: 19.37%]--[loss-2.960468: wl-3.943283, gl-1.974648]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 11:35:07]
2023.01.26-07:22:42:756:[step-34500/177600: 19.43%]--[loss-3.221984: wl-4.407644, gl-2.120073]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 12:38:27]
2023.01.26-07:24:11:856:[step-34600/177600: 19.48%]--[loss-2.725408: wl-3.441970, gl-1.864915]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 11:17:16]
End of epoch 39 / 200 	 Time Taken: 798 sec
2023.01.26-07:25:43:68:[step-34700/177600: 19.54%]--[loss-2.792973: wl-3.549325, gl-1.905641]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 11:23:11]
2023.01.26-07:27:13:168:[step-34800/177600: 19.59%]--[loss-2.795142: wl-3.683810, gl-1.874190]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 12:32:37]
2023.01.26-07:28:43:268:[step-34900/177600: 19.65%]--[loss-2.963972: wl-3.950853, gl-1.976259]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 11:33:33]
2023.01.26-07:30:13:368:[step-35000/177600: 19.71%]--[loss-2.949230: wl-3.862858, gl-1.983516]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 11:27:30]
2023.01.26-07:31:43:468:[step-35100/177600: 19.76%]--[loss-2.958724: wl-3.883667, gl-1.987807]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 10:59:18]
2023.01.26-07:33:13:568:[step-35200/177600: 19.82%]--[loss-3.110869: wl-4.640729, gl-1.950687]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 11:23:22]
2023.01.26-07:34:43:668:[step-35300/177600: 19.88%]--[loss-2.837640: wl-3.824326, gl-1.881559]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 10:57:27]
2023.01.26-07:36:12:768:[step-35400/177600: 19.93%]--[loss-2.824375: wl-3.941255, gl-1.839061]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 10:57:35]
2023.01.26-07:37:42:868:[step-35500/177600: 19.99%]--[loss-2.757350: wl-3.955741, gl-1.768415]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 10:57:39]
End of epoch 40 / 200 	 Time Taken: 799 sec
saving the model at the end of epoch 40, iters 35520
2023.01.26-07:39:13:80:[step-35600/177600: 20.05%]--[loss-3.245904: wl-4.548854, gl-2.108691]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 10:47:58]
2023.01.26-07:40:44:180:[step-35700/177600: 20.10%]--[loss-2.781792: wl-3.786587, gl-1.835146]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 11:07:51]
2023.01.26-07:42:15:280:[step-35800/177600: 20.16%]--[loss-3.076816: wl-4.084177, gl-2.055772]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 11:13:12]
2023.01.26-07:43:45:380:[step-35900/177600: 20.21%]--[loss-2.841742: wl-3.661627, gl-1.926335]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 10:57:38]
2023.01.26-07:45:14:480:[step-36000/177600: 20.27%]--[loss-3.097213: wl-4.661102, gl-1.931937]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 10:55:35]
2023.01.26-07:46:45:580:[step-36100/177600: 20.33%]--[loss-3.115764: wl-4.613373, gl-1.962420]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 12:10:10]
2023.01.26-07:48:16:680:[step-36200/177600: 20.38%]--[loss-3.237845: wl-4.865532, gl-2.021462]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 10:53:41]
2023.01.26-07:49:46:780:[step-36300/177600: 20.44%]--[loss-2.960569: wl-3.894768, gl-1.986877]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 11:49:02]
2023.01.26-07:51:17:880:[step-36400/177600: 20.50%]--[loss-2.922952: wl-3.678759, gl-2.003263]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 11:53:35]
End of epoch 41 / 200 	 Time Taken: 803 sec
2023.01.26-07:52:48:92:[step-36500/177600: 20.55%]--[loss-3.225326: wl-4.595225, gl-2.076519]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 10:45:11]
2023.01.26-07:54:17:192:[step-36600/177600: 20.61%]--[loss-2.730551: wl-3.701965, gl-1.805059]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 10:48:02]
2023.01.26-07:55:47:292:[step-36700/177600: 20.66%]--[loss-2.889135: wl-3.920819, gl-1.908931]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 12:04:13]
2023.01.26-07:57:16:392:[step-36800/177600: 20.72%]--[loss-2.821350: wl-3.519228, gl-1.941543]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 10:39:54]
2023.01.26-07:58:46:492:[step-36900/177600: 20.78%]--[loss-2.747040: wl-3.531991, gl-1.864042]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 10:47:07]
2023.01.26-08:00:16:592:[step-37000/177600: 20.83%]--[loss-2.794093: wl-3.646671, gl-1.882425]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 10:27:37]
2023.01.26-08:01:45:692:[step-37100/177600: 20.89%]--[loss-3.011630: wl-3.971519, gl-2.018750]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 10:25:13]
2023.01.26-08:03:14:792:[step-37200/177600: 20.95%]--[loss-3.222955: wl-4.773622, gl-2.029549]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 10:29:54]
End of epoch 42 / 200 	 Time Taken: 796 sec
2023.01.26-08:04:45:4:[step-37300/177600: 21.00%]--[loss-3.148736: wl-4.369424, gl-2.056380]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 10:36:49]
2023.01.26-08:06:15:104:[step-37400/177600: 21.06%]--[loss-2.980793: wl-4.020940, gl-1.975559]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 13:13:45]
2023.01.26-08:07:44:204:[step-37500/177600: 21.11%]--[loss-2.827966: wl-3.714771, gl-1.899274]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 10:43:46]
2023.01.26-08:09:14:304:[step-37600/177600: 21.17%]--[loss-3.040953: wl-3.988820, gl-2.043748]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 11:00:49]
2023.01.26-08:10:43:404:[step-37700/177600: 21.23%]--[loss-2.783417: wl-3.323968, gl-1.952425]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 11:44:17]
2023.01.26-08:12:13:504:[step-37800/177600: 21.28%]--[loss-2.869250: wl-3.886526, gl-1.897618]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 11:36:38]
2023.01.26-08:13:42:604:[step-37900/177600: 21.34%]--[loss-3.057392: wl-4.201733, gl-2.006959]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 10:38:10]
2023.01.26-08:15:12:704:[step-38000/177600: 21.40%]--[loss-3.002429: wl-3.946829, gl-2.015722]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 10:50:25]
2023.01.26-08:16:41:804:[step-38100/177600: 21.45%]--[loss-2.968589: wl-3.833352, gl-2.010251]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 11:24:36]
End of epoch 43 / 200 	 Time Taken: 796 sec
2023.01.26-08:18:13:16:[step-38200/177600: 21.51%]--[loss-2.952406: wl-3.727493, gl-2.020533]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 10:50:52]
2023.01.26-08:19:42:116:[step-38300/177600: 21.57%]--[loss-2.743989: wl-3.490022, gl-1.871483]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 10:43:19]
2023.01.26-08:21:12:216:[step-38400/177600: 21.62%]--[loss-2.975720: wl-4.276375, gl-1.906626]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 10:16:19]
2023.01.26-08:22:43:316:[step-38500/177600: 21.68%]--[loss-2.897197: wl-3.578241, gl-2.002637]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 10:24:58]
2023.01.26-08:24:13:416:[step-38600/177600: 21.73%]--[loss-2.836931: wl-3.700034, gl-1.911923]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 10:36:41]
2023.01.26-08:25:43:516:[step-38700/177600: 21.79%]--[loss-2.835764: wl-3.904627, gl-1.859608]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 10:46:23]
2023.01.26-08:27:13:616:[step-38800/177600: 21.85%]--[loss-2.814324: wl-3.569622, gl-1.921918]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 10:13:43]
2023.01.26-08:28:42:716:[step-38900/177600: 21.90%]--[loss-2.836375: wl-3.895221, gl-1.862570]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 10:08:31]
2023.01.26-08:30:12:816:[step-39000/177600: 21.96%]--[loss-2.970636: wl-4.093843, gl-1.947175]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 11:30:39]
End of epoch 44 / 200 	 Time Taken: 799 sec
2023.01.26-08:31:44:28:[step-39100/177600: 22.02%]--[loss-2.902817: wl-3.782239, gl-1.957257]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 10:03:52]
2023.01.26-08:33:14:128:[step-39200/177600: 22.07%]--[loss-2.822735: wl-3.747196, gl-1.885936]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 10:48:58]
2023.01.26-08:34:44:228:[step-39300/177600: 22.13%]--[loss-3.074173: wl-4.293936, gl-2.000689]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 11:24:52]
2023.01.26-08:36:13:328:[step-39400/177600: 22.18%]--[loss-2.842290: wl-3.903201, gl-1.866490]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 10:02:18]
2023.01.26-08:37:43:428:[step-39500/177600: 22.24%]--[loss-3.181956: wl-4.896524, gl-1.957825]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 10:01:07]
2023.01.26-08:39:12:528:[step-39600/177600: 22.30%]--[loss-2.781679: wl-3.530594, gl-1.899031]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 10:05:19]
2023.01.26-08:40:42:628:[step-39700/177600: 22.35%]--[loss-2.950080: wl-4.150335, gl-1.912496]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 9:59:21]
2023.01.26-08:42:12:728:[step-39800/177600: 22.41%]--[loss-2.780972: wl-3.761604, gl-1.840571]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 9:55:34]
2023.01.26-08:43:41:828:[step-39900/177600: 22.47%]--[loss-3.126403: wl-4.206982, gl-2.074658]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 10:23:49]
End of epoch 45 / 200 	 Time Taken: 798 sec
saving the model at the end of epoch 45, iters 39960
2023.01.26-08:45:13:40:[step-40000/177600: 22.52%]--[loss-3.000211: wl-4.097191, gl-1.975914]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 9:44:07]
2023.01.26-08:46:43:140:[step-40100/177600: 22.58%]--[loss-2.899098: wl-3.851731, gl-1.936166]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 9:45:23]
2023.01.26-08:48:13:240:[step-40200/177600: 22.64%]--[loss-2.928330: wl-4.271417, gl-1.860476]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 9:44:16]
2023.01.26-08:49:44:340:[step-40300/177600: 22.69%]--[loss-2.752899: wl-3.646134, gl-1.841366]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 10:24:10]
2023.01.26-08:51:14:440:[step-40400/177600: 22.75%]--[loss-3.113453: wl-4.430995, gl-2.005704]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 9:35:45]
2023.01.26-08:52:43:540:[step-40500/177600: 22.80%]--[loss-2.908282: wl-3.674526, gl-1.989650]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 9:39:21]
2023.01.26-08:54:13:640:[step-40600/177600: 22.86%]--[loss-2.761463: wl-3.594418, gl-1.862858]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 11:55:04]
2023.01.26-08:55:44:740:[step-40700/177600: 22.92%]--[loss-2.971297: wl-3.829503, gl-2.013921]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 9:43:38]
2023.01.26-08:57:14:840:[step-40800/177600: 22.97%]--[loss-2.884141: wl-3.900019, gl-1.909136]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 9:42:57]
End of epoch 46 / 200 	 Time Taken: 801 sec
2023.01.26-08:58:44:52:[step-40900/177600: 23.03%]--[loss-2.981412: wl-3.996706, gl-1.982236]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 10:52:04]
2023.01.26-09:00:14:152:[step-41000/177600: 23.09%]--[loss-2.773086: wl-3.686568, gl-1.851444]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 9:59:58]
2023.01.26-09:01:45:252:[step-41100/177600: 23.14%]--[loss-2.765373: wl-3.704592, gl-1.839226]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 9:34:23]
2023.01.26-09:03:16:352:[step-41200/177600: 23.20%]--[loss-3.042860: wl-3.998317, gl-2.043281]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 14:29:23]
2023.01.26-09:04:46:452:[step-41300/177600: 23.25%]--[loss-2.936427: wl-3.829814, gl-1.978974]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 9:30:25]
2023.01.26-09:06:16:552:[step-41400/177600: 23.31%]--[loss-2.826737: wl-3.652525, gl-1.913606]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 9:34:49]
2023.01.26-09:07:46:652:[step-41500/177600: 23.37%]--[loss-2.857474: wl-3.838686, gl-1.897802]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 10:29:32]
2023.01.26-09:09:16:752:[step-41600/177600: 23.42%]--[loss-3.077728: wl-4.955940, gl-1.838743]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 9:44:22]
2023.01.26-09:10:47:852:[step-41700/177600: 23.48%]--[loss-2.721570: wl-3.680233, gl-1.801512]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 9:16:08]
End of epoch 47 / 200 	 Time Taken: 802 sec
2023.01.26-09:12:18:64:[step-41800/177600: 23.54%]--[loss-2.794063: wl-3.653452, gl-1.880700]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 9:21:24]
2023.01.26-09:13:49:164:[step-41900/177600: 23.59%]--[loss-2.778624: wl-3.771780, gl-1.835679]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 9:35:24]
2023.01.26-09:15:19:264:[step-42000/177600: 23.65%]--[loss-2.614231: wl-3.338764, gl-1.779540]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 9:47:32]
2023.01.26-09:16:50:364:[step-42100/177600: 23.70%]--[loss-2.923311: wl-4.269715, gl-1.855882]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 9:54:50]
2023.01.26-09:18:20:464:[step-42200/177600: 23.76%]--[loss-2.836184: wl-3.863419, gl-1.870329]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 9:37:43]
2023.01.26-09:19:51:564:[step-42300/177600: 23.82%]--[loss-2.867212: wl-3.927249, gl-1.885400]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 10:28:13]
2023.01.26-09:21:22:664:[step-42400/177600: 23.87%]--[loss-2.884372: wl-4.356503, gl-1.795247]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 9:14:19]
2023.01.26-09:22:52:764:[step-42500/177600: 23.93%]--[loss-2.857624: wl-3.660301, gl-1.942549]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 9:07:41]
2023.01.26-09:24:23:864:[step-42600/177600: 23.99%]--[loss-2.709686: wl-3.500484, gl-1.834565]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 9:06:14]
End of epoch 48 / 200 	 Time Taken: 805 sec
2023.01.26-09:25:55:76:[step-42700/177600: 24.04%]--[loss-2.854939: wl-3.926080, gl-1.873419]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 9:23:01]
2023.01.26-09:27:25:176:[step-42800/177600: 24.10%]--[loss-3.165406: wl-4.400825, gl-2.065199]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 9:30:55]
2023.01.26-09:28:55:276:[step-42900/177600: 24.16%]--[loss-3.038220: wl-4.116306, gl-2.009143]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 9:24:27]
2023.01.26-09:30:25:376:[step-43000/177600: 24.21%]--[loss-2.949363: wl-4.124098, gl-1.918339]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 9:20:00]
2023.01.26-09:31:56:476:[step-43100/177600: 24.27%]--[loss-3.096040: wl-4.048705, gl-2.083864]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 10:23:39]
2023.01.26-09:33:28:576:[step-43200/177600: 24.32%]--[loss-2.782769: wl-3.790280, gl-1.835199]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 9:19:32]
2023.01.26-09:34:59:676:[step-43300/177600: 24.38%]--[loss-2.982243: wl-3.938365, gl-1.997651]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 9:17:53]
2023.01.26-09:36:30:776:[step-43400/177600: 24.44%]--[loss-3.005206: wl-3.949113, gl-2.017928]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 9:17:33]
2023.01.26-09:38:00:876:[step-43500/177600: 24.49%]--[loss-2.756789: wl-3.825665, gl-1.800373]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 9:21:02]
End of epoch 49 / 200 	 Time Taken: 806 sec
2023.01.26-09:39:31:88:[step-43600/177600: 24.55%]--[loss-3.042142: wl-3.688855, gl-2.119928]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 8:54:19]
2023.01.26-09:41:01:188:[step-43700/177600: 24.61%]--[loss-2.865998: wl-3.490610, gl-1.993345]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 9:38:10]
2023.01.26-09:42:31:288:[step-43800/177600: 24.66%]--[loss-2.937836: wl-3.723959, gl-2.006846]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 8:55:03]
2023.01.26-09:44:00:388:[step-43900/177600: 24.72%]--[loss-3.284102: wl-4.319434, gl-2.204244]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 8:54:49]
2023.01.26-09:45:30:488:[step-44000/177600: 24.77%]--[loss-3.306175: wl-4.596277, gl-2.157106]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 8:55:25]
2023.01.26-09:47:00:588:[step-44100/177600: 24.83%]--[loss-3.079115: wl-3.801797, gl-2.128666]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 9:15:01]
2023.01.26-09:48:29:688:[step-44200/177600: 24.89%]--[loss-2.894852: wl-4.123855, gl-1.863889]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 9:06:42]
2023.01.26-09:50:00:788:[step-44300/177600: 24.94%]--[loss-3.235493: wl-4.368933, gl-2.143260]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 10:13:43]
2023.01.26-09:51:29:888:[step-44400/177600: 25.00%]--[loss-3.503397: wl-4.963783, gl-2.262452]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 8:48:42]
End of epoch 50 / 200 	 Time Taken: 798 sec
saving the model at the end of epoch 50, iters 44400
2023.01.26-09:53:01:100:[step-44500/177600: 25.06%]--[loss-3.042612: wl-4.030873, gl-2.034894]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 9:08:58]
2023.01.26-09:54:31:200:[step-44600/177600: 25.11%]--[loss-3.099087: wl-3.965868, gl-2.107620]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 10:07:39]
2023.01.26-09:56:01:300:[step-44700/177600: 25.17%]--[loss-3.115915: wl-4.343552, gl-2.030027]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 8:42:13]
2023.01.26-09:57:30:400:[step-44800/177600: 25.23%]--[loss-3.061646: wl-3.594949, gl-2.162909]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 9:20:03]
2023.01.26-09:59:00:500:[step-44900/177600: 25.28%]--[loss-3.043114: wl-4.032383, gl-2.035018]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 8:33:06]
2023.01.26-10:00:31:600:[step-45000/177600: 25.34%]--[loss-2.894055: wl-3.585255, gl-1.997742]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 9:55:05]
2023.01.26-10:02:02:700:[step-45100/177600: 25.39%]--[loss-3.093982: wl-4.189913, gl-2.046504]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 8:40:57]
2023.01.26-10:03:33:800:[step-45200/177600: 25.45%]--[loss-2.975546: wl-4.105467, gl-1.949179]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 10:30:46]
End of epoch 51 / 200 	 Time Taken: 807 sec
2023.01.26-10:05:12:12:[step-45300/177600: 25.51%]--[loss-3.271382: wl-4.153129, gl-2.233100]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 9:53:54]
2023.01.26-10:06:49:112:[step-45400/177600: 25.56%]--[loss-3.129826: wl-4.298994, gl-2.055078]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 10:22:59]
2023.01.26-10:08:26:212:[step-45500/177600: 25.62%]--[loss-3.016347: wl-4.199613, gl-1.966444]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 8:50:04]
2023.01.26-10:09:55:312:[step-45600/177600: 25.68%]--[loss-3.232144: wl-4.498443, gl-2.107533]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 8:27:10]
2023.01.26-10:11:25:412:[step-45700/177600: 25.73%]--[loss-3.076636: wl-3.847606, gl-2.114734]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 8:43:46]
2023.01.26-10:12:55:512:[step-45800/177600: 25.79%]--[loss-3.134780: wl-4.310543, gl-2.057145]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 9:57:25]
2023.01.26-10:14:24:612:[step-45900/177600: 25.84%]--[loss-3.253744: wl-4.190238, gl-2.206184]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 8:26:04]
2023.01.26-10:15:54:712:[step-46000/177600: 25.90%]--[loss-3.292921: wl-4.148125, gl-2.255890]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 8:26:55]
2023.01.26-10:17:23:812:[step-46100/177600: 25.96%]--[loss-3.140818: wl-3.833973, gl-2.182324]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 10:04:39]
End of epoch 52 / 200 	 Time Taken: 816 sec
2023.01.26-10:19:00:24:[step-46200/177600: 26.01%]--[loss-3.084504: wl-3.914050, gl-2.105992]--[lr: pb-0.000050, pf-0.000049]--[ETA-1 day, 11:16:28]
2023.01.26-10:20:36:124:[step-46300/177600: 26.07%]--[loss-2.794092: wl-3.601646, gl-1.893680]--[lr: pb-0.000050, pf-0.000049]--[ETA-1 day, 9:56:51]
2023.01.26-10:22:14:224:[step-46400/177600: 26.13%]--[loss-3.014350: wl-4.401972, gl-1.913857]--[lr: pb-0.000050, pf-0.000049]--[ETA-1 day, 13:47:34]
2023.01.26-10:23:48:324:[step-46500/177600: 26.18%]--[loss-3.069063: wl-4.003444, gl-2.068202]--[lr: pb-0.000050, pf-0.000049]--[ETA-1 day, 8:27:40]
2023.01.26-10:25:18:424:[step-46600/177600: 26.24%]--[loss-3.228742: wl-4.070620, gl-2.211087]--[lr: pb-0.000050, pf-0.000049]--[ETA-1 day, 8:27:31]
2023.01.26-10:26:48:524:[step-46700/177600: 26.30%]--[loss-3.051900: wl-3.890457, gl-2.079286]--[lr: pb-0.000050, pf-0.000049]--[ETA-1 day, 10:05:42]
2023.01.26-10:28:20:624:[step-46800/177600: 26.35%]--[loss-2.926050: wl-3.580366, gl-2.030959]--[lr: pb-0.000050, pf-0.000049]--[ETA-1 day, 11:52:52]
2023.01.26-10:29:56:724:[step-46900/177600: 26.41%]--[loss-3.224154: wl-4.701694, gl-2.048731]--[lr: pb-0.000050, pf-0.000049]--[ETA-1 day, 9:27:39]
2023.01.26-10:31:32:824:[step-47000/177600: 26.46%]--[loss-3.411814: wl-5.109822, gl-2.134359]--[lr: pb-0.000050, pf-0.000049]--[ETA-1 day, 12:41:23]
End of epoch 53 / 200 	 Time Taken: 840 sec
2023.01.26-10:33:11:36:[step-47100/177600: 26.52%]--[loss-2.841720: wl-3.807925, gl-1.889739]--[lr: pb-0.000050, pf-0.000049]--[ETA-1 day, 8:02:40]
2023.01.26-10:34:41:136:[step-47200/177600: 26.58%]--[loss-3.091459: wl-3.886733, gl-2.119776]--[lr: pb-0.000050, pf-0.000049]--[ETA-1 day, 8:00:04]
2023.01.26-10:36:13:236:[step-47300/177600: 26.63%]--[loss-2.845646: wl-3.583717, gl-1.949716]--[lr: pb-0.000050, pf-0.000049]--[ETA-1 day, 10:03:42]
2023.01.26-10:37:49:336:[step-47400/177600: 26.69%]--[loss-3.123005: wl-3.897108, gl-2.148728]--[lr: pb-0.000050, pf-0.000049]--[ETA-1 day, 9:57:45]
2023.01.26-10:39:26:436:[step-47500/177600: 26.75%]--[loss-3.055969: wl-3.770420, gl-2.113364]--[lr: pb-0.000050, pf-0.000049]--[ETA-1 day, 10:57:32]
2023.01.26-10:41:04:536:[step-47600/177600: 26.80%]--[loss-3.035676: wl-4.340343, gl-1.950590]--[lr: pb-0.000050, pf-0.000049]--[ETA-1 day, 7:59:02]
2023.01.26-10:42:34:636:[step-47700/177600: 26.86%]--[loss-3.110375: wl-4.764437, gl-1.919266]--[lr: pb-0.000050, pf-0.000049]--[ETA-1 day, 8:08:56]
2023.01.26-10:44:04:736:[step-47800/177600: 26.91%]--[loss-3.048902: wl-3.945846, gl-2.062441]--[lr: pb-0.000050, pf-0.000049]--[ETA-1 day, 7:48:07]
2023.01.26-10:45:40:836:[step-47900/177600: 26.97%]--[loss-3.070499: wl-4.187155, gl-2.023710]--[lr: pb-0.000050, pf-0.000049]--[ETA-1 day, 10:52:24]
End of epoch 54 / 200 	 Time Taken: 835 sec
2023.01.26-10:47:19:48:[step-48000/177600: 27.03%]--[loss-2.693931: wl-3.461885, gl-1.828459]--[lr: pb-0.000050, pf-0.000049]--[ETA-1 day, 10:38:44]
2023.01.26-10:48:58:148:[step-48100/177600: 27.08%]--[loss-2.900863: wl-3.783579, gl-1.954968]--[lr: pb-0.000050, pf-0.000049]--[ETA-1 day, 21:10:26]
2023.01.26-10:50:30:248:[step-48200/177600: 27.14%]--[loss-2.992775: wl-3.683607, gl-2.071874]--[lr: pb-0.000050, pf-0.000049]--[ETA-1 day, 7:48:26]
2023.01.26-10:52:00:348:[step-48300/177600: 27.20%]--[loss-3.095478: wl-3.933783, gl-2.112032]--[lr: pb-0.000050, pf-0.000049]--[ETA-1 day, 10:00:24]
2023.01.26-10:53:36:448:[step-48400/177600: 27.25%]--[loss-3.232119: wl-3.998385, gl-2.232522]--[lr: pb-0.000050, pf-0.000049]--[ETA-1 day, 12:29:46]
2023.01.26-10:55:13:548:[step-48500/177600: 27.31%]--[loss-3.123319: wl-4.374231, gl-2.029761]--[lr: pb-0.000050, pf-0.000049]--[ETA-1 day, 10:01:45]
2023.01.26-10:56:50:648:[step-48600/177600: 27.36%]--[loss-2.823541: wl-3.709870, gl-1.896073]--[lr: pb-0.000050, pf-0.000049]--[ETA-1 day, 10:43:16]
2023.01.26-10:58:22:748:[step-48700/177600: 27.42%]--[loss-3.077087: wl-4.113064, gl-2.048821]--[lr: pb-0.000050, pf-0.000049]--[ETA-1 day, 9:57:51]
2023.01.26-10:59:51:848:[step-48800/177600: 27.48%]--[loss-3.141183: wl-4.524197, gl-2.010134]--[lr: pb-0.000050, pf-0.000049]--[ETA-1 day, 7:45:37]
End of epoch 55 / 200 	 Time Taken: 836 sec
saving the model at the end of epoch 55, iters 48840
2023.01.26-11:01:23:60:[step-48900/177600: 27.53%]--[loss-3.006665: wl-3.881465, gl-2.036299]--[lr: pb-0.000050, pf-0.000048]--[ETA-1 day, 7:39:59]
2023.01.26-11:02:52:160:[step-49000/177600: 27.59%]--[loss-3.105472: wl-4.383283, gl-2.009652]--[lr: pb-0.000050, pf-0.000048]--[ETA-1 day, 7:35:00]
2023.01.26-11:04:22:260:[step-49100/177600: 27.65%]--[loss-3.005284: wl-3.500435, gl-2.130176]--[lr: pb-0.000050, pf-0.000048]--[ETA-1 day, 9:11:28]
2023.01.26-11:05:59:360:[step-49200/177600: 27.70%]--[loss-3.072971: wl-4.092539, gl-2.049837]--[lr: pb-0.000050, pf-0.000048]--[ETA-1 day, 9:30:37]
2023.01.26-11:07:35:460:[step-49300/177600: 27.76%]--[loss-2.766855: wl-3.481257, gl-1.896541]--[lr: pb-0.000050, pf-0.000048]--[ETA-1 day, 10:29:27]
2023.01.26-11:09:12:560:[step-49400/177600: 27.82%]--[loss-2.907807: wl-3.864979, gl-1.941562]--[lr: pb-0.000050, pf-0.000048]--[ETA-1 day, 8:12:58]
2023.01.26-11:10:45:660:[step-49500/177600: 27.87%]--[loss-3.162936: wl-4.240290, gl-2.102863]--[lr: pb-0.000050, pf-0.000048]--[ETA-1 day, 7:48:09]
2023.01.26-11:12:17:760:[step-49600/177600: 27.93%]--[loss-3.081970: wl-3.885678, gl-2.110551]--[lr: pb-0.000050, pf-0.000048]--[ETA-1 day, 11:49:36]
2023.01.26-11:13:53:860:[step-49700/177600: 27.98%]--[loss-2.860183: wl-3.723271, gl-1.929365]--[lr: pb-0.000050, pf-0.000048]--[ETA-1 day, 9:47:39]
End of epoch 56 / 200 	 Time Taken: 832 sec
2023.01.26-11:15:31:72:[step-49800/177600: 28.04%]--[loss-2.854577: wl-3.828808, gl-1.897375]--[lr: pb-0.000050, pf-0.000048]--[ETA-1 day, 9:31:59]
2023.01.26-11:17:09:172:[step-49900/177600: 28.10%]--[loss-3.004728: wl-3.996549, gl-2.005590]--[lr: pb-0.000050, pf-0.000048]--[ETA-1 day, 7:35:01]
2023.01.26-11:18:39:272:[step-50000/177600: 28.15%]--[loss-2.939805: wl-3.908830, gl-1.962597]--[lr: pb-0.000050, pf-0.000048]--[ETA-1 day, 7:21:47]
2023.01.26-11:20:08:372:[step-50100/177600: 28.21%]--[loss-3.036664: wl-4.286992, gl-1.964916]--[lr: pb-0.000050, pf-0.000048]--[ETA-1 day, 7:53:09]
2023.01.26-11:21:38:472:[step-50200/177600: 28.27%]--[loss-3.013259: wl-3.969390, gl-2.020911]--[lr: pb-0.000050, pf-0.000048]--[ETA-1 day, 7:34:35]
2023.01.26-11:23:14:572:[step-50300/177600: 28.32%]--[loss-3.137069: wl-4.074736, gl-2.118385]--[lr: pb-0.000050, pf-0.000048]--[ETA-1 day, 9:14:00]
2023.01.26-11:24:50:672:[step-50400/177600: 28.38%]--[loss-3.109324: wl-4.053555, gl-2.095935]--[lr: pb-0.000050, pf-0.000048]--[ETA-1 day, 10:29:22]
2023.01.26-11:26:27:772:[step-50500/177600: 28.43%]--[loss-3.167846: wl-4.174788, gl-2.124149]--[lr: pb-0.000050, pf-0.000048]--[ETA-1 day, 9:30:39]
2023.01.26-11:28:02:872:[step-50600/177600: 28.49%]--[loss-2.857280: wl-3.654882, gl-1.943560]--[lr: pb-0.000050, pf-0.000048]--[ETA-1 day, 8:35:12]
End of epoch 57 / 200 	 Time Taken: 836 sec
2023.01.26-11:29:38:84:[step-50700/177600: 28.55%]--[loss-3.003306: wl-4.038642, gl-1.993645]--[lr: pb-0.000050, pf-0.000048]--[ETA-1 day, 8:33:52]
2023.01.26-11:31:13:184:[step-50800/177600: 28.60%]--[loss-3.073379: wl-3.838217, gl-2.113825]--[lr: pb-0.000050, pf-0.000048]--[ETA-1 day, 9:51:34]
2023.01.26-11:32:51:284:[step-50900/177600: 28.66%]--[loss-2.820435: wl-3.807905, gl-1.868458]--[lr: pb-0.000050, pf-0.000048]--[ETA-1 day, 8:59:35]
2023.01.26-11:34:25:384:[step-51000/177600: 28.72%]--[loss-3.173161: wl-3.714025, gl-2.244654]--[lr: pb-0.000050, pf-0.000048]--[ETA-1 day, 8:43:40]
2023.01.26-11:36:01:484:[step-51100/177600: 28.77%]--[loss-2.964590: wl-3.919164, gl-1.984799]--[lr: pb-0.000050, pf-0.000048]--[ETA-1 day, 8:16:17]
2023.01.26-11:37:36:584:[step-51200/177600: 28.83%]--[loss-2.703953: wl-3.533782, gl-1.820507]--[lr: pb-0.000050, pf-0.000048]--[ETA-1 day, 9:09:32]
2023.01.26-11:39:09:684:[step-51300/177600: 28.89%]--[loss-2.950415: wl-3.766262, gl-2.008850]--[lr: pb-0.000050, pf-0.000048]--[ETA-1 day, 8:09:52]
2023.01.26-11:40:39:784:[step-51400/177600: 28.94%]--[loss-2.918107: wl-3.937659, gl-1.933692]--[lr: pb-0.000050, pf-0.000048]--[ETA-1 day, 7:24:56]
2023.01.26-11:42:10:884:[step-51500/177600: 29.00%]--[loss-2.926569: wl-3.994003, gl-1.928068]--[lr: pb-0.000050, pf-0.000048]--[ETA-1 day, 6:55:30]
End of epoch 58 / 200 	 Time Taken: 837 sec
2023.01.26-11:43:46:96:[step-51600/177600: 29.05%]--[loss-2.914136: wl-3.956986, gl-1.924890]--[lr: pb-0.000050, pf-0.000047]--[ETA-1 day, 11:57:10]
2023.01.26-11:45:22:196:[step-51700/177600: 29.11%]--[loss-3.080468: wl-4.565623, gl-1.939062]--[lr: pb-0.000050, pf-0.000047]--[ETA-1 day, 8:33:52]
2023.01.26-11:46:59:296:[step-51800/177600: 29.17%]--[loss-2.936826: wl-3.714229, gl-2.008269]--[lr: pb-0.000050, pf-0.000047]--[ETA-1 day, 9:24:55]
2023.01.26-11:48:34:396:[step-51900/177600: 29.22%]--[loss-2.917995: wl-3.954849, gl-1.929283]--[lr: pb-0.000050, pf-0.000047]--[ETA-1 day, 6:48:07]
2023.01.26-11:50:04:496:[step-52000/177600: 29.28%]--[loss-3.009406: wl-3.914649, gl-2.030744]--[lr: pb-0.000050, pf-0.000047]--[ETA-1 day, 9:12:27]
2023.01.26-11:51:41:596:[step-52100/177600: 29.34%]--[loss-3.066272: wl-4.117463, gl-2.036906]--[lr: pb-0.000050, pf-0.000047]--[ETA-1 day, 9:48:51]
2023.01.26-11:53:17:696:[step-52200/177600: 29.39%]--[loss-3.249816: wl-4.115561, gl-2.220925]--[lr: pb-0.000050, pf-0.000047]--[ETA-1 day, 8:57:28]
2023.01.26-11:54:54:796:[step-52300/177600: 29.45%]--[loss-3.107343: wl-4.332882, gl-2.024123]--[lr: pb-0.000050, pf-0.000047]--[ETA-1 day, 8:31:07]
End of epoch 59 / 200 	 Time Taken: 845 sec
2023.01.26-11:56:28:8:[step-52400/177600: 29.50%]--[loss-2.996532: wl-4.194392, gl-1.947934]--[lr: pb-0.000050, pf-0.000047]--[ETA-1 day, 7:20:17]
2023.01.26-11:57:57:108:[step-52500/177600: 29.56%]--[loss-3.003227: wl-4.335861, gl-1.919261]--[lr: pb-0.000050, pf-0.000047]--[ETA-1 day, 9:52:34]
2023.01.26-11:59:29:208:[step-52600/177600: 29.62%]--[loss-2.701922: wl-3.334960, gl-1.868183]--[lr: pb-0.000050, pf-0.000047]--[ETA-1 day, 8:36:09]
2023.01.26-12:01:05:308:[step-52700/177600: 29.67%]--[loss-3.119701: wl-4.294943, gl-2.045965]--[lr: pb-0.000050, pf-0.000047]--[ETA-1 day, 8:40:23]
2023.01.26-12:02:41:408:[step-52800/177600: 29.73%]--[loss-3.325405: wl-3.906179, gl-2.348860]--[lr: pb-0.000050, pf-0.000047]--[ETA-1 day, 9:05:05]
2023.01.26-12:04:18:508:[step-52900/177600: 29.79%]--[loss-3.290718: wl-4.532361, gl-2.157628]--[lr: pb-0.000050, pf-0.000047]--[ETA-1 day, 10:55:15]
2023.01.26-12:05:49:608:[step-53000/177600: 29.84%]--[loss-3.102623: wl-4.201385, gl-2.052277]--[lr: pb-0.000050, pf-0.000047]--[ETA-1 day, 7:59:20]
2023.01.26-12:07:20:708:[step-53100/177600: 29.90%]--[loss-3.011766: wl-3.707278, gl-2.084946]--[lr: pb-0.000050, pf-0.000047]--[ETA-1 day, 6:38:42]
2023.01.26-12:08:53:808:[step-53200/177600: 29.95%]--[loss-3.486609: wl-4.189720, gl-2.439179]--[lr: pb-0.000050, pf-0.000047]--[ETA-1 day, 10:16:46]
End of epoch 60 / 200 	 Time Taken: 830 sec
saving the model at the end of epoch 60, iters 53280
2023.01.26-12:10:31:20:[step-53300/177600: 30.01%]--[loss-2.871548: wl-3.759218, gl-1.931744]--[lr: pb-0.000050, pf-0.000047]--[ETA-1 day, 10:37:22]
2023.01.26-12:12:07:120:[step-53400/177600: 30.07%]--[loss-3.011249: wl-3.631091, gl-2.103476]--[lr: pb-0.000050, pf-0.000047]--[ETA-1 day, 12:10:15]
2023.01.26-12:13:44:220:[step-53500/177600: 30.12%]--[loss-2.971126: wl-4.109402, gl-1.943775]--[lr: pb-0.000050, pf-0.000047]--[ETA-1 day, 9:43:17]
2023.01.26-12:15:13:320:[step-53600/177600: 30.18%]--[loss-3.089414: wl-3.923510, gl-2.108537]--[lr: pb-0.000050, pf-0.000047]--[ETA-1 day, 6:53:16]
2023.01.26-12:16:42:420:[step-53700/177600: 30.24%]--[loss-2.767217: wl-3.584392, gl-1.871119]--[lr: pb-0.000050, pf-0.000047]--[ETA-1 day, 6:24:18]
2023.01.26-12:18:12:520:[step-53800/177600: 30.29%]--[loss-3.128839: wl-3.992659, gl-2.130674]--[lr: pb-0.000050, pf-0.000047]--[ETA-1 day, 7:50:24]
2023.01.26-12:19:42:620:[step-53900/177600: 30.35%]--[loss-2.914229: wl-4.189460, gl-1.866864]--[lr: pb-0.000050, pf-0.000047]--[ETA-1 day, 7:12:10]
2023.01.26-12:21:13:720:[step-54000/177600: 30.41%]--[loss-2.738594: wl-3.535768, gl-1.854652]--[lr: pb-0.000050, pf-0.000047]--[ETA-1 day, 6:27:01]
2023.01.26-12:22:43:820:[step-54100/177600: 30.46%]--[loss-2.997938: wl-3.945261, gl-2.011623]--[lr: pb-0.000050, pf-0.000047]--[ETA-1 day, 6:27:52]
End of epoch 61 / 200 	 Time Taken: 813 sec
2023.01.26-12:24:15:32:[step-54200/177600: 30.52%]--[loss-3.119804: wl-3.907287, gl-2.142983]--[lr: pb-0.000050, pf-0.000046]--[ETA-1 day, 7:40:21]
2023.01.26-12:25:46:132:[step-54300/177600: 30.57%]--[loss-2.907388: wl-3.959843, gl-1.917427]--[lr: pb-0.000050, pf-0.000046]--[ETA-1 day, 6:27:21]
2023.01.26-12:27:15:232:[step-54400/177600: 30.63%]--[loss-2.904483: wl-3.874595, gl-1.935835]--[lr: pb-0.000050, pf-0.000046]--[ETA-1 day, 6:14:19]
2023.01.26-12:28:45:332:[step-54500/177600: 30.69%]--[loss-2.844782: wl-3.990659, gl-1.847117]--[lr: pb-0.000050, pf-0.000046]--[ETA-1 day, 6:58:21]
2023.01.26-12:30:14:432:[step-54600/177600: 30.74%]--[loss-3.132883: wl-4.346735, gl-2.046199]--[lr: pb-0.000050, pf-0.000046]--[ETA-1 day, 6:20:13]
2023.01.26-12:31:45:532:[step-54700/177600: 30.80%]--[loss-3.024620: wl-4.004164, gl-2.023579]--[lr: pb-0.000050, pf-0.000046]--[ETA-1 day, 6:27:34]
2023.01.26-12:33:15:632:[step-54800/177600: 30.86%]--[loss-3.180176: wl-4.767213, gl-1.988372]--[lr: pb-0.000050, pf-0.000046]--[ETA-1 day, 6:10:32]
2023.01.26-12:34:45:732:[step-54900/177600: 30.91%]--[loss-3.093745: wl-3.731613, gl-2.160841]--[lr: pb-0.000050, pf-0.000046]--[ETA-1 day, 7:16:16]
2023.01.26-12:36:16:832:[step-55000/177600: 30.97%]--[loss-3.147113: wl-4.223745, gl-2.091177]--[lr: pb-0.000050, pf-0.000046]--[ETA-1 day, 6:41:28]
End of epoch 62 / 200 	 Time Taken: 802 sec
2023.01.26-12:37:49:44:[step-55100/177600: 31.02%]--[loss-2.830328: wl-3.510492, gl-1.952705]--[lr: pb-0.000050, pf-0.000046]--[ETA-1 day, 6:27:15]
2023.01.26-12:39:20:144:[step-55200/177600: 31.08%]--[loss-3.209506: wl-4.466226, gl-2.092949]--[lr: pb-0.000050, pf-0.000046]--[ETA-1 day, 6:49:26]
2023.01.26-12:40:51:244:[step-55300/177600: 31.14%]--[loss-2.867039: wl-3.555840, gl-1.978079]--[lr: pb-0.000050, pf-0.000046]--[ETA-1 day, 6:27:21]
2023.01.26-12:42:22:344:[step-55400/177600: 31.19%]--[loss-2.964192: wl-3.984748, gl-1.968005]--[lr: pb-0.000050, pf-0.000046]--[ETA-1 day, 8:19:57]
2023.01.26-12:43:54:444:[step-55500/177600: 31.25%]--[loss-2.982014: wl-3.913802, gl-2.003563]--[lr: pb-0.000050, pf-0.000046]--[ETA-1 day, 6:14:17]
2023.01.26-12:45:25:544:[step-55600/177600: 31.31%]--[loss-3.134374: wl-4.163301, gl-2.093548]--[lr: pb-0.000050, pf-0.000046]--[ETA-1 day, 6:42:30]
2023.01.26-12:46:56:644:[step-55700/177600: 31.36%]--[loss-3.252905: wl-4.730983, gl-2.070160]--[lr: pb-0.000050, pf-0.000046]--[ETA-1 day, 6:07:51]
2023.01.26-12:48:27:744:[step-55800/177600: 31.42%]--[loss-2.756625: wl-3.528523, gl-1.874495]--[lr: pb-0.000050, pf-0.000046]--[ETA-1 day, 6:23:21]
2023.01.26-12:49:59:844:[step-55900/177600: 31.48%]--[loss-3.230317: wl-4.328311, gl-2.148239]--[lr: pb-0.000050, pf-0.000046]--[ETA-1 day, 8:28:37]
End of epoch 63 / 200 	 Time Taken: 812 sec
2023.01.26-12:51:32:56:[step-56000/177600: 31.53%]--[loss-2.971739: wl-3.782395, gl-2.026140]--[lr: pb-0.000050, pf-0.000046]--[ETA-1 day, 7:03:08]
2023.01.26-12:53:04:156:[step-56100/177600: 31.59%]--[loss-2.846226: wl-3.910605, gl-1.868575]--[lr: pb-0.000050, pf-0.000046]--[ETA-1 day, 6:39:56]
2023.01.26-12:54:35:256:[step-56200/177600: 31.64%]--[loss-2.968418: wl-4.189072, gl-1.921150]--[lr: pb-0.000050, pf-0.000046]--[ETA-1 day, 6:45:30]
2023.01.26-12:56:07:356:[step-56300/177600: 31.70%]--[loss-2.963029: wl-4.166584, gl-1.921383]--[lr: pb-0.000050, pf-0.000046]--[ETA-1 day, 8:16:42]
2023.01.26-12:57:38:456:[step-56400/177600: 31.76%]--[loss-3.056737: wl-4.319824, gl-1.976781]--[lr: pb-0.000050, pf-0.000046]--[ETA-1 day, 6:33:20]
2023.01.26-12:59:09:556:[step-56500/177600: 31.81%]--[loss-2.800542: wl-3.871000, gl-1.832792]--[lr: pb-0.000050, pf-0.000046]--[ETA-1 day, 6:25:56]
2023.01.26-13:00:40:656:[step-56600/177600: 31.87%]--[loss-2.899835: wl-3.961474, gl-1.909466]--[lr: pb-0.000050, pf-0.000046]--[ETA-1 day, 6:44:42]
2023.01.26-13:02:11:756:[step-56700/177600: 31.93%]--[loss-2.967135: wl-3.948772, gl-1.979942]--[lr: pb-0.000050, pf-0.000046]--[ETA-1 day, 5:50:14]
2023.01.26-13:03:43:856:[step-56800/177600: 31.98%]--[loss-3.132170: wl-3.883981, gl-2.161175]--[lr: pb-0.000050, pf-0.000046]--[ETA-1 day, 7:14:27]
End of epoch 64 / 200 	 Time Taken: 812 sec
2023.01.26-13:05:15:68:[step-56900/177600: 32.04%]--[loss-3.124629: wl-3.897997, gl-2.150130]--[lr: pb-0.000050, pf-0.000045]--[ETA-1 day, 6:01:10]
2023.01.26-13:06:47:168:[step-57000/177600: 32.09%]--[loss-2.952389: wl-3.899191, gl-1.977591]--[lr: pb-0.000050, pf-0.000045]--[ETA-1 day, 6:20:59]
2023.01.26-13:08:18:268:[step-57100/177600: 32.15%]--[loss-2.660562: wl-3.408851, gl-1.808349]--[lr: pb-0.000050, pf-0.000045]--[ETA-1 day, 6:54:45]
2023.01.26-13:09:50:368:[step-57200/177600: 32.21%]--[loss-3.099396: wl-4.753095, gl-1.911122]--[lr: pb-0.000050, pf-0.000045]--[ETA-1 day, 6:27:54]
2023.01.26-13:11:21:468:[step-57300/177600: 32.26%]--[loss-2.830050: wl-3.662112, gl-1.914522]--[lr: pb-0.000050, pf-0.000045]--[ETA-1 day, 6:22:54]
2023.01.26-13:12:53:568:[step-57400/177600: 32.32%]--[loss-2.854288: wl-3.915851, gl-1.875325]--[lr: pb-0.000050, pf-0.000045]--[ETA-1 day, 6:26:02]
2023.01.26-13:14:24:668:[step-57500/177600: 32.38%]--[loss-3.010050: wl-3.924518, gl-2.028921]--[lr: pb-0.000050, pf-0.000045]--[ETA-1 day, 6:20:33]
2023.01.26-13:15:55:768:[step-57600/177600: 32.43%]--[loss-2.858233: wl-3.867328, gl-1.891401]--[lr: pb-0.000050, pf-0.000045]--[ETA-1 day, 7:47:31]
2023.01.26-13:17:26:868:[step-57700/177600: 32.49%]--[loss-2.924455: wl-3.982080, gl-1.928935]--[lr: pb-0.000050, pf-0.000045]--[ETA-1 day, 6:04:02]
End of epoch 65 / 200 	 Time Taken: 812 sec
saving the model at the end of epoch 65, iters 57720
2023.01.26-13:19:00:80:[step-57800/177600: 32.55%]--[loss-3.001716: wl-3.905755, gl-2.025277]--[lr: pb-0.000050, pf-0.000045]--[ETA-1 day, 6:00:35]
2023.01.26-13:20:31:180:[step-57900/177600: 32.60%]--[loss-3.126441: wl-4.161445, gl-2.086080]--[lr: pb-0.000050, pf-0.000045]--[ETA-1 day, 6:13:29]
2023.01.26-13:22:02:280:[step-58000/177600: 32.66%]--[loss-2.979781: wl-4.066414, gl-1.963177]--[lr: pb-0.000050, pf-0.000045]--[ETA-1 day, 6:38:02]
2023.01.26-13:23:34:380:[step-58100/177600: 32.71%]--[loss-2.877840: wl-3.774476, gl-1.934221]--[lr: pb-0.000050, pf-0.000045]--[ETA-1 day, 6:35:39]
2023.01.26-13:25:05:480:[step-58200/177600: 32.77%]--[loss-2.852511: wl-3.824781, gl-1.896316]--[lr: pb-0.000050, pf-0.000045]--[ETA-1 day, 6:01:24]
2023.01.26-13:26:36:580:[step-58300/177600: 32.83%]--[loss-3.144193: wl-4.481019, gl-2.023938]--[lr: pb-0.000050, pf-0.000045]--[ETA-1 day, 6:35:04]
2023.01.26-13:28:07:680:[step-58400/177600: 32.88%]--[loss-2.867972: wl-3.890722, gl-1.895292]--[lr: pb-0.000050, pf-0.000045]--[ETA-1 day, 7:00:32]
2023.01.26-13:29:39:780:[step-58500/177600: 32.94%]--[loss-2.749489: wl-3.869326, gl-1.782158]--[lr: pb-0.000050, pf-0.000045]--[ETA-1 day, 5:32:48]
2023.01.26-13:31:11:880:[step-58600/177600: 33.00%]--[loss-2.799212: wl-3.597306, gl-1.899885]--[lr: pb-0.000050, pf-0.000045]--[ETA-1 day, 5:23:00]
End of epoch 66 / 200 	 Time Taken: 812 sec
2023.01.26-13:32:43:92:[step-58700/177600: 33.05%]--[loss-2.913506: wl-3.858242, gl-1.948945]--[lr: pb-0.000050, pf-0.000045]--[ETA-1 day, 6:10:05]
2023.01.26-13:34:14:192:[step-58800/177600: 33.11%]--[loss-3.000468: wl-4.445252, gl-1.889155]--[lr: pb-0.000050, pf-0.000045]--[ETA-1 day, 6:11:38]
2023.01.26-13:35:46:292:[step-58900/177600: 33.16%]--[loss-2.884140: wl-3.773120, gl-1.940860]--[lr: pb-0.000050, pf-0.000045]--[ETA-1 day, 5:51:21]
2023.01.26-13:37:17:392:[step-59000/177600: 33.22%]--[loss-2.933772: wl-3.800910, gl-1.983544]--[lr: pb-0.000050, pf-0.000045]--[ETA-1 day, 5:47:43]
2023.01.26-13:38:49:492:[step-59100/177600: 33.28%]--[loss-3.187568: wl-4.839477, gl-1.977699]--[lr: pb-0.000050, pf-0.000045]--[ETA-1 day, 5:56:41]
2023.01.26-13:40:20:592:[step-59200/177600: 33.33%]--[loss-2.865779: wl-3.673647, gl-1.947368]--[lr: pb-0.000050, pf-0.000045]--[ETA-1 day, 5:55:38]
2023.01.26-13:41:51:692:[step-59300/177600: 33.39%]--[loss-2.916659: wl-3.904524, gl-1.940528]--[lr: pb-0.000050, pf-0.000045]--[ETA-1 day, 5:58:00]
2023.01.26-13:43:22:792:[step-59400/177600: 33.45%]--[loss-3.108051: wl-4.082430, gl-2.087443]--[lr: pb-0.000050, pf-0.000045]--[ETA-1 day, 5:10:56]
End of epoch 67 / 200 	 Time Taken: 811 sec
2023.01.26-13:44:55:4:[step-59500/177600: 33.50%]--[loss-3.172057: wl-3.755346, gl-2.233220]--[lr: pb-0.000050, pf-0.000044]--[ETA-1 day, 7:17:14]
2023.01.26-13:46:26:104:[step-59600/177600: 33.56%]--[loss-2.910172: wl-3.711071, gl-1.982404]--[lr: pb-0.000050, pf-0.000044]--[ETA-1 day, 5:10:58]
2023.01.26-13:47:57:204:[step-59700/177600: 33.61%]--[loss-2.718380: wl-3.775302, gl-1.774554]--[lr: pb-0.000050, pf-0.000044]--[ETA-1 day, 7:03:03]
2023.01.26-13:49:28:304:[step-59800/177600: 33.67%]--[loss-2.975754: wl-4.189713, gl-1.928326]--[lr: pb-0.000050, pf-0.000044]--[ETA-1 day, 6:02:33]
2023.01.26-13:50:59:404:[step-59900/177600: 33.73%]--[loss-3.117798: wl-4.386237, gl-2.021238]--[lr: pb-0.000050, pf-0.000044]--[ETA-1 day, 5:35:58]
2023.01.26-13:52:31:504:[step-60000/177600: 33.78%]--[loss-2.695353: wl-3.504797, gl-1.819154]--[lr: pb-0.000050, pf-0.000044]--[ETA-1 day, 5:43:45]
2023.01.26-13:54:03:604:[step-60100/177600: 33.84%]--[loss-2.967865: wl-4.175005, gl-1.924114]--[lr: pb-0.000050, pf-0.000044]--[ETA-1 day, 5:38:17]
2023.01.26-13:55:34:704:[step-60200/177600: 33.90%]--[loss-2.879148: wl-4.124228, gl-1.848090]--[lr: pb-0.000050, pf-0.000044]--[ETA-1 day, 5:19:17]
2023.01.26-13:57:05:804:[step-60300/177600: 33.95%]--[loss-3.022008: wl-3.837340, gl-2.062674]--[lr: pb-0.000050, pf-0.000044]--[ETA-1 day, 5:34:23]
End of epoch 68 / 200 	 Time Taken: 812 sec
2023.01.26-13:58:38:16:[step-60400/177600: 34.01%]--[loss-3.076037: wl-4.079631, gl-2.056129]--[lr: pb-0.000050, pf-0.000044]--[ETA-1 day, 5:34:16]
2023.01.26-14:00:09:116:[step-60500/177600: 34.07%]--[loss-3.003732: wl-4.043399, gl-1.992882]--[lr: pb-0.000050, pf-0.000044]--[ETA-1 day, 5:33:47]
2023.01.26-14:01:40:216:[step-60600/177600: 34.12%]--[loss-2.816491: wl-3.651830, gl-1.903534]--[lr: pb-0.000050, pf-0.000044]--[ETA-1 day, 5:09:44]
2023.01.26-14:03:12:316:[step-60700/177600: 34.18%]--[loss-2.948064: wl-3.970471, gl-1.955446]--[lr: pb-0.000050, pf-0.000044]--[ETA-1 day, 6:15:00]
2023.01.26-14:04:43:416:[step-60800/177600: 34.23%]--[loss-3.242780: wl-4.400349, gl-2.142692]--[lr: pb-0.000050, pf-0.000044]--[ETA-1 day, 6:23:00]
2023.01.26-14:06:14:516:[step-60900/177600: 34.29%]--[loss-2.939709: wl-3.768469, gl-1.997592]--[lr: pb-0.000050, pf-0.000044]--[ETA-1 day, 6:21:01]
2023.01.26-14:07:46:616:[step-61000/177600: 34.35%]--[loss-3.059212: wl-4.157300, gl-2.019887]--[lr: pb-0.000050, pf-0.000044]--[ETA-1 day, 5:22:55]
2023.01.26-14:09:17:716:[step-61100/177600: 34.40%]--[loss-3.088258: wl-4.378093, gl-1.993735]--[lr: pb-0.000050, pf-0.000044]--[ETA-1 day, 6:08:41]
2023.01.26-14:10:48:816:[step-61200/177600: 34.46%]--[loss-2.997021: wl-3.913566, gl-2.018630]--[lr: pb-0.000050, pf-0.000044]--[ETA-1 day, 4:41:21]
End of epoch 69 / 200 	 Time Taken: 811 sec
2023.01.26-14:12:21:28:[step-61300/177600: 34.52%]--[loss-2.975953: wl-3.901426, gl-2.000597]--[lr: pb-0.000050, pf-0.000044]--[ETA-1 day, 4:32:30]
2023.01.26-14:13:52:128:[step-61400/177600: 34.57%]--[loss-2.883276: wl-3.941905, gl-1.897799]--[lr: pb-0.000050, pf-0.000044]--[ETA-1 day, 4:40:59]
2023.01.26-14:15:23:228:[step-61500/177600: 34.63%]--[loss-2.798397: wl-3.610969, gl-1.895655]--[lr: pb-0.000050, pf-0.000044]--[ETA-1 day, 5:24:30]
2023.01.26-14:16:55:328:[step-61600/177600: 34.68%]--[loss-3.213662: wl-4.304046, gl-2.137651]--[lr: pb-0.000050, pf-0.000044]--[ETA-1 day, 4:49:25]
2023.01.26-14:18:26:428:[step-61700/177600: 34.74%]--[loss-3.076810: wl-4.417260, gl-1.972495]--[lr: pb-0.000050, pf-0.000044]--[ETA-1 day, 4:54:07]
2023.01.26-14:19:57:528:[step-61800/177600: 34.80%]--[loss-2.654245: wl-3.255674, gl-1.840326]--[lr: pb-0.000050, pf-0.000044]--[ETA-1 day, 4:35:55]
2023.01.26-14:21:28:628:[step-61900/177600: 34.85%]--[loss-2.928046: wl-4.052553, gl-1.914908]--[lr: pb-0.000050, pf-0.000044]--[ETA-1 day, 5:18:09]
2023.01.26-14:22:59:728:[step-62000/177600: 34.91%]--[loss-2.922480: wl-4.166786, gl-1.880783]--[lr: pb-0.000050, pf-0.000044]--[ETA-1 day, 5:48:07]
2023.01.26-14:24:31:828:[step-62100/177600: 34.97%]--[loss-2.995789: wl-3.822778, gl-2.040094]--[lr: pb-0.000050, pf-0.000044]--[ETA-1 day, 5:05:31]
End of epoch 70 / 200 	 Time Taken: 811 sec
saving the model at the end of epoch 70, iters 62160
2023.01.26-14:26:04:40:[step-62200/177600: 35.02%]--[loss-3.022549: wl-4.575233, gl-1.878741]--[lr: pb-0.000050, pf-0.000043]--[ETA-1 day, 5:03:29]
2023.01.26-14:27:35:140:[step-62300/177600: 35.08%]--[loss-3.035538: wl-4.359588, gl-1.945641]--[lr: pb-0.000050, pf-0.000043]--[ETA-1 day, 5:08:33]
2023.01.26-14:29:07:240:[step-62400/177600: 35.14%]--[loss-2.832447: wl-3.950070, gl-1.844930]--[lr: pb-0.000050, pf-0.000043]--[ETA-1 day, 5:06:30]
2023.01.26-14:30:39:340:[step-62500/177600: 35.19%]--[loss-2.774432: wl-3.875018, gl-1.805678]--[lr: pb-0.000050, pf-0.000043]--[ETA-1 day, 5:15:37]
2023.01.26-14:32:11:440:[step-62600/177600: 35.25%]--[loss-2.905294: wl-3.707453, gl-1.978431]--[lr: pb-0.000050, pf-0.000043]--[ETA-1 day, 4:34:40]
2023.01.26-14:33:43:540:[step-62700/177600: 35.30%]--[loss-3.274301: wl-4.430760, gl-2.166611]--[lr: pb-0.000050, pf-0.000043]--[ETA-1 day, 4:19:31]
2023.01.26-14:35:14:640:[step-62800/177600: 35.36%]--[loss-2.939669: wl-4.493646, gl-1.816258]--[lr: pb-0.000050, pf-0.000043]--[ETA-1 day, 4:09:59]
2023.01.26-14:36:47:740:[step-62900/177600: 35.42%]--[loss-3.032134: wl-4.369831, gl-1.939676]--[lr: pb-0.000050, pf-0.000043]--[ETA-1 day, 4:55:36]
2023.01.26-14:38:20:840:[step-63000/177600: 35.47%]--[loss-2.860576: wl-3.624327, gl-1.954494]--[lr: pb-0.000050, pf-0.000043]--[ETA-1 day, 8:09:34]
End of epoch 71 / 200 	 Time Taken: 819 sec
2023.01.26-14:39:55:52:[step-63100/177600: 35.53%]--[loss-2.585435: wl-3.210594, gl-1.782786]--[lr: pb-0.000050, pf-0.000043]--[ETA-1 day, 4:55:38]
2023.01.26-14:41:26:152:[step-63200/177600: 35.59%]--[loss-2.810016: wl-4.115323, gl-1.781185]--[lr: pb-0.000050, pf-0.000043]--[ETA-1 day, 4:23:26]
2023.01.26-14:42:57:252:[step-63300/177600: 35.64%]--[loss-3.008879: wl-4.198216, gl-1.959325]--[lr: pb-0.000050, pf-0.000043]--[ETA-1 day, 4:27:16]
2023.01.26-14:44:28:352:[step-63400/177600: 35.70%]--[loss-2.899728: wl-3.708941, gl-1.972493]--[lr: pb-0.000050, pf-0.000043]--[ETA-1 day, 4:44:19]
2023.01.26-14:45:59:452:[step-63500/177600: 35.75%]--[loss-3.017695: wl-4.187625, gl-1.970789]--[lr: pb-0.000050, pf-0.000043]--[ETA-1 day, 4:35:22]
2023.01.26-14:47:31:552:[step-63600/177600: 35.81%]--[loss-2.870268: wl-3.518090, gl-1.990746]--[lr: pb-0.000050, pf-0.000043]--[ETA-1 day, 5:10:50]
2023.01.26-14:49:02:652:[step-63700/177600: 35.87%]--[loss-2.961984: wl-3.666450, gl-2.045372]--[lr: pb-0.000050, pf-0.000043]--[ETA-1 day, 6:19:09]
2023.01.26-14:50:33:752:[step-63800/177600: 35.92%]--[loss-3.068871: wl-4.394897, gl-1.970146]--[lr: pb-0.000050, pf-0.000043]--[ETA-1 day, 4:44:06]
2023.01.26-14:52:05:852:[step-63900/177600: 35.98%]--[loss-3.026000: wl-4.113201, gl-1.997700]--[lr: pb-0.000050, pf-0.000043]--[ETA-1 day, 5:02:40]
End of epoch 72 / 200 	 Time Taken: 811 sec
2023.01.26-14:53:37:64:[step-64000/177600: 36.04%]--[loss-2.838883: wl-3.836432, gl-1.879775]--[lr: pb-0.000050, pf-0.000043]--[ETA-1 day, 4:58:33]
2023.01.26-14:55:09:164:[step-64100/177600: 36.09%]--[loss-2.848249: wl-3.648726, gl-1.936068]--[lr: pb-0.000050, pf-0.000043]--[ETA-1 day, 4:01:43]
2023.01.26-14:56:40:264:[step-64200/177600: 36.15%]--[loss-2.868105: wl-3.801790, gl-1.917657]--[lr: pb-0.000050, pf-0.000043]--[ETA-1 day, 4:28:15]
2023.01.26-14:58:11:364:[step-64300/177600: 36.20%]--[loss-3.142305: wl-4.432251, gl-2.034242]--[lr: pb-0.000050, pf-0.000043]--[ETA-1 day, 4:34:06]
2023.01.26-14:59:42:464:[step-64400/177600: 36.26%]--[loss-3.015970: wl-4.054128, gl-2.002438]--[lr: pb-0.000050, pf-0.000043]--[ETA-1 day, 5:06:08]
2023.01.26-15:01:13:564:[step-64500/177600: 36.32%]--[loss-3.177477: wl-4.666175, gl-2.010933]--[lr: pb-0.000050, pf-0.000043]--[ETA-1 day, 4:40:51]
2023.01.26-15:02:44:664:[step-64600/177600: 36.37%]--[loss-2.893748: wl-3.861112, gl-1.928470]--[lr: pb-0.000050, pf-0.000043]--[ETA-1 day, 4:55:42]
2023.01.26-15:04:16:764:[step-64700/177600: 36.43%]--[loss-3.180956: wl-4.382065, gl-2.085440]--[lr: pb-0.000050, pf-0.000043]--[ETA-1 day, 3:44:08]
2023.01.26-15:05:47:864:[step-64800/177600: 36.49%]--[loss-2.811315: wl-3.808136, gl-1.859281]--[lr: pb-0.000050, pf-0.000043]--[ETA-1 day, 3:58:51]
End of epoch 73 / 200 	 Time Taken: 811 sec
2023.01.26-15:07:19:76:[step-64900/177600: 36.54%]--[loss-3.044423: wl-4.187015, gl-1.997669]--[lr: pb-0.000050, pf-0.000042]--[ETA-1 day, 3:40:25]
2023.01.26-15:08:50:176:[step-65000/177600: 36.60%]--[loss-2.781640: wl-3.868297, gl-1.814566]--[lr: pb-0.000050, pf-0.000042]--[ETA-1 day, 5:23:52]
2023.01.26-15:10:21:276:[step-65100/177600: 36.66%]--[loss-2.901968: wl-4.315743, gl-1.823033]--[lr: pb-0.000050, pf-0.000042]--[ETA-1 day, 4:18:40]
2023.01.26-15:11:52:376:[step-65200/177600: 36.71%]--[loss-3.011518: wl-3.972928, gl-2.018286]--[lr: pb-0.000050, pf-0.000042]--[ETA-1 day, 4:21:46]
2023.01.26-15:13:24:476:[step-65300/177600: 36.77%]--[loss-2.984096: wl-4.198236, gl-1.934537]--[lr: pb-0.000050, pf-0.000042]--[ETA-1 day, 5:04:28]
2023.01.26-15:14:54:576:[step-65400/177600: 36.82%]--[loss-2.852194: wl-3.876505, gl-1.883068]--[lr: pb-0.000050, pf-0.000042]--[ETA-1 day, 3:55:34]
2023.01.26-15:16:26:676:[step-65500/177600: 36.88%]--[loss-2.992151: wl-3.888081, gl-2.020130]--[lr: pb-0.000050, pf-0.000042]--[ETA-1 day, 4:42:00]
2023.01.26-15:17:56:776:[step-65600/177600: 36.94%]--[loss-2.968217: wl-3.957003, gl-1.978966]--[lr: pb-0.000050, pf-0.000042]--[ETA-1 day, 4:15:53]
2023.01.26-15:19:27:876:[step-65700/177600: 36.99%]--[loss-2.743884: wl-3.603014, gl-1.843131]--[lr: pb-0.000050, pf-0.000042]--[ETA-1 day, 4:43:50]
End of epoch 74 / 200 	 Time Taken: 809 sec
2023.01.26-15:21:00:88:[step-65800/177600: 37.05%]--[loss-3.073905: wl-4.642405, gl-1.913304]--[lr: pb-0.000050, pf-0.000042]--[ETA-1 day, 5:53:18]
2023.01.26-15:22:31:188:[step-65900/177600: 37.11%]--[loss-3.061235: wl-4.092395, gl-2.038136]--[lr: pb-0.000050, pf-0.000042]--[ETA-1 day, 3:54:17]
2023.01.26-15:24:02:288:[step-66000/177600: 37.16%]--[loss-2.945465: wl-3.673045, gl-2.027204]--[lr: pb-0.000050, pf-0.000042]--[ETA-1 day, 5:02:12]
2023.01.26-15:25:34:388:[step-66100/177600: 37.22%]--[loss-2.871662: wl-3.810202, gl-1.919111]--[lr: pb-0.000050, pf-0.000042]--[ETA-1 day, 4:05:17]
2023.01.26-15:27:05:488:[step-66200/177600: 37.27%]--[loss-3.069207: wl-3.837135, gl-2.109923]--[lr: pb-0.000050, pf-0.000042]--[ETA-1 day, 4:03:46]
2023.01.26-15:28:36:588:[step-66300/177600: 37.33%]--[loss-2.998713: wl-3.980582, gl-2.003567]--[lr: pb-0.000050, pf-0.000042]--[ETA-1 day, 3:54:44]
2023.01.26-15:30:08:688:[step-66400/177600: 37.39%]--[loss-3.196290: wl-4.090738, gl-2.173605]--[lr: pb-0.000050, pf-0.000042]--[ETA-1 day, 4:12:56]
2023.01.26-15:31:39:788:[step-66500/177600: 37.44%]--[loss-2.947552: wl-4.025054, gl-1.941288]--[lr: pb-0.000050, pf-0.000042]--[ETA-1 day, 3:52:47]
2023.01.26-15:33:10:888:[step-66600/177600: 37.50%]--[loss-2.832994: wl-3.924699, gl-1.851819]--[lr: pb-0.000050, pf-0.000042]--[ETA-1 day, 4:58:54]
End of epoch 75 / 200 	 Time Taken: 812 sec
saving the model at the end of epoch 75, iters 66600
2023.01.26-15:34:43:100:[step-66700/177600: 37.56%]--[loss-2.835123: wl-3.895277, gl-1.861303]--[lr: pb-0.000050, pf-0.000042]--[ETA-1 day, 3:55:36]
2023.01.26-15:36:15:200:[step-66800/177600: 37.61%]--[loss-2.918723: wl-3.880912, gl-1.948495]--[lr: pb-0.000050, pf-0.000042]--[ETA-1 day, 3:52:38]
2023.01.26-15:37:46:300:[step-66900/177600: 37.67%]--[loss-2.797073: wl-3.486483, gl-1.925452]--[lr: pb-0.000050, pf-0.000042]--[ETA-1 day, 3:22:33]
2023.01.26-15:39:17:400:[step-67000/177600: 37.73%]--[loss-2.914828: wl-3.968451, gl-1.922715]--[lr: pb-0.000050, pf-0.000042]--[ETA-1 day, 4:54:01]
2023.01.26-15:40:48:500:[step-67100/177600: 37.78%]--[loss-2.860529: wl-4.014926, gl-1.856798]--[lr: pb-0.000050, pf-0.000042]--[ETA-1 day, 4:05:19]
2023.01.26-15:42:19:600:[step-67200/177600: 37.84%]--[loss-2.925694: wl-3.912919, gl-1.947464]--[lr: pb-0.000050, pf-0.000042]--[ETA-1 day, 4:02:12]
2023.01.26-15:43:50:700:[step-67300/177600: 37.89%]--[loss-2.892118: wl-3.662850, gl-1.976405]--[lr: pb-0.000050, pf-0.000042]--[ETA-1 day, 3:46:08]
2023.01.26-15:45:22:800:[step-67400/177600: 37.95%]--[loss-2.989312: wl-3.953219, gl-2.001008]--[lr: pb-0.000050, pf-0.000042]--[ETA-1 day, 5:40:01]
End of epoch 76 / 200 	 Time Taken: 810 sec
2023.01.26-15:46:54:12:[step-67500/177600: 38.01%]--[loss-3.156409: wl-4.182760, gl-2.110719]--[lr: pb-0.000050, pf-0.000041]--[ETA-1 day, 3:17:47]
2023.01.26-15:48:25:112:[step-67600/177600: 38.06%]--[loss-2.853724: wl-3.633525, gl-1.945343]--[lr: pb-0.000050, pf-0.000041]--[ETA-1 day, 3:44:49]
2023.01.26-15:49:55:212:[step-67700/177600: 38.12%]--[loss-3.204146: wl-4.292825, gl-2.130940]--[lr: pb-0.000050, pf-0.000041]--[ETA-1 day, 3:08:58]
2023.01.26-15:51:27:312:[step-67800/177600: 38.18%]--[loss-2.909520: wl-4.169066, gl-1.867254]--[lr: pb-0.000050, pf-0.000041]--[ETA-1 day, 3:40:07]
2023.01.26-15:52:58:412:[step-67900/177600: 38.23%]--[loss-2.813155: wl-3.522356, gl-1.932566]--[lr: pb-0.000050, pf-0.000041]--[ETA-1 day, 3:07:48]
2023.01.26-15:54:29:512:[step-68000/177600: 38.29%]--[loss-2.723627: wl-3.685346, gl-1.802291]--[lr: pb-0.000050, pf-0.000041]--[ETA-1 day, 3:51:19]
2023.01.26-15:56:00:612:[step-68100/177600: 38.34%]--[loss-2.754045: wl-3.680184, gl-1.833999]--[lr: pb-0.000050, pf-0.000041]--[ETA-1 day, 3:34:04]
2023.01.26-15:57:31:712:[step-68200/177600: 38.40%]--[loss-2.843224: wl-4.039387, gl-1.833377]--[lr: pb-0.000050, pf-0.000041]--[ETA-1 day, 2:58:03]
2023.01.26-15:59:02:812:[step-68300/177600: 38.46%]--[loss-2.991152: wl-3.904540, gl-2.015017]--[lr: pb-0.000050, pf-0.000041]--[ETA-1 day, 3:54:37]
End of epoch 77 / 200 	 Time Taken: 809 sec
2023.01.26-16:00:35:24:[step-68400/177600: 38.51%]--[loss-3.086207: wl-4.495374, gl-1.962364]--[lr: pb-0.000050, pf-0.000041]--[ETA-1 day, 2:59:49]
2023.01.26-16:02:06:124:[step-68500/177600: 38.57%]--[loss-2.981407: wl-4.046573, gl-1.969764]--[lr: pb-0.000050, pf-0.000041]--[ETA-1 day, 3:05:31]
2023.01.26-16:03:37:224:[step-68600/177600: 38.63%]--[loss-3.463920: wl-5.187902, gl-2.166945]--[lr: pb-0.000050, pf-0.000041]--[ETA-1 day, 3:39:33]
2023.01.26-16:05:08:324:[step-68700/177600: 38.68%]--[loss-3.275149: wl-4.923065, gl-2.044383]--[lr: pb-0.000050, pf-0.000041]--[ETA-1 day, 2:49:51]
2023.01.26-16:06:39:424:[step-68800/177600: 38.74%]--[loss-3.063894: wl-4.318509, gl-1.984267]--[lr: pb-0.000050, pf-0.000041]--[ETA-1 day, 3:52:55]
2023.01.26-16:08:11:524:[step-68900/177600: 38.80%]--[loss-2.904192: wl-3.899987, gl-1.929196]--[lr: pb-0.000050, pf-0.000041]--[ETA-1 day, 3:00:34]
2023.01.26-16:09:42:624:[step-69000/177600: 38.85%]--[loss-3.120962: wl-4.453908, gl-2.007485]--[lr: pb-0.000050, pf-0.000041]--[ETA-1 day, 3:14:29]
2023.01.26-16:11:13:724:[step-69100/177600: 38.91%]--[loss-2.719899: wl-3.725183, gl-1.788604]--[lr: pb-0.000050, pf-0.000041]--[ETA-1 day, 2:56:58]
2023.01.26-16:12:44:824:[step-69200/177600: 38.96%]--[loss-3.061342: wl-4.135861, gl-2.027377]--[lr: pb-0.000050, pf-0.000041]--[ETA-1 day, 3:39:02]
End of epoch 78 / 200 	 Time Taken: 810 sec
2023.01.26-16:14:16:36:[step-69300/177600: 39.02%]--[loss-2.897602: wl-4.046577, gl-1.885958]--[lr: pb-0.000050, pf-0.000041]--[ETA-1 day, 3:22:15]
2023.01.26-16:15:48:136:[step-69400/177600: 39.08%]--[loss-2.923798: wl-3.954503, gl-1.935173]--[lr: pb-0.000050, pf-0.000041]--[ETA-1 day, 2:48:08]
2023.01.26-16:17:19:236:[step-69500/177600: 39.13%]--[loss-2.911272: wl-3.707484, gl-1.984401]--[lr: pb-0.000050, pf-0.000041]--[ETA-1 day, 3:32:08]
2023.01.26-16:18:50:336:[step-69600/177600: 39.19%]--[loss-2.728773: wl-3.651305, gl-1.815946]--[lr: pb-0.000050, pf-0.000041]--[ETA-1 day, 2:31:08]
2023.01.26-16:20:22:436:[step-69700/177600: 39.25%]--[loss-3.042518: wl-4.498569, gl-1.917876]--[lr: pb-0.000050, pf-0.000041]--[ETA-1 day, 3:05:09]
2023.01.26-16:21:52:536:[step-69800/177600: 39.30%]--[loss-3.100121: wl-4.171327, gl-2.057289]--[lr: pb-0.000050, pf-0.000041]--[ETA-1 day, 3:47:16]
2023.01.26-16:23:24:636:[step-69900/177600: 39.36%]--[loss-2.894586: wl-3.921097, gl-1.914312]--[lr: pb-0.000050, pf-0.000041]--[ETA-1 day, 2:36:37]
2023.01.26-16:24:55:736:[step-70000/177600: 39.41%]--[loss-3.022657: wl-4.053108, gl-2.009381]--[lr: pb-0.000050, pf-0.000041]--[ETA-1 day, 4:17:54]
2023.01.26-16:26:26:836:[step-70100/177600: 39.47%]--[loss-2.713721: wl-3.576873, gl-1.819503]--[lr: pb-0.000050, pf-0.000041]--[ETA-1 day, 2:26:37]
End of epoch 79 / 200 	 Time Taken: 810 sec
2023.01.26-16:27:58:48:[step-70200/177600: 39.53%]--[loss-3.024930: wl-4.077553, gl-2.005542]--[lr: pb-0.000050, pf-0.000040]--[ETA-1 day, 3:18:01]
2023.01.26-16:29:30:148:[step-70300/177600: 39.58%]--[loss-2.818967: wl-3.475121, gl-1.950187]--[lr: pb-0.000050, pf-0.000040]--[ETA-1 day, 3:12:48]
2023.01.26-16:31:01:248:[step-70400/177600: 39.64%]--[loss-2.791448: wl-3.762332, gl-1.850865]--[lr: pb-0.000050, pf-0.000040]--[ETA-1 day, 2:24:46]
2023.01.26-16:32:32:348:[step-70500/177600: 39.70%]--[loss-2.962753: wl-3.833959, gl-2.004263]--[lr: pb-0.000050, pf-0.000040]--[ETA-1 day, 2:57:52]
2023.01.26-16:34:03:448:[step-70600/177600: 39.75%]--[loss-2.657655: wl-3.408141, gl-1.805619]--[lr: pb-0.000050, pf-0.000040]--[ETA-1 day, 3:19:33]
2023.01.26-16:35:34:548:[step-70700/177600: 39.81%]--[loss-2.888040: wl-4.356959, gl-1.798800]--[lr: pb-0.000050, pf-0.000040]--[ETA-1 day, 3:12:02]
2023.01.26-16:37:06:648:[step-70800/177600: 39.86%]--[loss-3.015825: wl-3.875132, gl-2.047042]--[lr: pb-0.000050, pf-0.000040]--[ETA-1 day, 2:53:07]
2023.01.26-16:38:37:748:[step-70900/177600: 39.92%]--[loss-2.894382: wl-4.014067, gl-1.890865]--[lr: pb-0.000050, pf-0.000040]--[ETA-1 day, 3:06:31]
2023.01.26-16:40:09:848:[step-71000/177600: 39.98%]--[loss-2.871008: wl-4.141616, gl-1.835604]--[lr: pb-0.000050, pf-0.000040]--[ETA-1 day, 2:27:48]
End of epoch 80 / 200 	 Time Taken: 812 sec
saving the model at the end of epoch 80, iters 71040
2023.01.26-16:41:42:60:[step-71100/177600: 40.03%]--[loss-2.935457: wl-3.716249, gl-2.006395]--[lr: pb-0.000050, pf-0.000040]--[ETA-1 day, 2:26:12]
2023.01.26-16:43:15:160:[step-71200/177600: 40.09%]--[loss-3.219759: wl-4.909814, gl-1.992306]--[lr: pb-0.000050, pf-0.000040]--[ETA-1 day, 3:08:11]
2023.01.26-16:44:48:260:[step-71300/177600: 40.15%]--[loss-2.730021: wl-3.479291, gl-1.860198]--[lr: pb-0.000050, pf-0.000040]--[ETA-1 day, 2:05:08]
2023.01.26-16:46:19:360:[step-71400/177600: 40.20%]--[loss-2.837441: wl-3.743796, gl-1.901492]--[lr: pb-0.000050, pf-0.000040]--[ETA-1 day, 3:22:53]
2023.01.26-16:47:50:460:[step-71500/177600: 40.26%]--[loss-2.792209: wl-3.738554, gl-1.857570]--[lr: pb-0.000050, pf-0.000040]--[ETA-1 day, 3:03:17]
2023.01.26-16:49:21:560:[step-71600/177600: 40.32%]--[loss-2.907352: wl-3.871430, gl-1.939494]--[lr: pb-0.000050, pf-0.000040]--[ETA-1 day, 3:17:36]
2023.01.26-16:50:53:660:[step-71700/177600: 40.37%]--[loss-2.893042: wl-4.223129, gl-1.837259]--[lr: pb-0.000050, pf-0.000040]--[ETA-1 day, 3:20:07]
2023.01.26-16:52:23:760:[step-71800/177600: 40.43%]--[loss-2.846008: wl-3.816306, gl-1.891932]--[lr: pb-0.000050, pf-0.000040]--[ETA-1 day, 1:57:25]
2023.01.26-16:53:55:860:[step-71900/177600: 40.48%]--[loss-2.887959: wl-3.781244, gl-1.942648]--[lr: pb-0.000050, pf-0.000040]--[ETA-1 day, 2:38:58]
End of epoch 81 / 200 	 Time Taken: 814 sec
2023.01.26-16:55:27:72:[step-72000/177600: 40.54%]--[loss-3.242994: wl-4.140683, gl-2.207823]--[lr: pb-0.000050, pf-0.000040]--[ETA-1 day, 2:17:39]
2023.01.26-16:56:59:172:[step-72100/177600: 40.60%]--[loss-3.020275: wl-4.586931, gl-1.873542]--[lr: pb-0.000050, pf-0.000040]--[ETA-1 day, 2:29:36]
2023.01.26-16:58:30:272:[step-72200/177600: 40.65%]--[loss-2.944830: wl-4.108977, gl-1.917586]--[lr: pb-0.000050, pf-0.000040]--[ETA-1 day, 3:05:33]
2023.01.26-17:00:03:372:[step-72300/177600: 40.71%]--[loss-2.958880: wl-3.954859, gl-1.970165]--[lr: pb-0.000050, pf-0.000040]--[ETA-1 day, 2:54:01]
2023.01.26-17:01:34:472:[step-72400/177600: 40.77%]--[loss-2.911234: wl-4.003932, gl-1.910251]--[lr: pb-0.000050, pf-0.000040]--[ETA-1 day, 2:39:42]
2023.01.26-17:03:06:572:[step-72500/177600: 40.82%]--[loss-3.163883: wl-4.842852, gl-1.953170]--[lr: pb-0.000050, pf-0.000040]--[ETA-1 day, 3:00:15]
2023.01.26-17:04:37:672:[step-72600/177600: 40.88%]--[loss-2.814141: wl-3.812781, gl-1.860945]--[lr: pb-0.000050, pf-0.000040]--[ETA-1 day, 2:13:20]
2023.01.26-17:06:08:772:[step-72700/177600: 40.93%]--[loss-2.894257: wl-4.120818, gl-1.864053]--[lr: pb-0.000050, pf-0.000040]--[ETA-1 day, 2:20:09]
2023.01.26-17:07:39:872:[step-72800/177600: 40.99%]--[loss-3.015477: wl-4.259040, gl-1.950717]--[lr: pb-0.000050, pf-0.000040]--[ETA-1 day, 1:52:24]
End of epoch 82 / 200 	 Time Taken: 813 sec
2023.01.26-17:09:12:84:[step-72900/177600: 41.05%]--[loss-2.941824: wl-3.733404, gl-2.008473]--[lr: pb-0.000050, pf-0.000039]--[ETA-1 day, 1:50:44]
2023.01.26-17:10:43:184:[step-73000/177600: 41.10%]--[loss-2.742395: wl-3.767051, gl-1.800633]--[lr: pb-0.000050, pf-0.000039]--[ETA-1 day, 3:12:15]
2023.01.26-17:12:14:284:[step-73100/177600: 41.16%]--[loss-2.811777: wl-3.552683, gl-1.923606]--[lr: pb-0.000050, pf-0.000039]--[ETA-1 day, 1:52:44]
2023.01.26-17:13:44:384:[step-73200/177600: 41.22%]--[loss-2.841724: wl-3.752411, gl-1.903621]--[lr: pb-0.000050, pf-0.000039]--[ETA-1 day, 2:10:46]
2023.01.26-17:15:16:484:[step-73300/177600: 41.27%]--[loss-3.059879: wl-3.911058, gl-2.082114]--[lr: pb-0.000050, pf-0.000039]--[ETA-1 day, 1:43:12]
2023.01.26-17:16:47:584:[step-73400/177600: 41.33%]--[loss-2.828824: wl-4.008624, gl-1.826668]--[lr: pb-0.000050, pf-0.000039]--[ETA-1 day, 1:49:47]
2023.01.26-17:18:18:684:[step-73500/177600: 41.39%]--[loss-3.100157: wl-4.175428, gl-2.056300]--[lr: pb-0.000050, pf-0.000039]--[ETA-1 day, 2:34:56]
2023.01.26-17:19:49:784:[step-73600/177600: 41.44%]--[loss-2.874848: wl-3.765286, gl-1.933526]--[lr: pb-0.000050, pf-0.000039]--[ETA-1 day, 2:39:00]
2023.01.26-17:21:21:884:[step-73700/177600: 41.50%]--[loss-3.200262: wl-4.201564, gl-2.149871]--[lr: pb-0.000050, pf-0.000039]--[ETA-1 day, 3:49:50]
End of epoch 83 / 200 	 Time Taken: 811 sec
2023.01.26-17:22:54:96:[step-73800/177600: 41.55%]--[loss-2.868830: wl-3.827639, gl-1.911921]--[lr: pb-0.000050, pf-0.000039]--[ETA-1 day, 2:18:37]
2023.01.26-17:24:26:196:[step-73900/177600: 41.61%]--[loss-2.790137: wl-3.850491, gl-1.827514]--[lr: pb-0.000050, pf-0.000039]--[ETA-1 day, 2:53:24]
2023.01.26-17:25:58:296:[step-74000/177600: 41.67%]--[loss-2.769610: wl-3.538272, gl-1.885042]--[lr: pb-0.000050, pf-0.000039]--[ETA-1 day, 1:44:01]
2023.01.26-17:27:29:396:[step-74100/177600: 41.72%]--[loss-2.883651: wl-4.010548, gl-1.881014]--[lr: pb-0.000050, pf-0.000039]--[ETA-1 day, 5:01:57]
2023.01.26-17:29:01:496:[step-74200/177600: 41.78%]--[loss-3.049605: wl-4.872456, gl-1.831491]--[lr: pb-0.000050, pf-0.000039]--[ETA-1 day, 2:42:12]
2023.01.26-17:30:32:596:[step-74300/177600: 41.84%]--[loss-2.970351: wl-4.138115, gl-1.935822]--[lr: pb-0.000050, pf-0.000039]--[ETA-1 day, 5:34:55]
2023.01.26-17:32:04:696:[step-74400/177600: 41.89%]--[loss-2.886852: wl-4.141078, gl-1.851583]--[lr: pb-0.000050, pf-0.000039]--[ETA-1 day, 1:33:28]
2023.01.26-17:33:34:796:[step-74500/177600: 41.95%]--[loss-2.881106: wl-3.836757, gl-1.921916]--[lr: pb-0.000050, pf-0.000039]--[ETA-1 day, 2:05:45]
End of epoch 84 / 200 	 Time Taken: 813 sec
2023.01.26-17:35:07:8:[step-74600/177600: 42.00%]--[loss-2.718860: wl-3.763326, gl-1.778028]--[lr: pb-0.000050, pf-0.000039]--[ETA-1 day, 4:28:47]
2023.01.26-17:36:38:108:[step-74700/177600: 42.06%]--[loss-2.835924: wl-4.141882, gl-1.800454]--[lr: pb-0.000050, pf-0.000039]--[ETA-1 day, 1:30:33]
2023.01.26-17:38:09:208:[step-74800/177600: 42.12%]--[loss-2.750617: wl-3.788266, gl-1.803551]--[lr: pb-0.000050, pf-0.000039]--[ETA-1 day, 1:22:12]
2023.01.26-17:39:40:308:[step-74900/177600: 42.17%]--[loss-2.697576: wl-3.496131, gl-1.823543]--[lr: pb-0.000050, pf-0.000039]--[ETA-1 day, 1:49:07]
2023.01.26-17:41:12:408:[step-75000/177600: 42.23%]--[loss-2.818499: wl-3.542238, gl-1.932940]--[lr: pb-0.000050, pf-0.000039]--[ETA-1 day, 1:23:50]
2023.01.26-17:42:44:508:[step-75100/177600: 42.29%]--[loss-3.109714: wl-4.612003, gl-1.956713]--[lr: pb-0.000050, pf-0.000039]--[ETA-1 day, 3:28:24]
2023.01.26-17:44:15:608:[step-75200/177600: 42.34%]--[loss-2.992681: wl-3.987137, gl-1.995896]--[lr: pb-0.000050, pf-0.000039]--[ETA-1 day, 1:52:33]
2023.01.26-17:45:46:708:[step-75300/177600: 42.40%]--[loss-2.852663: wl-4.051447, gl-1.839801]--[lr: pb-0.000050, pf-0.000039]--[ETA-1 day, 1:11:26]
2023.01.26-17:47:17:808:[step-75400/177600: 42.45%]--[loss-2.911781: wl-4.184578, gl-1.865637]--[lr: pb-0.000050, pf-0.000039]--[ETA-1 day, 1:45:09]
End of epoch 85 / 200 	 Time Taken: 812 sec
saving the model at the end of epoch 85, iters 75480
2023.01.26-17:48:51:20:[step-75500/177600: 42.51%]--[loss-2.869049: wl-3.859620, gl-1.904144]--[lr: pb-0.000050, pf-0.000038]--[ETA-1 day, 1:05:11]
2023.01.26-17:50:22:120:[step-75600/177600: 42.57%]--[loss-3.251951: wl-4.199338, gl-2.202116]--[lr: pb-0.000050, pf-0.000038]--[ETA-1 day, 1:10:56]
2023.01.26-17:51:53:220:[step-75700/177600: 42.62%]--[loss-2.677829: wl-3.566302, gl-1.786254]--[lr: pb-0.000050, pf-0.000038]--[ETA-1 day, 2:12:34]
2023.01.26-17:53:24:320:[step-75800/177600: 42.68%]--[loss-2.965523: wl-4.542804, gl-1.829822]--[lr: pb-0.000050, pf-0.000038]--[ETA-1 day, 2:03:06]
2023.01.26-17:54:56:420:[step-75900/177600: 42.74%]--[loss-3.046353: wl-4.810791, gl-1.843656]--[lr: pb-0.000050, pf-0.000038]--[ETA-1 day, 1:05:46]
2023.01.26-17:56:27:520:[step-76000/177600: 42.79%]--[loss-2.874135: wl-3.603294, gl-1.973312]--[lr: pb-0.000050, pf-0.000038]--[ETA-1 day, 1:28:22]
2023.01.26-17:57:58:620:[step-76100/177600: 42.85%]--[loss-2.922032: wl-3.915993, gl-1.943034]--[lr: pb-0.000050, pf-0.000038]--[ETA-1 day, 1:11:03]
2023.01.26-17:59:29:720:[step-76200/177600: 42.91%]--[loss-3.010385: wl-4.250039, gl-1.947876]--[lr: pb-0.000050, pf-0.000038]--[ETA-1 day, 2:01:00]
2023.01.26-18:01:00:820:[step-76300/177600: 42.96%]--[loss-2.875620: wl-3.904219, gl-1.899565]--[lr: pb-0.000050, pf-0.000038]--[ETA-1 day, 0:59:51]
End of epoch 86 / 200 	 Time Taken: 810 sec
2023.01.26-18:02:33:32:[step-76400/177600: 43.02%]--[loss-2.949085: wl-3.857466, gl-1.984718]--[lr: pb-0.000050, pf-0.000038]--[ETA-1 day, 2:44:07]
2023.01.26-18:04:03:132:[step-76500/177600: 43.07%]--[loss-2.828492: wl-3.620526, gl-1.923361]--[lr: pb-0.000050, pf-0.000038]--[ETA-1 day, 1:57:57]
2023.01.26-18:05:35:232:[step-76600/177600: 43.13%]--[loss-2.732096: wl-3.870123, gl-1.764565]--[lr: pb-0.000050, pf-0.000038]--[ETA-1 day, 1:29:46]
2023.01.26-18:07:06:332:[step-76700/177600: 43.19%]--[loss-2.723196: wl-3.466896, gl-1.856472]--[lr: pb-0.000050, pf-0.000038]--[ETA-1 day, 1:40:26]
2023.01.26-18:08:37:432:[step-76800/177600: 43.24%]--[loss-2.999396: wl-4.513819, gl-1.870941]--[lr: pb-0.000050, pf-0.000038]--[ETA-1 day, 1:17:09]
2023.01.26-18:10:08:532:[step-76900/177600: 43.30%]--[loss-2.772674: wl-3.478534, gl-1.903040]--[lr: pb-0.000050, pf-0.000038]--[ETA-1 day, 1:24:23]
2023.01.26-18:11:40:632:[step-77000/177600: 43.36%]--[loss-3.015433: wl-4.386421, gl-1.918827]--[lr: pb-0.000050, pf-0.000038]--[ETA-1 day, 1:15:26]
2023.01.26-18:13:11:732:[step-77100/177600: 43.41%]--[loss-2.875734: wl-3.917979, gl-1.896239]--[lr: pb-0.000050, pf-0.000038]--[ETA-1 day, 2:27:28]
2023.01.26-18:14:42:832:[step-77200/177600: 43.47%]--[loss-2.915879: wl-3.744857, gl-1.979665]--[lr: pb-0.000050, pf-0.000038]--[ETA-1 day, 0:48:44]
End of epoch 87 / 200 	 Time Taken: 810 sec
2023.01.26-18:16:14:44:[step-77300/177600: 43.52%]--[loss-2.888299: wl-4.137321, gl-1.853968]--[lr: pb-0.000050, pf-0.000038]--[ETA-1 day, 1:47:53]
2023.01.26-18:17:45:144:[step-77400/177600: 43.58%]--[loss-2.859009: wl-4.013596, gl-1.855610]--[lr: pb-0.000050, pf-0.000038]--[ETA-1 day, 1:57:07]
2023.01.26-18:19:17:244:[step-77500/177600: 43.64%]--[loss-2.910478: wl-3.825773, gl-1.954035]--[lr: pb-0.000050, pf-0.000038]--[ETA-1 day, 2:26:23]
2023.01.26-18:20:48:344:[step-77600/177600: 43.69%]--[loss-2.929710: wl-4.019348, gl-1.924873]--[lr: pb-0.000050, pf-0.000038]--[ETA-1 day, 0:47:07]
2023.01.26-18:22:19:444:[step-77700/177600: 43.75%]--[loss-2.890324: wl-3.965205, gl-1.899023]--[lr: pb-0.000050, pf-0.000038]--[ETA-1 day, 0:33:00]
2023.01.26-18:23:51:544:[step-77800/177600: 43.81%]--[loss-2.670817: wl-3.879057, gl-1.701053]--[lr: pb-0.000050, pf-0.000038]--[ETA-1 day, 2:04:13]
2023.01.26-18:25:23:644:[step-77900/177600: 43.86%]--[loss-2.681369: wl-3.552819, gl-1.793164]--[lr: pb-0.000050, pf-0.000038]--[ETA-1 day, 0:50:42]
2023.01.26-18:26:54:744:[step-78000/177600: 43.92%]--[loss-2.757699: wl-3.680598, gl-1.837550]--[lr: pb-0.000050, pf-0.000038]--[ETA-1 day, 0:33:24]
2023.01.26-18:28:25:844:[step-78100/177600: 43.98%]--[loss-2.763779: wl-3.737992, gl-1.829281]--[lr: pb-0.000050, pf-0.000038]--[ETA-1 day, 2:34:48]
End of epoch 88 / 200 	 Time Taken: 812 sec
2023.01.26-18:29:58:56:[step-78200/177600: 44.03%]--[loss-2.699958: wl-3.544389, gl-1.813861]--[lr: pb-0.000050, pf-0.000037]--[ETA-1 day, 1:09:13]
2023.01.26-18:31:29:156:[step-78300/177600: 44.09%]--[loss-2.823957: wl-3.752527, gl-1.885825]--[lr: pb-0.000050, pf-0.000037]--[ETA-1 day, 1:53:20]
2023.01.26-18:33:01:256:[step-78400/177600: 44.14%]--[loss-2.870282: wl-3.681752, gl-1.949844]--[lr: pb-0.000050, pf-0.000037]--[ETA-1 day, 1:25:55]
2023.01.26-18:34:32:356:[step-78500/177600: 44.20%]--[loss-2.772108: wl-3.728036, gl-1.840099]--[lr: pb-0.000050, pf-0.000037]--[ETA-1 day, 0:29:13]
2023.01.26-18:36:03:456:[step-78600/177600: 44.26%]--[loss-2.806838: wl-3.890031, gl-1.834330]--[lr: pb-0.000050, pf-0.000037]--[ETA-1 day, 1:02:19]
2023.01.26-18:37:34:556:[step-78700/177600: 44.31%]--[loss-2.754476: wl-3.812988, gl-1.801229]--[lr: pb-0.000050, pf-0.000037]--[ETA-1 day, 1:17:17]
2023.01.26-18:39:05:656:[step-78800/177600: 44.37%]--[loss-2.994474: wl-4.605976, gl-1.842980]--[lr: pb-0.000050, pf-0.000037]--[ETA-1 day, 1:23:04]
2023.01.26-18:40:37:756:[step-78900/177600: 44.43%]--[loss-2.748410: wl-3.556370, gl-1.859317]--[lr: pb-0.000050, pf-0.000037]--[ETA-1 day, 0:25:36]
2023.01.26-18:42:08:856:[step-79000/177600: 44.48%]--[loss-2.939959: wl-4.276080, gl-1.870939]--[lr: pb-0.000050, pf-0.000037]--[ETA-1 day, 2:01:36]
End of epoch 89 / 200 	 Time Taken: 812 sec
2023.01.26-18:43:41:68:[step-79100/177600: 44.54%]--[loss-2.776245: wl-3.687670, gl-1.854327]--[lr: pb-0.000050, pf-0.000037]--[ETA-1 day, 0:26:45]
2023.01.26-18:45:13:168:[step-79200/177600: 44.59%]--[loss-2.910560: wl-4.093990, gl-1.887063]--[lr: pb-0.000050, pf-0.000037]--[ETA-1 day, 0:44:17]
2023.01.26-18:46:45:268:[step-79300/177600: 44.65%]--[loss-2.793827: wl-3.688035, gl-1.871818]--[lr: pb-0.000050, pf-0.000037]--[ETA-1 day, 0:50:38]
2023.01.26-18:48:19:368:[step-79400/177600: 44.71%]--[loss-2.716622: wl-3.600134, gl-1.816588]--[lr: pb-0.000050, pf-0.000037]--[ETA-1 day, 1:03:40]
2023.01.26-18:49:52:468:[step-79500/177600: 44.76%]--[loss-2.948925: wl-4.344012, gl-1.862922]--[lr: pb-0.000050, pf-0.000037]--[ETA-1 day, 1:06:19]
2023.01.26-18:51:25:568:[step-79600/177600: 44.82%]--[loss-2.890866: wl-3.754200, gl-1.952316]--[lr: pb-0.000050, pf-0.000037]--[ETA-1 day, 2:08:52]
2023.01.26-18:52:57:668:[step-79700/177600: 44.88%]--[loss-2.873906: wl-3.921587, gl-1.893510]--[lr: pb-0.000050, pf-0.000037]--[ETA-1 day, 1:08:32]
2023.01.26-18:54:27:768:[step-79800/177600: 44.93%]--[loss-2.770549: wl-3.768807, gl-1.828347]--[lr: pb-0.000050, pf-0.000037]--[ETA-1 day, 0:32:53]
2023.01.26-18:55:59:868:[step-79900/177600: 44.99%]--[loss-2.605813: wl-3.448783, gl-1.743617]--[lr: pb-0.000050, pf-0.000037]--[ETA-1 day, 0:31:50]
End of epoch 90 / 200 	 Time Taken: 819 sec
saving the model at the end of epoch 90, iters 79920
2023.01.26-18:57:31:80:[step-80000/177600: 45.05%]--[loss-2.614105: wl-3.320761, gl-1.783915]--[lr: pb-0.000050, pf-0.000037]--[ETA-1 day, 0:20:29]
2023.01.26-18:59:03:180:[step-80100/177600: 45.10%]--[loss-3.464174: wl-5.065433, gl-2.197816]--[lr: pb-0.000050, pf-0.000037]--[ETA-1 day, 0:54:50]
2023.01.26-19:00:34:280:[step-80200/177600: 45.16%]--[loss-2.706246: wl-3.810725, gl-1.753565]--[lr: pb-0.000050, pf-0.000037]--[ETA-1 day, 0:08:57]
2023.01.26-19:02:05:380:[step-80300/177600: 45.21%]--[loss-2.833212: wl-3.858692, gl-1.868539]--[lr: pb-0.000050, pf-0.000037]--[ETA-1 day, 0:04:26]
2023.01.26-19:03:37:480:[step-80400/177600: 45.27%]--[loss-2.755396: wl-3.520211, gl-1.875343]--[lr: pb-0.000050, pf-0.000037]--[ETA-1 day, 0:34:58]
2023.01.26-19:05:08:580:[step-80500/177600: 45.33%]--[loss-3.048124: wl-4.103744, gl-2.022188]--[lr: pb-0.000050, pf-0.000037]--[ETA-1 day, 0:05:56]
2023.01.26-19:06:39:680:[step-80600/177600: 45.38%]--[loss-2.996982: wl-4.221131, gl-1.941699]--[lr: pb-0.000050, pf-0.000037]--[ETA-1 day, 0:00:03]
2023.01.26-19:08:10:780:[step-80700/177600: 45.44%]--[loss-2.876575: wl-4.028129, gl-1.869543]--[lr: pb-0.000050, pf-0.000037]--[ETA-1 day, 0:07:29]
2023.01.26-19:09:42:880:[step-80800/177600: 45.50%]--[loss-2.714520: wl-3.648060, gl-1.802505]--[lr: pb-0.000050, pf-0.000037]--[ETA-1 day, 0:01:34]
End of epoch 91 / 200 	 Time Taken: 811 sec
2023.01.26-19:11:14:92:[step-80900/177600: 45.55%]--[loss-2.894173: wl-3.837109, gl-1.934896]--[lr: pb-0.000050, pf-0.000036]--[ETA-1 day, 0:19:03]
2023.01.26-19:12:45:192:[step-81000/177600: 45.61%]--[loss-2.626287: wl-3.582603, gl-1.730636]--[lr: pb-0.000050, pf-0.000036]--[ETA-23:56:03]
2023.01.26-19:14:17:292:[step-81100/177600: 45.66%]--[loss-2.812209: wl-3.467511, gl-1.945332]--[lr: pb-0.000050, pf-0.000036]--[ETA-1 day, 1:07:21]
2023.01.26-19:15:48:392:[step-81200/177600: 45.72%]--[loss-2.691839: wl-3.968680, gl-1.699669]--[lr: pb-0.000050, pf-0.000036]--[ETA-1 day, 0:46:41]
2023.01.26-19:17:19:492:[step-81300/177600: 45.78%]--[loss-3.151854: wl-4.525224, gl-2.020548]--[lr: pb-0.000050, pf-0.000036]--[ETA-1 day, 0:11:04]
2023.01.26-19:18:51:592:[step-81400/177600: 45.83%]--[loss-2.724078: wl-3.540365, gl-1.838987]--[lr: pb-0.000050, pf-0.000036]--[ETA-1 day, 0:12:17]
2023.01.26-19:20:22:692:[step-81500/177600: 45.89%]--[loss-2.852451: wl-3.583808, gl-1.956499]--[lr: pb-0.000050, pf-0.000036]--[ETA-1 day, 0:27:48]
2023.01.26-19:21:53:792:[step-81600/177600: 45.95%]--[loss-2.791100: wl-3.825139, gl-1.834815]--[lr: pb-0.000050, pf-0.000036]--[ETA-23:40:23]
End of epoch 92 / 200 	 Time Taken: 812 sec
2023.01.26-19:23:26:4:[step-81700/177600: 46.00%]--[loss-2.856335: wl-4.095048, gl-1.832573]--[lr: pb-0.000050, pf-0.000036]--[ETA-1 day, 0:25:07]
2023.01.26-19:24:58:104:[step-81800/177600: 46.06%]--[loss-2.742529: wl-3.901538, gl-1.767144]--[lr: pb-0.000050, pf-0.000036]--[ETA-1 day, 0:18:41]
2023.01.26-19:26:29:204:[step-81900/177600: 46.11%]--[loss-2.917786: wl-4.053321, gl-1.904455]--[lr: pb-0.000050, pf-0.000036]--[ETA-23:39:57]
2023.01.26-19:28:00:304:[step-82000/177600: 46.17%]--[loss-2.934791: wl-3.963584, gl-1.943895]--[lr: pb-0.000050, pf-0.000036]--[ETA-1 day, 0:09:26]
2023.01.26-19:29:32:404:[step-82100/177600: 46.23%]--[loss-2.833468: wl-3.916704, gl-1.854292]--[lr: pb-0.000050, pf-0.000036]--[ETA-1 day, 0:55:02]
2023.01.26-19:31:03:504:[step-82200/177600: 46.28%]--[loss-2.799340: wl-4.012276, gl-1.796271]--[lr: pb-0.000050, pf-0.000036]--[ETA-23:41:47]
2023.01.26-19:32:35:604:[step-82300/177600: 46.34%]--[loss-2.731379: wl-3.831658, gl-1.773465]--[lr: pb-0.000050, pf-0.000036]--[ETA-1 day, 0:39:33]
2023.01.26-19:34:06:704:[step-82400/177600: 46.40%]--[loss-2.803023: wl-3.829197, gl-1.845724]--[lr: pb-0.000050, pf-0.000036]--[ETA-1 day, 0:42:51]
2023.01.26-19:35:38:804:[step-82500/177600: 46.45%]--[loss-2.886098: wl-4.137500, gl-1.851723]--[lr: pb-0.000050, pf-0.000036]--[ETA-1 day, 0:01:14]
End of epoch 93 / 200 	 Time Taken: 814 sec
2023.01.26-19:37:11:16:[step-82600/177600: 46.51%]--[loss-2.878131: wl-4.096126, gl-1.854099]--[lr: pb-0.000050, pf-0.000036]--[ETA-1 day, 1:30:05]
2023.01.26-19:38:42:116:[step-82700/177600: 46.57%]--[loss-2.710717: wl-3.665371, gl-1.794374]--[lr: pb-0.000050, pf-0.000036]--[ETA-1 day, 0:02:00]
2023.01.26-19:40:13:216:[step-82800/177600: 46.62%]--[loss-2.690606: wl-3.414097, gl-1.837081]--[lr: pb-0.000050, pf-0.000036]--[ETA-23:42:34]
2023.01.26-19:41:44:316:[step-82900/177600: 46.68%]--[loss-2.749619: wl-4.031580, gl-1.741724]--[lr: pb-0.000050, pf-0.000036]--[ETA-23:38:44]
2023.01.26-19:43:16:416:[step-83000/177600: 46.73%]--[loss-2.723468: wl-3.702531, gl-1.797835]--[lr: pb-0.000050, pf-0.000036]--[ETA-23:50:40]
2023.01.26-19:44:47:516:[step-83100/177600: 46.79%]--[loss-3.007929: wl-4.546814, gl-1.871226]--[lr: pb-0.000050, pf-0.000036]--[ETA-23:57:46]
2023.01.26-19:46:18:616:[step-83200/177600: 46.85%]--[loss-3.016453: wl-3.932420, gl-2.033348]--[lr: pb-0.000050, pf-0.000036]--[ETA-1 day, 0:06:23]
2023.01.26-19:47:50:716:[step-83300/177600: 46.90%]--[loss-3.043274: wl-4.861084, gl-1.828003]--[lr: pb-0.000050, pf-0.000036]--[ETA-23:16:39]
2023.01.26-19:49:21:816:[step-83400/177600: 46.96%]--[loss-2.857460: wl-3.990299, gl-1.859885]--[lr: pb-0.000050, pf-0.000036]--[ETA-1 day, 0:55:24]
End of epoch 94 / 200 	 Time Taken: 810 sec
2023.01.26-19:50:53:28:[step-83500/177600: 47.02%]--[loss-2.639560: wl-3.564619, gl-1.748405]--[lr: pb-0.000050, pf-0.000035]--[ETA-23:24:58]
2023.01.26-19:52:24:128:[step-83600/177600: 47.07%]--[loss-2.887822: wl-4.108183, gl-1.860776]--[lr: pb-0.000050, pf-0.000035]--[ETA-1 day, 0:17:38]
2023.01.26-19:53:56:228:[step-83700/177600: 47.13%]--[loss-2.602914: wl-3.619366, gl-1.698072]--[lr: pb-0.000050, pf-0.000035]--[ETA-23:34:54]
2023.01.26-19:55:27:328:[step-83800/177600: 47.18%]--[loss-2.979242: wl-4.537393, gl-1.844894]--[lr: pb-0.000050, pf-0.000035]--[ETA-23:46:14]
2023.01.26-19:56:58:428:[step-83900/177600: 47.24%]--[loss-2.783966: wl-3.550890, gl-1.896243]--[lr: pb-0.000050, pf-0.000035]--[ETA-23:07:28]
2023.01.26-19:58:29:528:[step-84000/177600: 47.30%]--[loss-2.692480: wl-3.522204, gl-1.811929]--[lr: pb-0.000050, pf-0.000035]--[ETA-23:38:35]
2023.01.26-20:00:01:628:[step-84100/177600: 47.35%]--[loss-2.913640: wl-4.062789, gl-1.897943]--[lr: pb-0.000050, pf-0.000035]--[ETA-23:27:13]
2023.01.26-20:01:31:728:[step-84200/177600: 47.41%]--[loss-2.863765: wl-3.932077, gl-1.880746]--[lr: pb-0.000050, pf-0.000035]--[ETA-23:31:10]
2023.01.26-20:03:03:828:[step-84300/177600: 47.47%]--[loss-2.858393: wl-4.053822, gl-1.844938]--[lr: pb-0.000050, pf-0.000035]--[ETA-23:50:05]
End of epoch 95 / 200 	 Time Taken: 811 sec
saving the model at the end of epoch 95, iters 84360
2023.01.26-20:04:36:40:[step-84400/177600: 47.52%]--[loss-2.708049: wl-3.974470, gl-1.714431]--[lr: pb-0.000050, pf-0.000035]--[ETA-23:05:18]
2023.01.26-20:06:07:140:[step-84500/177600: 47.58%]--[loss-2.826909: wl-4.201210, gl-1.776606]--[lr: pb-0.000050, pf-0.000035]--[ETA-22:51:31]
2023.01.26-20:07:38:240:[step-84600/177600: 47.64%]--[loss-2.727331: wl-3.649801, gl-1.814881]--[lr: pb-0.000050, pf-0.000035]--[ETA-22:57:22]
2023.01.26-20:09:09:340:[step-84700/177600: 47.69%]--[loss-2.587118: wl-3.627305, gl-1.680291]--[lr: pb-0.000050, pf-0.000035]--[ETA-22:59:45]
2023.01.26-20:10:40:440:[step-84800/177600: 47.75%]--[loss-2.782432: wl-3.806108, gl-1.830905]--[lr: pb-0.000050, pf-0.000035]--[ETA-23:56:10]
2023.01.26-20:12:11:540:[step-84900/177600: 47.80%]--[loss-2.903871: wl-4.539234, gl-1.769062]--[lr: pb-0.000050, pf-0.000035]--[ETA-23:21:49]
2023.01.26-20:13:42:640:[step-85000/177600: 47.86%]--[loss-2.702198: wl-3.704140, gl-1.776163]--[lr: pb-0.000050, pf-0.000035]--[ETA-22:56:54]
2023.01.26-20:15:13:740:[step-85100/177600: 47.92%]--[loss-2.794727: wl-3.811382, gl-1.841881]--[lr: pb-0.000050, pf-0.000035]--[ETA-23:13:49]
2023.01.26-20:16:44:840:[step-85200/177600: 47.97%]--[loss-2.929045: wl-4.024881, gl-1.922825]--[lr: pb-0.000050, pf-0.000035]--[ETA-23:26:15]
End of epoch 96 / 200 	 Time Taken: 809 sec
2023.01.26-20:18:17:52:[step-85300/177600: 48.03%]--[loss-2.848839: wl-3.826261, gl-1.892274]--[lr: pb-0.000050, pf-0.000035]--[ETA-23:24:02]
2023.01.26-20:19:48:152:[step-85400/177600: 48.09%]--[loss-2.834823: wl-3.778229, gl-1.890265]--[lr: pb-0.000050, pf-0.000035]--[ETA-23:42:19]
2023.01.26-20:21:19:252:[step-85500/177600: 48.14%]--[loss-2.816975: wl-4.009099, gl-1.814700]--[lr: pb-0.000050, pf-0.000035]--[ETA-23:54:48]
2023.01.26-20:22:50:352:[step-85600/177600: 48.20%]--[loss-2.886781: wl-3.818879, gl-1.932062]--[lr: pb-0.000050, pf-0.000035]--[ETA-23:17:47]
2023.01.26-20:24:24:452:[step-85700/177600: 48.25%]--[loss-2.923447: wl-4.022711, gl-1.917769]--[lr: pb-0.000050, pf-0.000035]--[ETA-1 day, 0:19:54]
2023.01.26-20:26:03:552:[step-85800/177600: 48.31%]--[loss-2.959025: wl-3.934891, gl-1.975302]--[lr: pb-0.000050, pf-0.000035]--[ETA-1 day, 1:39:22]
2023.01.26-20:27:42:652:[step-85900/177600: 48.37%]--[loss-2.651850: wl-3.696737, gl-1.727666]--[lr: pb-0.000050, pf-0.000035]--[ETA-1 day, 1:52:39]
2023.01.26-20:29:22:752:[step-86000/177600: 48.42%]--[loss-2.753607: wl-3.930340, gl-1.771022]--[lr: pb-0.000050, pf-0.000035]--[ETA-23:45:47]
2023.01.26-20:31:03:852:[step-86100/177600: 48.48%]--[loss-2.688588: wl-3.762624, gl-1.747932]--[lr: pb-0.000050, pf-0.000035]--[ETA-1 day, 3:39:46]
End of epoch 97 / 200 	 Time Taken: 847 sec
2023.01.26-20:32:35:64:[step-86200/177600: 48.54%]--[loss-2.851966: wl-3.783942, gl-1.905981]--[lr: pb-0.000050, pf-0.000034]--[ETA-23:40:51]
2023.01.26-20:34:06:164:[step-86300/177600: 48.59%]--[loss-2.803442: wl-3.664474, gl-1.887323]--[lr: pb-0.000050, pf-0.000034]--[ETA-22:57:51]
2023.01.26-20:35:37:264:[step-86400/177600: 48.65%]--[loss-2.746309: wl-3.910831, gl-1.768602]--[lr: pb-0.000050, pf-0.000034]--[ETA-23:14:40]
2023.01.26-20:37:08:364:[step-86500/177600: 48.70%]--[loss-2.662143: wl-3.621632, gl-1.756734]--[lr: pb-0.000050, pf-0.000034]--[ETA-22:25:01]
2023.01.26-20:38:39:464:[step-86600/177600: 48.76%]--[loss-2.575473: wl-3.524914, gl-1.694245]--[lr: pb-0.000050, pf-0.000034]--[ETA-22:35:41]
2023.01.26-20:40:09:564:[step-86700/177600: 48.82%]--[loss-2.764703: wl-3.844260, gl-1.803638]--[lr: pb-0.000050, pf-0.000034]--[ETA-23:10:22]
2023.01.26-20:41:40:664:[step-86800/177600: 48.87%]--[loss-2.946048: wl-3.821976, gl-1.990554]--[lr: pb-0.000050, pf-0.000034]--[ETA-22:35:52]
2023.01.26-20:43:11:764:[step-86900/177600: 48.93%]--[loss-2.783051: wl-3.617337, gl-1.878717]--[lr: pb-0.000050, pf-0.000034]--[ETA-22:30:21]
2023.01.26-20:44:42:864:[step-87000/177600: 48.99%]--[loss-2.641824: wl-3.818079, gl-1.687304]--[lr: pb-0.000050, pf-0.000034]--[ETA-23:28:51]
End of epoch 98 / 200 	 Time Taken: 808 sec
2023.01.26-20:46:14:76:[step-87100/177600: 49.04%]--[loss-2.663329: wl-3.340634, gl-1.828170]--[lr: pb-0.000050, pf-0.000034]--[ETA-22:28:44]
2023.01.26-20:47:45:176:[step-87200/177600: 49.10%]--[loss-2.807179: wl-3.817393, gl-1.852831]--[lr: pb-0.000050, pf-0.000034]--[ETA-23:37:02]
2023.01.26-20:49:22:276:[step-87300/177600: 49.16%]--[loss-3.030848: wl-3.953365, gl-2.042507]--[lr: pb-0.000050, pf-0.000034]--[ETA-1 day, 0:06:16]
2023.01.26-20:51:04:376:[step-87400/177600: 49.21%]--[loss-3.085946: wl-4.219560, gl-2.031056]--[lr: pb-0.000050, pf-0.000034]--[ETA-23:05:49]
2023.01.26-20:52:46:476:[step-87500/177600: 49.27%]--[loss-2.814017: wl-3.888996, gl-1.841768]--[lr: pb-0.000050, pf-0.000034]--[ETA-1 day, 0:34:40]
2023.01.26-20:54:25:576:[step-87600/177600: 49.32%]--[loss-2.781695: wl-3.741939, gl-1.846210]--[lr: pb-0.000050, pf-0.000034]--[ETA-23:08:42]
2023.01.26-20:56:05:676:[step-87700/177600: 49.38%]--[loss-2.943488: wl-4.349917, gl-1.856009]--[lr: pb-0.000050, pf-0.000034]--[ETA-1 day, 0:55:14]
2023.01.26-20:57:37:776:[step-87800/177600: 49.44%]--[loss-2.689157: wl-3.769685, gl-1.746735]--[lr: pb-0.000050, pf-0.000034]--[ETA-1 day, 0:51:01]
2023.01.26-20:59:14:876:[step-87900/177600: 49.49%]--[loss-2.642704: wl-3.526756, gl-1.761015]--[lr: pb-0.000050, pf-0.000034]--[ETA-23:54:46]
End of epoch 99 / 200 	 Time Taken: 861 sec
2023.01.26-21:00:54:88:[step-88000/177600: 49.55%]--[loss-2.613171: wl-3.316260, gl-1.784106]--[lr: pb-0.000050, pf-0.000034]--[ETA-23:08:02]
2023.01.26-21:02:34:188:[step-88100/177600: 49.61%]--[loss-2.664759: wl-3.714710, gl-1.736082]--[lr: pb-0.000050, pf-0.000034]--[ETA-1 day, 0:38:18]
2023.01.26-21:04:13:288:[step-88200/177600: 49.66%]--[loss-2.795663: wl-3.571694, gl-1.902740]--[lr: pb-0.000050, pf-0.000034]--[ETA-23:28:39]
2023.01.26-21:05:49:388:[step-88300/177600: 49.72%]--[loss-2.978907: wl-4.093019, gl-1.955652]--[lr: pb-0.000050, pf-0.000034]--[ETA-23:14:44]
2023.01.26-21:07:28:488:[step-88400/177600: 49.77%]--[loss-2.796885: wl-3.974366, gl-1.803293]--[lr: pb-0.000050, pf-0.000034]--[ETA-23:23:08]
2023.01.26-21:09:08:588:[step-88500/177600: 49.83%]--[loss-2.761077: wl-3.569654, gl-1.868664]--[lr: pb-0.000050, pf-0.000034]--[ETA-1 day, 0:28:22]
2023.01.26-21:10:46:688:[step-88600/177600: 49.89%]--[loss-2.765441: wl-3.902190, gl-1.789893]--[lr: pb-0.000050, pf-0.000034]--[ETA-23:36:42]
2023.01.26-21:12:27:788:[step-88700/177600: 49.94%]--[loss-2.670564: wl-3.746929, gl-1.733831]--[lr: pb-0.000050, pf-0.000034]--[ETA-1 day, 4:36:01]
2023.01.26-21:14:01:888:[step-88800/177600: 50.00%]--[loss-2.890326: wl-4.127109, gl-1.858549]--[lr: pb-0.000050, pf-0.000034]--[ETA-23:24:59]
End of epoch 100 / 200 	 Time Taken: 876 sec
saving the model at the end of epoch 100, iters 88800
2023.01.26-21:15:43:100:[step-88900/177600: 50.06%]--[loss-2.925052: wl-4.333108, gl-1.841775]--[lr: pb-0.000050, pf-0.000033]--[ETA-1 day, 0:39:42]
2023.01.26-21:17:21:200:[step-89000/177600: 50.11%]--[loss-2.815328: wl-3.619603, gl-1.910427]--[lr: pb-0.000050, pf-0.000033]--[ETA-1 day, 1:53:40]
2023.01.26-21:19:00:300:[step-89100/177600: 50.17%]--[loss-3.193293: wl-4.630535, gl-2.035659]--[lr: pb-0.000050, pf-0.000033]--[ETA-23:29:02]
2023.01.26-21:20:40:400:[step-89200/177600: 50.23%]--[loss-3.300777: wl-5.412793, gl-1.947579]--[lr: pb-0.000050, pf-0.000033]--[ETA-1 day, 1:07:26]
2023.01.26-21:22:14:500:[step-89300/177600: 50.28%]--[loss-2.892431: wl-4.192266, gl-1.844365]--[lr: pb-0.000050, pf-0.000033]--[ETA-22:06:27]
2023.01.26-21:23:51:600:[step-89400/177600: 50.34%]--[loss-2.813622: wl-4.377977, gl-1.719128]--[lr: pb-0.000050, pf-0.000033]--[ETA-22:41:04]
2023.01.26-21:25:30:700:[step-89500/177600: 50.39%]--[loss-2.614019: wl-3.716854, gl-1.684806]--[lr: pb-0.000050, pf-0.000033]--[ETA-22:58:18]
2023.01.26-21:27:10:800:[step-89600/177600: 50.45%]--[loss-2.734639: wl-3.788597, gl-1.787490]--[lr: pb-0.000050, pf-0.000033]--[ETA-23:50:43]
End of epoch 101 / 200 	 Time Taken: 876 sec
2023.01.26-21:28:53:12:[step-89700/177600: 50.51%]--[loss-2.908615: wl-4.398684, gl-1.808944]--[lr: pb-0.000050, pf-0.000033]--[ETA-23:33:02]
2023.01.26-21:30:32:112:[step-89800/177600: 50.56%]--[loss-2.963006: wl-4.376249, gl-1.868943]--[lr: pb-0.000050, pf-0.000033]--[ETA-23:30:28]
2023.01.26-21:32:08:212:[step-89900/177600: 50.62%]--[loss-2.587926: wl-3.555847, gl-1.698964]--[lr: pb-0.000050, pf-0.000033]--[ETA-22:38:27]
2023.01.26-21:33:47:312:[step-90000/177600: 50.68%]--[loss-2.617663: wl-3.606727, gl-1.715982]--[lr: pb-0.000050, pf-0.000033]--[ETA-1 day, 1:10:19]
2023.01.26-21:35:27:412:[step-90100/177600: 50.73%]--[loss-3.017540: wl-4.449603, gl-1.905139]--[lr: pb-0.000050, pf-0.000033]--[ETA-1 day, 0:05:34]
2023.01.26-21:37:07:512:[step-90200/177600: 50.79%]--[loss-3.049154: wl-4.310233, gl-1.971596]--[lr: pb-0.000050, pf-0.000033]--[ETA-1 day, 0:06:58]
2023.01.26-21:38:47:612:[step-90300/177600: 50.84%]--[loss-2.749310: wl-3.857852, gl-1.784847]--[lr: pb-0.000050, pf-0.000033]--[ETA-1 day, 2:42:23]
2023.01.26-21:40:20:712:[step-90400/177600: 50.90%]--[loss-2.811299: wl-3.741550, gl-1.875911]--[lr: pb-0.000050, pf-0.000033]--[ETA-21:57:29]
2023.01.26-21:41:55:812:[step-90500/177600: 50.96%]--[loss-2.667086: wl-3.609403, gl-1.764735]--[lr: pb-0.000050, pf-0.000033]--[ETA-22:35:13]
End of epoch 102 / 200 	 Time Taken: 871 sec
2023.01.26-21:43:35:24:[step-90600/177600: 51.01%]--[loss-3.064187: wl-4.664421, gl-1.898082]--[lr: pb-0.000050, pf-0.000033]--[ETA-1 day, 0:44:26]
2023.01.26-21:45:15:124:[step-90700/177600: 51.07%]--[loss-3.064211: wl-4.203805, gl-2.013260]--[lr: pb-0.000050, pf-0.000033]--[ETA-23:10:03]
2023.01.26-21:46:55:224:[step-90800/177600: 51.13%]--[loss-2.704153: wl-3.842100, gl-1.743628]--[lr: pb-0.000050, pf-0.000033]--[ETA-23:00:30]
2023.01.26-21:48:36:324:[step-90900/177600: 51.18%]--[loss-2.784286: wl-3.631841, gl-1.876326]--[lr: pb-0.000050, pf-0.000033]--[ETA-23:36:22]
2023.01.26-21:50:08:424:[step-91000/177600: 51.24%]--[loss-2.926125: wl-4.502544, gl-1.800489]--[lr: pb-0.000050, pf-0.000033]--[ETA-21:21:52]
2023.01.26-21:51:40:524:[step-91100/177600: 51.30%]--[loss-2.867748: wl-3.852667, gl-1.904582]--[lr: pb-0.000050, pf-0.000033]--[ETA-21:56:40]
2023.01.26-21:53:18:624:[step-91200/177600: 51.35%]--[loss-2.833687: wl-3.633413, gl-1.925334]--[lr: pb-0.000050, pf-0.000033]--[ETA-22:30:46]
2023.01.26-21:54:57:724:[step-91300/177600: 51.41%]--[loss-2.859354: wl-4.248965, gl-1.797113]--[lr: pb-0.000050, pf-0.000033]--[ETA-22:37:55]
2023.01.26-21:56:37:824:[step-91400/177600: 51.46%]--[loss-2.659220: wl-3.549380, gl-1.771876]--[lr: pb-0.000050, pf-0.000033]--[ETA-22:21:41]
End of epoch 103 / 200 	 Time Taken: 871 sec
2023.01.26-21:58:19:36:[step-91500/177600: 51.52%]--[loss-3.045753: wl-4.792496, gl-1.847630]--[lr: pb-0.000050, pf-0.000032]--[ETA-22:06:38]
2023.01.26-21:59:53:136:[step-91600/177600: 51.58%]--[loss-2.925229: wl-3.880757, gl-1.955040]--[lr: pb-0.000050, pf-0.000032]--[ETA-22:04:42]
2023.01.26-22:01:28:236:[step-91700/177600: 51.63%]--[loss-2.669744: wl-3.729198, gl-1.737444]--[lr: pb-0.000050, pf-0.000032]--[ETA-1 day, 1:13:57]
2023.01.26-22:03:08:336:[step-91800/177600: 51.69%]--[loss-2.830846: wl-3.908103, gl-1.853820]--[lr: pb-0.000050, pf-0.000032]--[ETA-23:39:08]
2023.01.26-22:04:48:436:[step-91900/177600: 51.75%]--[loss-2.679025: wl-3.507772, gl-1.802082]--[lr: pb-0.000050, pf-0.000032]--[ETA-1 day, 0:42:47]
2023.01.26-22:06:27:536:[step-92000/177600: 51.80%]--[loss-2.773355: wl-3.799777, gl-1.823411]--[lr: pb-0.000050, pf-0.000032]--[ETA-21:45:50]
2023.01.26-22:07:59:636:[step-92100/177600: 51.86%]--[loss-2.831045: wl-4.239914, gl-1.771066]--[lr: pb-0.000050, pf-0.000032]--[ETA-21:32:45]
2023.01.26-22:09:31:736:[step-92200/177600: 51.91%]--[loss-2.724996: wl-3.582968, gl-1.829254]--[lr: pb-0.000050, pf-0.000032]--[ETA-21:40:13]
2023.01.26-22:11:05:836:[step-92300/177600: 51.97%]--[loss-2.738405: wl-3.784365, gl-1.792314]--[lr: pb-0.000050, pf-0.000032]--[ETA-22:58:14]
End of epoch 104 / 200 	 Time Taken: 855 sec
2023.01.26-22:12:47:48:[step-92400/177600: 52.03%]--[loss-2.618813: wl-3.651666, gl-1.705897]--[lr: pb-0.000050, pf-0.000032]--[ETA-22:20:19]
2023.01.26-22:14:29:148:[step-92500/177600: 52.08%]--[loss-2.908962: wl-4.195673, gl-1.860044]--[lr: pb-0.000050, pf-0.000032]--[ETA-23:18:11]
2023.01.26-22:16:09:248:[step-92600/177600: 52.14%]--[loss-2.713499: wl-3.888174, gl-1.741456]--[lr: pb-0.000050, pf-0.000032]--[ETA-22:45:39]
2023.01.26-22:17:51:348:[step-92700/177600: 52.20%]--[loss-2.841616: wl-4.128196, gl-1.809566]--[lr: pb-0.000050, pf-0.000032]--[ETA-23:14:41]
2023.01.26-22:19:23:448:[step-92800/177600: 52.25%]--[loss-2.696726: wl-3.753170, gl-1.758433]--[lr: pb-0.000050, pf-0.000032]--[ETA-21:34:22]
2023.01.26-22:20:55:548:[step-92900/177600: 52.31%]--[loss-2.800540: wl-4.053221, gl-1.787235]--[lr: pb-0.000050, pf-0.000032]--[ETA-20:47:51]
2023.01.26-22:22:27:648:[step-93000/177600: 52.36%]--[loss-2.799579: wl-3.745403, gl-1.863229]--[lr: pb-0.000050, pf-0.000032]--[ETA-22:02:29]
2023.01.26-22:24:01:748:[step-93100/177600: 52.42%]--[loss-3.040672: wl-4.234045, gl-1.982161]--[lr: pb-0.000050, pf-0.000032]--[ETA-1 day, 0:05:03]
2023.01.26-22:25:42:848:[step-93200/177600: 52.48%]--[loss-2.693778: wl-3.662891, gl-1.778055]--[lr: pb-0.000050, pf-0.000032]--[ETA-21:42:07]
End of epoch 105 / 200 	 Time Taken: 864 sec
saving the model at the end of epoch 105, iters 93240
2023.01.26-22:27:25:60:[step-93300/177600: 52.53%]--[loss-2.630503: wl-3.694216, gl-1.706949]--[lr: pb-0.000050, pf-0.000032]--[ETA-21:36:06]
2023.01.26-22:29:06:160:[step-93400/177600: 52.59%]--[loss-2.771363: wl-3.856136, gl-1.807329]--[lr: pb-0.000050, pf-0.000032]--[ETA-22:28:06]
2023.01.26-22:30:48:260:[step-93500/177600: 52.65%]--[loss-2.705216: wl-3.615427, gl-1.801360]--[lr: pb-0.000050, pf-0.000032]--[ETA-22:55:48]
2023.01.26-22:32:19:360:[step-93600/177600: 52.70%]--[loss-2.588297: wl-3.450439, gl-1.725688]--[lr: pb-0.000050, pf-0.000032]--[ETA-20:37:25]
2023.01.26-22:33:51:460:[step-93700/177600: 52.76%]--[loss-2.755568: wl-3.750112, gl-1.818040]--[lr: pb-0.000050, pf-0.000032]--[ETA-21:47:06]
2023.01.26-22:35:22:560:[step-93800/177600: 52.82%]--[loss-2.576829: wl-3.452663, gl-1.713663]--[lr: pb-0.000050, pf-0.000032]--[ETA-20:56:50]
2023.01.26-22:37:00:660:[step-93900/177600: 52.87%]--[loss-2.820390: wl-4.035456, gl-1.811526]--[lr: pb-0.000050, pf-0.000032]--[ETA-23:02:32]
2023.01.26-22:38:42:760:[step-94000/177600: 52.93%]--[loss-2.583851: wl-3.459922, gl-1.718870]--[lr: pb-0.000050, pf-0.000032]--[ETA-1 day, 0:06:10]
2023.01.26-22:40:22:860:[step-94100/177600: 52.98%]--[loss-3.001946: wl-4.561011, gl-1.861694]--[lr: pb-0.000050, pf-0.000032]--[ETA-21:46:53]
End of epoch 106 / 200 	 Time Taken: 867 sec
2023.01.26-22:42:05:72:[step-94200/177600: 53.04%]--[loss-2.874228: wl-4.010311, gl-1.871651]--[lr: pb-0.000050, pf-0.000031]--[ETA-22:08:03]
2023.01.26-22:43:43:172:[step-94300/177600: 53.10%]--[loss-2.693523: wl-3.688674, gl-1.771355]--[lr: pb-0.000050, pf-0.000031]--[ETA-20:59:13]
2023.01.26-22:45:15:272:[step-94400/177600: 53.15%]--[loss-2.805588: wl-3.962358, gl-1.814998]--[lr: pb-0.000050, pf-0.000031]--[ETA-21:54:00]
2023.01.26-22:46:56:372:[step-94500/177600: 53.21%]--[loss-2.633464: wl-3.531067, gl-1.750697]--[lr: pb-0.000050, pf-0.000031]--[ETA-21:12:13]
2023.01.26-22:48:36:472:[step-94600/177600: 53.27%]--[loss-2.730230: wl-3.367853, gl-1.888266]--[lr: pb-0.000050, pf-0.000031]--[ETA-23:27:03]
2023.01.26-22:50:18:572:[step-94700/177600: 53.32%]--[loss-2.778188: wl-3.952113, gl-1.790160]--[lr: pb-0.000050, pf-0.000031]--[ETA-21:15:32]
2023.01.26-22:51:58:672:[step-94800/177600: 53.38%]--[loss-2.908045: wl-4.104816, gl-1.881841]--[lr: pb-0.000050, pf-0.000031]--[ETA-22:14:05]
2023.01.26-22:53:37:772:[step-94900/177600: 53.43%]--[loss-2.757167: wl-3.870940, gl-1.789432]--[lr: pb-0.000050, pf-0.000031]--[ETA-20:16:15]
2023.01.26-22:55:07:872:[step-95000/177600: 53.49%]--[loss-2.986757: wl-4.954386, gl-1.748161]--[lr: pb-0.000050, pf-0.000031]--[ETA-20:35:26]
End of epoch 107 / 200 	 Time Taken: 871 sec
2023.01.26-22:56:40:84:[step-95100/177600: 53.55%]--[loss-2.635634: wl-3.690789, gl-1.712937]--[lr: pb-0.000050, pf-0.000031]--[ETA-20:44:16]
2023.01.26-22:58:11:184:[step-95200/177600: 53.60%]--[loss-2.769204: wl-3.830313, gl-1.811625]--[lr: pb-0.000050, pf-0.000031]--[ETA-22:17:54]
2023.01.26-22:59:42:284:[step-95300/177600: 53.66%]--[loss-2.812707: wl-4.223556, gl-1.756818]--[lr: pb-0.000050, pf-0.000031]--[ETA-20:21:08]
2023.01.26-23:01:14:384:[step-95400/177600: 53.72%]--[loss-2.834487: wl-3.624618, gl-1.928332]--[lr: pb-0.000050, pf-0.000031]--[ETA-20:17:38]
2023.01.26-23:02:45:484:[step-95500/177600: 53.77%]--[loss-2.706142: wl-3.898235, gl-1.731584]--[lr: pb-0.000050, pf-0.000031]--[ETA-20:46:42]
2023.01.26-23:04:17:584:[step-95600/177600: 53.83%]--[loss-2.836418: wl-4.218347, gl-1.781831]--[lr: pb-0.000050, pf-0.000031]--[ETA-20:45:43]
2023.01.26-23:05:48:684:[step-95700/177600: 53.89%]--[loss-2.917242: wl-4.268595, gl-1.850093]--[lr: pb-0.000050, pf-0.000031]--[ETA-20:09:08]
2023.01.26-23:07:21:784:[step-95800/177600: 53.94%]--[loss-2.656650: wl-3.524603, gl-1.775500]--[lr: pb-0.000050, pf-0.000031]--[ETA-20:29:36]
2023.01.26-23:08:55:884:[step-95900/177600: 54.00%]--[loss-2.620533: wl-3.377783, gl-1.776087]--[lr: pb-0.000050, pf-0.000031]--[ETA-21:46:56]
End of epoch 108 / 200 	 Time Taken: 816 sec
2023.01.26-23:10:27:96:[step-96000/177600: 54.05%]--[loss-2.454673: wl-3.513907, gl-1.576196]--[lr: pb-0.000050, pf-0.000031]--[ETA-21:32:44]
2023.01.26-23:11:58:196:[step-96100/177600: 54.11%]--[loss-2.688668: wl-3.538466, gl-1.804052]--[lr: pb-0.000050, pf-0.000031]--[ETA-20:28:47]
2023.01.26-23:13:29:296:[step-96200/177600: 54.17%]--[loss-2.901433: wl-4.061966, gl-1.885942]--[lr: pb-0.000050, pf-0.000031]--[ETA-20:15:12]
2023.01.26-23:15:00:396:[step-96300/177600: 54.22%]--[loss-2.865981: wl-3.821527, gl-1.910599]--[lr: pb-0.000050, pf-0.000031]--[ETA-20:34:26]
2023.01.26-23:16:32:496:[step-96400/177600: 54.28%]--[loss-2.731884: wl-3.988159, gl-1.734845]--[lr: pb-0.000050, pf-0.000031]--[ETA-20:04:39]
2023.01.26-23:18:04:596:[step-96500/177600: 54.34%]--[loss-2.834257: wl-3.930745, gl-1.851571]--[lr: pb-0.000050, pf-0.000031]--[ETA-20:16:06]
2023.01.26-23:19:35:696:[step-96600/177600: 54.39%]--[loss-3.006815: wl-4.549574, gl-1.869421]--[lr: pb-0.000050, pf-0.000031]--[ETA-20:15:11]
2023.01.26-23:21:07:796:[step-96700/177600: 54.45%]--[loss-2.872290: wl-4.055263, gl-1.858475]--[lr: pb-0.000050, pf-0.000031]--[ETA-20:18:37]
End of epoch 109 / 200 	 Time Taken: 812 sec
2023.01.26-23:22:40:8:[step-96800/177600: 54.50%]--[loss-2.929921: wl-3.839488, gl-1.970049]--[lr: pb-0.000050, pf-0.000030]--[ETA-20:01:25]
2023.01.26-23:24:11:108:[step-96900/177600: 54.56%]--[loss-2.649572: wl-3.606483, gl-1.747951]--[lr: pb-0.000050, pf-0.000030]--[ETA-19:52:17]
2023.01.26-23:25:42:208:[step-97000/177600: 54.62%]--[loss-2.913005: wl-3.995669, gl-1.914088]--[lr: pb-0.000050, pf-0.000030]--[ETA-20:13:58]
2023.01.26-23:27:14:308:[step-97100/177600: 54.67%]--[loss-2.656042: wl-3.635852, gl-1.747079]--[lr: pb-0.000050, pf-0.000030]--[ETA-20:09:22]
2023.01.26-23:28:45:408:[step-97200/177600: 54.73%]--[loss-2.798168: wl-4.201620, gl-1.747763]--[lr: pb-0.000050, pf-0.000030]--[ETA-20:19:04]
2023.01.26-23:30:17:508:[step-97300/177600: 54.79%]--[loss-2.685703: wl-3.840772, gl-1.725510]--[lr: pb-0.000050, pf-0.000030]--[ETA-19:58:55]
2023.01.26-23:31:48:608:[step-97400/177600: 54.84%]--[loss-2.827073: wl-4.322086, gl-1.746552]--[lr: pb-0.000050, pf-0.000030]--[ETA-20:45:17]
2023.01.26-23:33:19:708:[step-97500/177600: 54.90%]--[loss-2.654758: wl-3.673106, gl-1.736481]--[lr: pb-0.000050, pf-0.000030]--[ETA-22:14:51]
2023.01.26-23:34:50:808:[step-97600/177600: 54.95%]--[loss-3.020976: wl-4.840891, gl-1.810753]--[lr: pb-0.000050, pf-0.000030]--[ETA-19:51:12]
End of epoch 110 / 200 	 Time Taken: 811 sec
saving the model at the end of epoch 110, iters 97680
2023.01.26-23:36:22:20:[step-97700/177600: 55.01%]--[loss-2.691613: wl-3.798550, gl-1.741976]--[lr: pb-0.000050, pf-0.000030]--[ETA-20:51:54]
2023.01.26-23:37:53:120:[step-97800/177600: 55.07%]--[loss-2.697970: wl-3.772413, gl-1.754867]--[lr: pb-0.000050, pf-0.000030]--[ETA-19:45:04]
2023.01.26-23:39:25:220:[step-97900/177600: 55.12%]--[loss-3.008246: wl-4.732243, gl-1.825186]--[lr: pb-0.000050, pf-0.000030]--[ETA-20:12:58]
2023.01.26-23:40:56:320:[step-98000/177600: 55.18%]--[loss-2.679583: wl-3.660173, gl-1.764540]--[lr: pb-0.000050, pf-0.000030]--[ETA-20:53:48]
2023.01.26-23:42:28:420:[step-98100/177600: 55.24%]--[loss-2.617411: wl-3.655665, gl-1.703495]--[lr: pb-0.000050, pf-0.000030]--[ETA-20:01:27]
2023.01.26-23:43:59:520:[step-98200/177600: 55.29%]--[loss-2.649482: wl-3.619382, gl-1.744637]--[lr: pb-0.000050, pf-0.000030]--[ETA-22:07:12]
2023.01.26-23:45:30:620:[step-98300/177600: 55.35%]--[loss-3.126750: wl-4.985135, gl-1.880467]--[lr: pb-0.000050, pf-0.000030]--[ETA-20:31:56]
2023.01.26-23:47:02:720:[step-98400/177600: 55.41%]--[loss-2.590759: wl-3.556197, gl-1.701709]--[lr: pb-0.000050, pf-0.000030]--[ETA-19:56:29]
2023.01.26-23:48:33:820:[step-98500/177600: 55.46%]--[loss-2.931177: wl-4.577240, gl-1.786867]--[lr: pb-0.000050, pf-0.000030]--[ETA-20:02:30]
End of epoch 111 / 200 	 Time Taken: 812 sec
2023.01.26-23:50:06:32:[step-98600/177600: 55.52%]--[loss-2.743760: wl-3.623350, gl-1.837922]--[lr: pb-0.000050, pf-0.000030]--[ETA-19:34:20]
2023.01.26-23:51:37:132:[step-98700/177600: 55.57%]--[loss-2.705092: wl-3.854688, gl-1.741420]--[lr: pb-0.000050, pf-0.000030]--[ETA-20:26:36]
2023.01.26-23:53:08:232:[step-98800/177600: 55.63%]--[loss-3.002156: wl-4.405044, gl-1.900895]--[lr: pb-0.000050, pf-0.000030]--[ETA-19:31:22]
2023.01.26-23:54:39:332:[step-98900/177600: 55.69%]--[loss-2.677533: wl-3.709412, gl-1.750180]--[lr: pb-0.000050, pf-0.000030]--[ETA-20:00:59]
2023.01.26-23:56:11:432:[step-99000/177600: 55.74%]--[loss-2.697714: wl-3.623646, gl-1.791802]--[lr: pb-0.000050, pf-0.000030]--[ETA-20:15:50]
2023.01.26-23:57:42:532:[step-99100/177600: 55.80%]--[loss-2.689167: wl-3.327932, gl-1.857183]--[lr: pb-0.000050, pf-0.000030]--[ETA-19:21:57]
2023.01.26-23:59:13:632:[step-99200/177600: 55.86%]--[loss-2.705971: wl-3.630440, gl-1.798361]--[lr: pb-0.000050, pf-0.000030]--[ETA-19:47:10]
2023.01.27-00:00:44:732:[step-99300/177600: 55.91%]--[loss-2.773679: wl-3.840690, gl-1.813507]--[lr: pb-0.000050, pf-0.000030]--[ETA-20:02:05]
2023.01.27-00:02:15:832:[step-99400/177600: 55.97%]--[loss-3.127331: wl-4.470231, gl-2.009773]--[lr: pb-0.000050, pf-0.000030]--[ETA-20:08:29]
End of epoch 112 / 200 	 Time Taken: 810 sec
2023.01.27-00:03:47:44:[step-99500/177600: 56.02%]--[loss-2.721455: wl-3.874648, gl-1.752793]--[lr: pb-0.000050, pf-0.000029]--[ETA-19:12:40]
2023.01.27-00:05:18:144:[step-99600/177600: 56.08%]--[loss-2.704018: wl-3.806021, gl-1.752513]--[lr: pb-0.000050, pf-0.000029]--[ETA-19:59:22]
2023.01.27-00:06:49:244:[step-99700/177600: 56.14%]--[loss-2.824985: wl-4.035765, gl-1.816044]--[lr: pb-0.000050, pf-0.000029]--[ETA-19:38:12]
2023.01.27-00:08:20:344:[step-99800/177600: 56.19%]--[loss-2.958344: wl-4.166920, gl-1.916613]--[lr: pb-0.000050, pf-0.000029]--[ETA-19:14:44]
2023.01.27-00:09:51:444:[step-99900/177600: 56.25%]--[loss-2.916849: wl-4.095596, gl-1.892950]--[lr: pb-0.000050, pf-0.000029]--[ETA-19:12:00]
2023.01.27-00:11:22:544:[step-100000/177600: 56.31%]--[loss-2.758334: wl-4.218239, gl-1.703774]--[lr: pb-0.000050, pf-0.000029]--[ETA-19:12:41]
2023.01.27-00:12:53:644:[step-100100/177600: 56.36%]--[loss-2.855487: wl-3.982861, gl-1.859772]--[lr: pb-0.000050, pf-0.000029]--[ETA-19:50:30]
2023.01.27-00:14:24:744:[step-100200/177600: 56.42%]--[loss-2.612252: wl-3.489937, gl-1.739768]--[lr: pb-0.000050, pf-0.000029]--[ETA-19:10:06]
2023.01.27-00:15:56:844:[step-100300/177600: 56.48%]--[loss-2.600041: wl-3.546461, gl-1.713426]--[lr: pb-0.000050, pf-0.000029]--[ETA-19:33:38]
End of epoch 113 / 200 	 Time Taken: 809 sec
2023.01.27-00:17:28:56:[step-100400/177600: 56.53%]--[loss-2.709626: wl-3.668275, gl-1.792557]--[lr: pb-0.000050, pf-0.000029]--[ETA-19:56:42]
2023.01.27-00:18:59:156:[step-100500/177600: 56.59%]--[loss-2.732805: wl-3.984080, gl-1.736785]--[lr: pb-0.000050, pf-0.000029]--[ETA-19:19:13]
2023.01.27-00:20:30:256:[step-100600/177600: 56.64%]--[loss-2.757284: wl-3.943732, gl-1.771351]--[lr: pb-0.000050, pf-0.000029]--[ETA-19:15:22]
2023.01.27-00:22:01:356:[step-100700/177600: 56.70%]--[loss-2.754706: wl-3.926664, gl-1.773040]--[lr: pb-0.000050, pf-0.000029]--[ETA-18:56:40]
2023.01.27-00:23:32:456:[step-100800/177600: 56.76%]--[loss-2.686401: wl-3.571126, gl-1.793620]--[lr: pb-0.000050, pf-0.000029]--[ETA-19:25:56]
2023.01.27-00:25:03:556:[step-100900/177600: 56.81%]--[loss-2.996603: wl-4.340334, gl-1.911519]--[lr: pb-0.000050, pf-0.000029]--[ETA-20:39:10]
2023.01.27-00:26:35:656:[step-101000/177600: 56.87%]--[loss-2.868515: wl-3.968887, gl-1.876294]--[lr: pb-0.000050, pf-0.000029]--[ETA-19:08:40]
2023.01.27-00:28:06:756:[step-101100/177600: 56.93%]--[loss-2.760054: wl-3.924639, gl-1.778894]--[lr: pb-0.000050, pf-0.000029]--[ETA-19:17:31]
2023.01.27-00:29:37:856:[step-101200/177600: 56.98%]--[loss-2.692435: wl-3.734951, gl-1.758698]--[lr: pb-0.000050, pf-0.000029]--[ETA-19:26:54]
End of epoch 114 / 200 	 Time Taken: 811 sec
2023.01.27-00:31:11:68:[step-101300/177600: 57.04%]--[loss-2.997293: wl-4.366027, gl-1.905787]--[lr: pb-0.000050, pf-0.000029]--[ETA-18:49:15]
2023.01.27-00:32:43:168:[step-101400/177600: 57.09%]--[loss-3.006416: wl-3.998050, gl-2.006903]--[lr: pb-0.000050, pf-0.000029]--[ETA-19:34:46]
2023.01.27-00:34:14:268:[step-101500/177600: 57.15%]--[loss-3.031094: wl-4.254241, gl-1.967534]--[lr: pb-0.000050, pf-0.000029]--[ETA-19:31:24]
2023.01.27-00:35:45:368:[step-101600/177600: 57.21%]--[loss-2.659087: wl-3.883583, gl-1.688191]--[lr: pb-0.000050, pf-0.000029]--[ETA-19:18:51]
2023.01.27-00:37:16:468:[step-101700/177600: 57.26%]--[loss-2.726213: wl-3.965720, gl-1.734783]--[lr: pb-0.000050, pf-0.000029]--[ETA-19:18:25]
2023.01.27-00:38:47:568:[step-101800/177600: 57.32%]--[loss-2.807731: wl-3.918797, gl-1.828032]--[lr: pb-0.000050, pf-0.000029]--[ETA-19:15:52]
2023.01.27-00:40:18:668:[step-101900/177600: 57.38%]--[loss-2.583039: wl-3.649683, gl-1.670618]--[lr: pb-0.000050, pf-0.000029]--[ETA-19:05:13]
2023.01.27-00:41:50:768:[step-102000/177600: 57.43%]--[loss-2.919456: wl-4.084607, gl-1.898304]--[lr: pb-0.000050, pf-0.000029]--[ETA-19:21:00]
2023.01.27-00:43:21:868:[step-102100/177600: 57.49%]--[loss-2.878328: wl-3.893607, gl-1.904927]--[lr: pb-0.000050, pf-0.000029]--[ETA-19:01:12]
End of epoch 115 / 200 	 Time Taken: 812 sec
saving the model at the end of epoch 115, iters 102120
2023.01.27-00:44:54:80:[step-102200/177600: 57.55%]--[loss-2.770781: wl-3.844536, gl-1.809647]--[lr: pb-0.000050, pf-0.000028]--[ETA-18:48:46]
2023.01.27-00:46:25:180:[step-102300/177600: 57.60%]--[loss-2.694102: wl-4.105522, gl-1.667721]--[lr: pb-0.000050, pf-0.000028]--[ETA-18:45:46]
2023.01.27-00:47:56:280:[step-102400/177600: 57.66%]--[loss-2.942119: wl-4.711366, gl-1.764278]--[lr: pb-0.000050, pf-0.000028]--[ETA-19:02:48]
2023.01.27-00:49:26:380:[step-102500/177600: 57.71%]--[loss-2.926489: wl-4.215410, gl-1.872636]--[lr: pb-0.000050, pf-0.000028]--[ETA-19:28:05]
2023.01.27-00:50:58:480:[step-102600/177600: 57.77%]--[loss-2.860172: wl-3.845066, gl-1.898906]--[lr: pb-0.000050, pf-0.000028]--[ETA-19:17:59]
2023.01.27-00:52:29:580:[step-102700/177600: 57.83%]--[loss-2.667762: wl-3.362901, gl-1.827036]--[lr: pb-0.000050, pf-0.000028]--[ETA-19:17:49]
2023.01.27-00:54:00:680:[step-102800/177600: 57.88%]--[loss-2.658509: wl-3.974068, gl-1.664992]--[lr: pb-0.000050, pf-0.000028]--[ETA-18:33:20]
2023.01.27-00:55:31:780:[step-102900/177600: 57.94%]--[loss-2.956496: wl-4.353580, gl-1.868101]--[lr: pb-0.000050, pf-0.000028]--[ETA-18:25:34]
2023.01.27-00:57:02:880:[step-103000/177600: 58.00%]--[loss-2.893429: wl-3.918606, gl-1.913777]--[lr: pb-0.000050, pf-0.000028]--[ETA-21:03:26]
End of epoch 116 / 200 	 Time Taken: 809 sec
2023.01.27-00:58:35:92:[step-103100/177600: 58.05%]--[loss-2.966166: wl-4.199081, gl-1.916395]--[lr: pb-0.000050, pf-0.000028]--[ETA-18:51:03]
2023.01.27-01:00:06:192:[step-103200/177600: 58.11%]--[loss-2.605685: wl-3.907069, gl-1.628918]--[lr: pb-0.000050, pf-0.000028]--[ETA-18:48:17]
2023.01.27-01:01:37:292:[step-103300/177600: 58.16%]--[loss-2.858432: wl-3.924827, gl-1.877225]--[lr: pb-0.000050, pf-0.000028]--[ETA-18:39:41]
2023.01.27-01:03:08:392:[step-103400/177600: 58.22%]--[loss-2.947706: wl-4.056781, gl-1.933511]--[lr: pb-0.000050, pf-0.000028]--[ETA-18:23:34]
2023.01.27-01:04:40:492:[step-103500/177600: 58.28%]--[loss-2.932951: wl-4.198208, gl-1.883399]--[lr: pb-0.000050, pf-0.000028]--[ETA-18:46:26]
2023.01.27-01:06:12:592:[step-103600/177600: 58.33%]--[loss-2.802829: wl-3.840595, gl-1.842680]--[lr: pb-0.000050, pf-0.000028]--[ETA-18:35:42]
2023.01.27-01:07:43:692:[step-103700/177600: 58.39%]--[loss-2.780678: wl-3.998897, gl-1.780954]--[lr: pb-0.000050, pf-0.000028]--[ETA-19:14:41]
2023.01.27-01:09:14:792:[step-103800/177600: 58.45%]--[loss-2.657511: wl-3.879412, gl-1.687658]--[lr: pb-0.000050, pf-0.000028]--[ETA-19:29:50]
End of epoch 117 / 200 	 Time Taken: 812 sec
2023.01.27-01:10:46:4:[step-103900/177600: 58.50%]--[loss-2.746557: wl-3.680957, gl-1.826318]--[lr: pb-0.000050, pf-0.000028]--[ETA-18:04:26]
2023.01.27-01:12:19:104:[step-104000/177600: 58.56%]--[loss-2.785156: wl-4.084862, gl-1.763940]--[lr: pb-0.000050, pf-0.000028]--[ETA-19:28:20]
2023.01.27-01:13:53:204:[step-104100/177600: 58.61%]--[loss-2.761874: wl-3.879488, gl-1.792002]--[lr: pb-0.000050, pf-0.000028]--[ETA-18:56:55]
2023.01.27-01:15:25:304:[step-104200/177600: 58.67%]--[loss-2.753147: wl-3.734473, gl-1.819529]--[lr: pb-0.000050, pf-0.000028]--[ETA-18:30:46]
2023.01.27-01:17:00:404:[step-104300/177600: 58.73%]--[loss-2.677203: wl-3.753116, gl-1.738924]--[lr: pb-0.000050, pf-0.000028]--[ETA-20:35:26]
2023.01.27-01:18:35:504:[step-104400/177600: 58.78%]--[loss-2.959441: wl-4.443498, gl-1.848567]--[lr: pb-0.000050, pf-0.000028]--[ETA-18:51:24]
2023.01.27-01:20:09:604:[step-104500/177600: 58.84%]--[loss-2.665601: wl-3.871570, gl-1.697708]--[lr: pb-0.000050, pf-0.000028]--[ETA-20:47:01]
2023.01.27-01:21:43:704:[step-104600/177600: 58.90%]--[loss-2.559193: wl-3.505216, gl-1.682889]--[lr: pb-0.000050, pf-0.000028]--[ETA-18:44:17]
2023.01.27-01:23:17:804:[step-104700/177600: 58.95%]--[loss-2.768707: wl-3.946765, gl-1.782016]--[lr: pb-0.000050, pf-0.000028]--[ETA-18:38:04]
End of epoch 118 / 200 	 Time Taken: 835 sec
2023.01.27-01:24:54:16:[step-104800/177600: 59.01%]--[loss-2.620960: wl-3.610917, gl-1.718231]--[lr: pb-0.000050, pf-0.000027]--[ETA-18:39:49]
2023.01.27-01:26:27:116:[step-104900/177600: 59.07%]--[loss-3.315037: wl-5.609224, gl-1.912731]--[lr: pb-0.000050, pf-0.000027]--[ETA-17:54:12]
2023.01.27-01:27:57:216:[step-105000/177600: 59.12%]--[loss-2.677764: wl-3.540971, gl-1.792522]--[lr: pb-0.000050, pf-0.000027]--[ETA-18:29:32]
2023.01.27-01:29:27:316:[step-105100/177600: 59.18%]--[loss-2.693611: wl-3.889490, gl-1.721238]--[lr: pb-0.000050, pf-0.000027]--[ETA-17:48:55]
2023.01.27-01:30:56:416:[step-105200/177600: 59.23%]--[loss-2.716118: wl-3.488654, gl-1.843955]--[lr: pb-0.000050, pf-0.000027]--[ETA-17:47:27]
2023.01.27-01:32:26:516:[step-105300/177600: 59.29%]--[loss-2.996540: wl-4.445666, gl-1.885123]--[lr: pb-0.000050, pf-0.000027]--[ETA-17:45:11]
2023.01.27-01:33:56:616:[step-105400/177600: 59.35%]--[loss-2.740335: wl-4.030011, gl-1.732832]--[lr: pb-0.000050, pf-0.000027]--[ETA-17:46:51]
2023.01.27-01:35:26:716:[step-105500/177600: 59.40%]--[loss-2.869707: wl-4.192169, gl-1.821665]--[lr: pb-0.000050, pf-0.000027]--[ETA-18:55:53]
2023.01.27-01:36:56:816:[step-105600/177600: 59.46%]--[loss-2.728682: wl-4.255374, gl-1.664839]--[lr: pb-0.000050, pf-0.000027]--[ETA-17:59:56]
End of epoch 119 / 200 	 Time Taken: 803 sec
2023.01.27-01:38:28:28:[step-105700/177600: 59.52%]--[loss-2.666616: wl-3.872845, gl-1.698405]--[lr: pb-0.000050, pf-0.000027]--[ETA-18:59:38]
2023.01.27-01:39:58:128:[step-105800/177600: 59.57%]--[loss-2.805763: wl-3.944430, gl-1.819655]--[lr: pb-0.000050, pf-0.000027]--[ETA-17:42:21]
2023.01.27-01:41:28:228:[step-105900/177600: 59.63%]--[loss-2.769085: wl-4.014241, gl-1.765525]--[lr: pb-0.000050, pf-0.000027]--[ETA-17:36:40]
2023.01.27-01:42:57:328:[step-106000/177600: 59.68%]--[loss-2.720221: wl-3.907821, gl-1.743266]--[lr: pb-0.000050, pf-0.000027]--[ETA-18:23:40]
2023.01.27-01:44:27:428:[step-106100/177600: 59.74%]--[loss-2.892132: wl-4.447358, gl-1.780292]--[lr: pb-0.000050, pf-0.000027]--[ETA-17:59:40]
2023.01.27-01:45:57:528:[step-106200/177600: 59.80%]--[loss-2.690802: wl-3.628338, gl-1.783717]--[lr: pb-0.000050, pf-0.000027]--[ETA-17:44:06]
2023.01.27-01:47:27:628:[step-106300/177600: 59.85%]--[loss-2.698854: wl-3.891347, gl-1.726017]--[lr: pb-0.000050, pf-0.000027]--[ETA-18:26:24]
2023.01.27-01:48:57:728:[step-106400/177600: 59.91%]--[loss-2.668548: wl-4.125336, gl-1.637214]--[lr: pb-0.000050, pf-0.000027]--[ETA-17:33:58]
2023.01.27-01:50:27:828:[step-106500/177600: 59.97%]--[loss-2.883289: wl-4.212997, gl-1.830040]--[lr: pb-0.000050, pf-0.000027]--[ETA-17:37:02]
End of epoch 120 / 200 	 Time Taken: 800 sec
saving the model at the end of epoch 120, iters 106560
2023.01.27-01:51:59:40:[step-106600/177600: 60.02%]--[loss-2.918385: wl-4.192145, gl-1.870349]--[lr: pb-0.000050, pf-0.000027]--[ETA-18:11:38]
2023.01.27-01:53:30:140:[step-106700/177600: 60.08%]--[loss-2.805409: wl-4.315230, gl-1.726602]--[lr: pb-0.000050, pf-0.000027]--[ETA-18:03:50]
2023.01.27-01:55:00:240:[step-106800/177600: 60.14%]--[loss-3.134592: wl-4.719164, gl-1.954801]--[lr: pb-0.000050, pf-0.000027]--[ETA-17:39:06]
2023.01.27-01:56:30:340:[step-106900/177600: 60.19%]--[loss-2.452690: wl-3.463821, gl-1.586735]--[lr: pb-0.000050, pf-0.000027]--[ETA-18:53:07]
2023.01.27-01:58:02:440:[step-107000/177600: 60.25%]--[loss-2.794072: wl-3.970559, gl-1.801433]--[lr: pb-0.000050, pf-0.000027]--[ETA-17:27:35]
2023.01.27-01:59:32:540:[step-107100/177600: 60.30%]--[loss-2.898241: wl-4.068336, gl-1.881157]--[lr: pb-0.000050, pf-0.000027]--[ETA-17:22:43]
2023.01.27-02:01:02:640:[step-107200/177600: 60.36%]--[loss-2.500149: wl-3.408415, gl-1.648046]--[lr: pb-0.000050, pf-0.000027]--[ETA-18:23:18]
2023.01.27-02:02:32:740:[step-107300/177600: 60.42%]--[loss-2.721666: wl-4.171237, gl-1.678856]--[lr: pb-0.000050, pf-0.000027]--[ETA-17:31:00]
2023.01.27-02:04:01:840:[step-107400/177600: 60.47%]--[loss-2.651517: wl-3.667483, gl-1.734647]--[lr: pb-0.000050, pf-0.000027]--[ETA-17:15:05]
End of epoch 121 / 200 	 Time Taken: 801 sec
2023.01.27-02:05:32:52:[step-107500/177600: 60.53%]--[loss-2.832719: wl-4.294896, gl-1.758995]--[lr: pb-0.000050, pf-0.000026]--[ETA-17:06:02]
2023.01.27-02:07:01:152:[step-107600/177600: 60.59%]--[loss-2.979664: wl-4.287100, gl-1.907889]--[lr: pb-0.000050, pf-0.000026]--[ETA-17:15:14]
2023.01.27-02:08:30:252:[step-107700/177600: 60.64%]--[loss-2.877725: wl-4.133717, gl-1.844296]--[lr: pb-0.000050, pf-0.000026]--[ETA-17:07:24]
2023.01.27-02:10:00:352:[step-107800/177600: 60.70%]--[loss-2.585984: wl-3.556257, gl-1.696920]--[lr: pb-0.000050, pf-0.000026]--[ETA-17:09:42]
2023.01.27-02:11:31:452:[step-107900/177600: 60.75%]--[loss-2.573748: wl-3.596891, gl-1.674525]--[lr: pb-0.000050, pf-0.000026]--[ETA-17:09:15]
2023.01.27-02:13:01:552:[step-108000/177600: 60.81%]--[loss-2.732766: wl-4.021383, gl-1.727420]--[lr: pb-0.000050, pf-0.000026]--[ETA-17:18:09]
2023.01.27-02:14:30:652:[step-108100/177600: 60.87%]--[loss-2.827791: wl-4.086977, gl-1.806046]--[lr: pb-0.000050, pf-0.000026]--[ETA-17:07:12]
2023.01.27-02:16:00:752:[step-108200/177600: 60.92%]--[loss-2.617339: wl-3.617782, gl-1.712894]--[lr: pb-0.000050, pf-0.000026]--[ETA-17:47:45]
2023.01.27-02:17:29:852:[step-108300/177600: 60.98%]--[loss-2.742242: wl-3.769813, gl-1.799789]--[lr: pb-0.000050, pf-0.000026]--[ETA-17:11:44]
End of epoch 122 / 200 	 Time Taken: 798 sec
2023.01.27-02:19:01:64:[step-108400/177600: 61.04%]--[loss-2.962009: wl-4.832964, gl-1.753768]--[lr: pb-0.000050, pf-0.000026]--[ETA-17:01:42]
2023.01.27-02:20:31:164:[step-108500/177600: 61.09%]--[loss-2.989486: wl-4.648380, gl-1.827391]--[lr: pb-0.000050, pf-0.000026]--[ETA-17:56:39]
2023.01.27-02:22:01:264:[step-108600/177600: 61.15%]--[loss-2.820277: wl-4.135428, gl-1.786420]--[lr: pb-0.000050, pf-0.000026]--[ETA-16:54:34]
2023.01.27-02:23:31:364:[step-108700/177600: 61.20%]--[loss-2.584468: wl-3.423101, gl-1.728693]--[lr: pb-0.000050, pf-0.000026]--[ETA-16:56:27]
2023.01.27-02:25:01:464:[step-108800/177600: 61.26%]--[loss-3.244493: wl-5.018881, gl-1.989773]--[lr: pb-0.000050, pf-0.000026]--[ETA-17:23:13]
2023.01.27-02:26:31:564:[step-108900/177600: 61.32%]--[loss-2.790789: wl-3.851096, gl-1.828015]--[lr: pb-0.000050, pf-0.000026]--[ETA-16:51:13]
2023.01.27-02:28:01:664:[step-109000/177600: 61.37%]--[loss-2.776542: wl-3.677685, gl-1.857120]--[lr: pb-0.000050, pf-0.000026]--[ETA-17:03:59]
2023.01.27-02:29:31:764:[step-109100/177600: 61.43%]--[loss-2.658667: wl-3.825957, gl-1.702178]--[lr: pb-0.000050, pf-0.000026]--[ETA-16:51:45]
2023.01.27-02:31:01:864:[step-109200/177600: 61.49%]--[loss-2.572327: wl-3.505863, gl-1.695861]--[lr: pb-0.000050, pf-0.000026]--[ETA-17:03:24]
End of epoch 123 / 200 	 Time Taken: 800 sec
2023.01.27-02:32:32:76:[step-109300/177600: 61.54%]--[loss-3.114893: wl-4.724766, gl-1.933702]--[lr: pb-0.000050, pf-0.000026]--[ETA-16:47:57]
2023.01.27-02:34:02:176:[step-109400/177600: 61.60%]--[loss-2.734936: wl-3.962386, gl-1.744340]--[lr: pb-0.000050, pf-0.000026]--[ETA-16:56:46]
2023.01.27-02:35:32:276:[step-109500/177600: 61.66%]--[loss-2.483836: wl-3.324971, gl-1.652594]--[lr: pb-0.000050, pf-0.000026]--[ETA-16:53:06]
2023.01.27-02:37:01:376:[step-109600/177600: 61.71%]--[loss-2.700009: wl-3.779868, gl-1.755042]--[lr: pb-0.000050, pf-0.000026]--[ETA-16:44:29]
2023.01.27-02:38:31:476:[step-109700/177600: 61.77%]--[loss-3.033454: wl-4.501533, gl-1.908071]--[lr: pb-0.000050, pf-0.000026]--[ETA-16:47:21]
2023.01.27-02:40:01:576:[step-109800/177600: 61.82%]--[loss-2.777655: wl-4.143933, gl-1.741671]--[lr: pb-0.000050, pf-0.000026]--[ETA-17:29:39]
2023.01.27-02:41:30:676:[step-109900/177600: 61.88%]--[loss-2.653711: wl-3.827673, gl-1.696792]--[lr: pb-0.000050, pf-0.000026]--[ETA-17:05:14]
2023.01.27-02:43:02:776:[step-110000/177600: 61.94%]--[loss-2.526480: wl-3.519400, gl-1.646630]--[lr: pb-0.000050, pf-0.000026]--[ETA-16:37:30]
2023.01.27-02:44:32:876:[step-110100/177600: 61.99%]--[loss-2.840984: wl-4.378013, gl-1.746481]--[lr: pb-0.000050, pf-0.000026]--[ETA-17:34:58]
End of epoch 124 / 200 	 Time Taken: 799 sec
2023.01.27-02:46:03:88:[step-110200/177600: 62.05%]--[loss-2.673937: wl-3.773917, gl-1.730458]--[lr: pb-0.000050, pf-0.000025]--[ETA-16:36:21]
2023.01.27-02:47:32:188:[step-110300/177600: 62.11%]--[loss-2.668205: wl-3.686695, gl-1.746531]--[lr: pb-0.000050, pf-0.000025]--[ETA-16:40:27]
2023.01.27-02:49:02:288:[step-110400/177600: 62.16%]--[loss-3.013018: wl-4.700507, gl-1.837891]--[lr: pb-0.000050, pf-0.000025]--[ETA-17:11:46]
2023.01.27-02:50:32:388:[step-110500/177600: 62.22%]--[loss-2.573926: wl-3.468095, gl-1.706903]--[lr: pb-0.000050, pf-0.000025]--[ETA-16:32:50]
2023.01.27-02:52:02:488:[step-110600/177600: 62.27%]--[loss-2.496097: wl-3.306021, gl-1.669591]--[lr: pb-0.000050, pf-0.000025]--[ETA-16:42:16]
2023.01.27-02:53:31:588:[step-110700/177600: 62.33%]--[loss-2.804284: wl-3.968596, gl-1.812135]--[lr: pb-0.000050, pf-0.000025]--[ETA-16:41:37]
2023.01.27-02:55:02:688:[step-110800/177600: 62.39%]--[loss-2.746496: wl-3.717930, gl-1.817014]--[lr: pb-0.000050, pf-0.000025]--[ETA-16:49:59]
2023.01.27-02:56:32:788:[step-110900/177600: 62.44%]--[loss-2.517323: wl-3.512069, gl-1.639305]--[lr: pb-0.000050, pf-0.000025]--[ETA-16:47:33]
2023.01.27-02:58:02:888:[step-111000/177600: 62.50%]--[loss-2.576766: wl-3.538366, gl-1.692175]--[lr: pb-0.000050, pf-0.000025]--[ETA-16:29:45]
End of epoch 125 / 200 	 Time Taken: 800 sec
saving the model at the end of epoch 125, iters 111000
2023.01.27-02:59:34:100:[step-111100/177600: 62.56%]--[loss-2.633796: wl-3.451985, gl-1.770800]--[lr: pb-0.000050, pf-0.000025]--[ETA-16:34:45]
2023.01.27-03:01:04:200:[step-111200/177600: 62.61%]--[loss-2.637363: wl-3.580231, gl-1.742305]--[lr: pb-0.000050, pf-0.000025]--[ETA-16:12:58]
2023.01.27-03:02:34:300:[step-111300/177600: 62.67%]--[loss-3.046319: wl-4.857576, gl-1.831925]--[lr: pb-0.000050, pf-0.000025]--[ETA-16:18:43]
2023.01.27-03:04:04:400:[step-111400/177600: 62.73%]--[loss-2.688020: wl-3.688751, gl-1.765832]--[lr: pb-0.000050, pf-0.000025]--[ETA-16:39:05]
2023.01.27-03:05:34:500:[step-111500/177600: 62.78%]--[loss-2.881914: wl-5.004749, gl-1.630727]--[lr: pb-0.000050, pf-0.000025]--[ETA-16:18:29]
2023.01.27-03:07:04:600:[step-111600/177600: 62.84%]--[loss-3.003509: wl-4.545959, gl-1.867019]--[lr: pb-0.000050, pf-0.000025]--[ETA-16:12:58]
2023.01.27-03:08:34:700:[step-111700/177600: 62.89%]--[loss-2.615276: wl-3.769513, gl-1.672898]--[lr: pb-0.000050, pf-0.000025]--[ETA-16:56:27]
2023.01.27-03:10:04:800:[step-111800/177600: 62.95%]--[loss-2.934834: wl-4.126740, gl-1.903149]--[lr: pb-0.000050, pf-0.000025]--[ETA-16:10:03]
End of epoch 126 / 200 	 Time Taken: 801 sec
2023.01.27-03:11:36:12:[step-111900/177600: 63.01%]--[loss-2.677844: wl-3.874336, gl-1.709260]--[lr: pb-0.000050, pf-0.000025]--[ETA-16:11:19]
2023.01.27-03:13:07:112:[step-112000/177600: 63.06%]--[loss-3.002066: wl-4.885915, gl-1.780587]--[lr: pb-0.000050, pf-0.000025]--[ETA-16:06:41]
2023.01.27-03:14:38:212:[step-112100/177600: 63.12%]--[loss-2.724206: wl-3.922431, gl-1.743598]--[lr: pb-0.000050, pf-0.000025]--[ETA-16:14:00]
2023.01.27-03:16:08:312:[step-112200/177600: 63.18%]--[loss-2.728364: wl-3.546582, gl-1.841718]--[lr: pb-0.000050, pf-0.000025]--[ETA-16:14:52]
2023.01.27-03:17:40:412:[step-112300/177600: 63.23%]--[loss-2.676105: wl-3.949368, gl-1.688764]--[lr: pb-0.000050, pf-0.000025]--[ETA-16:11:37]
2023.01.27-03:19:10:512:[step-112400/177600: 63.29%]--[loss-2.613019: wl-3.654803, gl-1.699318]--[lr: pb-0.000050, pf-0.000025]--[ETA-16:03:53]
2023.01.27-03:20:40:612:[step-112500/177600: 63.34%]--[loss-2.709762: wl-3.914416, gl-1.731158]--[lr: pb-0.000050, pf-0.000025]--[ETA-16:15:28]
2023.01.27-03:22:11:712:[step-112600/177600: 63.40%]--[loss-2.752439: wl-4.001169, gl-1.752147]--[lr: pb-0.000050, pf-0.000025]--[ETA-17:43:37]
2023.01.27-03:23:41:812:[step-112700/177600: 63.46%]--[loss-2.581141: wl-3.768690, gl-1.638968]--[lr: pb-0.000050, pf-0.000025]--[ETA-15:58:18]
End of epoch 127 / 200 	 Time Taken: 804 sec
2023.01.27-03:25:11:24:[step-112800/177600: 63.51%]--[loss-2.593508: wl-3.796038, gl-1.644498]--[lr: pb-0.000050, pf-0.000024]--[ETA-15:56:06]
2023.01.27-03:26:43:124:[step-112900/177600: 63.57%]--[loss-3.048890: wl-4.881066, gl-1.828623]--[lr: pb-0.000050, pf-0.000024]--[ETA-18:08:07]
2023.01.27-03:28:13:224:[step-113000/177600: 63.63%]--[loss-2.786264: wl-4.111741, gl-1.758329]--[lr: pb-0.000050, pf-0.000024]--[ETA-16:18:01]
2023.01.27-03:29:46:324:[step-113100/177600: 63.68%]--[loss-2.940002: wl-4.590993, gl-1.792254]--[lr: pb-0.000050, pf-0.000024]--[ETA-16:09:24]
2023.01.27-03:31:17:424:[step-113200/177600: 63.74%]--[loss-2.685427: wl-3.946491, gl-1.698804]--[lr: pb-0.000050, pf-0.000024]--[ETA-15:54:36]
2023.01.27-03:32:47:524:[step-113300/177600: 63.80%]--[loss-2.815551: wl-4.129052, gl-1.783288]--[lr: pb-0.000050, pf-0.000024]--[ETA-15:46:21]
2023.01.27-03:34:17:624:[step-113400/177600: 63.85%]--[loss-2.679883: wl-3.839878, gl-1.719913]--[lr: pb-0.000050, pf-0.000024]--[ETA-15:47:38]
2023.01.27-03:35:47:724:[step-113500/177600: 63.91%]--[loss-2.692604: wl-3.683332, gl-1.771771]--[lr: pb-0.000050, pf-0.000024]--[ETA-16:25:54]
2023.01.27-03:37:17:824:[step-113600/177600: 63.96%]--[loss-2.527584: wl-3.667600, gl-1.610684]--[lr: pb-0.000050, pf-0.000024]--[ETA-16:49:13]
End of epoch 128 / 200 	 Time Taken: 805 sec
2023.01.27-03:38:49:36:[step-113700/177600: 64.02%]--[loss-2.597275: wl-3.469596, gl-1.729876]--[lr: pb-0.000050, pf-0.000024]--[ETA-16:50:07]
2023.01.27-03:40:20:136:[step-113800/177600: 64.08%]--[loss-2.606820: wl-3.472493, gl-1.738696]--[lr: pb-0.000050, pf-0.000024]--[ETA-16:20:24]
2023.01.27-03:41:49:236:[step-113900/177600: 64.13%]--[loss-2.901951: wl-4.251219, gl-1.839146]--[lr: pb-0.000050, pf-0.000024]--[ETA-15:41:28]
2023.01.27-03:43:19:336:[step-114000/177600: 64.19%]--[loss-2.874684: wl-3.853869, gl-1.911216]--[lr: pb-0.000050, pf-0.000024]--[ETA-15:38:29]
2023.01.27-03:44:49:436:[step-114100/177600: 64.25%]--[loss-2.656873: wl-3.581692, gl-1.761450]--[lr: pb-0.000050, pf-0.000024]--[ETA-16:27:35]
2023.01.27-03:46:19:536:[step-114200/177600: 64.30%]--[loss-2.696503: wl-3.500645, gl-1.821341]--[lr: pb-0.000050, pf-0.000024]--[ETA-15:38:11]
2023.01.27-03:47:49:636:[step-114300/177600: 64.36%]--[loss-2.625248: wl-3.837112, gl-1.665970]--[lr: pb-0.000050, pf-0.000024]--[ETA-16:39:02]
2023.01.27-03:49:19:736:[step-114400/177600: 64.41%]--[loss-2.745626: wl-3.764936, gl-1.804392]--[lr: pb-0.000050, pf-0.000024]--[ETA-15:37:38]
2023.01.27-03:50:49:836:[step-114500/177600: 64.47%]--[loss-2.669510: wl-3.745480, gl-1.733140]--[lr: pb-0.000050, pf-0.000024]--[ETA-15:29:21]
End of epoch 129 / 200 	 Time Taken: 800 sec
2023.01.27-03:52:20:48:[step-114600/177600: 64.53%]--[loss-2.709342: wl-3.617198, gl-1.805043]--[lr: pb-0.000050, pf-0.000024]--[ETA-15:28:30]
2023.01.27-03:53:50:148:[step-114700/177600: 64.58%]--[loss-2.717497: wl-3.689632, gl-1.795089]--[lr: pb-0.000050, pf-0.000024]--[ETA-15:31:55]
2023.01.27-03:55:20:248:[step-114800/177600: 64.64%]--[loss-2.607170: wl-3.871808, gl-1.639218]--[lr: pb-0.000050, pf-0.000024]--[ETA-15:35:48]
2023.01.27-03:56:49:348:[step-114900/177600: 64.70%]--[loss-2.744070: wl-4.025331, gl-1.737737]--[lr: pb-0.000050, pf-0.000024]--[ETA-15:36:42]
2023.01.27-03:58:19:448:[step-115000/177600: 64.75%]--[loss-2.657567: wl-3.918937, gl-1.677833]--[lr: pb-0.000050, pf-0.000024]--[ETA-15:34:04]
2023.01.27-03:59:49:548:[step-115100/177600: 64.81%]--[loss-2.685569: wl-3.701257, gl-1.760255]--[lr: pb-0.000050, pf-0.000024]--[ETA-15:35:00]
2023.01.27-04:01:19:648:[step-115200/177600: 64.86%]--[loss-2.768728: wl-4.014529, gl-1.765096]--[lr: pb-0.000050, pf-0.000024]--[ETA-15:37:25]
2023.01.27-04:02:49:748:[step-115300/177600: 64.92%]--[loss-2.647373: wl-4.002784, gl-1.646677]--[lr: pb-0.000050, pf-0.000024]--[ETA-15:21:07]
2023.01.27-04:04:18:848:[step-115400/177600: 64.98%]--[loss-2.494789: wl-3.392333, gl-1.646706]--[lr: pb-0.000050, pf-0.000024]--[ETA-15:55:59]
End of epoch 130 / 200 	 Time Taken: 798 sec
saving the model at the end of epoch 130, iters 115440
2023.01.27-04:05:50:60:[step-115500/177600: 65.03%]--[loss-2.824368: wl-4.100842, gl-1.799158]--[lr: pb-0.000050, pf-0.000023]--[ETA-15:30:58]
2023.01.27-04:07:21:160:[step-115600/177600: 65.09%]--[loss-3.014338: wl-4.824440, gl-1.808228]--[lr: pb-0.000050, pf-0.000023]--[ETA-15:30:59]
2023.01.27-04:08:51:260:[step-115700/177600: 65.15%]--[loss-2.680826: wl-3.968415, gl-1.688722]--[lr: pb-0.000050, pf-0.000023]--[ETA-15:51:18]
2023.01.27-04:10:21:360:[step-115800/177600: 65.20%]--[loss-2.731136: wl-3.607852, gl-1.829173]--[lr: pb-0.000050, pf-0.000023]--[ETA-15:19:08]
2023.01.27-04:11:51:460:[step-115900/177600: 65.26%]--[loss-2.615047: wl-3.642468, gl-1.704430]--[lr: pb-0.000050, pf-0.000023]--[ETA-15:16:34]
2023.01.27-04:13:21:560:[step-116000/177600: 65.32%]--[loss-2.643469: wl-3.766023, gl-1.701964]--[lr: pb-0.000050, pf-0.000023]--[ETA-15:50:18]
2023.01.27-04:14:51:660:[step-116100/177600: 65.37%]--[loss-2.840409: wl-4.163795, gl-1.799461]--[lr: pb-0.000050, pf-0.000023]--[ETA-15:14:43]
2023.01.27-04:16:22:760:[step-116200/177600: 65.43%]--[loss-2.526066: wl-3.468856, gl-1.658852]--[lr: pb-0.000050, pf-0.000023]--[ETA-15:13:27]
2023.01.27-04:17:52:860:[step-116300/177600: 65.48%]--[loss-2.542117: wl-3.560345, gl-1.652031]--[lr: pb-0.000050, pf-0.000023]--[ETA-15:37:30]
End of epoch 131 / 200 	 Time Taken: 801 sec
2023.01.27-04:19:23:72:[step-116400/177600: 65.54%]--[loss-2.862877: wl-4.715474, gl-1.684009]--[lr: pb-0.000050, pf-0.000023]--[ETA-15:06:45]
2023.01.27-04:20:53:172:[step-116500/177600: 65.60%]--[loss-2.827509: wl-4.336620, gl-1.743354]--[lr: pb-0.000050, pf-0.000023]--[ETA-15:08:03]
2023.01.27-04:22:24:272:[step-116600/177600: 65.65%]--[loss-2.678374: wl-3.651903, gl-1.765399]--[lr: pb-0.000050, pf-0.000023]--[ETA-15:44:38]
2023.01.27-04:23:55:372:[step-116700/177600: 65.71%]--[loss-2.867119: wl-4.014227, gl-1.863563]--[lr: pb-0.000050, pf-0.000023]--[ETA-15:08:20]
2023.01.27-04:25:25:472:[step-116800/177600: 65.77%]--[loss-2.650002: wl-3.583517, gl-1.754123]--[lr: pb-0.000050, pf-0.000023]--[ETA-15:06:15]
2023.01.27-04:26:55:572:[step-116900/177600: 65.82%]--[loss-2.678016: wl-3.699220, gl-1.753211]--[lr: pb-0.000050, pf-0.000023]--[ETA-15:02:23]
2023.01.27-04:28:25:672:[step-117000/177600: 65.88%]--[loss-2.749465: wl-3.604857, gl-1.848250]--[lr: pb-0.000050, pf-0.000023]--[ETA-15:05:38]
2023.01.27-04:29:54:772:[step-117100/177600: 65.93%]--[loss-2.899211: wl-4.890985, gl-1.676465]--[lr: pb-0.000050, pf-0.000023]--[ETA-15:08:04]
2023.01.27-04:31:24:872:[step-117200/177600: 65.99%]--[loss-2.788858: wl-4.644946, gl-1.627622]--[lr: pb-0.000050, pf-0.000023]--[ETA-15:22:00]
End of epoch 132 / 200 	 Time Taken: 800 sec
2023.01.27-04:32:56:84:[step-117300/177600: 66.05%]--[loss-2.866354: wl-4.073960, gl-1.847863]--[lr: pb-0.000050, pf-0.000023]--[ETA-15:01:21]
2023.01.27-04:34:26:184:[step-117400/177600: 66.10%]--[loss-2.696576: wl-3.739214, gl-1.761772]--[lr: pb-0.000050, pf-0.000023]--[ETA-14:44:21]
2023.01.27-04:35:55:284:[step-117500/177600: 66.16%]--[loss-2.646126: wl-3.787652, gl-1.699213]--[lr: pb-0.000050, pf-0.000023]--[ETA-14:49:23]
2023.01.27-04:37:25:384:[step-117600/177600: 66.22%]--[loss-2.738515: wl-4.117153, gl-1.709227]--[lr: pb-0.000050, pf-0.000023]--[ETA-15:23:53]
2023.01.27-04:38:55:484:[step-117700/177600: 66.27%]--[loss-2.590229: wl-3.689310, gl-1.667902]--[lr: pb-0.000050, pf-0.000023]--[ETA-14:40:58]
2023.01.27-04:40:25:584:[step-117800/177600: 66.33%]--[loss-2.562068: wl-3.686657, gl-1.640404]--[lr: pb-0.000050, pf-0.000023]--[ETA-14:57:03]
2023.01.27-04:41:55:684:[step-117900/177600: 66.39%]--[loss-2.802782: wl-3.819109, gl-1.848005]--[lr: pb-0.000050, pf-0.000023]--[ETA-15:17:32]
2023.01.27-04:43:24:784:[step-118000/177600: 66.44%]--[loss-2.489346: wl-3.510044, gl-1.611835]--[lr: pb-0.000050, pf-0.000023]--[ETA-14:38:17]
2023.01.27-04:44:54:884:[step-118100/177600: 66.50%]--[loss-2.776963: wl-3.871675, gl-1.809044]--[lr: pb-0.000050, pf-0.000023]--[ETA-14:30:37]
End of epoch 133 / 200 	 Time Taken: 799 sec
2023.01.27-04:46:26:96:[step-118200/177600: 66.55%]--[loss-2.753972: wl-3.955511, gl-1.765095]--[lr: pb-0.000050, pf-0.000022]--[ETA-15:22:40]
2023.01.27-04:47:56:196:[step-118300/177600: 66.61%]--[loss-2.694774: wl-3.983223, gl-1.698968]--[lr: pb-0.000050, pf-0.000022]--[ETA-14:40:52]
2023.01.27-04:49:27:296:[step-118400/177600: 66.67%]--[loss-2.818377: wl-4.084316, gl-1.797298]--[lr: pb-0.000050, pf-0.000022]--[ETA-14:46:19]
2023.01.27-04:50:57:396:[step-118500/177600: 66.72%]--[loss-2.649091: wl-3.564867, gl-1.757874]--[lr: pb-0.000050, pf-0.000022]--[ETA-15:05:39]
2023.01.27-04:52:26:496:[step-118600/177600: 66.78%]--[loss-2.946831: wl-4.299349, gl-1.871994]--[lr: pb-0.000050, pf-0.000022]--[ETA-14:28:20]
2023.01.27-04:53:56:596:[step-118700/177600: 66.84%]--[loss-2.587973: wl-3.569353, gl-1.695635]--[lr: pb-0.000050, pf-0.000022]--[ETA-14:28:51]
2023.01.27-04:55:25:696:[step-118800/177600: 66.89%]--[loss-2.586949: wl-3.743068, gl-1.651182]--[lr: pb-0.000050, pf-0.000022]--[ETA-14:30:16]
2023.01.27-04:56:55:796:[step-118900/177600: 66.95%]--[loss-3.045968: wl-4.243572, gl-1.985075]--[lr: pb-0.000050, pf-0.000022]--[ETA-14:33:11]
End of epoch 134 / 200 	 Time Taken: 799 sec
2023.01.27-04:58:25:8:[step-119000/177600: 67.00%]--[loss-2.863005: wl-4.591392, gl-1.715157]--[lr: pb-0.000050, pf-0.000022]--[ETA-14:23:26]
2023.01.27-04:59:55:108:[step-119100/177600: 67.06%]--[loss-2.738344: wl-3.886379, gl-1.766749]--[lr: pb-0.000050, pf-0.000022]--[ETA-14:42:54]
2023.01.27-05:01:25:208:[step-119200/177600: 67.12%]--[loss-2.757052: wl-3.851979, gl-1.794057]--[lr: pb-0.000050, pf-0.000022]--[ETA-14:44:14]
2023.01.27-05:02:55:308:[step-119300/177600: 67.17%]--[loss-2.925746: wl-4.424052, gl-1.819733]--[lr: pb-0.000050, pf-0.000022]--[ETA-14:30:17]
2023.01.27-05:04:25:408:[step-119400/177600: 67.23%]--[loss-2.683599: wl-3.908568, gl-1.706457]--[lr: pb-0.000050, pf-0.000022]--[ETA-14:17:53]
2023.01.27-05:05:54:508:[step-119500/177600: 67.29%]--[loss-2.814926: wl-4.271204, gl-1.747125]--[lr: pb-0.000050, pf-0.000022]--[ETA-14:53:48]
2023.01.27-05:07:24:608:[step-119600/177600: 67.34%]--[loss-2.589201: wl-3.559953, gl-1.699212]--[lr: pb-0.000050, pf-0.000022]--[ETA-14:13:05]
2023.01.27-05:08:54:708:[step-119700/177600: 67.40%]--[loss-2.449752: wl-3.497315, gl-1.575423]--[lr: pb-0.000050, pf-0.000022]--[ETA-14:13:32]
2023.01.27-05:10:23:808:[step-119800/177600: 67.45%]--[loss-2.679189: wl-3.832643, gl-1.721028]--[lr: pb-0.000050, pf-0.000022]--[ETA-14:12:02]
End of epoch 135 / 200 	 Time Taken: 797 sec
saving the model at the end of epoch 135, iters 119880
2023.01.27-05:11:54:20:[step-119900/177600: 67.51%]--[loss-2.461380: wl-3.366388, gl-1.619783]--[lr: pb-0.000050, pf-0.000022]--[ETA-14:27:39]
2023.01.27-05:13:24:120:[step-120000/177600: 67.57%]--[loss-3.008065: wl-4.628601, gl-1.850915]--[lr: pb-0.000050, pf-0.000022]--[ETA-14:39:26]
2023.01.27-05:14:56:220:[step-120100/177600: 67.62%]--[loss-2.590342: wl-3.596575, gl-1.691198]--[lr: pb-0.000050, pf-0.000022]--[ETA-14:44:33]
2023.01.27-05:16:27:320:[step-120200/177600: 67.68%]--[loss-2.899569: wl-4.262387, gl-1.833972]--[lr: pb-0.000050, pf-0.000022]--[ETA-14:22:42]
2023.01.27-05:17:58:420:[step-120300/177600: 67.74%]--[loss-2.861988: wl-4.450092, gl-1.749465]--[lr: pb-0.000050, pf-0.000022]--[ETA-14:13:42]
2023.01.27-05:19:29:520:[step-120400/177600: 67.79%]--[loss-2.688542: wl-3.929674, gl-1.706123]--[lr: pb-0.000050, pf-0.000022]--[ETA-14:10:09]
2023.01.27-05:21:00:620:[step-120500/177600: 67.85%]--[loss-2.649536: wl-3.855475, gl-1.685668]--[lr: pb-0.000050, pf-0.000022]--[ETA-14:02:22]
2023.01.27-05:22:30:720:[step-120600/177600: 67.91%]--[loss-2.773051: wl-4.165426, gl-1.731695]--[lr: pb-0.000050, pf-0.000022]--[ETA-14:22:18]
2023.01.27-05:24:00:820:[step-120700/177600: 67.96%]--[loss-2.771137: wl-3.976033, gl-1.777128]--[lr: pb-0.000050, pf-0.000022]--[ETA-14:18:31]
End of epoch 136 / 200 	 Time Taken: 806 sec
2023.01.27-05:25:31:32:[step-120800/177600: 68.02%]--[loss-2.686907: wl-3.749756, gl-1.749468]--[lr: pb-0.000050, pf-0.000021]--[ETA-14:08:23]
2023.01.27-05:27:02:132:[step-120900/177600: 68.07%]--[loss-2.494928: wl-3.537513, gl-1.610550]--[lr: pb-0.000050, pf-0.000021]--[ETA-14:28:03]
2023.01.27-05:28:32:232:[step-121000/177600: 68.13%]--[loss-2.667998: wl-3.827368, gl-1.711156]--[lr: pb-0.000050, pf-0.000021]--[ETA-14:24:22]
2023.01.27-05:30:04:332:[step-121100/177600: 68.19%]--[loss-2.698631: wl-3.763550, gl-1.757744]--[lr: pb-0.000050, pf-0.000021]--[ETA-14:04:10]
2023.01.27-05:31:34:432:[step-121200/177600: 68.24%]--[loss-2.670935: wl-4.001932, gl-1.670452]--[lr: pb-0.000050, pf-0.000021]--[ETA-14:03:09]
2023.01.27-05:33:04:532:[step-121300/177600: 68.30%]--[loss-2.762501: wl-4.077871, gl-1.743033]--[lr: pb-0.000050, pf-0.000021]--[ETA-13:58:44]
2023.01.27-05:34:34:632:[step-121400/177600: 68.36%]--[loss-2.509752: wl-3.427871, gl-1.652784]--[lr: pb-0.000050, pf-0.000021]--[ETA-14:23:06]
2023.01.27-05:36:05:732:[step-121500/177600: 68.41%]--[loss-2.649637: wl-3.583531, gl-1.753754]--[lr: pb-0.000050, pf-0.000021]--[ETA-13:48:20]
2023.01.27-05:37:35:832:[step-121600/177600: 68.47%]--[loss-2.654969: wl-3.865483, gl-1.688599]--[lr: pb-0.000050, pf-0.000021]--[ETA-13:52:16]
End of epoch 137 / 200 	 Time Taken: 805 sec
2023.01.27-05:39:07:44:[step-121700/177600: 68.52%]--[loss-2.762192: wl-4.208739, gl-1.710007]--[lr: pb-0.000050, pf-0.000021]--[ETA-13:47:08]
2023.01.27-05:40:37:144:[step-121800/177600: 68.58%]--[loss-2.855968: wl-4.012857, gl-1.852754]--[lr: pb-0.000050, pf-0.000021]--[ETA-13:44:20]
2023.01.27-05:42:08:244:[step-121900/177600: 68.64%]--[loss-2.783544: wl-3.947479, gl-1.796674]--[lr: pb-0.000050, pf-0.000021]--[ETA-14:46:20]
2023.01.27-05:43:38:344:[step-122000/177600: 68.69%]--[loss-2.551202: wl-3.669875, gl-1.633733]--[lr: pb-0.000050, pf-0.000021]--[ETA-13:53:15]
2023.01.27-05:45:09:444:[step-122100/177600: 68.75%]--[loss-2.554817: wl-3.560078, gl-1.664798]--[lr: pb-0.000050, pf-0.000021]--[ETA-13:51:48]
2023.01.27-05:46:39:544:[step-122200/177600: 68.81%]--[loss-2.679432: wl-3.810606, gl-1.726780]--[lr: pb-0.000050, pf-0.000021]--[ETA-13:44:21]
2023.01.27-05:48:09:644:[step-122300/177600: 68.86%]--[loss-2.645707: wl-3.710677, gl-1.718038]--[lr: pb-0.000050, pf-0.000021]--[ETA-13:55:54]
2023.01.27-05:49:40:744:[step-122400/177600: 68.92%]--[loss-2.840369: wl-4.623702, gl-1.684443]--[lr: pb-0.000050, pf-0.000021]--[ETA-13:54:05]
2023.01.27-05:51:10:844:[step-122500/177600: 68.98%]--[loss-2.565120: wl-3.783237, gl-1.619311]--[lr: pb-0.000050, pf-0.000021]--[ETA-13:35:04]
End of epoch 138 / 200 	 Time Taken: 802 sec
2023.01.27-05:52:41:56:[step-122600/177600: 69.03%]--[loss-2.721178: wl-4.026826, gl-1.714471]--[lr: pb-0.000050, pf-0.000021]--[ETA-14:03:52]
2023.01.27-05:54:11:156:[step-122700/177600: 69.09%]--[loss-2.576128: wl-3.818358, gl-1.621538]--[lr: pb-0.000050, pf-0.000021]--[ETA-13:36:34]
2023.01.27-05:55:41:256:[step-122800/177600: 69.14%]--[loss-2.811288: wl-4.161273, gl-1.770970]--[lr: pb-0.000050, pf-0.000021]--[ETA-13:33:31]
2023.01.27-05:57:11:356:[step-122900/177600: 69.20%]--[loss-2.711862: wl-4.133111, gl-1.678584]--[lr: pb-0.000050, pf-0.000021]--[ETA-15:11:26]
2023.01.27-05:58:41:456:[step-123000/177600: 69.26%]--[loss-3.175819: wl-5.533687, gl-1.792397]--[lr: pb-0.000050, pf-0.000021]--[ETA-13:24:29]
2023.01.27-06:00:11:556:[step-123100/177600: 69.31%]--[loss-2.610499: wl-3.591167, gl-1.712707]--[lr: pb-0.000050, pf-0.000021]--[ETA-13:32:30]
2023.01.27-06:01:41:656:[step-123200/177600: 69.37%]--[loss-2.563602: wl-3.653119, gl-1.650323]--[lr: pb-0.000050, pf-0.000021]--[ETA-14:10:31]
2023.01.27-06:03:11:756:[step-123300/177600: 69.43%]--[loss-2.554718: wl-3.658917, gl-1.639988]--[lr: pb-0.000050, pf-0.000021]--[ETA-13:21:40]
2023.01.27-06:04:40:856:[step-123400/177600: 69.48%]--[loss-2.715664: wl-3.875599, gl-1.746764]--[lr: pb-0.000050, pf-0.000021]--[ETA-13:20:55]
End of epoch 139 / 200 	 Time Taken: 800 sec
2023.01.27-06:06:11:68:[step-123500/177600: 69.54%]--[loss-2.674057: wl-3.912549, gl-1.695919]--[lr: pb-0.000050, pf-0.000020]--[ETA-13:29:08]
2023.01.27-06:07:41:168:[step-123600/177600: 69.59%]--[loss-2.624165: wl-3.580311, gl-1.729087]--[lr: pb-0.000050, pf-0.000020]--[ETA-13:11:45]
2023.01.27-06:09:12:268:[step-123700/177600: 69.65%]--[loss-2.636350: wl-3.697143, gl-1.712064]--[lr: pb-0.000050, pf-0.000020]--[ETA-13:16:20]
2023.01.27-06:10:42:368:[step-123800/177600: 69.71%]--[loss-2.713535: wl-4.190599, gl-1.665885]--[lr: pb-0.000050, pf-0.000020]--[ETA-13:25:36]
2023.01.27-06:12:12:468:[step-123900/177600: 69.76%]--[loss-2.771894: wl-4.197073, gl-1.722625]--[lr: pb-0.000050, pf-0.000020]--[ETA-13:43:05]
2023.01.27-06:13:43:568:[step-124000/177600: 69.82%]--[loss-2.805187: wl-3.911301, gl-1.827362]--[lr: pb-0.000050, pf-0.000020]--[ETA-13:45:32]
2023.01.27-06:15:13:668:[step-124100/177600: 69.88%]--[loss-2.662200: wl-3.825765, gl-1.705759]--[lr: pb-0.000050, pf-0.000020]--[ETA-13:06:42]
2023.01.27-06:16:43:768:[step-124200/177600: 69.93%]--[loss-2.637482: wl-3.947387, gl-1.650635]--[lr: pb-0.000050, pf-0.000020]--[ETA-14:02:41]
2023.01.27-06:18:12:868:[step-124300/177600: 69.99%]--[loss-2.716881: wl-3.886331, gl-1.745298]--[lr: pb-0.000050, pf-0.000020]--[ETA-13:07:39]
End of epoch 140 / 200 	 Time Taken: 800 sec
saving the model at the end of epoch 140, iters 124320
2023.01.27-06:19:44:80:[step-124400/177600: 70.05%]--[loss-2.591056: wl-3.623876, gl-1.685087]--[lr: pb-0.000050, pf-0.000020]--[ETA-13:07:13]
2023.01.27-06:21:14:180:[step-124500/177600: 70.10%]--[loss-2.912225: wl-4.511519, gl-1.784345]--[lr: pb-0.000050, pf-0.000020]--[ETA-13:05:02]
2023.01.27-06:22:44:280:[step-124600/177600: 70.16%]--[loss-2.716073: wl-3.567658, gl-1.824159]--[lr: pb-0.000050, pf-0.000020]--[ETA-13:01:48]
2023.01.27-06:24:13:380:[step-124700/177600: 70.21%]--[loss-2.629438: wl-3.662363, gl-1.713847]--[lr: pb-0.000050, pf-0.000020]--[ETA-12:59:41]
2023.01.27-06:25:44:480:[step-124800/177600: 70.27%]--[loss-2.546861: wl-3.655582, gl-1.632966]--[lr: pb-0.000050, pf-0.000020]--[ETA-13:19:40]
2023.01.27-06:27:14:580:[step-124900/177600: 70.33%]--[loss-2.592404: wl-3.970916, gl-1.599676]--[lr: pb-0.000050, pf-0.000020]--[ETA-13:08:45]
2023.01.27-06:28:46:680:[step-125000/177600: 70.38%]--[loss-2.925379: wl-4.347850, gl-1.838417]--[lr: pb-0.000050, pf-0.000020]--[ETA-12:56:47]
2023.01.27-06:30:16:780:[step-125100/177600: 70.44%]--[loss-2.560473: wl-3.655083, gl-1.646702]--[lr: pb-0.000050, pf-0.000020]--[ETA-13:13:51]
2023.01.27-06:31:46:880:[step-125200/177600: 70.50%]--[loss-2.638184: wl-3.587323, gl-1.741353]--[lr: pb-0.000050, pf-0.000020]--[ETA-13:11:06]
End of epoch 141 / 200 	 Time Taken: 801 sec
2023.01.27-06:33:17:92:[step-125300/177600: 70.55%]--[loss-2.558744: wl-3.537383, gl-1.674399]--[lr: pb-0.000050, pf-0.000020]--[ETA-13:25:34]
2023.01.27-06:34:47:192:[step-125400/177600: 70.61%]--[loss-2.648230: wl-3.791741, gl-1.700294]--[lr: pb-0.000050, pf-0.000020]--[ETA-13:28:31]
2023.01.27-06:36:17:292:[step-125500/177600: 70.66%]--[loss-2.671158: wl-3.818897, gl-1.716433]--[lr: pb-0.000050, pf-0.000020]--[ETA-12:54:05]
2023.01.27-06:37:47:392:[step-125600/177600: 70.72%]--[loss-2.625340: wl-3.574706, gl-1.731664]--[lr: pb-0.000050, pf-0.000020]--[ETA-12:46:40]
2023.01.27-06:39:17:492:[step-125700/177600: 70.78%]--[loss-2.519799: wl-3.737740, gl-1.585364]--[lr: pb-0.000050, pf-0.000020]--[ETA-13:16:20]
2023.01.27-06:40:47:592:[step-125800/177600: 70.83%]--[loss-2.816249: wl-3.887615, gl-1.844345]--[lr: pb-0.000050, pf-0.000020]--[ETA-12:46:08]
2023.01.27-06:42:17:692:[step-125900/177600: 70.89%]--[loss-2.598914: wl-3.837403, gl-1.639563]--[lr: pb-0.000050, pf-0.000020]--[ETA-12:54:01]
2023.01.27-06:43:46:792:[step-126000/177600: 70.95%]--[loss-2.828120: wl-4.098462, gl-1.803505]--[lr: pb-0.000050, pf-0.000020]--[ETA-12:51:15]
End of epoch 142 / 200 	 Time Taken: 800 sec
2023.01.27-06:45:18:4:[step-126100/177600: 71.00%]--[loss-2.587758: wl-3.584805, gl-1.691557]--[lr: pb-0.000050, pf-0.000019]--[ETA-12:41:16]
2023.01.27-06:46:47:104:[step-126200/177600: 71.06%]--[loss-2.728867: wl-4.212463, gl-1.675751]--[lr: pb-0.000050, pf-0.000019]--[ETA-12:47:42]
2023.01.27-06:48:17:204:[step-126300/177600: 71.11%]--[loss-2.644100: wl-3.886337, gl-1.672516]--[lr: pb-0.000050, pf-0.000019]--[ETA-12:35:33]
2023.01.27-06:49:47:304:[step-126400/177600: 71.17%]--[loss-2.860617: wl-4.169050, gl-1.818354]--[lr: pb-0.000050, pf-0.000019]--[ETA-12:34:33]
2023.01.27-06:51:16:404:[step-126500/177600: 71.23%]--[loss-2.687454: wl-4.015205, gl-1.683653]--[lr: pb-0.000050, pf-0.000019]--[ETA-12:45:28]
2023.01.27-06:52:46:504:[step-126600/177600: 71.28%]--[loss-2.650035: wl-4.014489, gl-1.646413]--[lr: pb-0.000050, pf-0.000019]--[ETA-12:34:46]
2023.01.27-06:54:16:604:[step-126700/177600: 71.34%]--[loss-2.961751: wl-4.094216, gl-1.938197]--[lr: pb-0.000050, pf-0.000019]--[ETA-12:44:12]
2023.01.27-06:55:45:704:[step-126800/177600: 71.40%]--[loss-2.759726: wl-4.079411, gl-1.739873]--[lr: pb-0.000050, pf-0.000019]--[ETA-12:30:40]
2023.01.27-06:57:15:804:[step-126900/177600: 71.45%]--[loss-2.686148: wl-3.608218, gl-1.784093]--[lr: pb-0.000050, pf-0.000019]--[ETA-12:32:42]
End of epoch 143 / 200 	 Time Taken: 798 sec
2023.01.27-06:58:47:16:[step-127000/177600: 71.51%]--[loss-2.574741: wl-3.754058, gl-1.636227]--[lr: pb-0.000050, pf-0.000019]--[ETA-12:58:22]
2023.01.27-07:00:17:116:[step-127100/177600: 71.57%]--[loss-2.610516: wl-3.746422, gl-1.673910]--[lr: pb-0.000050, pf-0.000019]--[ETA-12:37:51]
2023.01.27-07:01:47:216:[step-127200/177600: 71.62%]--[loss-2.736409: wl-4.037274, gl-1.727091]--[lr: pb-0.000050, pf-0.000019]--[ETA-12:28:56]
2023.01.27-07:03:18:316:[step-127300/177600: 71.68%]--[loss-2.571168: wl-3.785623, gl-1.624762]--[lr: pb-0.000050, pf-0.000019]--[ETA-13:10:42]
2023.01.27-07:04:48:416:[step-127400/177600: 71.73%]--[loss-2.554328: wl-3.601941, gl-1.653843]--[lr: pb-0.000050, pf-0.000019]--[ETA-12:24:50]
2023.01.27-07:06:17:516:[step-127500/177600: 71.79%]--[loss-2.529400: wl-3.708642, gl-1.602240]--[lr: pb-0.000050, pf-0.000019]--[ETA-12:22:37]
2023.01.27-07:07:48:616:[step-127600/177600: 71.85%]--[loss-2.515566: wl-3.706365, gl-1.588974]--[lr: pb-0.000050, pf-0.000019]--[ETA-13:02:03]
2023.01.27-07:09:18:716:[step-127700/177600: 71.90%]--[loss-2.744201: wl-4.126848, gl-1.712489]--[lr: pb-0.000050, pf-0.000019]--[ETA-12:19:51]
2023.01.27-07:10:48:816:[step-127800/177600: 71.96%]--[loss-2.748613: wl-3.986053, gl-1.752100]--[lr: pb-0.000050, pf-0.000019]--[ETA-12:29:02]
End of epoch 144 / 200 	 Time Taken: 801 sec
2023.01.27-07:12:19:28:[step-127900/177600: 72.02%]--[loss-2.470738: wl-3.352930, gl-1.632506]--[lr: pb-0.000050, pf-0.000019]--[ETA-12:32:17]
2023.01.27-07:13:49:128:[step-128000/177600: 72.07%]--[loss-2.498449: wl-3.424113, gl-1.642421]--[lr: pb-0.000050, pf-0.000019]--[ETA-12:09:02]
2023.01.27-07:15:18:228:[step-128100/177600: 72.13%]--[loss-2.658649: wl-3.693582, gl-1.735253]--[lr: pb-0.000050, pf-0.000019]--[ETA-12:11:17]
2023.01.27-07:16:48:328:[step-128200/177600: 72.18%]--[loss-2.719611: wl-3.707523, gl-1.792730]--[lr: pb-0.000050, pf-0.000019]--[ETA-12:15:49]
2023.01.27-07:18:17:428:[step-128300/177600: 72.24%]--[loss-2.798100: wl-4.305816, gl-1.721646]--[lr: pb-0.000050, pf-0.000019]--[ETA-12:09:10]
2023.01.27-07:19:47:528:[step-128400/177600: 72.30%]--[loss-2.656880: wl-3.868598, gl-1.689730]--[lr: pb-0.000050, pf-0.000019]--[ETA-12:04:39]
2023.01.27-07:21:16:628:[step-128500/177600: 72.35%]--[loss-2.421976: wl-3.270617, gl-1.604322]--[lr: pb-0.000050, pf-0.000019]--[ETA-12:00:39]
2023.01.27-07:22:46:728:[step-128600/177600: 72.41%]--[loss-2.672256: wl-3.841941, gl-1.711770]--[lr: pb-0.000050, pf-0.000019]--[ETA-12:35:39]
2023.01.27-07:24:16:828:[step-128700/177600: 72.47%]--[loss-2.636793: wl-3.667288, gl-1.719971]--[lr: pb-0.000050, pf-0.000019]--[ETA-12:02:00]
End of epoch 145 / 200 	 Time Taken: 796 sec
saving the model at the end of epoch 145, iters 128760
2023.01.27-07:25:47:40:[step-128800/177600: 72.52%]--[loss-2.491445: wl-3.470829, gl-1.623738]--[lr: pb-0.000050, pf-0.000018]--[ETA-12:00:02]
2023.01.27-07:27:18:140:[step-128900/177600: 72.58%]--[loss-2.544536: wl-3.790449, gl-1.596924]--[lr: pb-0.000050, pf-0.000018]--[ETA-12:41:24]
2023.01.27-07:28:48:240:[step-129000/177600: 72.64%]--[loss-2.583849: wl-3.648045, gl-1.671838]--[lr: pb-0.000050, pf-0.000018]--[ETA-12:18:47]
2023.01.27-07:30:21:340:[step-129100/177600: 72.69%]--[loss-2.659375: wl-3.721890, gl-1.728903]--[lr: pb-0.000050, pf-0.000018]--[ETA-12:18:29]
2023.01.27-07:31:54:440:[step-129200/177600: 72.75%]--[loss-2.630432: wl-3.801574, gl-1.680038]--[lr: pb-0.000050, pf-0.000018]--[ETA-12:21:32]
2023.01.27-07:33:23:540:[step-129300/177600: 72.80%]--[loss-2.593172: wl-3.439551, gl-1.733284]--[lr: pb-0.000050, pf-0.000018]--[ETA-11:58:29]
2023.01.27-07:34:53:640:[step-129400/177600: 72.86%]--[loss-2.525615: wl-3.530740, gl-1.642930]--[lr: pb-0.000050, pf-0.000018]--[ETA-11:59:10]
2023.01.27-07:36:24:740:[step-129500/177600: 72.92%]--[loss-2.632407: wl-3.610466, gl-1.729791]--[lr: pb-0.000050, pf-0.000018]--[ETA-12:22:42]
2023.01.27-07:37:55:840:[step-129600/177600: 72.97%]--[loss-2.861067: wl-4.445258, gl-1.749753]--[lr: pb-0.000050, pf-0.000018]--[ETA-12:11:24]
End of epoch 146 / 200 	 Time Taken: 808 sec
2023.01.27-07:39:26:52:[step-129700/177600: 73.03%]--[loss-2.676136: wl-3.937444, gl-1.691775]--[lr: pb-0.000050, pf-0.000018]--[ETA-11:44:46]
2023.01.27-07:40:55:152:[step-129800/177600: 73.09%]--[loss-2.643987: wl-3.668833, gl-1.726779]--[lr: pb-0.000050, pf-0.000018]--[ETA-12:15:25]
2023.01.27-07:42:25:252:[step-129900/177600: 73.14%]--[loss-2.769822: wl-3.992842, gl-1.771611]--[lr: pb-0.000050, pf-0.000018]--[ETA-11:44:06]
2023.01.27-07:43:55:352:[step-130000/177600: 73.20%]--[loss-2.603294: wl-3.722952, gl-1.672556]--[lr: pb-0.000050, pf-0.000018]--[ETA-11:44:44]
2023.01.27-07:45:25:452:[step-130100/177600: 73.25%]--[loss-2.697050: wl-3.687542, gl-1.775165]--[lr: pb-0.000050, pf-0.000018]--[ETA-12:08:06]
2023.01.27-07:46:55:552:[step-130200/177600: 73.31%]--[loss-2.643739: wl-3.872116, gl-1.675710]--[lr: pb-0.000050, pf-0.000018]--[ETA-11:43:31]
2023.01.27-07:48:24:652:[step-130300/177600: 73.37%]--[loss-2.861885: wl-4.213033, gl-1.808627]--[lr: pb-0.000050, pf-0.000018]--[ETA-11:39:06]
2023.01.27-07:49:54:752:[step-130400/177600: 73.42%]--[loss-2.642646: wl-3.579495, gl-1.747772]--[lr: pb-0.000050, pf-0.000018]--[ETA-11:46:56]
2023.01.27-07:51:24:852:[step-130500/177600: 73.48%]--[loss-2.601712: wl-3.618141, gl-1.697176]--[lr: pb-0.000050, pf-0.000018]--[ETA-11:35:51]
End of epoch 147 / 200 	 Time Taken: 798 sec
2023.01.27-07:52:55:64:[step-130600/177600: 73.54%]--[loss-2.930140: wl-4.616447, gl-1.776028]--[lr: pb-0.000050, pf-0.000018]--[ETA-11:44:44]
2023.01.27-07:54:26:164:[step-130700/177600: 73.59%]--[loss-2.848711: wl-4.217162, gl-1.794420]--[lr: pb-0.000050, pf-0.000018]--[ETA-11:43:09]
2023.01.27-07:55:57:264:[step-130800/177600: 73.65%]--[loss-2.792433: wl-4.087418, gl-1.770578]--[lr: pb-0.000050, pf-0.000018]--[ETA-11:34:27]
2023.01.27-07:57:28:364:[step-130900/177600: 73.70%]--[loss-2.760110: wl-4.369573, gl-1.667717]--[lr: pb-0.000050, pf-0.000018]--[ETA-11:37:16]
2023.01.27-07:58:58:464:[step-131000/177600: 73.76%]--[loss-2.689338: wl-3.853953, gl-1.725850]--[lr: pb-0.000050, pf-0.000018]--[ETA-11:30:36]
2023.01.27-08:00:28:564:[step-131100/177600: 73.82%]--[loss-2.504731: wl-3.735111, gl-1.570954]--[lr: pb-0.000050, pf-0.000018]--[ETA-11:27:34]
2023.01.27-08:01:58:664:[step-131200/177600: 73.87%]--[loss-2.902672: wl-4.394537, gl-1.804037]--[lr: pb-0.000050, pf-0.000018]--[ETA-11:23:01]
2023.01.27-08:03:29:764:[step-131300/177600: 73.93%]--[loss-2.910374: wl-4.337738, gl-1.825939]--[lr: pb-0.000050, pf-0.000018]--[ETA-11:26:43]
2023.01.27-08:04:59:864:[step-131400/177600: 73.99%]--[loss-2.553218: wl-3.668219, gl-1.636163]--[lr: pb-0.000050, pf-0.000018]--[ETA-11:28:35]
End of epoch 148 / 200 	 Time Taken: 804 sec
2023.01.27-08:06:31:76:[step-131500/177600: 74.04%]--[loss-2.736768: wl-3.743774, gl-1.800824]--[lr: pb-0.000050, pf-0.000017]--[ETA-11:18:49]
2023.01.27-08:08:00:176:[step-131600/177600: 74.10%]--[loss-2.870285: wl-4.385847, gl-1.773823]--[lr: pb-0.000050, pf-0.000017]--[ETA-11:26:41]
2023.01.27-08:09:30:276:[step-131700/177600: 74.16%]--[loss-2.652814: wl-3.668868, gl-1.735597]--[lr: pb-0.000050, pf-0.000017]--[ETA-11:28:38]
2023.01.27-08:11:00:376:[step-131800/177600: 74.21%]--[loss-2.496112: wl-3.428074, gl-1.639093]--[lr: pb-0.000050, pf-0.000017]--[ETA-11:39:16]
2023.01.27-08:12:30:476:[step-131900/177600: 74.27%]--[loss-2.659644: wl-3.692709, gl-1.736467]--[lr: pb-0.000050, pf-0.000017]--[ETA-11:15:29]
2023.01.27-08:14:00:576:[step-132000/177600: 74.32%]--[loss-2.484853: wl-3.475480, gl-1.615983]--[lr: pb-0.000050, pf-0.000017]--[ETA-11:25:10]
2023.01.27-08:15:29:676:[step-132100/177600: 74.38%]--[loss-2.633639: wl-3.669809, gl-1.716187]--[lr: pb-0.000050, pf-0.000017]--[ETA-11:08:54]
2023.01.27-08:16:59:776:[step-132200/177600: 74.44%]--[loss-2.576571: wl-3.506023, gl-1.700065]--[lr: pb-0.000050, pf-0.000017]--[ETA-11:10:06]
2023.01.27-08:18:28:876:[step-132300/177600: 74.49%]--[loss-2.633676: wl-3.587759, gl-1.736736]--[lr: pb-0.000050, pf-0.000017]--[ETA-11:38:45]
End of epoch 149 / 200 	 Time Taken: 798 sec
2023.01.27-08:19:59:88:[step-132400/177600: 74.55%]--[loss-2.764134: wl-4.106866, gl-1.737417]--[lr: pb-0.000050, pf-0.000017]--[ETA-11:06:48]
2023.01.27-08:21:29:188:[step-132500/177600: 74.61%]--[loss-2.654730: wl-3.909580, gl-1.677335]--[lr: pb-0.000050, pf-0.000017]--[ETA-11:51:24]
2023.01.27-08:22:59:288:[step-132600/177600: 74.66%]--[loss-2.624130: wl-3.771840, gl-1.681170]--[lr: pb-0.000050, pf-0.000017]--[ETA-11:33:02]
2023.01.27-08:24:29:388:[step-132700/177600: 74.72%]--[loss-2.483771: wl-3.471992, gl-1.615773]--[lr: pb-0.000050, pf-0.000017]--[ETA-11:04:22]
2023.01.27-08:25:59:488:[step-132800/177600: 74.77%]--[loss-2.595800: wl-3.789384, gl-1.648454]--[lr: pb-0.000050, pf-0.000017]--[ETA-10:58:40]
2023.01.27-08:27:29:588:[step-132900/177600: 74.83%]--[loss-2.472576: wl-3.336546, gl-1.638439]--[lr: pb-0.000050, pf-0.000017]--[ETA-11:06:09]
2023.01.27-08:28:59:688:[step-133000/177600: 74.89%]--[loss-2.630311: wl-3.740360, gl-1.695221]--[lr: pb-0.000050, pf-0.000017]--[ETA-11:12:29]
2023.01.27-08:30:30:788:[step-133100/177600: 74.94%]--[loss-2.652155: wl-3.428339, gl-1.795071]--[lr: pb-0.000050, pf-0.000017]--[ETA-10:58:28]
2023.01.27-08:31:59:888:[step-133200/177600: 75.00%]--[loss-2.709872: wl-3.878470, gl-1.740254]--[lr: pb-0.000050, pf-0.000017]--[ETA-11:06:13]
End of epoch 150 / 200 	 Time Taken: 800 sec
saving the model at the end of epoch 150, iters 133200
2023.01.27-08:33:32:100:[step-133300/177600: 75.06%]--[loss-2.788861: wl-4.107489, gl-1.761989]--[lr: pb-0.000050, pf-0.000017]--[ETA-11:06:21]
2023.01.27-08:35:02:200:[step-133400/177600: 75.11%]--[loss-2.516142: wl-3.451330, gl-1.653310]--[lr: pb-0.000050, pf-0.000017]--[ETA-10:57:20]
2023.01.27-08:36:31:300:[step-133500/177600: 75.17%]--[loss-2.688799: wl-4.064276, gl-1.672730]--[lr: pb-0.000050, pf-0.000017]--[ETA-11:13:56]
2023.01.27-08:38:01:400:[step-133600/177600: 75.23%]--[loss-2.750797: wl-3.980188, gl-1.755750]--[lr: pb-0.000050, pf-0.000017]--[ETA-10:54:35]
2023.01.27-08:39:31:500:[step-133700/177600: 75.28%]--[loss-2.649158: wl-4.212980, gl-1.595913]--[lr: pb-0.000050, pf-0.000017]--[ETA-11:36:10]
2023.01.27-08:41:03:600:[step-133800/177600: 75.34%]--[loss-2.569068: wl-3.608783, gl-1.666873]--[lr: pb-0.000050, pf-0.000017]--[ETA-10:55:56]
2023.01.27-08:42:33:700:[step-133900/177600: 75.39%]--[loss-2.682128: wl-3.593445, gl-1.783767]--[lr: pb-0.000050, pf-0.000017]--[ETA-10:52:59]
2023.01.27-08:44:03:800:[step-134000/177600: 75.45%]--[loss-2.427664: wl-3.395756, gl-1.578725]--[lr: pb-0.000050, pf-0.000017]--[ETA-10:42:06]
End of epoch 151 / 200 	 Time Taken: 802 sec
2023.01.27-08:45:34:12:[step-134100/177600: 75.51%]--[loss-2.655375: wl-3.822287, gl-1.699803]--[lr: pb-0.000050, pf-0.000016]--[ETA-10:43:16]
2023.01.27-08:47:04:112:[step-134200/177600: 75.56%]--[loss-2.952765: wl-4.988457, gl-1.705650]--[lr: pb-0.000050, pf-0.000016]--[ETA-11:06:12]
2023.01.27-08:48:33:212:[step-134300/177600: 75.62%]--[loss-2.713466: wl-3.769383, gl-1.771120]--[lr: pb-0.000050, pf-0.000016]--[ETA-10:48:46]
2023.01.27-08:50:03:312:[step-134400/177600: 75.68%]--[loss-2.976061: wl-4.670413, gl-1.808457]--[lr: pb-0.000050, pf-0.000016]--[ETA-10:46:55]
2023.01.27-08:51:33:412:[step-134500/177600: 75.73%]--[loss-2.682190: wl-4.387448, gl-1.585328]--[lr: pb-0.000050, pf-0.000016]--[ETA-11:06:13]
2023.01.27-08:53:04:512:[step-134600/177600: 75.79%]--[loss-2.524427: wl-3.724517, gl-1.593298]--[lr: pb-0.000050, pf-0.000016]--[ETA-10:39:27]
2023.01.27-08:54:34:612:[step-134700/177600: 75.84%]--[loss-2.557120: wl-3.601513, gl-1.656742]--[lr: pb-0.000050, pf-0.000016]--[ETA-10:53:59]
2023.01.27-08:56:04:712:[step-134800/177600: 75.90%]--[loss-2.806783: wl-4.359925, gl-1.716802]--[lr: pb-0.000050, pf-0.000016]--[ETA-11:08:46]
2023.01.27-08:57:34:812:[step-134900/177600: 75.96%]--[loss-2.815880: wl-4.216410, gl-1.761777]--[lr: pb-0.000050, pf-0.000016]--[ETA-10:35:20]
End of epoch 152 / 200 	 Time Taken: 800 sec
2023.01.27-08:59:05:24:[step-135000/177600: 76.01%]--[loss-2.681543: wl-3.811645, gl-1.728631]--[lr: pb-0.000050, pf-0.000016]--[ETA-10:25:28]
2023.01.27-09:00:35:124:[step-135100/177600: 76.07%]--[loss-2.691980: wl-3.942768, gl-1.706289]--[lr: pb-0.000050, pf-0.000016]--[ETA-10:26:35]
2023.01.27-09:02:04:224:[step-135200/177600: 76.13%]--[loss-2.645313: wl-3.910094, gl-1.667789]--[lr: pb-0.000050, pf-0.000016]--[ETA-10:23:44]
2023.01.27-09:03:34:324:[step-135300/177600: 76.18%]--[loss-2.628514: wl-3.790278, gl-1.680944]--[lr: pb-0.000050, pf-0.000016]--[ETA-10:52:14]
2023.01.27-09:05:05:424:[step-135400/177600: 76.24%]--[loss-2.731486: wl-3.994672, gl-1.732818]--[lr: pb-0.000050, pf-0.000016]--[ETA-10:22:30]
2023.01.27-09:06:34:524:[step-135500/177600: 76.30%]--[loss-2.599597: wl-3.773185, gl-1.656300]--[lr: pb-0.000050, pf-0.000016]--[ETA-10:26:22]
2023.01.27-09:08:03:624:[step-135600/177600: 76.35%]--[loss-2.684228: wl-3.948121, gl-1.697198]--[lr: pb-0.000050, pf-0.000016]--[ETA-10:38:32]
2023.01.27-09:09:34:724:[step-135700/177600: 76.41%]--[loss-2.782235: wl-3.983020, gl-1.786481]--[lr: pb-0.000050, pf-0.000016]--[ETA-10:24:33]
2023.01.27-09:11:03:824:[step-135800/177600: 76.46%]--[loss-2.559136: wl-3.673623, gl-1.640730]--[lr: pb-0.000050, pf-0.000016]--[ETA-10:24:31]
End of epoch 153 / 200 	 Time Taken: 797 sec
2023.01.27-09:12:34:36:[step-135900/177600: 76.52%]--[loss-2.581027: wl-3.760663, gl-1.640861]--[lr: pb-0.000050, pf-0.000016]--[ETA-10:16:47]
2023.01.27-09:14:04:136:[step-136000/177600: 76.58%]--[loss-2.677736: wl-3.660670, gl-1.762568]--[lr: pb-0.000050, pf-0.000016]--[ETA-10:14:24]
2023.01.27-09:15:33:236:[step-136100/177600: 76.63%]--[loss-2.584716: wl-3.516881, gl-1.705496]--[lr: pb-0.000050, pf-0.000016]--[ETA-10:23:04]
2023.01.27-09:17:03:336:[step-136200/177600: 76.69%]--[loss-2.771219: wl-4.112893, gl-1.742996]--[lr: pb-0.000050, pf-0.000016]--[ETA-10:23:53]
2023.01.27-09:18:33:436:[step-136300/177600: 76.75%]--[loss-2.645336: wl-3.589814, gl-1.747883]--[lr: pb-0.000050, pf-0.000016]--[ETA-10:10:55]
2023.01.27-09:20:04:536:[step-136400/177600: 76.80%]--[loss-2.599375: wl-3.954080, gl-1.610855]--[lr: pb-0.000050, pf-0.000016]--[ETA-10:47:57]
2023.01.27-09:21:34:636:[step-136500/177600: 76.86%]--[loss-2.572886: wl-3.536745, gl-1.688700]--[lr: pb-0.000050, pf-0.000016]--[ETA-10:23:40]
2023.01.27-09:23:04:736:[step-136600/177600: 76.91%]--[loss-2.501508: wl-3.613937, gl-1.598024]--[lr: pb-0.000050, pf-0.000016]--[ETA-10:05:24]
2023.01.27-09:24:33:836:[step-136700/177600: 76.97%]--[loss-2.797918: wl-4.030922, gl-1.790187]--[lr: pb-0.000050, pf-0.000016]--[ETA-10:15:16]
End of epoch 154 / 200 	 Time Taken: 798 sec
2023.01.27-09:26:04:48:[step-136800/177600: 77.03%]--[loss-2.649423: wl-3.702314, gl-1.723845]--[lr: pb-0.000050, pf-0.000015]--[ETA-10:01:17]
2023.01.27-09:27:33:148:[step-136900/177600: 77.08%]--[loss-2.834188: wl-4.266162, gl-1.767647]--[lr: pb-0.000050, pf-0.000015]--[ETA-10:09:40]
2023.01.27-09:29:02:248:[step-137000/177600: 77.14%]--[loss-2.816484: wl-4.503275, gl-1.690666]--[lr: pb-0.000050, pf-0.000015]--[ETA-9:57:24]
2023.01.27-09:30:32:348:[step-137100/177600: 77.20%]--[loss-2.439773: wl-3.391430, gl-1.591915]--[lr: pb-0.000050, pf-0.000015]--[ETA-10:01:32]
2023.01.27-09:32:01:448:[step-137200/177600: 77.25%]--[loss-2.554093: wl-3.462174, gl-1.688550]--[lr: pb-0.000050, pf-0.000015]--[ETA-9:57:35]
2023.01.27-09:33:31:548:[step-137300/177600: 77.31%]--[loss-2.371139: wl-3.298948, gl-1.546402]--[lr: pb-0.000050, pf-0.000015]--[ETA-9:56:58]
2023.01.27-09:35:01:648:[step-137400/177600: 77.36%]--[loss-2.540308: wl-3.652476, gl-1.627190]--[lr: pb-0.000050, pf-0.000015]--[ETA-9:54:52]
2023.01.27-09:36:30:748:[step-137500/177600: 77.42%]--[loss-2.672477: wl-3.873171, gl-1.704185]--[lr: pb-0.000050, pf-0.000015]--[ETA-9:55:48]
2023.01.27-09:38:00:848:[step-137600/177600: 77.48%]--[loss-2.679420: wl-4.170167, gl-1.636879]--[lr: pb-0.000050, pf-0.000015]--[ETA-9:52:18]
End of epoch 155 / 200 	 Time Taken: 796 sec
saving the model at the end of epoch 155, iters 137640
2023.01.27-09:39:32:60:[step-137700/177600: 77.53%]--[loss-2.688930: wl-3.812553, gl-1.735791]--[lr: pb-0.000050, pf-0.000015]--[ETA-9:52:03]
2023.01.27-09:41:02:160:[step-137800/177600: 77.59%]--[loss-2.673926: wl-3.820251, gl-1.718864]--[lr: pb-0.000050, pf-0.000015]--[ETA-9:57:25]
2023.01.27-09:42:32:260:[step-137900/177600: 77.65%]--[loss-2.586162: wl-3.763583, gl-1.645266]--[lr: pb-0.000050, pf-0.000015]--[ETA-9:47:31]
2023.01.27-09:44:02:360:[step-138000/177600: 77.70%]--[loss-2.930916: wl-4.684911, gl-1.759688]--[lr: pb-0.000050, pf-0.000015]--[ETA-9:47:01]
2023.01.27-09:45:33:460:[step-138100/177600: 77.76%]--[loss-2.656496: wl-3.804217, gl-1.705441]--[lr: pb-0.000050, pf-0.000015]--[ETA-9:45:23]
2023.01.27-09:47:04:560:[step-138200/177600: 77.82%]--[loss-2.787883: wl-3.935293, gl-1.804060]--[lr: pb-0.000050, pf-0.000015]--[ETA-9:41:00]
2023.01.27-09:48:35:660:[step-138300/177600: 77.87%]--[loss-2.608548: wl-4.104131, gl-1.582515]--[lr: pb-0.000050, pf-0.000015]--[ETA-9:57:10]
2023.01.27-09:50:07:760:[step-138400/177600: 77.93%]--[loss-2.726875: wl-3.986399, gl-1.730275]--[lr: pb-0.000050, pf-0.000015]--[ETA-10:03:27]
2023.01.27-09:51:37:860:[step-138500/177600: 77.98%]--[loss-2.584488: wl-3.564248, gl-1.693426]--[lr: pb-0.000050, pf-0.000015]--[ETA-10:13:47]
End of epoch 156 / 200 	 Time Taken: 805 sec
2023.01.27-09:53:08:72:[step-138600/177600: 78.04%]--[loss-2.634561: wl-3.700418, gl-1.709457]--[lr: pb-0.000050, pf-0.000015]--[ETA-10:03:24]
2023.01.27-09:54:38:172:[step-138700/177600: 78.10%]--[loss-2.552432: wl-3.876750, gl-1.583244]--[lr: pb-0.000050, pf-0.000015]--[ETA-9:42:15]
2023.01.27-09:56:10:272:[step-138800/177600: 78.15%]--[loss-2.651248: wl-3.786317, gl-1.704668]--[lr: pb-0.000050, pf-0.000015]--[ETA-9:36:06]
2023.01.27-09:57:40:372:[step-138900/177600: 78.21%]--[loss-2.504897: wl-3.476985, gl-1.635651]--[lr: pb-0.000050, pf-0.000015]--[ETA-9:49:44]
2023.01.27-09:59:10:472:[step-139000/177600: 78.27%]--[loss-2.550678: wl-3.550972, gl-1.662935]--[lr: pb-0.000050, pf-0.000015]--[ETA-9:46:29]
2023.01.27-10:00:40:572:[step-139100/177600: 78.32%]--[loss-2.541490: wl-3.670763, gl-1.623799]--[lr: pb-0.000050, pf-0.000015]--[ETA-9:29:42]
2023.01.27-10:02:09:672:[step-139200/177600: 78.38%]--[loss-2.692959: wl-3.926719, gl-1.711279]--[lr: pb-0.000050, pf-0.000015]--[ETA-9:28:28]
2023.01.27-10:03:39:772:[step-139300/177600: 78.43%]--[loss-2.563864: wl-3.590312, gl-1.666286]--[lr: pb-0.000050, pf-0.000015]--[ETA-9:47:23]
2023.01.27-10:05:08:872:[step-139400/177600: 78.49%]--[loss-2.788461: wl-3.977466, gl-1.794094]--[lr: pb-0.000050, pf-0.000015]--[ETA-9:24:32]
End of epoch 157 / 200 	 Time Taken: 800 sec
2023.01.27-10:06:39:84:[step-139500/177600: 78.55%]--[loss-2.697759: wl-4.276813, gl-1.628556]--[lr: pb-0.000050, pf-0.000014]--[ETA-9:29:57]
2023.01.27-10:08:08:184:[step-139600/177600: 78.60%]--[loss-2.658209: wl-4.227570, gl-1.601317]--[lr: pb-0.000050, pf-0.000014]--[ETA-9:20:47]
2023.01.27-10:09:38:284:[step-139700/177600: 78.66%]--[loss-2.676142: wl-3.875822, gl-1.707186]--[lr: pb-0.000050, pf-0.000014]--[ETA-9:20:24]
2023.01.27-10:11:07:384:[step-139800/177600: 78.72%]--[loss-2.452398: wl-3.521677, gl-1.571979]--[lr: pb-0.000050, pf-0.000014]--[ETA-9:19:02]
2023.01.27-10:12:36:484:[step-139900/177600: 78.77%]--[loss-2.469229: wl-3.528733, gl-1.587046]--[lr: pb-0.000050, pf-0.000014]--[ETA-9:19:56]
2023.01.27-10:14:06:584:[step-140000/177600: 78.83%]--[loss-2.568998: wl-3.630900, gl-1.661273]--[lr: pb-0.000050, pf-0.000014]--[ETA-9:14:55]
2023.01.27-10:15:35:684:[step-140100/177600: 78.89%]--[loss-2.590422: wl-4.032093, gl-1.582399]--[lr: pb-0.000050, pf-0.000014]--[ETA-9:18:40]
2023.01.27-10:17:04:784:[step-140200/177600: 78.94%]--[loss-2.636312: wl-4.002585, gl-1.635666]--[lr: pb-0.000050, pf-0.000014]--[ETA-9:13:10]
2023.01.27-10:18:33:884:[step-140300/177600: 79.00%]--[loss-2.563442: wl-3.635281, gl-1.654622]--[lr: pb-0.000050, pf-0.000014]--[ETA-9:17:23]
End of epoch 158 / 200 	 Time Taken: 794 sec
2023.01.27-10:20:03:96:[step-140400/177600: 79.05%]--[loss-2.510498: wl-3.746165, gl-1.573956]--[lr: pb-0.000050, pf-0.000014]--[ETA-9:10:38]
2023.01.27-10:21:33:196:[step-140500/177600: 79.11%]--[loss-2.515083: wl-3.495796, gl-1.641134]--[lr: pb-0.000050, pf-0.000014]--[ETA-9:07:47]
2023.01.27-10:23:03:296:[step-140600/177600: 79.17%]--[loss-2.553528: wl-3.623182, gl-1.647732]--[lr: pb-0.000050, pf-0.000014]--[ETA-9:06:06]
2023.01.27-10:24:33:396:[step-140700/177600: 79.22%]--[loss-2.629622: wl-3.739540, gl-1.694737]--[lr: pb-0.000050, pf-0.000014]--[ETA-9:05:24]
2023.01.27-10:26:03:496:[step-140800/177600: 79.28%]--[loss-2.542522: wl-3.861470, gl-1.577155]--[lr: pb-0.000050, pf-0.000014]--[ETA-9:11:04]
2023.01.27-10:27:33:596:[step-140900/177600: 79.34%]--[loss-2.835496: wl-4.199132, gl-1.785713]--[lr: pb-0.000050, pf-0.000014]--[ETA-9:01:29]
2023.01.27-10:29:03:696:[step-141000/177600: 79.39%]--[loss-2.613521: wl-3.923855, gl-1.632557]--[lr: pb-0.000050, pf-0.000014]--[ETA-9:19:24]
2023.01.27-10:30:33:796:[step-141100/177600: 79.45%]--[loss-2.676256: wl-3.846438, gl-1.714646]--[lr: pb-0.000050, pf-0.000014]--[ETA-8:59:52]
End of epoch 159 / 200 	 Time Taken: 798 sec
2023.01.27-10:32:04:8:[step-141200/177600: 79.50%]--[loss-2.767316: wl-3.956094, gl-1.778293]--[lr: pb-0.000050, pf-0.000014]--[ETA-9:05:46]
2023.01.27-10:33:34:108:[step-141300/177600: 79.56%]--[loss-2.531906: wl-3.649767, gl-1.619464]--[lr: pb-0.000050, pf-0.000014]--[ETA-9:33:02]
2023.01.27-10:35:05:208:[step-141400/177600: 79.62%]--[loss-2.595160: wl-3.767697, gl-1.653236]--[lr: pb-0.000050, pf-0.000014]--[ETA-9:14:39]
2023.01.27-10:36:35:308:[step-141500/177600: 79.67%]--[loss-2.434026: wl-3.440396, gl-1.573927]--[lr: pb-0.000050, pf-0.000014]--[ETA-8:56:29]
2023.01.27-10:38:04:408:[step-141600/177600: 79.73%]--[loss-2.704685: wl-4.512912, gl-1.576457]--[lr: pb-0.000050, pf-0.000014]--[ETA-8:51:48]
2023.01.27-10:39:35:508:[step-141700/177600: 79.79%]--[loss-2.441147: wl-3.595508, gl-1.542270]--[lr: pb-0.000050, pf-0.000014]--[ETA-8:52:21]
2023.01.27-10:41:05:608:[step-141800/177600: 79.84%]--[loss-2.472220: wl-3.444359, gl-1.611130]--[lr: pb-0.000050, pf-0.000014]--[ETA-8:53:19]
2023.01.27-10:42:36:708:[step-141900/177600: 79.90%]--[loss-2.773879: wl-4.465479, gl-1.657509]--[lr: pb-0.000050, pf-0.000014]--[ETA-8:57:04]
2023.01.27-10:44:06:808:[step-142000/177600: 79.95%]--[loss-2.556414: wl-3.982170, gl-1.560871]--[lr: pb-0.000050, pf-0.000014]--[ETA-8:50:56]
End of epoch 160 / 200 	 Time Taken: 802 sec
saving the model at the end of epoch 160, iters 142080
2023.01.27-10:45:38:20:[step-142100/177600: 80.01%]--[loss-2.432349: wl-3.461379, gl-1.567004]--[lr: pb-0.000050, pf-0.000013]--[ETA-9:07:08]
2023.01.27-10:47:08:120:[step-142200/177600: 80.07%]--[loss-2.698347: wl-3.986779, gl-1.701652]--[lr: pb-0.000050, pf-0.000013]--[ETA-9:21:49]
2023.01.27-10:48:37:220:[step-142300/177600: 80.12%]--[loss-2.582520: wl-3.815524, gl-1.628639]--[lr: pb-0.000050, pf-0.000013]--[ETA-8:41:28]
2023.01.27-10:50:08:320:[step-142400/177600: 80.18%]--[loss-2.548737: wl-3.882993, gl-1.577989]--[lr: pb-0.000050, pf-0.000013]--[ETA-8:57:21]
2023.01.27-10:51:38:420:[step-142500/177600: 80.24%]--[loss-2.806069: wl-3.717075, gl-1.876800]--[lr: pb-0.000050, pf-0.000013]--[ETA-8:39:49]
2023.01.27-10:53:08:520:[step-142600/177600: 80.29%]--[loss-2.547939: wl-3.547064, gl-1.661173]--[lr: pb-0.000050, pf-0.000013]--[ETA-8:46:19]
2023.01.27-10:54:38:620:[step-142700/177600: 80.35%]--[loss-2.775434: wl-4.042528, gl-1.764802]--[lr: pb-0.000050, pf-0.000013]--[ETA-9:02:39]
2023.01.27-10:56:08:720:[step-142800/177600: 80.41%]--[loss-2.363235: wl-3.282666, gl-1.542568]--[lr: pb-0.000050, pf-0.000013]--[ETA-8:34:32]
2023.01.27-10:57:38:820:[step-142900/177600: 80.46%]--[loss-2.592988: wl-3.672970, gl-1.674746]--[lr: pb-0.000050, pf-0.000013]--[ETA-8:33:18]
End of epoch 161 / 200 	 Time Taken: 799 sec
2023.01.27-10:59:08:32:[step-143000/177600: 80.52%]--[loss-2.819614: wl-4.466677, gl-1.702945]--[lr: pb-0.000050, pf-0.000013]--[ETA-8:54:23]
2023.01.27-11:00:38:132:[step-143100/177600: 80.57%]--[loss-2.500294: wl-3.543791, gl-1.614346]--[lr: pb-0.000050, pf-0.000013]--[ETA-8:30:42]
2023.01.27-11:02:07:232:[step-143200/177600: 80.63%]--[loss-2.561022: wl-3.790367, gl-1.613430]--[lr: pb-0.000050, pf-0.000013]--[ETA-8:27:38]
2023.01.27-11:03:37:332:[step-143300/177600: 80.69%]--[loss-2.638267: wl-3.812961, gl-1.685026]--[lr: pb-0.000050, pf-0.000013]--[ETA-8:26:21]
2023.01.27-11:05:06:432:[step-143400/177600: 80.74%]--[loss-2.806293: wl-4.034043, gl-1.797782]--[lr: pb-0.000050, pf-0.000013]--[ETA-8:24:25]
2023.01.27-11:06:35:532:[step-143500/177600: 80.80%]--[loss-2.543692: wl-3.580939, gl-1.648457]--[lr: pb-0.000050, pf-0.000013]--[ETA-8:23:14]
2023.01.27-11:08:04:632:[step-143600/177600: 80.86%]--[loss-2.615577: wl-3.645244, gl-1.704266]--[lr: pb-0.000050, pf-0.000013]--[ETA-8:22:18]
2023.01.27-11:09:34:732:[step-143700/177600: 80.91%]--[loss-2.520085: wl-3.420654, gl-1.664922]--[lr: pb-0.000050, pf-0.000013]--[ETA-8:19:15]
2023.01.27-11:11:04:832:[step-143800/177600: 80.97%]--[loss-2.601920: wl-3.607819, gl-1.699966]--[lr: pb-0.000050, pf-0.000013]--[ETA-8:25:38]
End of epoch 162 / 200 	 Time Taken: 796 sec
2023.01.27-11:12:36:44:[step-143900/177600: 81.02%]--[loss-2.869989: wl-4.789665, gl-1.672573]--[lr: pb-0.000050, pf-0.000013]--[ETA-8:43:29]
2023.01.27-11:14:06:144:[step-144000/177600: 81.08%]--[loss-2.512449: wl-3.713554, gl-1.584060]--[lr: pb-0.000050, pf-0.000013]--[ETA-8:27:31]
2023.01.27-11:15:37:244:[step-144100/177600: 81.14%]--[loss-2.433592: wl-3.557553, gl-1.544204]--[lr: pb-0.000050, pf-0.000013]--[ETA-8:29:10]
2023.01.27-11:17:08:344:[step-144200/177600: 81.19%]--[loss-2.459320: wl-3.373092, gl-1.616047]--[lr: pb-0.000050, pf-0.000013]--[ETA-8:34:36]
2023.01.27-11:18:39:444:[step-144300/177600: 81.25%]--[loss-2.702502: wl-3.686449, gl-1.780890]--[lr: pb-0.000050, pf-0.000013]--[ETA-8:11:20]
2023.01.27-11:20:09:544:[step-144400/177600: 81.31%]--[loss-2.616942: wl-3.840843, gl-1.656731]--[lr: pb-0.000050, pf-0.000013]--[ETA-8:18:32]
2023.01.27-11:21:38:644:[step-144500/177600: 81.36%]--[loss-2.625988: wl-3.804878, gl-1.674768]--[lr: pb-0.000050, pf-0.000013]--[ETA-8:10:41]
2023.01.27-11:23:08:744:[step-144600/177600: 81.42%]--[loss-2.583953: wl-3.785311, gl-1.637625]--[lr: pb-0.000050, pf-0.000013]--[ETA-8:08:02]
2023.01.27-11:24:38:844:[step-144700/177600: 81.48%]--[loss-2.743804: wl-3.797328, gl-1.794472]--[lr: pb-0.000050, pf-0.000013]--[ETA-8:10:46]
End of epoch 163 / 200 	 Time Taken: 803 sec
2023.01.27-11:26:09:56:[step-144800/177600: 81.53%]--[loss-2.877382: wl-4.526101, gl-1.745856]--[lr: pb-0.000050, pf-0.000012]--[ETA-8:09:02]
2023.01.27-11:27:38:156:[step-144900/177600: 81.59%]--[loss-2.481105: wl-3.524352, gl-1.600017]--[lr: pb-0.000050, pf-0.000012]--[ETA-8:03:18]
2023.01.27-11:29:09:256:[step-145000/177600: 81.64%]--[loss-2.472061: wl-3.735801, gl-1.538111]--[lr: pb-0.000050, pf-0.000012]--[ETA-8:16:24]
2023.01.27-11:30:40:356:[step-145100/177600: 81.70%]--[loss-2.734339: wl-3.869889, gl-1.766867]--[lr: pb-0.000050, pf-0.000012]--[ETA-8:05:54]
2023.01.27-11:32:11:456:[step-145200/177600: 81.76%]--[loss-2.565507: wl-3.772538, gl-1.622373]--[lr: pb-0.000050, pf-0.000012]--[ETA-7:59:16]
2023.01.27-11:33:41:556:[step-145300/177600: 81.81%]--[loss-2.440673: wl-3.538512, gl-1.556045]--[lr: pb-0.000050, pf-0.000012]--[ETA-7:58:41]
2023.01.27-11:35:11:656:[step-145400/177600: 81.87%]--[loss-2.669761: wl-3.987634, gl-1.672852]--[lr: pb-0.000050, pf-0.000012]--[ETA-7:56:21]
2023.01.27-11:36:41:756:[step-145500/177600: 81.93%]--[loss-2.553307: wl-3.583315, gl-1.657478]--[lr: pb-0.000050, pf-0.000012]--[ETA-8:37:59]
2023.01.27-11:38:11:856:[step-145600/177600: 81.98%]--[loss-2.671520: wl-3.618288, gl-1.766948]--[lr: pb-0.000050, pf-0.000012]--[ETA-7:51:15]
End of epoch 164 / 200 	 Time Taken: 801 sec
2023.01.27-11:39:41:68:[step-145700/177600: 82.04%]--[loss-2.579889: wl-3.767550, gl-1.638001]--[lr: pb-0.000050, pf-0.000012]--[ETA-8:09:11]
2023.01.27-11:41:11:168:[step-145800/177600: 82.09%]--[loss-2.685518: wl-4.143735, gl-1.649584]--[lr: pb-0.000050, pf-0.000012]--[ETA-7:47:06]
2023.01.27-11:42:41:268:[step-145900/177600: 82.15%]--[loss-2.485421: wl-3.536022, gl-1.601415]--[lr: pb-0.000050, pf-0.000012]--[ETA-7:53:55]
2023.01.27-11:44:11:368:[step-146000/177600: 82.21%]--[loss-2.622850: wl-3.733925, gl-1.689369]--[lr: pb-0.000050, pf-0.000012]--[ETA-8:03:44]
2023.01.27-11:45:41:468:[step-146100/177600: 82.26%]--[loss-2.502695: wl-3.523832, gl-1.621737]--[lr: pb-0.000050, pf-0.000012]--[ETA-7:45:30]
2023.01.27-11:48:34:568:[step-146200/177600: 82.32%]--[loss-2.594152: wl-4.205916, gl-1.542673]--[lr: pb-0.000050, pf-0.000012]--[ETA-8:15:40]
2023.01.27-11:50:13:668:[step-146300/177600: 82.38%]--[loss-2.504835: wl-3.684593, gl-1.583687]--[lr: pb-0.000050, pf-0.000012]--[ETA-8:06:05]
2023.01.27-11:51:57:768:[step-146400/177600: 82.43%]--[loss-2.512084: wl-3.638716, gl-1.602406]--[lr: pb-0.000050, pf-0.000012]--[ETA-7:34:58]
2023.01.27-11:53:25:868:[step-146500/177600: 82.49%]--[loss-2.557797: wl-3.661677, gl-1.642378]--[lr: pb-0.000050, pf-0.000012]--[ETA-7:33:54]
End of epoch 165 / 200 	 Time Taken: 903 sec
saving the model at the end of epoch 165, iters 146520
2023.01.27-11:54:55:80:[step-146600/177600: 82.55%]--[loss-2.444012: wl-3.436770, gl-1.584819]--[lr: pb-0.000050, pf-0.000012]--[ETA-7:36:19]
2023.01.27-11:56:24:180:[step-146700/177600: 82.60%]--[loss-2.508214: wl-3.692511, gl-1.585086]--[lr: pb-0.000050, pf-0.000012]--[ETA-7:35:46]
2023.01.27-11:57:53:280:[step-146800/177600: 82.66%]--[loss-2.740174: wl-4.054812, gl-1.726471]--[lr: pb-0.000050, pf-0.000012]--[ETA-7:45:36]
2023.01.27-11:59:27:380:[step-146900/177600: 82.71%]--[loss-2.840025: wl-4.709426, gl-1.662668]--[lr: pb-0.000050, pf-0.000012]--[ETA-9:11:29]
2023.01.27-12:01:06:480:[step-147000/177600: 82.77%]--[loss-2.505824: wl-3.633309, gl-1.597497]--[lr: pb-0.000050, pf-0.000012]--[ETA-7:56:45]
2023.01.27-12:02:44:580:[step-147100/177600: 82.83%]--[loss-2.613478: wl-3.838286, gl-1.653906]--[lr: pb-0.000050, pf-0.000012]--[ETA-7:28:26]
2023.01.27-12:04:13:680:[step-147200/177600: 82.88%]--[loss-2.515222: wl-3.630034, gl-1.607713]--[lr: pb-0.000050, pf-0.000012]--[ETA-8:01:36]
2023.01.27-12:05:53:780:[step-147300/177600: 82.94%]--[loss-2.767484: wl-4.384099, gl-1.671460]--[lr: pb-0.000050, pf-0.000012]--[ETA-8:23:10]
2023.01.27-12:07:38:880:[step-147400/177600: 83.00%]--[loss-2.546405: wl-3.581440, gl-1.651045]--[lr: pb-0.000050, pf-0.000012]--[ETA-7:22:47]
End of epoch 166 / 200 	 Time Taken: 841 sec
2023.01.27-12:09:08:92:[step-147500/177600: 83.05%]--[loss-2.936846: wl-5.006354, gl-1.685258]--[lr: pb-0.000050, pf-0.000011]--[ETA-7:53:28]
2023.01.27-12:10:40:192:[step-147600/177600: 83.11%]--[loss-2.448503: wl-3.547604, gl-1.561602]--[lr: pb-0.000050, pf-0.000011]--[ETA-8:12:36]
2023.01.27-12:12:20:292:[step-147700/177600: 83.16%]--[loss-2.871651: wl-4.585546, gl-1.725264]--[lr: pb-0.000050, pf-0.000011]--[ETA-7:58:17]
2023.01.27-12:14:08:392:[step-147800/177600: 83.22%]--[loss-2.704156: wl-4.008752, gl-1.701968]--[lr: pb-0.000050, pf-0.000011]--[ETA-7:14:49]
2023.01.27-12:15:36:492:[step-147900/177600: 83.28%]--[loss-2.426416: wl-3.440797, gl-1.566217]--[lr: pb-0.000050, pf-0.000011]--[ETA-7:16:04]
2023.01.27-12:17:05:592:[step-148000/177600: 83.33%]--[loss-2.635270: wl-3.789572, gl-1.687877]--[lr: pb-0.000050, pf-0.000011]--[ETA-7:15:14]
2023.01.27-12:18:33:692:[step-148100/177600: 83.39%]--[loss-2.346769: wl-3.396739, gl-1.497585]--[lr: pb-0.000050, pf-0.000011]--[ETA-7:13:05]
2023.01.27-12:20:02:792:[step-148200/177600: 83.45%]--[loss-2.999987: wl-5.154977, gl-1.711243]--[lr: pb-0.000050, pf-0.000011]--[ETA-7:11:22]
End of epoch 167 / 200 	 Time Taken: 821 sec
2023.01.27-12:21:31:4:[step-148300/177600: 83.50%]--[loss-2.590435: wl-3.733039, gl-1.657176]--[lr: pb-0.000050, pf-0.000011]--[ETA-7:13:33]
2023.01.27-12:23:01:104:[step-148400/177600: 83.56%]--[loss-2.541391: wl-3.598261, gl-1.641825]--[lr: pb-0.000050, pf-0.000011]--[ETA-7:21:31]
2023.01.27-12:24:30:204:[step-148500/177600: 83.61%]--[loss-2.588045: wl-3.924414, gl-1.606941]--[lr: pb-0.000050, pf-0.000011]--[ETA-7:17:15]
2023.01.27-12:25:59:304:[step-148600/177600: 83.67%]--[loss-2.659468: wl-3.760017, gl-1.719464]--[lr: pb-0.000050, pf-0.000011]--[ETA-7:09:17]
2023.01.27-12:27:28:404:[step-148700/177600: 83.73%]--[loss-2.712586: wl-4.336413, gl-1.628482]--[lr: pb-0.000050, pf-0.000011]--[ETA-7:06:08]
2023.01.27-12:28:57:504:[step-148800/177600: 83.78%]--[loss-2.501101: wl-3.701077, gl-1.575832]--[lr: pb-0.000050, pf-0.000011]--[ETA-7:05:39]
2023.01.27-12:30:26:604:[step-148900/177600: 83.84%]--[loss-2.575448: wl-3.610618, gl-1.672793]--[lr: pb-0.000050, pf-0.000011]--[ETA-7:02:14]
2023.01.27-12:31:55:704:[step-149000/177600: 83.90%]--[loss-2.590639: wl-3.824026, gl-1.634633]--[lr: pb-0.000050, pf-0.000011]--[ETA-7:01:57]
2023.01.27-12:33:24:804:[step-149100/177600: 83.95%]--[loss-2.558570: wl-3.591649, gl-1.660658]--[lr: pb-0.000050, pf-0.000011]--[ETA-7:04:09]
End of epoch 168 / 200 	 Time Taken: 792 sec
2023.01.27-12:34:54:16:[step-149200/177600: 84.01%]--[loss-2.563926: wl-3.640038, gl-1.653916]--[lr: pb-0.000050, pf-0.000011]--[ETA-6:59:25]
2023.01.27-12:36:24:116:[step-149300/177600: 84.07%]--[loss-2.674995: wl-3.802591, gl-1.724347]--[lr: pb-0.000050, pf-0.000011]--[ETA-7:00:48]
2023.01.27-12:37:53:216:[step-149400/177600: 84.12%]--[loss-2.913609: wl-4.365324, gl-1.822278]--[lr: pb-0.000050, pf-0.000011]--[ETA-7:00:21]
2023.01.27-12:39:22:316:[step-149500/177600: 84.18%]--[loss-2.558803: wl-3.590541, gl-1.661168]--[lr: pb-0.000050, pf-0.000011]--[ETA-6:54:30]
2023.01.27-12:40:51:416:[step-149600/177600: 84.23%]--[loss-2.597004: wl-3.853829, gl-1.633547]--[lr: pb-0.000050, pf-0.000011]--[ETA-6:58:29]
2023.01.27-12:42:21:516:[step-149700/177600: 84.29%]--[loss-2.677605: wl-3.933225, gl-1.694298]--[lr: pb-0.000050, pf-0.000011]--[ETA-6:50:51]
2023.01.27-12:43:50:616:[step-149800/177600: 84.35%]--[loss-2.663751: wl-4.150939, gl-1.626016]--[lr: pb-0.000050, pf-0.000011]--[ETA-6:50:59]
2023.01.27-12:45:20:716:[step-149900/177600: 84.40%]--[loss-2.589411: wl-3.788746, gl-1.642225]--[lr: pb-0.000050, pf-0.000011]--[ETA-6:48:41]
2023.01.27-12:46:48:816:[step-150000/177600: 84.46%]--[loss-2.438815: wl-3.384583, gl-1.592669]--[lr: pb-0.000050, pf-0.000011]--[ETA-6:48:35]
End of epoch 169 / 200 	 Time Taken: 793 sec
2023.01.27-12:48:19:28:[step-150100/177600: 84.52%]--[loss-2.748371: wl-4.235339, gl-1.689536]--[lr: pb-0.000050, pf-0.000010]--[ETA-7:00:53]
2023.01.27-12:49:49:128:[step-150200/177600: 84.57%]--[loss-2.940341: wl-4.783173, gl-1.744548]--[lr: pb-0.000050, pf-0.000010]--[ETA-6:47:10]
2023.01.27-12:51:19:228:[step-150300/177600: 84.63%]--[loss-2.552709: wl-3.825743, gl-1.596274]--[lr: pb-0.000050, pf-0.000010]--[ETA-6:43:04]
2023.01.27-12:52:48:328:[step-150400/177600: 84.68%]--[loss-2.670780: wl-4.429289, gl-1.563458]--[lr: pb-0.000050, pf-0.000010]--[ETA-6:42:37]
2023.01.27-12:54:17:428:[step-150500/177600: 84.74%]--[loss-2.430984: wl-3.401421, gl-1.580629]--[lr: pb-0.000050, pf-0.000010]--[ETA-6:41:24]
2023.01.27-12:55:46:528:[step-150600/177600: 84.80%]--[loss-2.775324: wl-4.549058, gl-1.638060]--[lr: pb-0.000050, pf-0.000010]--[ETA-6:40:05]
2023.01.27-12:57:15:628:[step-150700/177600: 84.85%]--[loss-2.646301: wl-3.823054, gl-1.690537]--[lr: pb-0.000050, pf-0.000010]--[ETA-6:36:31]
2023.01.27-12:58:44:728:[step-150800/177600: 84.91%]--[loss-2.742811: wl-4.258386, gl-1.678215]--[lr: pb-0.000050, pf-0.000010]--[ETA-6:40:26]
2023.01.27-13:00:13:828:[step-150900/177600: 84.97%]--[loss-2.520018: wl-3.906804, gl-1.543317]--[lr: pb-0.000050, pf-0.000010]--[ETA-6:34:14]
End of epoch 170 / 200 	 Time Taken: 793 sec
saving the model at the end of epoch 170, iters 150960
2023.01.27-13:01:43:40:[step-151000/177600: 85.02%]--[loss-2.694863: wl-3.920546, gl-1.714726]--[lr: pb-0.000050, pf-0.000010]--[ETA-6:34:36]
2023.01.27-13:03:13:140:[step-151100/177600: 85.08%]--[loss-2.626976: wl-3.510856, gl-1.749262]--[lr: pb-0.000050, pf-0.000010]--[ETA-6:29:38]
2023.01.27-13:04:44:240:[step-151200/177600: 85.14%]--[loss-2.504724: wl-3.482256, gl-1.634160]--[lr: pb-0.000050, pf-0.000010]--[ETA-6:44:45]
2023.01.27-13:06:15:340:[step-151300/177600: 85.19%]--[loss-2.720408: wl-3.811593, gl-1.767510]--[lr: pb-0.000050, pf-0.000010]--[ETA-6:40:40]
2023.01.27-13:07:45:440:[step-151400/177600: 85.25%]--[loss-2.867620: wl-4.800560, gl-1.667480]--[lr: pb-0.000050, pf-0.000010]--[ETA-6:30:28]
2023.01.27-13:09:15:540:[step-151500/177600: 85.30%]--[loss-2.721138: wl-3.971392, gl-1.728290]--[lr: pb-0.000050, pf-0.000010]--[ETA-6:24:22]
2023.01.27-13:10:44:640:[step-151600/177600: 85.36%]--[loss-2.715683: wl-4.458157, gl-1.601144]--[lr: pb-0.000050, pf-0.000010]--[ETA-6:24:41]
2023.01.27-13:12:14:740:[step-151700/177600: 85.42%]--[loss-2.592986: wl-3.763569, gl-1.652093]--[lr: pb-0.000050, pf-0.000010]--[ETA-6:22:28]
2023.01.27-13:13:43:840:[step-151800/177600: 85.47%]--[loss-2.506764: wl-3.573045, gl-1.613503]--[lr: pb-0.000050, pf-0.000010]--[ETA-6:31:09]
End of epoch 171 / 200 	 Time Taken: 799 sec
2023.01.27-13:15:14:52:[step-151900/177600: 85.53%]--[loss-2.992820: wl-5.317164, gl-1.663529]--[lr: pb-0.000050, pf-0.000010]--[ETA-6:20:08]
2023.01.27-13:16:43:152:[step-152000/177600: 85.59%]--[loss-2.579531: wl-3.762226, gl-1.638975]--[lr: pb-0.000050, pf-0.000010]--[ETA-6:18:21]
2023.01.27-13:18:12:252:[step-152100/177600: 85.64%]--[loss-2.733889: wl-3.698663, gl-1.809223]--[lr: pb-0.000050, pf-0.000010]--[ETA-6:17:32]
2023.01.27-13:19:43:352:[step-152200/177600: 85.70%]--[loss-2.461749: wl-3.601968, gl-1.561257]--[lr: pb-0.000050, pf-0.000010]--[ETA-6:28:48]
2023.01.27-13:21:12:452:[step-152300/177600: 85.75%]--[loss-2.673810: wl-4.009009, gl-1.671558]--[lr: pb-0.000050, pf-0.000010]--[ETA-6:15:02]
2023.01.27-13:22:41:552:[step-152400/177600: 85.81%]--[loss-2.573617: wl-4.106729, gl-1.546935]--[lr: pb-0.000050, pf-0.000010]--[ETA-6:19:09]
2023.01.27-13:24:11:652:[step-152500/177600: 85.87%]--[loss-2.548075: wl-3.784626, gl-1.601919]--[lr: pb-0.000050, pf-0.000010]--[ETA-6:12:20]
2023.01.27-13:25:40:752:[step-152600/177600: 85.92%]--[loss-2.652137: wl-4.136593, gl-1.617989]--[lr: pb-0.000050, pf-0.000010]--[ETA-6:09:23]
2023.01.27-13:27:08:852:[step-152700/177600: 85.98%]--[loss-2.514596: wl-3.792016, gl-1.566592]--[lr: pb-0.000050, pf-0.000010]--[ETA-6:06:04]
End of epoch 172 / 200 	 Time Taken: 794 sec
2023.01.27-13:28:40:64:[step-152800/177600: 86.04%]--[loss-2.646761: wl-4.227106, gl-1.589985]--[lr: pb-0.000050, pf-0.000009]--[ETA-6:33:36]
2023.01.27-13:30:12:164:[step-152900/177600: 86.09%]--[loss-2.417072: wl-3.521175, gl-1.536778]--[lr: pb-0.000050, pf-0.000009]--[ETA-6:05:18]
2023.01.27-13:31:41:264:[step-153000/177600: 86.15%]--[loss-2.528005: wl-3.667289, gl-1.611182]--[lr: pb-0.000050, pf-0.000009]--[ETA-6:02:08]
2023.01.27-13:33:10:364:[step-153100/177600: 86.20%]--[loss-2.661912: wl-4.009158, gl-1.659623]--[lr: pb-0.000050, pf-0.000009]--[ETA-6:02:56]
2023.01.27-13:34:39:464:[step-153200/177600: 86.26%]--[loss-2.571374: wl-3.693198, gl-1.648074]--[lr: pb-0.000050, pf-0.000009]--[ETA-6:03:11]
2023.01.27-13:36:09:564:[step-153300/177600: 86.32%]--[loss-2.654708: wl-4.149045, gl-1.617446]--[lr: pb-0.000050, pf-0.000009]--[ETA-5:59:41]
2023.01.27-13:37:39:664:[step-153400/177600: 86.37%]--[loss-2.480583: wl-3.618870, gl-1.575865]--[lr: pb-0.000050, pf-0.000009]--[ETA-5:57:42]
2023.01.27-13:39:08:764:[step-153500/177600: 86.43%]--[loss-2.527225: wl-3.796156, gl-1.578186]--[lr: pb-0.000050, pf-0.000009]--[ETA-5:56:53]
2023.01.27-13:40:37:864:[step-153600/177600: 86.49%]--[loss-2.567728: wl-4.027000, gl-1.560978]--[lr: pb-0.000050, pf-0.000009]--[ETA-6:00:52]
End of epoch 173 / 200 	 Time Taken: 798 sec
2023.01.27-13:42:08:76:[step-153700/177600: 86.54%]--[loss-2.541988: wl-3.923022, gl-1.561233]--[lr: pb-0.000050, pf-0.000009]--[ETA-5:57:08]
2023.01.27-13:43:39:176:[step-153800/177600: 86.60%]--[loss-2.520024: wl-3.662542, gl-1.604388]--[lr: pb-0.000050, pf-0.000009]--[ETA-5:56:50]
2023.01.27-13:45:09:276:[step-153900/177600: 86.66%]--[loss-2.601357: wl-3.744853, gl-1.665144]--[lr: pb-0.000050, pf-0.000009]--[ETA-5:50:13]
2023.01.27-13:46:39:376:[step-154000/177600: 86.71%]--[loss-2.556733: wl-3.547157, gl-1.669944]--[lr: pb-0.000050, pf-0.000009]--[ETA-6:01:17]
2023.01.27-13:48:10:476:[step-154100/177600: 86.77%]--[loss-2.502933: wl-3.585867, gl-1.606466]--[lr: pb-0.000050, pf-0.000009]--[ETA-5:46:42]
2023.01.27-13:49:40:576:[step-154200/177600: 86.82%]--[loss-2.851103: wl-4.329648, gl-1.768691]--[lr: pb-0.000050, pf-0.000009]--[ETA-5:49:04]
2023.01.27-13:51:09:676:[step-154300/177600: 86.88%]--[loss-2.680223: wl-4.091866, gl-1.657256]--[lr: pb-0.000050, pf-0.000009]--[ETA-5:43:24]
2023.01.27-13:52:38:776:[step-154400/177600: 86.94%]--[loss-2.635045: wl-3.883213, gl-1.664241]--[lr: pb-0.000050, pf-0.000009]--[ETA-5:54:13]
2023.01.27-13:54:09:876:[step-154500/177600: 86.99%]--[loss-2.527326: wl-3.573660, gl-1.633911]--[lr: pb-0.000050, pf-0.000009]--[ETA-5:41:32]
End of epoch 174 / 200 	 Time Taken: 800 sec
2023.01.27-13:55:40:88:[step-154600/177600: 87.05%]--[loss-2.623702: wl-3.734943, gl-1.689966]--[lr: pb-0.000050, pf-0.000009]--[ETA-5:39:10]
2023.01.27-13:57:10:188:[step-154700/177600: 87.11%]--[loss-2.586533: wl-3.648381, gl-1.674438]--[lr: pb-0.000050, pf-0.000009]--[ETA-5:40:31]
2023.01.27-13:58:40:288:[step-154800/177600: 87.16%]--[loss-2.557636: wl-3.643047, gl-1.646874]--[lr: pb-0.000050, pf-0.000009]--[ETA-5:54:03]
2023.01.27-14:00:10:388:[step-154900/177600: 87.22%]--[loss-2.759277: wl-3.820185, gl-1.804230]--[lr: pb-0.000050, pf-0.000009]--[ETA-5:34:53]
2023.01.27-14:01:40:488:[step-155000/177600: 87.27%]--[loss-2.776482: wl-4.380145, gl-1.681446]--[lr: pb-0.000050, pf-0.000009]--[ETA-5:34:59]
2023.01.27-14:03:12:588:[step-155100/177600: 87.33%]--[loss-2.598808: wl-3.852486, gl-1.635687]--[lr: pb-0.000050, pf-0.000009]--[ETA-5:34:06]
2023.01.27-14:04:42:688:[step-155200/177600: 87.39%]--[loss-2.386631: wl-3.252154, gl-1.573592]--[lr: pb-0.000050, pf-0.000009]--[ETA-5:33:44]
2023.01.27-14:06:12:788:[step-155300/177600: 87.44%]--[loss-2.527408: wl-3.464392, gl-1.661310]--[lr: pb-0.000050, pf-0.000009]--[ETA-5:31:36]
2023.01.27-14:07:42:888:[step-155400/177600: 87.50%]--[loss-2.549738: wl-3.742544, gl-1.614102]--[lr: pb-0.000050, pf-0.000009]--[ETA-5:47:57]
End of epoch 175 / 200 	 Time Taken: 802 sec
saving the model at the end of epoch 175, iters 155400
2023.01.27-14:09:13:100:[step-155500/177600: 87.56%]--[loss-2.766061: wl-4.436726, gl-1.656880]--[lr: pb-0.000050, pf-0.000008]--[ETA-5:28:38]
2023.01.27-14:10:43:200:[step-155600/177600: 87.61%]--[loss-2.531172: wl-3.895061, gl-1.557407]--[lr: pb-0.000050, pf-0.000008]--[ETA-5:26:27]
2023.01.27-14:12:14:300:[step-155700/177600: 87.67%]--[loss-2.457119: wl-3.561227, gl-1.566812]--[lr: pb-0.000050, pf-0.000008]--[ETA-5:31:31]
2023.01.27-14:13:44:400:[step-155800/177600: 87.73%]--[loss-2.655354: wl-4.028686, gl-1.648183]--[lr: pb-0.000050, pf-0.000008]--[ETA-5:23:20]
2023.01.27-14:15:14:500:[step-155900/177600: 87.78%]--[loss-2.842154: wl-4.309319, gl-1.764825]--[lr: pb-0.000050, pf-0.000008]--[ETA-5:23:26]
2023.01.27-14:16:44:600:[step-156000/177600: 87.84%]--[loss-2.734041: wl-3.960529, gl-1.743909]--[lr: pb-0.000050, pf-0.000008]--[ETA-5:22:35]
2023.01.27-14:18:13:700:[step-156100/177600: 87.89%]--[loss-2.522243: wl-3.690185, gl-1.599697]--[lr: pb-0.000050, pf-0.000008]--[ETA-5:21:57]
2023.01.27-14:19:49:800:[step-156200/177600: 87.95%]--[loss-2.460110: wl-3.343633, gl-1.624202]--[lr: pb-0.000050, pf-0.000008]--[ETA-5:33:53]
End of epoch 176 / 200 	 Time Taken: 811 sec
2023.01.27-14:21:27:12:[step-156300/177600: 88.01%]--[loss-2.511722: wl-3.619498, gl-1.606848]--[lr: pb-0.000050, pf-0.000008]--[ETA-5:44:08]
2023.01.27-14:24:15:112:[step-156400/177600: 88.06%]--[loss-2.762707: wl-4.320954, gl-1.682468]--[lr: pb-0.000050, pf-0.000008]--[ETA-5:37:05]
2023.01.27-14:25:51:212:[step-156500/177600: 88.12%]--[loss-2.612310: wl-3.996973, gl-1.613067]--[lr: pb-0.000050, pf-0.000008]--[ETA-5:30:32]
2023.01.27-14:27:27:312:[step-156600/177600: 88.18%]--[loss-2.755289: wl-4.019274, gl-1.750470]--[lr: pb-0.000050, pf-0.000008]--[ETA-5:30:43]
2023.01.27-14:33:30:412:[step-156700/177600: 88.23%]--[loss-2.637386: wl-3.666797, gl-1.720687]--[lr: pb-0.000050, pf-0.000008]--[ETA-5:30:25]
2023.01.27-14:35:06:512:[step-156800/177600: 88.29%]--[loss-2.604064: wl-3.946437, gl-1.617454]--[lr: pb-0.000050, pf-0.000008]--[ETA-5:28:05]
2023.01.27-14:36:41:612:[step-156900/177600: 88.34%]--[loss-2.557920: wl-3.851249, gl-1.595108]--[lr: pb-0.000050, pf-0.000008]--[ETA-5:09:18]
2023.01.27-14:38:11:712:[step-157000/177600: 88.40%]--[loss-2.716629: wl-4.235795, gl-1.657680]--[lr: pb-0.000050, pf-0.000008]--[ETA-5:03:19]
2023.01.27-14:39:41:812:[step-157100/177600: 88.46%]--[loss-2.596160: wl-3.958892, gl-1.606437]--[lr: pb-0.000050, pf-0.000008]--[ETA-5:20:45]
End of epoch 177 / 200 	 Time Taken: 1180 sec
2023.01.27-14:41:20:24:[step-157200/177600: 88.51%]--[loss-2.459876: wl-3.312942, gl-1.631641]--[lr: pb-0.000050, pf-0.000008]--[ETA-5:17:14]
2023.01.27-14:42:55:124:[step-157300/177600: 88.57%]--[loss-2.439212: wl-3.542080, gl-1.553692]--[lr: pb-0.000050, pf-0.000008]--[ETA-5:06:14]
2023.01.27-14:44:27:224:[step-157400/177600: 88.63%]--[loss-2.599010: wl-3.824227, gl-1.642954]--[lr: pb-0.000050, pf-0.000008]--[ETA-5:17:36]
2023.01.27-14:46:03:324:[step-157500/177600: 88.68%]--[loss-2.517372: wl-3.761046, gl-1.577110]--[lr: pb-0.000050, pf-0.000008]--[ETA-5:18:23]
2023.01.27-14:47:39:424:[step-157600/177600: 88.74%]--[loss-2.553378: wl-3.794688, gl-1.604706]--[lr: pb-0.000050, pf-0.000008]--[ETA-5:11:05]
2023.01.27-14:49:10:524:[step-157700/177600: 88.80%]--[loss-2.447445: wl-3.389104, gl-1.600169]--[lr: pb-0.000050, pf-0.000008]--[ETA-5:25:57]
2023.01.27-14:50:46:624:[step-157800/177600: 88.85%]--[loss-2.529251: wl-3.589123, gl-1.631970]--[lr: pb-0.000050, pf-0.000008]--[ETA-5:15:54]
2023.01.27-14:52:23:724:[step-157900/177600: 88.91%]--[loss-2.550894: wl-3.527050, gl-1.669132]--[lr: pb-0.000050, pf-0.000008]--[ETA-5:19:49]
2023.01.27-14:53:53:824:[step-158000/177600: 88.96%]--[loss-2.700887: wl-4.257651, gl-1.636474]--[lr: pb-0.000050, pf-0.000008]--[ETA-4:59:16]
End of epoch 178 / 200 	 Time Taken: 836 sec
2023.01.27-14:55:26:36:[step-158100/177600: 89.02%]--[loss-2.643248: wl-3.810470, gl-1.690630]--[lr: pb-0.000050, pf-0.000007]--[ETA-5:23:03]
2023.01.27-14:57:03:136:[step-158200/177600: 89.08%]--[loss-2.531697: wl-3.587759, gl-1.634757]--[lr: pb-0.000050, pf-0.000007]--[ETA-5:10:48]
2023.01.27-14:58:39:236:[step-158300/177600: 89.13%]--[loss-2.636974: wl-4.142051, gl-1.601461]--[lr: pb-0.000050, pf-0.000007]--[ETA-4:46:39]
2023.01.27-15:00:12:336:[step-158400/177600: 89.19%]--[loss-2.512582: wl-3.767731, gl-1.570649]--[lr: pb-0.000050, pf-0.000007]--[ETA-5:04:29]
2023.01.27-15:01:49:436:[step-158500/177600: 89.25%]--[loss-2.525133: wl-3.704583, gl-1.598987]--[lr: pb-0.000050, pf-0.000007]--[ETA-5:04:17]
2023.01.27-15:03:24:536:[step-158600/177600: 89.30%]--[loss-2.639146: wl-4.294003, gl-1.565645]--[lr: pb-0.000050, pf-0.000007]--[ETA-4:40:16]
2023.01.27-15:05:00:636:[step-158700/177600: 89.36%]--[loss-2.500424: wl-3.503078, gl-1.624654]--[lr: pb-0.000050, pf-0.000007]--[ETA-5:12:36]
2023.01.27-15:06:38:736:[step-158800/177600: 89.41%]--[loss-2.997588: wl-4.642179, gl-1.837043]--[lr: pb-0.000050, pf-0.000007]--[ETA-5:03:20]
2023.01.27-15:08:11:836:[step-158900/177600: 89.47%]--[loss-2.717161: wl-4.381414, gl-1.621807]--[lr: pb-0.000050, pf-0.000007]--[ETA-4:59:15]
End of epoch 179 / 200 	 Time Taken: 849 sec
2023.01.27-15:09:49:48:[step-159000/177600: 89.53%]--[loss-2.582625: wl-3.963919, gl-1.591645]--[lr: pb-0.000050, pf-0.000007]--[ETA-4:56:52]
2023.01.27-15:11:26:148:[step-159100/177600: 89.58%]--[loss-2.617937: wl-3.748990, gl-1.680689]--[lr: pb-0.000050, pf-0.000007]--[ETA-4:51:14]
2023.01.27-15:13:01:248:[step-159200/177600: 89.64%]--[loss-2.404750: wl-3.495982, gl-1.530755]--[lr: pb-0.000050, pf-0.000007]--[ETA-4:49:55]
2023.01.27-15:14:39:348:[step-159300/177600: 89.70%]--[loss-2.504252: wl-3.655535, gl-1.590369]--[lr: pb-0.000050, pf-0.000007]--[ETA-5:09:19]
2023.01.27-15:16:11:448:[step-159400/177600: 89.75%]--[loss-2.423536: wl-3.634544, gl-1.514900]--[lr: pb-0.000050, pf-0.000007]--[ETA-4:29:40]
2023.01.27-15:17:41:548:[step-159500/177600: 89.81%]--[loss-2.508664: wl-3.570925, gl-1.615932]--[lr: pb-0.000050, pf-0.000007]--[ETA-4:28:05]
2023.01.27-15:19:12:648:[step-159600/177600: 89.86%]--[loss-2.795190: wl-4.650550, gl-1.632552]--[lr: pb-0.000050, pf-0.000007]--[ETA-4:42:18]
2023.01.27-15:20:43:748:[step-159700/177600: 89.92%]--[loss-2.694905: wl-3.734477, gl-1.761285]--[lr: pb-0.000050, pf-0.000007]--[ETA-4:31:58]
2023.01.27-15:22:12:848:[step-159800/177600: 89.98%]--[loss-2.957926: wl-4.848555, gl-1.745788]--[lr: pb-0.000050, pf-0.000007]--[ETA-4:23:40]
End of epoch 180 / 200 	 Time Taken: 827 sec
saving the model at the end of epoch 180, iters 159840
2023.01.27-15:23:50:60:[step-159900/177600: 90.03%]--[loss-2.616225: wl-3.727731, gl-1.684293]--[lr: pb-0.000050, pf-0.000007]--[ETA-4:34:43]
2023.01.27-15:25:27:160:[step-160000/177600: 90.09%]--[loss-2.631215: wl-3.911565, gl-1.653324]--[lr: pb-0.000050, pf-0.000007]--[ETA-4:48:39]
2023.01.27-15:27:03:260:[step-160100/177600: 90.15%]--[loss-2.505246: wl-3.728270, gl-1.573178]--[lr: pb-0.000050, pf-0.000007]--[ETA-4:20:46]
2023.01.27-15:28:42:360:[step-160200/177600: 90.20%]--[loss-2.733630: wl-3.958821, gl-1.743924]--[lr: pb-0.000050, pf-0.000007]--[ETA-4:46:35]
2023.01.27-15:30:18:460:[step-160300/177600: 90.26%]--[loss-2.604928: wl-3.612578, gl-1.701783]--[lr: pb-0.000050, pf-0.000007]--[ETA-4:33:41]
2023.01.27-15:31:58:560:[step-160400/177600: 90.32%]--[loss-2.557422: wl-3.780632, gl-1.612264]--[lr: pb-0.000050, pf-0.000007]--[ETA-4:13:38]
2023.01.27-15:33:33:660:[step-160500/177600: 90.37%]--[loss-2.707258: wl-3.808327, gl-1.755177]--[lr: pb-0.000050, pf-0.000007]--[ETA-4:20:10]
2023.01.27-15:35:04:760:[step-160600/177600: 90.43%]--[loss-2.519427: wl-3.602602, gl-1.618777]--[lr: pb-0.000050, pf-0.000007]--[ETA-4:26:00]
2023.01.27-15:36:43:860:[step-160700/177600: 90.48%]--[loss-2.527373: wl-3.640521, gl-1.617242]--[lr: pb-0.000050, pf-0.000007]--[ETA-4:21:29]
End of epoch 181 / 200 	 Time Taken: 858 sec
2023.01.27-15:38:14:72:[step-160800/177600: 90.54%]--[loss-2.586136: wl-3.693510, gl-1.662759]--[lr: pb-0.000050, pf-0.000006]--[ETA-4:07:47]
2023.01.27-15:39:48:172:[step-160900/177600: 90.60%]--[loss-2.518727: wl-3.555361, gl-1.629887]--[lr: pb-0.000050, pf-0.000006]--[ETA-4:19:23]
2023.01.27-15:41:22:272:[step-161000/177600: 90.65%]--[loss-2.754580: wl-4.123561, gl-1.723690]--[lr: pb-0.000050, pf-0.000006]--[ETA-4:13:49]
2023.01.27-15:42:53:372:[step-161100/177600: 90.71%]--[loss-2.732018: wl-4.601943, gl-1.581532]--[lr: pb-0.000050, pf-0.000006]--[ETA-4:08:40]
2023.01.27-15:44:28:472:[step-161200/177600: 90.77%]--[loss-2.541886: wl-3.656390, gl-1.627789]--[lr: pb-0.000050, pf-0.000006]--[ETA-4:01:57]
2023.01.27-15:46:07:572:[step-161300/177600: 90.82%]--[loss-2.652306: wl-3.916676, gl-1.673137]--[lr: pb-0.000050, pf-0.000006]--[ETA-3:59:45]
2023.01.27-15:47:41:672:[step-161400/177600: 90.88%]--[loss-2.567078: wl-3.770863, gl-1.624363]--[lr: pb-0.000050, pf-0.000006]--[ETA-4:12:18]
2023.01.27-15:49:25:772:[step-161500/177600: 90.93%]--[loss-2.646888: wl-3.864076, gl-1.680870]--[lr: pb-0.000050, pf-0.000006]--[ETA-4:30:46]
2023.01.27-15:55:33:872:[step-161600/177600: 90.99%]--[loss-2.535344: wl-3.803738, gl-1.584410]--[lr: pb-0.000050, pf-0.000006]--[ETA-3:52:43]
End of epoch 182 / 200 	 Time Taken: 1118 sec
2023.01.27-15:57:03:84:[step-161700/177600: 91.05%]--[loss-2.752857: wl-3.973194, gl-1.759558]--[lr: pb-0.000050, pf-0.000006]--[ETA-3:52:15]
2023.01.27-15:58:34:184:[step-161800/177600: 91.10%]--[loss-2.500797: wl-3.719564, gl-1.570906]--[lr: pb-0.000050, pf-0.000006]--[ETA-3:51:56]
2023.01.27-16:00:02:284:[step-161900/177600: 91.16%]--[loss-2.589116: wl-3.717946, gl-1.659630]--[lr: pb-0.000050, pf-0.000006]--[ETA-3:49:59]
2023.01.27-16:01:33:384:[step-162000/177600: 91.22%]--[loss-2.823716: wl-4.289836, gl-1.751256]--[lr: pb-0.000050, pf-0.000006]--[ETA-3:50:40]
2023.01.27-16:03:03:484:[step-162100/177600: 91.27%]--[loss-2.587499: wl-3.700750, gl-1.662312]--[lr: pb-0.000050, pf-0.000006]--[ETA-3:49:55]
2023.01.27-16:04:33:584:[step-162200/177600: 91.33%]--[loss-2.508029: wl-3.349712, gl-1.670601]--[lr: pb-0.000050, pf-0.000006]--[ETA-3:50:31]
2023.01.27-16:06:02:684:[step-162300/177600: 91.39%]--[loss-2.680251: wl-4.084988, gl-1.659004]--[lr: pb-0.000050, pf-0.000006]--[ETA-3:45:21]
2023.01.27-16:07:31:784:[step-162400/177600: 91.44%]--[loss-2.519463: wl-3.390479, gl-1.671843]--[lr: pb-0.000050, pf-0.000006]--[ETA-3:57:48]
2023.01.27-16:09:01:884:[step-162500/177600: 91.50%]--[loss-2.693397: wl-3.922211, gl-1.712844]--[lr: pb-0.000050, pf-0.000006]--[ETA-3:47:44]
End of epoch 183 / 200 	 Time Taken: 797 sec
2023.01.27-16:10:32:96:[step-162600/177600: 91.55%]--[loss-2.503222: wl-3.760378, gl-1.563127]--[lr: pb-0.000050, pf-0.000006]--[ETA-3:47:01]
2023.01.27-16:12:03:196:[step-162700/177600: 91.61%]--[loss-2.639654: wl-3.921046, gl-1.659393]--[lr: pb-0.000050, pf-0.000006]--[ETA-3:51:15]
2023.01.27-16:13:33:296:[step-162800/177600: 91.67%]--[loss-2.439375: wl-3.617905, gl-1.534899]--[lr: pb-0.000050, pf-0.000006]--[ETA-3:39:20]
2023.01.27-16:15:03:396:[step-162900/177600: 91.72%]--[loss-2.667193: wl-3.943379, gl-1.681349]--[lr: pb-0.000050, pf-0.000006]--[ETA-3:37:09]
2023.01.27-16:16:32:496:[step-163000/177600: 91.78%]--[loss-2.556234: wl-4.067278, gl-1.539415]--[lr: pb-0.000050, pf-0.000006]--[ETA-3:40:07]
2023.01.27-16:18:01:596:[step-163100/177600: 91.84%]--[loss-2.507509: wl-3.571183, gl-1.614713]--[lr: pb-0.000050, pf-0.000006]--[ETA-3:33:43]
2023.01.27-16:19:30:696:[step-163200/177600: 91.89%]--[loss-2.467966: wl-3.476279, gl-1.598896]--[lr: pb-0.000050, pf-0.000006]--[ETA-3:37:05]
2023.01.27-16:21:01:796:[step-163300/177600: 91.95%]--[loss-2.443341: wl-3.435818, gl-1.584387]--[lr: pb-0.000050, pf-0.000006]--[ETA-3:39:12]
End of epoch 184 / 200 	 Time Taken: 798 sec
2023.01.27-16:22:32:8:[step-163400/177600: 92.00%]--[loss-2.500306: wl-3.764566, gl-1.559165]--[lr: pb-0.000050, pf-0.000005]--[ETA-3:40:19]
2023.01.27-16:24:01:108:[step-163500/177600: 92.06%]--[loss-2.871665: wl-3.990176, gl-1.874121]--[lr: pb-0.000050, pf-0.000005]--[ETA-3:28:43]
2023.01.27-16:25:31:208:[step-163600/177600: 92.12%]--[loss-2.524054: wl-3.658095, gl-1.609530]--[lr: pb-0.000050, pf-0.000005]--[ETA-3:28:22]
2023.01.27-16:27:01:308:[step-163700/177600: 92.17%]--[loss-2.928092: wl-5.270248, gl-1.610530]--[lr: pb-0.000050, pf-0.000005]--[ETA-3:26:10]
2023.01.27-16:28:30:408:[step-163800/177600: 92.23%]--[loss-2.498678: wl-3.535668, gl-1.614761]--[lr: pb-0.000050, pf-0.000005]--[ETA-3:24:32]
2023.01.27-16:29:59:508:[step-163900/177600: 92.29%]--[loss-2.412009: wl-3.588495, gl-1.514886]--[lr: pb-0.000050, pf-0.000005]--[ETA-3:22:24]
2023.01.27-16:31:28:608:[step-164000/177600: 92.34%]--[loss-2.560608: wl-3.730590, gl-1.627960]--[lr: pb-0.000050, pf-0.000005]--[ETA-3:20:57]
2023.01.27-16:32:57:708:[step-164100/177600: 92.40%]--[loss-2.524856: wl-3.557861, gl-1.635391]--[lr: pb-0.000050, pf-0.000005]--[ETA-3:19:47]
2023.01.27-16:34:26:808:[step-164200/177600: 92.45%]--[loss-2.723175: wl-3.766019, gl-1.781671]--[lr: pb-0.000050, pf-0.000005]--[ETA-3:18:02]
End of epoch 185 / 200 	 Time Taken: 794 sec
saving the model at the end of epoch 185, iters 164280
2023.01.27-16:35:58:20:[step-164300/177600: 92.51%]--[loss-2.574327: wl-3.556893, gl-1.685104]--[lr: pb-0.000050, pf-0.000005]--[ETA-3:19:37]
2023.01.27-16:37:27:120:[step-164400/177600: 92.57%]--[loss-2.523525: wl-3.846041, gl-1.562015]--[lr: pb-0.000050, pf-0.000005]--[ETA-3:15:42]
2023.01.27-16:38:56:220:[step-164500/177600: 92.62%]--[loss-2.497797: wl-3.635092, gl-1.589025]--[lr: pb-0.000050, pf-0.000005]--[ETA-3:13:23]
2023.01.27-16:40:25:320:[step-164600/177600: 92.68%]--[loss-2.491690: wl-3.624627, gl-1.585534]--[lr: pb-0.000050, pf-0.000005]--[ETA-3:11:53]
2023.01.27-16:41:54:420:[step-164700/177600: 92.74%]--[loss-2.517693: wl-3.647805, gl-1.605742]--[lr: pb-0.000050, pf-0.000005]--[ETA-3:10:41]
2023.01.27-16:43:24:520:[step-164800/177600: 92.79%]--[loss-2.395987: wl-3.451390, gl-1.533140]--[lr: pb-0.000050, pf-0.000005]--[ETA-3:09:58]
2023.01.27-16:44:54:620:[step-164900/177600: 92.85%]--[loss-2.613952: wl-3.947052, gl-1.627189]--[lr: pb-0.000050, pf-0.000005]--[ETA-3:07:21]
2023.01.27-16:46:23:720:[step-165000/177600: 92.91%]--[loss-2.811671: wl-4.289631, gl-1.739264]--[lr: pb-0.000050, pf-0.000005]--[ETA-3:06:16]
2023.01.27-16:47:52:820:[step-165100/177600: 92.96%]--[loss-2.584722: wl-3.797760, gl-1.635282]--[lr: pb-0.000050, pf-0.000005]--[ETA-3:04:29]
End of epoch 186 / 200 	 Time Taken: 794 sec
2023.01.27-16:49:22:32:[step-165200/177600: 93.02%]--[loss-2.513547: wl-3.782675, gl-1.567878]--[lr: pb-0.000050, pf-0.000005]--[ETA-3:05:18]
2023.01.27-16:50:52:132:[step-165300/177600: 93.07%]--[loss-2.350324: wl-3.373434, gl-1.506966]--[lr: pb-0.000050, pf-0.000005]--[ETA-3:03:56]
2023.01.27-16:52:22:232:[step-165400/177600: 93.13%]--[loss-2.458740: wl-3.739010, gl-1.523988]--[lr: pb-0.000050, pf-0.000005]--[ETA-3:00:30]
2023.01.27-16:53:51:332:[step-165500/177600: 93.19%]--[loss-2.801462: wl-4.644506, gl-1.640336]--[lr: pb-0.000050, pf-0.000005]--[ETA-2:58:43]
2023.01.27-16:55:19:432:[step-165600/177600: 93.24%]--[loss-2.589018: wl-3.774416, gl-1.645414]--[lr: pb-0.000050, pf-0.000005]--[ETA-3:00:24]
2023.01.27-16:56:48:532:[step-165700/177600: 93.30%]--[loss-2.571068: wl-3.676332, gl-1.651985]--[lr: pb-0.000050, pf-0.000005]--[ETA-2:55:50]
2023.01.27-16:58:18:632:[step-165800/177600: 93.36%]--[loss-2.745476: wl-4.359303, gl-1.655651]--[lr: pb-0.000050, pf-0.000005]--[ETA-2:54:31]
2023.01.27-16:59:47:732:[step-165900/177600: 93.41%]--[loss-2.601862: wl-3.895128, gl-1.628080]--[lr: pb-0.000050, pf-0.000005]--[ETA-2:52:41]
2023.01.27-17:01:17:832:[step-166000/177600: 93.47%]--[loss-2.543855: wl-3.786257, gl-1.597291]--[lr: pb-0.000050, pf-0.000005]--[ETA-2:56:37]
End of epoch 187 / 200 	 Time Taken: 794 sec
2023.01.27-17:02:47:44:[step-166100/177600: 93.52%]--[loss-2.671609: wl-4.049001, gl-1.659359]--[lr: pb-0.000050, pf-0.000004]--[ETA-2:49:52]
2023.01.27-17:04:16:144:[step-166200/177600: 93.58%]--[loss-2.720374: wl-4.354090, gl-1.631851]--[lr: pb-0.000050, pf-0.000004]--[ETA-2:48:00]
2023.01.27-17:05:45:244:[step-166300/177600: 93.64%]--[loss-2.509480: wl-3.343058, gl-1.673715]--[lr: pb-0.000050, pf-0.000004]--[ETA-2:46:59]
2023.01.27-17:07:14:344:[step-166400/177600: 93.69%]--[loss-2.767791: wl-4.070961, gl-1.750051]--[lr: pb-0.000050, pf-0.000004]--[ETA-2:45:36]
2023.01.27-17:08:44:444:[step-166500/177600: 93.75%]--[loss-2.569375: wl-3.730774, gl-1.636681]--[lr: pb-0.000050, pf-0.000004]--[ETA-2:43:56]
2023.01.27-17:10:13:544:[step-166600/177600: 93.81%]--[loss-2.484138: wl-3.732734, gl-1.550954]--[lr: pb-0.000050, pf-0.000004]--[ETA-2:42:55]
2023.01.27-17:11:42:644:[step-166700/177600: 93.86%]--[loss-2.592816: wl-3.734969, gl-1.659074]--[lr: pb-0.000050, pf-0.000004]--[ETA-2:41:15]
2023.01.27-17:13:10:744:[step-166800/177600: 93.92%]--[loss-2.558114: wl-3.622634, gl-1.652456]--[lr: pb-0.000050, pf-0.000004]--[ETA-2:39:51]
2023.01.27-17:14:39:844:[step-166900/177600: 93.98%]--[loss-2.611861: wl-3.783771, gl-1.665918]--[lr: pb-0.000050, pf-0.000004]--[ETA-2:38:18]
End of epoch 188 / 200 	 Time Taken: 791 sec
2023.01.27-17:16:09:56:[step-167000/177600: 94.03%]--[loss-2.583586: wl-3.599997, gl-1.683586]--[lr: pb-0.000050, pf-0.000004]--[ETA-2:36:11]
2023.01.27-17:17:38:156:[step-167100/177600: 94.09%]--[loss-2.670816: wl-3.891061, gl-1.698051]--[lr: pb-0.000050, pf-0.000004]--[ETA-2:38:58]
2023.01.27-17:19:07:256:[step-167200/177600: 94.14%]--[loss-2.439889: wl-3.497144, gl-1.565603]--[lr: pb-0.000050, pf-0.000004]--[ETA-2:33:36]
2023.01.27-17:20:37:356:[step-167300/177600: 94.20%]--[loss-2.446224: wl-3.250692, gl-1.633551]--[lr: pb-0.000050, pf-0.000004]--[ETA-2:34:20]
2023.01.27-17:22:05:456:[step-167400/177600: 94.26%]--[loss-2.633478: wl-4.102693, gl-1.607805]--[lr: pb-0.000050, pf-0.000004]--[ETA-2:30:20]
2023.01.27-17:23:35:556:[step-167500/177600: 94.31%]--[loss-2.565790: wl-4.060516, gl-1.550661]--[lr: pb-0.000050, pf-0.000004]--[ETA-2:29:44]
2023.01.27-17:25:04:656:[step-167600/177600: 94.37%]--[loss-2.666086: wl-4.040601, gl-1.655936]--[lr: pb-0.000050, pf-0.000004]--[ETA-2:27:29]
2023.01.27-17:26:33:756:[step-167700/177600: 94.43%]--[loss-2.598418: wl-3.778232, gl-1.653860]--[lr: pb-0.000050, pf-0.000004]--[ETA-2:26:01]
2023.01.27-17:28:02:856:[step-167800/177600: 94.48%]--[loss-2.664615: wl-3.825273, gl-1.708297]--[lr: pb-0.000050, pf-0.000004]--[ETA-2:35:32]
End of epoch 189 / 200 	 Time Taken: 792 sec
2023.01.27-17:29:33:68:[step-167900/177600: 94.54%]--[loss-2.475165: wl-3.656225, gl-1.561109]--[lr: pb-0.000050, pf-0.000004]--[ETA-2:22:52]
2023.01.27-17:31:02:168:[step-168000/177600: 94.59%]--[loss-2.498443: wl-3.489398, gl-1.626094]--[lr: pb-0.000050, pf-0.000004]--[ETA-2:23:25]
2023.01.27-17:32:32:268:[step-168100/177600: 94.65%]--[loss-2.435184: wl-3.727049, gl-1.503422]--[lr: pb-0.000050, pf-0.000004]--[ETA-2:25:14]
2023.01.27-17:34:01:368:[step-168200/177600: 94.71%]--[loss-2.470493: wl-3.610667, gl-1.567826]--[lr: pb-0.000050, pf-0.000004]--[ETA-2:19:40]
2023.01.27-17:35:31:468:[step-168300/177600: 94.76%]--[loss-2.726239: wl-4.121590, gl-1.695842]--[lr: pb-0.000050, pf-0.000004]--[ETA-2:18:26]
2023.01.27-17:37:00:568:[step-168400/177600: 94.82%]--[loss-2.479517: wl-3.747058, gl-1.542753]--[lr: pb-0.000050, pf-0.000004]--[ETA-2:17:36]
2023.01.27-17:38:29:668:[step-168500/177600: 94.88%]--[loss-2.661119: wl-3.796029, gl-1.712112]--[lr: pb-0.000050, pf-0.000004]--[ETA-2:14:30]
2023.01.27-17:40:00:768:[step-168600/177600: 94.93%]--[loss-2.751462: wl-4.211001, gl-1.698711]--[lr: pb-0.000050, pf-0.000004]--[ETA-2:12:23]
2023.01.27-17:41:28:868:[step-168700/177600: 94.99%]--[loss-2.648270: wl-3.798607, gl-1.698618]--[lr: pb-0.000050, pf-0.000004]--[ETA-2:12:08]
End of epoch 190 / 200 	 Time Taken: 795 sec
saving the model at the end of epoch 190, iters 168720
2023.01.27-17:43:00:80:[step-168800/177600: 95.05%]--[loss-2.866306: wl-4.420024, gl-1.761300]--[lr: pb-0.000050, pf-0.000003]--[ETA-2:19:15]
2023.01.27-17:44:29:180:[step-168900/177600: 95.10%]--[loss-2.512551: wl-3.912771, gl-1.534359]--[lr: pb-0.000050, pf-0.000003]--[ETA-2:08:14]
2023.01.27-17:45:59:280:[step-169000/177600: 95.16%]--[loss-2.563549: wl-3.791781, gl-1.615603]--[lr: pb-0.000050, pf-0.000003]--[ETA-2:08:22]
2023.01.27-17:47:30:380:[step-169100/177600: 95.21%]--[loss-2.459659: wl-3.670773, gl-1.541966]--[lr: pb-0.000050, pf-0.000003]--[ETA-2:04:47]
2023.01.27-17:48:58:480:[step-169200/177600: 95.27%]--[loss-2.542811: wl-3.721359, gl-1.612471]--[lr: pb-0.000050, pf-0.000003]--[ETA-2:04:20]
2023.01.27-17:50:28:580:[step-169300/177600: 95.33%]--[loss-2.611851: wl-4.164764, gl-1.570660]--[lr: pb-0.000050, pf-0.000003]--[ETA-2:02:44]
2023.01.27-17:51:58:680:[step-169400/177600: 95.38%]--[loss-2.536502: wl-3.756049, gl-1.597490]--[lr: pb-0.000050, pf-0.000003]--[ETA-2:02:11]
2023.01.27-17:53:28:780:[step-169500/177600: 95.44%]--[loss-2.711294: wl-4.026576, gl-1.704650]--[lr: pb-0.000050, pf-0.000003]--[ETA-1:59:59]
2023.01.27-17:54:57:880:[step-169600/177600: 95.50%]--[loss-2.743791: wl-4.119775, gl-1.713847]--[lr: pb-0.000050, pf-0.000003]--[ETA-2:01:49]
End of epoch 191 / 200 	 Time Taken: 797 sec
2023.01.27-17:56:31:92:[step-169700/177600: 95.55%]--[loss-2.525465: wl-3.555763, gl-1.636524]--[lr: pb-0.000050, pf-0.000003]--[ETA-1:56:22]
2023.01.27-17:58:02:192:[step-169800/177600: 95.61%]--[loss-2.507936: wl-3.703159, gl-1.582146]--[lr: pb-0.000050, pf-0.000003]--[ETA-2:04:03]
2023.01.27-17:59:33:292:[step-169900/177600: 95.66%]--[loss-2.613220: wl-3.749468, gl-1.675853]--[lr: pb-0.000050, pf-0.000003]--[ETA-1:54:35]
2023.01.27-18:01:04:392:[step-170000/177600: 95.72%]--[loss-2.589520: wl-3.836207, gl-1.630468]--[lr: pb-0.000050, pf-0.000003]--[ETA-1:53:44]
2023.01.27-18:02:34:492:[step-170100/177600: 95.78%]--[loss-2.645234: wl-4.115933, gl-1.616250]--[lr: pb-0.000050, pf-0.000003]--[ETA-1:56:54]
2023.01.27-18:04:06:592:[step-170200/177600: 95.83%]--[loss-2.609641: wl-3.921927, gl-1.629159]--[lr: pb-0.000050, pf-0.000003]--[ETA-1:57:40]
2023.01.27-18:05:37:692:[step-170300/177600: 95.89%]--[loss-2.438874: wl-3.528282, gl-1.556803]--[lr: pb-0.000050, pf-0.000003]--[ETA-1:48:00]
2023.01.27-18:07:06:792:[step-170400/177600: 95.95%]--[loss-2.477627: wl-3.745750, gl-1.541190]--[lr: pb-0.000050, pf-0.000003]--[ETA-1:46:39]
End of epoch 192 / 200 	 Time Taken: 807 sec
2023.01.27-18:08:37:4:[step-170500/177600: 96.00%]--[loss-2.675449: wl-3.720496, gl-1.745325]--[lr: pb-0.000050, pf-0.000003]--[ETA-1:46:19]
2023.01.27-18:10:07:104:[step-170600/177600: 96.06%]--[loss-2.604324: wl-3.757881, gl-1.664854]--[lr: pb-0.000050, pf-0.000003]--[ETA-1:43:28]
2023.01.27-18:11:36:204:[step-170700/177600: 96.11%]--[loss-2.628078: wl-3.864170, gl-1.662035]--[lr: pb-0.000050, pf-0.000003]--[ETA-1:41:59]
2023.01.27-18:13:07:304:[step-170800/177600: 96.17%]--[loss-2.388859: wl-3.510583, gl-1.511213]--[lr: pb-0.000050, pf-0.000003]--[ETA-1:48:19]
2023.01.27-18:14:37:404:[step-170900/177600: 96.23%]--[loss-2.506242: wl-3.834622, gl-1.547586]--[lr: pb-0.000050, pf-0.000003]--[ETA-1:46:20]
2023.01.27-18:16:09:504:[step-171000/177600: 96.28%]--[loss-2.455231: wl-3.768100, gl-1.513206]--[lr: pb-0.000050, pf-0.000003]--[ETA-1:40:08]
2023.01.27-18:17:39:604:[step-171100/177600: 96.34%]--[loss-2.505714: wl-3.797468, gl-1.556347]--[lr: pb-0.000050, pf-0.000003]--[ETA-1:36:06]
2023.01.27-18:19:08:704:[step-171200/177600: 96.40%]--[loss-2.568830: wl-3.824737, gl-1.612646]--[lr: pb-0.000050, pf-0.000003]--[ETA-1:34:51]
2023.01.27-18:20:42:804:[step-171300/177600: 96.45%]--[loss-2.586666: wl-3.822913, gl-1.630938]--[lr: pb-0.000050, pf-0.000003]--[ETA-1:41:44]
End of epoch 193 / 200 	 Time Taken: 810 sec
2023.01.27-18:22:20:16:[step-171400/177600: 96.51%]--[loss-2.558362: wl-4.011266, gl-1.555545]--[lr: pb-0.000050, pf-0.000002]--[ETA-1:38:39]
2023.01.27-18:23:51:116:[step-171500/177600: 96.57%]--[loss-2.454011: wl-3.602538, gl-1.553376]--[lr: pb-0.000050, pf-0.000002]--[ETA-1:31:10]
2023.01.27-18:25:28:216:[step-171600/177600: 96.62%]--[loss-2.722075: wl-3.951188, gl-1.734279]--[lr: pb-0.000050, pf-0.000002]--[ETA-1:34:54]
2023.01.27-18:27:05:316:[step-171700/177600: 96.68%]--[loss-2.685813: wl-3.978117, gl-1.691283]--[lr: pb-0.000050, pf-0.000002]--[ETA-1:32:50]
2023.01.27-18:28:35:416:[step-171800/177600: 96.73%]--[loss-2.353292: wl-3.480993, gl-1.483044]--[lr: pb-0.000050, pf-0.000002]--[ETA-1:30:12]
2023.01.27-18:30:06:516:[step-171900/177600: 96.79%]--[loss-2.582475: wl-4.015533, gl-1.578592]--[lr: pb-0.000050, pf-0.000002]--[ETA-1:24:15]
2023.01.27-18:31:35:616:[step-172000/177600: 96.85%]--[loss-2.735930: wl-4.131238, gl-1.703120]--[lr: pb-0.000050, pf-0.000002]--[ETA-1:22:56]
2023.01.27-18:33:06:716:[step-172100/177600: 96.90%]--[loss-2.415090: wl-3.588074, gl-1.518071]--[lr: pb-0.000050, pf-0.000002]--[ETA-1:29:17]
2023.01.27-18:34:44:816:[step-172200/177600: 96.96%]--[loss-2.349507: wl-3.472304, gl-1.481431]--[lr: pb-0.000050, pf-0.000002]--[ETA-1:36:10]
End of epoch 194 / 200 	 Time Taken: 830 sec
2023.01.27-18:36:20:28:[step-172300/177600: 97.02%]--[loss-3.036665: wl-5.543643, gl-1.650754]--[lr: pb-0.000050, pf-0.000002]--[ETA-1:20:27]
2023.01.27-18:37:49:128:[step-172400/177600: 97.07%]--[loss-2.420600: wl-3.509937, gl-1.543116]--[lr: pb-0.000050, pf-0.000002]--[ETA-1:16:42]
2023.01.27-18:39:20:228:[step-172500/177600: 97.13%]--[loss-2.661110: wl-4.089795, gl-1.638662]--[lr: pb-0.000050, pf-0.000002]--[ETA-1:15:09]
2023.01.27-18:40:53:328:[step-172600/177600: 97.18%]--[loss-2.396593: wl-3.479264, gl-1.526777]--[lr: pb-0.000050, pf-0.000002]--[ETA-1:18:54]
2023.01.27-18:42:30:428:[step-172700/177600: 97.24%]--[loss-2.633912: wl-3.984546, gl-1.637776]--[lr: pb-0.000050, pf-0.000002]--[ETA-1:18:45]
2023.01.27-18:44:03:528:[step-172800/177600: 97.30%]--[loss-2.604366: wl-3.823551, gl-1.648478]--[lr: pb-0.000050, pf-0.000002]--[ETA-1:10:57]
2023.01.27-18:45:39:628:[step-172900/177600: 97.35%]--[loss-2.552797: wl-3.729870, gl-1.620330]--[lr: pb-0.000050, pf-0.000002]--[ETA-1:14:38]
2023.01.27-18:47:18:728:[step-173000/177600: 97.41%]--[loss-2.932178: wl-4.900594, gl-1.707030]--[lr: pb-0.000050, pf-0.000002]--[ETA-1:20:07]
2023.01.27-18:48:51:828:[step-173100/177600: 97.47%]--[loss-2.478267: wl-3.630332, gl-1.570684]--[lr: pb-0.000050, pf-0.000002]--[ETA-1:12:41]
End of epoch 195 / 200 	 Time Taken: 835 sec
saving the model at the end of epoch 195, iters 173160
2023.01.27-18:50:27:40:[step-173200/177600: 97.52%]--[loss-2.309630: wl-3.126240, gl-1.528070]--[lr: pb-0.000050, pf-0.000002]--[ETA-1:06:42]
2023.01.27-18:51:56:140:[step-173300/177600: 97.58%]--[loss-2.606005: wl-3.686581, gl-1.684360]--[lr: pb-0.000050, pf-0.000002]--[ETA-1:03:34]
2023.01.27-18:53:25:240:[step-173400/177600: 97.64%]--[loss-2.523632: wl-3.785467, gl-1.577265]--[lr: pb-0.000050, pf-0.000002]--[ETA-1:03:31]
2023.01.27-18:54:55:340:[step-173500/177600: 97.69%]--[loss-2.416163: wl-3.423294, gl-1.560340]--[lr: pb-0.000050, pf-0.000002]--[ETA-1:01:36]
2023.01.27-18:56:26:440:[step-173600/177600: 97.75%]--[loss-2.842584: wl-3.986831, gl-1.845876]--[lr: pb-0.000050, pf-0.000002]--[ETA-1:01:39]
2023.01.27-18:57:56:540:[step-173700/177600: 97.80%]--[loss-2.797132: wl-4.147522, gl-1.760252]--[lr: pb-0.000050, pf-0.000002]--[ETA-0:57:47]
2023.01.27-18:59:26:640:[step-173800/177600: 97.86%]--[loss-2.484102: wl-3.509200, gl-1.606802]--[lr: pb-0.000050, pf-0.000002]--[ETA-0:56:08]
2023.01.27-19:00:56:740:[step-173900/177600: 97.92%]--[loss-2.547548: wl-3.808980, gl-1.595303]--[lr: pb-0.000050, pf-0.000002]--[ETA-0:54:44]
2023.01.27-19:02:27:840:[step-174000/177600: 97.97%]--[loss-2.648261: wl-3.952386, gl-1.660164]--[lr: pb-0.000050, pf-0.000002]--[ETA-0:55:58]
End of epoch 196 / 200 	 Time Taken: 799 sec
2023.01.27-19:03:57:52:[step-174100/177600: 98.03%]--[loss-2.452732: wl-3.244097, gl-1.641708]--[lr: pb-0.000050, pf-0.000001]--[ETA-0:51:39]
2023.01.27-19:05:28:152:[step-174200/177600: 98.09%]--[loss-2.614316: wl-3.973642, gl-1.620906]--[lr: pb-0.000050, pf-0.000001]--[ETA-0:52:13]
2023.01.27-19:06:57:252:[step-174300/177600: 98.14%]--[loss-2.473766: wl-3.762894, gl-1.533042]--[lr: pb-0.000050, pf-0.000001]--[ETA-0:50:38]
2023.01.27-19:08:28:352:[step-174400/177600: 98.20%]--[loss-2.544530: wl-3.791674, gl-1.596612]--[lr: pb-0.000050, pf-0.000001]--[ETA-0:47:09]
2023.01.27-19:10:00:452:[step-174500/177600: 98.25%]--[loss-2.534928: wl-3.689319, gl-1.612598]--[lr: pb-0.000050, pf-0.000001]--[ETA-0:47:33]
2023.01.27-19:11:29:552:[step-174600/177600: 98.31%]--[loss-2.536543: wl-3.991271, gl-1.538725]--[lr: pb-0.000050, pf-0.000001]--[ETA-0:44:15]
2023.01.27-19:12:59:652:[step-174700/177600: 98.37%]--[loss-2.728446: wl-4.436951, gl-1.619209]--[lr: pb-0.000050, pf-0.000001]--[ETA-0:43:02]
2023.01.27-19:14:29:752:[step-174800/177600: 98.42%]--[loss-2.442024: wl-3.582121, gl-1.546494]--[lr: pb-0.000050, pf-0.000001]--[ETA-0:42:23]
2023.01.27-19:15:59:852:[step-174900/177600: 98.48%]--[loss-2.445756: wl-3.660852, gl-1.530543]--[lr: pb-0.000050, pf-0.000001]--[ETA-0:40:34]
End of epoch 197 / 200 	 Time Taken: 801 sec
2023.01.27-19:17:30:64:[step-175000/177600: 98.54%]--[loss-2.365144: wl-3.542631, gl-1.479486]--[lr: pb-0.000050, pf-0.000001]--[ETA-0:40:01]
2023.01.27-19:19:01:164:[step-175100/177600: 98.59%]--[loss-2.550704: wl-3.688736, gl-1.628520]--[lr: pb-0.000050, pf-0.000001]--[ETA-0:37:01]
2023.01.27-19:20:30:264:[step-175200/177600: 98.65%]--[loss-2.759480: wl-4.198503, gl-1.709854]--[lr: pb-0.000050, pf-0.000001]--[ETA-0:35:36]
2023.01.27-19:22:00:364:[step-175300/177600: 98.70%]--[loss-2.411248: wl-3.465910, gl-1.544770]--[lr: pb-0.000050, pf-0.000001]--[ETA-0:35:29]
2023.01.27-19:23:30:464:[step-175400/177600: 98.76%]--[loss-2.505222: wl-4.016768, gl-1.501031]--[lr: pb-0.000050, pf-0.000001]--[ETA-0:32:48]
2023.01.27-19:25:00:564:[step-175500/177600: 98.82%]--[loss-2.646796: wl-3.979032, gl-1.652038]--[lr: pb-0.000050, pf-0.000001]--[ETA-0:31:06]
2023.01.27-19:26:29:664:[step-175600/177600: 98.87%]--[loss-2.484776: wl-3.662161, gl-1.569235]--[lr: pb-0.000050, pf-0.000001]--[ETA-0:29:33]
2023.01.27-19:28:00:764:[step-175700/177600: 98.93%]--[loss-2.586790: wl-3.875162, gl-1.618000]--[lr: pb-0.000050, pf-0.000001]--[ETA-0:28:06]
2023.01.27-19:29:31:864:[step-175800/177600: 98.99%]--[loss-2.514222: wl-3.897735, gl-1.539788]--[lr: pb-0.000050, pf-0.000001]--[ETA-0:26:41]
End of epoch 198 / 200 	 Time Taken: 800 sec
2023.01.27-19:31:03:76:[step-175900/177600: 99.04%]--[loss-2.682364: wl-4.479098, gl-1.562589]--[lr: pb-0.000050, pf-0.000001]--[ETA-0:25:05]
2023.01.27-19:32:35:176:[step-176000/177600: 99.10%]--[loss-2.595606: wl-3.808506, gl-1.643479]--[lr: pb-0.000050, pf-0.000001]--[ETA-0:26:35]
2023.01.27-19:34:07:276:[step-176100/177600: 99.16%]--[loss-2.603748: wl-3.731913, gl-1.670769]--[lr: pb-0.000050, pf-0.000001]--[ETA-0:22:54]
2023.01.27-19:35:38:376:[step-176200/177600: 99.21%]--[loss-2.714247: wl-3.886165, gl-1.742705]--[lr: pb-0.000050, pf-0.000001]--[ETA-0:21:15]
2023.01.27-19:37:09:476:[step-176300/177600: 99.27%]--[loss-2.907778: wl-4.540006, gl-1.772776]--[lr: pb-0.000050, pf-0.000001]--[ETA-0:19:13]
2023.01.27-19:38:39:576:[step-176400/177600: 99.32%]--[loss-2.501796: wl-3.533348, gl-1.618459]--[lr: pb-0.000050, pf-0.000001]--[ETA-0:17:44]
2023.01.27-19:40:09:676:[step-176500/177600: 99.38%]--[loss-2.389665: wl-3.442940, gl-1.528930]--[lr: pb-0.000050, pf-0.000001]--[ETA-0:16:17]
2023.01.27-19:41:39:776:[step-176600/177600: 99.44%]--[loss-2.765582: wl-4.113425, gl-1.737226]--[lr: pb-0.000050, pf-0.000001]--[ETA-0:14:59]
2023.01.27-19:43:09:876:[step-176700/177600: 99.49%]--[loss-2.675632: wl-4.349136, gl-1.588348]--[lr: pb-0.000050, pf-0.000001]--[ETA-0:13:19]
End of epoch 199 / 200 	 Time Taken: 807 sec
2023.01.27-19:44:41:88:[step-176800/177600: 99.55%]--[loss-2.613258: wl-3.776396, gl-1.669159]--[lr: pb-0.000050, pf-0.000000]--[ETA-0:11:49]
2023.01.27-19:46:12:188:[step-176900/177600: 99.61%]--[loss-2.484146: wl-3.516524, gl-1.605015]--[lr: pb-0.000050, pf-0.000000]--[ETA-0:10:32]
2023.01.27-19:47:43:288:[step-177000/177600: 99.66%]--[loss-2.581476: wl-3.838491, gl-1.621854]--[lr: pb-0.000050, pf-0.000000]--[ETA-0:08:51]
2023.01.27-19:49:13:388:[step-177100/177600: 99.72%]--[loss-2.353018: wl-3.240210, gl-1.542965]--[lr: pb-0.000050, pf-0.000000]--[ETA-0:08:12]
2023.01.27-19:50:44:488:[step-177200/177600: 99.77%]--[loss-2.382464: wl-3.363591, gl-1.541567]--[lr: pb-0.000050, pf-0.000000]--[ETA-0:05:53]
2023.01.27-19:52:14:588:[step-177300/177600: 99.83%]--[loss-2.582658: wl-3.741627, gl-1.647251]--[lr: pb-0.000050, pf-0.000000]--[ETA-0:04:26]
2023.01.27-19:53:44:688:[step-177400/177600: 99.89%]--[loss-2.453422: wl-3.845808, gl-1.491970]--[lr: pb-0.000050, pf-0.000000]--[ETA-0:02:57]
2023.01.27-19:55:14:788:[step-177500/177600: 99.94%]--[loss-2.658654: wl-3.843505, gl-1.697777]--[lr: pb-0.000050, pf-0.000000]--[ETA-0:01:28]
2023.01.27-19:56:44:888:[step-177600/177600: 100.00%]--[loss-2.604110: wl-4.093530, gl-1.580728]--[lr: pb-0.000050, pf-0.000000]--[ETA-0:13:05]
End of epoch 200 / 200 	 Time Taken: 803 sec
saving the model at the end of epoch 200, iters 177600
/opt/conda/lib/python3.7/site-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
