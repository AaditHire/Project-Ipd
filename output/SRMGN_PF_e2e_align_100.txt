/opt/conda/envs/srmgn/lib/python3.10/site-packages/torch/distributed/launch.py:180: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
------------ Options -------------
PBAFN_gen_checkpoint: checkpoints/SRMGN_PB_e2e_align_200/PBAFN_gen_epoch_201.pth
PBAFN_warp_checkpoint: checkpoints/SRMGN_PB_e2e_align_200/PBAFN_warp_epoch_201.pth
PFAFN_gen_checkpoint: None
PFAFN_warp_checkpoint: checkpoints/SRMGN_PF_stage1_align_100/PFAFN_warp_epoch_101.pth
align_corners: True
batchSize: 16
beta1: 0.5
checkpoints_dir: checkpoints
continue_train: False
data_type: 32
dataroot: ../dataset/Flow-Style-VTON/VITON_traindata
debug: False
display_freq: 100
display_winsize: 512
fineSize: 512
gpu_ids: [2]
input_nc: 3
isTrain: True
label_nc: 14
lambda_feat: 10.0
launcher: pytorch
loadSize: 512
load_pretrain: 
local_rank: 0
lr: 5e-05
max_dataset_size: inf
nThreads: 1
n_blocks_global: 4
n_blocks_local: 3
n_downsample_global: 4
n_layers_D: 3
n_local_enhancers: 1
name: SRMGN_PF_e2e_align_100
ndf: 64
netG: global
ngf: 64
niter: 50
niter_decay: 50
niter_fix_global: 0
no_flip: False
no_ganFeat_loss: False
no_html: False
no_lsgan: False
no_vgg_loss: False
norm: instance
num_D: 2
num_gpus: 1
output_nc: 3
phase: train
pool_size: 0
print_freq: 100
resize_or_crop: None
save_epoch_freq: 1
save_latest_freq: 1000
serial_batches: False
tf_log: False
tv_weight: 0.1
use_dropout: False
valroot: dataset/VITON_valdata/
verbose: False
which_epoch: latest
-------------- End ----------------
------------ Options -------------
PBAFN_gen_checkpoint: checkpoints/SRMGN_PB_e2e_align_200/PBAFN_gen_epoch_201.pth
PBAFN_warp_checkpoint: checkpoints/SRMGN_PB_e2e_align_200/PBAFN_warp_epoch_201.pth
PFAFN_gen_checkpoint: None
PFAFN_warp_checkpoint: checkpoints/SRMGN_PF_stage1_align_100/PFAFN_warp_epoch_101.pth
align_corners: True
batchSize: 16
beta1: 0.5
checkpoints_dir: checkpoints
continue_train: False
data_type: 32
dataroot: ../dataset/Flow-Style-VTON/VITON_traindata
debug: False
display_freq: 100
display_winsize: 512
fineSize: 512
gpu_ids: [2]
input_nc: 3
isTrain: True
label_nc: 14
lambda_feat: 10.0
launcher: pytorch
loadSize: 512
load_pretrain: 
local_rank: 0
lr: 5e-05
max_dataset_size: inf
nThreads: 1
n_blocks_global: 4
n_blocks_local: 3
n_downsample_global: 4
n_layers_D: 3
n_local_enhancers: 1
name: SRMGN_PF_e2e_align_100
ndf: 64
netG: global
ngf: 64
niter: 50
niter_decay: 50
niter_fix_global: 0
no_flip: False
no_ganFeat_loss: False
no_html: False
no_lsgan: False
no_vgg_loss: False
norm: instance
num_D: 2
num_gpus: 1
output_nc: 3
phase: train
pool_size: 0
print_freq: 100
resize_or_crop: None
save_epoch_freq: 1
save_latest_freq: 1000
serial_batches: False
tf_log: False
tv_weight: 0.1
use_dropout: False
valroot: dataset/VITON_valdata/
verbose: False
which_epoch: latest
-------------- End ----------------
------------ Options -------------
PBAFN_gen_checkpoint: checkpoints/SRMGN_PB_e2e_align_200/PBAFN_gen_epoch_201.pth
PBAFN_warp_checkpoint: checkpoints/SRMGN_PB_e2e_align_200/PBAFN_warp_epoch_201.pth
PFAFN_gen_checkpoint: None
PFAFN_warp_checkpoint: checkpoints/SRMGN_PF_stage1_align_100/PFAFN_warp_epoch_101.pth
align_corners: True
batchSize: 16
beta1: 0.5
checkpoints_dir: checkpoints
continue_train: False
data_type: 32
dataroot: ../dataset/Flow-Style-VTON/VITON_traindata
debug: False
display_freq: 100
display_winsize: 512
fineSize: 512
gpu_ids: [2]
input_nc: 3
isTrain: True
label_nc: 14
lambda_feat: 10.0
launcher: pytorch
loadSize: 512
load_pretrain: 
local_rank: 0
lr: 5e-05
max_dataset_size: inf
nThreads: 1
n_blocks_global: 4
n_blocks_local: 3
n_downsample_global: 4
n_layers_D: 3
n_local_enhancers: 1
name: SRMGN_PF_e2e_align_100
ndf: 64
netG: global
ngf: 64
niter: 50
niter_decay: 50
niter_fix_global: 0
no_flip: False
no_ganFeat_loss: False
no_html: False
no_lsgan: False
no_vgg_loss: False
norm: instance
num_D: 2
num_gpus: 1
output_nc: 3
phase: train
pool_size: 0
print_freq: 100
resize_or_crop: None
save_epoch_freq: 1
save_latest_freq: 1000
serial_batches: False
tf_log: False
tv_weight: 0.1
use_dropout: False
valroot: dataset/VITON_valdata/
verbose: False
which_epoch: latest
-------------- End ----------------
dataset [AlignedDataset] was created
../dataset/Flow-Style-VTON/VITON_traindata/train_label label
../dataset/Flow-Style-VTON/VITON_traindata/train_img img
../dataset/Flow-Style-VTON/VITON_traindata/train_edge edge
../dataset/Flow-Style-VTON/VITON_traindata/train_color color
/opt/conda/envs/srmgn/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.
  warnings.warn(warning.format(ret))
/opt/conda/envs/srmgn/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
2023.03.02-22:11:55:100:[step-100/88900: 0.11%]--[loss-5.470829: wl-4.267785, gl-4.403883]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:40:43]
2023.03.02-22:13:12:200:[step-200/88900: 0.22%]--[loss-4.337880: wl-3.931729, gl-3.354948]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:56:04]
2023.03.02-22:14:35:300:[step-300/88900: 0.34%]--[loss-4.621028: wl-4.715994, gl-3.442030]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:53:32]
2023.03.02-22:16:00:400:[step-400/88900: 0.45%]--[loss-4.700307: wl-4.316947, gl-3.621070]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:06:39]
2023.03.02-22:17:18:500:[step-500/88900: 0.56%]--[loss-4.804361: wl-4.075015, gl-3.785608]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:00:32]
2023.03.02-22:18:43:600:[step-600/88900: 0.67%]--[loss-5.035510: wl-5.594899, gl-3.636785]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:04:54]
2023.03.02-22:20:07:700:[step-700/88900: 0.79%]--[loss-4.271031: wl-4.257132, gl-3.206748]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:12:20]
2023.03.02-22:21:25:800:[step-800/88900: 0.90%]--[loss-4.565640: wl-4.495678, gl-3.441720]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:17:08]
End of epoch 1 / 100: train_loss: 4.578 	 time: 733 sec
Saving the model at the end of epoch 1, iters 889
2023.03.02-22:22:56:11:[step-900/88900: 1.01%]--[loss-3.826175: wl-4.066405, gl-2.809574]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:04:13]
2023.03.02-22:24:21:111:[step-1000/88900: 1.12%]--[loss-4.322471: wl-4.350343, gl-3.234885]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:54:12]
2023.03.02-22:25:46:211:[step-1100/88900: 1.24%]--[loss-3.682765: wl-4.025389, gl-2.676418]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:09:04]
2023.03.02-22:27:04:311:[step-1200/88900: 1.35%]--[loss-3.969024: wl-4.320086, gl-2.889002]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:09:21]
2023.03.02-22:28:29:411:[step-1300/88900: 1.46%]--[loss-3.983183: wl-4.331356, gl-2.900344]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:11:41]
2023.03.02-22:29:54:511:[step-1400/88900: 1.57%]--[loss-4.348150: wl-4.538232, gl-3.213592]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:48:57]
2023.03.02-22:31:13:611:[step-1500/88900: 1.69%]--[loss-3.597777: wl-3.976462, gl-2.603662]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:49:56]
2023.03.02-22:32:35:711:[step-1600/88900: 1.80%]--[loss-4.002753: wl-4.355684, gl-2.913832]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:15:42]
2023.03.02-22:33:51:811:[step-1700/88900: 1.91%]--[loss-3.846589: wl-4.556809, gl-2.707386]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:28:00]
End of epoch 2 / 100: train_loss: 3.955 	 time: 730 sec
Saving the model at the end of epoch 2, iters 1778
2023.03.02-22:35:15:22:[step-1800/88900: 2.02%]--[loss-3.861243: wl-4.089416, gl-2.838889]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:19:52]
2023.03.02-22:36:31:122:[step-1900/88900: 2.14%]--[loss-4.301883: wl-4.627549, gl-3.144996]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:12:34]
2023.03.02-22:37:47:222:[step-2000/88900: 2.25%]--[loss-4.177331: wl-4.493257, gl-3.054017]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:14:59]
2023.03.02-22:39:03:322:[step-2100/88900: 2.36%]--[loss-4.153620: wl-5.180654, gl-2.858457]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:13:21]
2023.03.02-22:40:19:422:[step-2200/88900: 2.47%]--[loss-3.860836: wl-4.421269, gl-2.755519]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:19:58]
2023.03.02-22:41:35:522:[step-2300/88900: 2.59%]--[loss-3.796882: wl-4.017358, gl-2.792543]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:17:24]
2023.03.02-22:42:52:622:[step-2400/88900: 2.70%]--[loss-3.474192: wl-3.884512, gl-2.503064]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:14:18]
2023.03.02-22:44:08:722:[step-2500/88900: 2.81%]--[loss-4.197320: wl-5.812202, gl-2.744269]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:20:00]
2023.03.02-22:45:24:822:[step-2600/88900: 2.92%]--[loss-3.813807: wl-4.059033, gl-2.799049]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:21:59]
End of epoch 3 / 100: train_loss: 3.809 	 time: 683 sec
Saving the model at the end of epoch 3, iters 2667
2023.03.02-22:46:48:33:[step-2700/88900: 3.04%]--[loss-3.941978: wl-4.487246, gl-2.820167]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:06:58]
2023.03.02-22:48:04:133:[step-2800/88900: 3.15%]--[loss-3.400538: wl-3.841126, gl-2.440256]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:02:21]
2023.03.02-22:49:20:233:[step-2900/88900: 3.26%]--[loss-3.928101: wl-4.059498, gl-2.913226]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:08:42]
2023.03.02-22:50:36:333:[step-3000/88900: 3.37%]--[loss-3.754551: wl-3.981201, gl-2.759251]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:08:25]
2023.03.02-22:51:53:433:[step-3100/88900: 3.49%]--[loss-3.849804: wl-4.289094, gl-2.777531]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:07:07]
2023.03.02-22:53:09:533:[step-3200/88900: 3.60%]--[loss-3.477626: wl-3.708445, gl-2.550514]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:01:25]
2023.03.02-22:54:25:633:[step-3300/88900: 3.71%]--[loss-3.701453: wl-4.102032, gl-2.675945]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:01:41]
2023.03.02-22:55:41:733:[step-3400/88900: 3.82%]--[loss-3.611702: wl-4.064801, gl-2.595502]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:03:42]
2023.03.02-22:56:57:833:[step-3500/88900: 3.94%]--[loss-3.815222: wl-4.182423, gl-2.769616]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:55:21]
End of epoch 4 / 100: train_loss: 3.718 	 time: 683 sec
Saving the model at the end of epoch 4, iters 3556
2023.03.02-22:58:20:44:[step-3600/88900: 4.05%]--[loss-3.408068: wl-3.760959, gl-2.467828]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:55:13]
2023.03.02-22:59:36:144:[step-3700/88900: 4.16%]--[loss-3.847896: wl-4.498970, gl-2.723154]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:10:39]
2023.03.02-23:01:03:244:[step-3800/88900: 4.27%]--[loss-3.500978: wl-3.916790, gl-2.521780]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:36:28]
2023.03.02-23:02:31:344:[step-3900/88900: 4.39%]--[loss-3.707879: wl-4.020595, gl-2.702730]--[lr: pb-0.000050, pf-0.000050]--[ETA-23:09:07]
2023.03.02-23:03:58:444:[step-4000/88900: 4.50%]--[loss-3.544371: wl-4.347261, gl-2.457556]--[lr: pb-0.000050, pf-0.000050]--[ETA-20:28:45]
2023.03.02-23:05:26:544:[step-4100/88900: 4.61%]--[loss-3.863595: wl-4.356277, gl-2.774526]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:04:31]
2023.03.02-23:06:54:644:[step-4200/88900: 4.72%]--[loss-3.658435: wl-4.524864, gl-2.527219]--[lr: pb-0.000050, pf-0.000050]--[ETA-20:08:38]
2023.03.02-23:08:22:744:[step-4300/88900: 4.84%]--[loss-3.801663: wl-4.921208, gl-2.571361]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:21:39]
2023.03.02-23:09:50:844:[step-4400/88900: 4.95%]--[loss-3.493556: wl-3.828935, gl-2.536322]--[lr: pb-0.000050, pf-0.000050]--[ETA-20:51:04]
End of epoch 5 / 100: train_loss: 3.649 	 time: 767 sec
Saving the model at the end of epoch 5, iters 4445
2023.03.02-23:11:23:55:[step-4500/88900: 5.06%]--[loss-3.403603: wl-3.998908, gl-2.403876]--[lr: pb-0.000050, pf-0.000050]--[ETA-21:27:39]
2023.03.02-23:12:52:155:[step-4600/88900: 5.17%]--[loss-3.507735: wl-4.169025, gl-2.465479]--[lr: pb-0.000050, pf-0.000050]--[ETA-21:41:34]
2023.03.02-23:14:18:255:[step-4700/88900: 5.29%]--[loss-3.500915: wl-4.127678, gl-2.468996]--[lr: pb-0.000050, pf-0.000050]--[ETA-21:15:51]
2023.03.02-23:15:55:355:[step-4800/88900: 5.40%]--[loss-3.225504: wl-3.655494, gl-2.311630]--[lr: pb-0.000050, pf-0.000050]--[ETA-22:11:27]
2023.03.02-23:17:23:455:[step-4900/88900: 5.51%]--[loss-3.835702: wl-4.884255, gl-2.614639]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:58:27]
2023.03.02-23:19:02:555:[step-5000/88900: 5.62%]--[loss-3.644483: wl-4.224839, gl-2.588273]--[lr: pb-0.000050, pf-0.000050]--[ETA-21:00:48]
2023.03.02-23:20:28:655:[step-5100/88900: 5.74%]--[loss-3.508693: wl-4.126567, gl-2.477052]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:21:07]
2023.03.02-23:21:56:755:[step-5200/88900: 5.85%]--[loss-3.459688: wl-3.744923, gl-2.523458]--[lr: pb-0.000050, pf-0.000050]--[ETA-22:33:16]
2023.03.02-23:23:25:855:[step-5300/88900: 5.96%]--[loss-3.580541: wl-4.173782, gl-2.537096]--[lr: pb-0.000050, pf-0.000050]--[ETA-20:06:22]
End of epoch 6 / 100: train_loss: 3.613 	 time: 805 sec
Saving the model at the end of epoch 6, iters 5334
2023.03.02-23:24:59:66:[step-5400/88900: 6.07%]--[loss-3.512150: wl-4.072993, gl-2.493901]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:35:15]
2023.03.02-23:26:28:166:[step-5500/88900: 6.19%]--[loss-3.973220: wl-5.006107, gl-2.721693]--[lr: pb-0.000050, pf-0.000050]--[ETA-22:21:41]
2023.03.02-23:27:58:266:[step-5600/88900: 6.30%]--[loss-3.525436: wl-4.355602, gl-2.436535]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:20:49]
2023.03.02-23:29:27:366:[step-5700/88900: 6.41%]--[loss-3.886778: wl-4.716508, gl-2.707651]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:29:01]
2023.03.02-23:30:55:466:[step-5800/88900: 6.52%]--[loss-3.325223: wl-3.808796, gl-2.373024]--[lr: pb-0.000050, pf-0.000050]--[ETA-23:02:58]
2023.03.02-23:32:24:566:[step-5900/88900: 6.64%]--[loss-3.313628: wl-3.820187, gl-2.358582]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:57:12]
2023.03.02-23:33:52:666:[step-6000/88900: 6.75%]--[loss-3.423689: wl-3.941078, gl-2.438419]--[lr: pb-0.000050, pf-0.000050]--[ETA-21:12:22]
2023.03.02-23:35:22:766:[step-6100/88900: 6.86%]--[loss-3.513533: wl-4.257707, gl-2.449106]--[lr: pb-0.000050, pf-0.000050]--[ETA-21:02:31]
2023.03.02-23:36:50:866:[step-6200/88900: 6.97%]--[loss-3.629794: wl-4.120592, gl-2.599646]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:55:42]
End of epoch 7 / 100: train_loss: 3.571 	 time: 795 sec
Saving the model at the end of epoch 7, iters 6223
2023.03.02-23:38:24:77:[step-6300/88900: 7.09%]--[loss-3.294794: wl-4.044315, gl-2.283715]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:20:19]
2023.03.02-23:39:54:177:[step-6400/88900: 7.20%]--[loss-3.444351: wl-4.065640, gl-2.427941]--[lr: pb-0.000050, pf-0.000050]--[ETA-21:27:58]
2023.03.02-23:41:22:277:[step-6500/88900: 7.31%]--[loss-3.409795: wl-4.115119, gl-2.381015]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:01:16]
2023.03.02-23:42:50:377:[step-6600/88900: 7.42%]--[loss-3.879372: wl-5.260861, gl-2.564156]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:35:57]
2023.03.02-23:44:18:477:[step-6700/88900: 7.54%]--[loss-3.451966: wl-4.368084, gl-2.359945]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:31:39]
2023.03.02-23:45:46:577:[step-6800/88900: 7.65%]--[loss-3.583970: wl-4.255763, gl-2.520029]--[lr: pb-0.000050, pf-0.000050]--[ETA-22:19:31]
2023.03.02-23:47:13:677:[step-6900/88900: 7.76%]--[loss-3.637866: wl-4.499236, gl-2.513057]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:10:42]
2023.03.02-23:48:41:777:[step-7000/88900: 7.87%]--[loss-3.608482: wl-4.214535, gl-2.554848]--[lr: pb-0.000050, pf-0.000050]--[ETA-21:18:05]
2023.03.02-23:50:06:877:[step-7100/88900: 7.99%]--[loss-3.624489: wl-4.577868, gl-2.480022]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:57:14]
End of epoch 8 / 100: train_loss: 3.536 	 time: 786 sec
Saving the model at the end of epoch 8, iters 7112
2023.03.02-23:51:43:88:[step-7200/88900: 8.10%]--[loss-3.685011: wl-5.005520, gl-2.433631]--[lr: pb-0.000050, pf-0.000050]--[ETA-21:18:07]
2023.03.02-23:53:12:188:[step-7300/88900: 8.21%]--[loss-3.352410: wl-3.803746, gl-2.401474]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:22:15]
2023.03.02-23:54:40:288:[step-7400/88900: 8.32%]--[loss-3.514977: wl-4.426315, gl-2.408399]--[lr: pb-0.000050, pf-0.000050]--[ETA-20:36:24]
2023.03.02-23:56:08:388:[step-7500/88900: 8.44%]--[loss-3.585951: wl-4.391422, gl-2.488096]--[lr: pb-0.000050, pf-0.000050]--[ETA-20:33:45]
2023.03.02-23:57:36:488:[step-7600/88900: 8.55%]--[loss-3.559833: wl-4.282576, gl-2.489189]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:39:17]
2023.03.02-23:59:03:588:[step-7700/88900: 8.66%]--[loss-3.547058: wl-4.310745, gl-2.469372]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:19:11]
2023.03.03-00:00:30:688:[step-7800/88900: 8.77%]--[loss-3.425196: wl-4.524022, gl-2.294190]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:56:58]
2023.03.03-00:01:58:788:[step-7900/88900: 8.89%]--[loss-3.745477: wl-4.668941, gl-2.578241]--[lr: pb-0.000050, pf-0.000050]--[ETA-21:28:38]
2023.03.03-00:03:23:888:[step-8000/88900: 9.00%]--[loss-3.535892: wl-4.372000, gl-2.442893]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:46:41]
End of epoch 9 / 100: train_loss: 3.508 	 time: 787 sec
Saving the model at the end of epoch 9, iters 8001
2023.03.03-00:05:00:99:[step-8100/88900: 9.11%]--[loss-3.442622: wl-4.303605, gl-2.366721]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:47:41]
2023.03.03-00:06:28:199:[step-8200/88900: 9.22%]--[loss-3.587381: wl-4.380470, gl-2.492263]--[lr: pb-0.000050, pf-0.000050]--[ETA-21:36:12]
2023.03.03-00:07:55:299:[step-8300/88900: 9.34%]--[loss-3.439005: wl-4.321628, gl-2.358598]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:08:12]
2023.03.03-00:09:23:399:[step-8400/88900: 9.45%]--[loss-4.160999: wl-5.025347, gl-2.904662]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:57:59]
2023.03.03-00:10:50:499:[step-8500/88900: 9.56%]--[loss-3.594027: wl-4.931055, gl-2.361263]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:51:12]
2023.03.03-00:12:18:599:[step-8600/88900: 9.67%]--[loss-3.676058: wl-4.942626, gl-2.440402]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:16:50]
2023.03.03-00:13:45:699:[step-8700/88900: 9.79%]--[loss-3.430061: wl-4.214251, gl-2.376498]--[lr: pb-0.000050, pf-0.000050]--[ETA-20:47:53]
2023.03.03-00:15:14:799:[step-8800/88900: 9.90%]--[loss-3.546679: wl-4.523206, gl-2.415878]--[lr: pb-0.000050, pf-0.000050]--[ETA-21:23:11]
End of epoch 10 / 100: train_loss: 3.484 	 time: 786 sec
Saving the model at the end of epoch 10, iters 8890
2023.03.03-00:16:48:10:[step-8900/88900: 10.01%]--[loss-3.584361: wl-4.516121, gl-2.455330]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:14:05]
2023.03.03-00:18:16:110:[step-9000/88900: 10.12%]--[loss-3.738418: wl-4.388483, gl-2.641298]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:04:46]
2023.03.03-00:19:44:210:[step-9100/88900: 10.24%]--[loss-3.357979: wl-3.882907, gl-2.387252]--[lr: pb-0.000050, pf-0.000050]--[ETA-21:50:20]
2023.03.03-00:21:12:310:[step-9200/88900: 10.35%]--[loss-3.460948: wl-4.235182, gl-2.402152]--[lr: pb-0.000050, pf-0.000050]--[ETA-20:30:22]
2023.03.03-00:22:39:410:[step-9300/88900: 10.46%]--[loss-3.650969: wl-4.600903, gl-2.500743]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:27:49]
2023.03.03-00:24:07:510:[step-9400/88900: 10.57%]--[loss-3.689825: wl-4.643837, gl-2.528866]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:23:06]
2023.03.03-00:25:42:610:[step-9500/88900: 10.69%]--[loss-3.576356: wl-4.702420, gl-2.400751]--[lr: pb-0.000050, pf-0.000050]--[ETA-20:29:26]
2023.03.03-00:27:14:710:[step-9600/88900: 10.80%]--[loss-3.376817: wl-4.206576, gl-2.325173]--[lr: pb-0.000050, pf-0.000050]--[ETA-22:45:01]
2023.03.03-00:28:47:810:[step-9700/88900: 10.91%]--[loss-3.605632: wl-4.507874, gl-2.478664]--[lr: pb-0.000050, pf-0.000050]--[ETA-22:23:48]
End of epoch 11 / 100: train_loss: 3.449 	 time: 807 sec
Saving the model at the end of epoch 11, iters 9779
2023.03.03-00:30:28:21:[step-9800/88900: 11.02%]--[loss-3.430653: wl-4.540762, gl-2.295462]--[lr: pb-0.000050, pf-0.000050]--[ETA-20:02:12]
2023.03.03-00:32:02:121:[step-9900/88900: 11.14%]--[loss-3.844117: wl-4.909013, gl-2.616864]--[lr: pb-0.000050, pf-0.000050]--[ETA-21:17:32]
2023.03.03-00:33:36:221:[step-10000/88900: 11.25%]--[loss-3.339113: wl-3.929096, gl-2.356839]--[lr: pb-0.000050, pf-0.000050]--[ETA-22:05:33]
2023.03.03-00:35:06:321:[step-10100/88900: 11.36%]--[loss-3.388104: wl-4.343257, gl-2.302290]--[lr: pb-0.000050, pf-0.000050]--[ETA-22:16:20]
2023.03.03-00:36:32:421:[step-10200/88900: 11.47%]--[loss-3.451563: wl-4.391499, gl-2.353688]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:27:48]
2023.03.03-00:37:49:521:[step-10300/88900: 11.59%]--[loss-3.392931: wl-3.980967, gl-2.397689]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:32:42]
2023.03.03-00:39:06:621:[step-10400/88900: 11.70%]--[loss-3.421565: wl-4.010472, gl-2.418947]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:35:56]
2023.03.03-00:40:34:721:[step-10500/88900: 11.81%]--[loss-3.778695: wl-4.874324, gl-2.560114]--[lr: pb-0.000050, pf-0.000050]--[ETA-20:05:54]
2023.03.03-00:42:00:821:[step-10600/88900: 11.92%]--[loss-3.703131: wl-4.718197, gl-2.523582]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:03:58]
End of epoch 12 / 100: train_loss: 3.423 	 time: 777 sec
Saving the model at the end of epoch 12, iters 10668
2023.03.03-00:43:33:32:[step-10700/88900: 12.04%]--[loss-3.259283: wl-4.058815, gl-2.244579]--[lr: pb-0.000050, pf-0.000050]--[ETA-20:15:52]
2023.03.03-00:45:01:132:[step-10800/88900: 12.15%]--[loss-3.532795: wl-4.205946, gl-2.481308]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:33:41]
2023.03.03-00:46:28:232:[step-10900/88900: 12.26%]--[loss-3.595119: wl-4.343879, gl-2.509149]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:47:18]
2023.03.03-00:47:53:332:[step-11000/88900: 12.37%]--[loss-3.503540: wl-4.318287, gl-2.423968]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:46:18]
2023.03.03-00:49:19:432:[step-11100/88900: 12.49%]--[loss-3.515607: wl-4.333309, gl-2.432280]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:22:11]
2023.03.03-00:50:44:532:[step-11200/88900: 12.60%]--[loss-3.282098: wl-4.000121, gl-2.282067]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:06:28]
2023.03.03-00:52:11:632:[step-11300/88900: 12.71%]--[loss-3.182090: wl-3.780098, gl-2.237066]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:20:30]
2023.03.03-00:53:37:732:[step-11400/88900: 12.82%]--[loss-3.124949: wl-3.917285, gl-2.145628]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:19:17]
2023.03.03-00:55:03:832:[step-11500/88900: 12.94%]--[loss-3.292915: wl-4.063271, gl-2.277098]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:10:04]
End of epoch 13 / 100: train_loss: 3.408 	 time: 773 sec
Saving the model at the end of epoch 13, iters 11557
2023.03.03-00:56:37:43:[step-11600/88900: 13.05%]--[loss-3.397290: wl-4.530431, gl-2.264682]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:21:55]
2023.03.03-00:58:03:143:[step-11700/88900: 13.16%]--[loss-3.357815: wl-4.198240, gl-2.308255]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:55:10]
2023.03.03-00:59:29:243:[step-11800/88900: 13.27%]--[loss-3.556952: wl-4.707315, gl-2.380123]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:12:21]
2023.03.03-01:00:56:343:[step-11900/88900: 13.39%]--[loss-3.109139: wl-3.642994, gl-2.198391]--[lr: pb-0.000050, pf-0.000050]--[ETA-20:10:28]
2023.03.03-01:02:22:443:[step-12000/88900: 13.50%]--[loss-3.409593: wl-4.219051, gl-2.354830]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:35:57]
2023.03.03-01:03:47:543:[step-12100/88900: 13.61%]--[loss-3.383169: wl-4.187262, gl-2.336354]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:57:31]
2023.03.03-01:05:14:643:[step-12200/88900: 13.72%]--[loss-3.136657: wl-3.813571, gl-2.183264]--[lr: pb-0.000050, pf-0.000050]--[ETA-20:14:56]
2023.03.03-01:06:40:743:[step-12300/88900: 13.84%]--[loss-3.294233: wl-4.189201, gl-2.246933]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:09:52]
2023.03.03-01:08:06:843:[step-12400/88900: 13.95%]--[loss-3.695375: wl-5.281267, gl-2.375058]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:00:47]
End of epoch 14 / 100: train_loss: 3.387 	 time: 772 sec
Saving the model at the end of epoch 14, iters 12446
2023.03.03-01:09:41:54:[step-12500/88900: 14.06%]--[loss-3.027030: wl-3.768579, gl-2.084886]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:20:59]
2023.03.03-01:11:09:154:[step-12600/88900: 14.17%]--[loss-3.265447: wl-4.033433, gl-2.257089]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:45:20]
2023.03.03-01:12:36:254:[step-12700/88900: 14.29%]--[loss-3.310026: wl-4.295686, gl-2.236105]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:09:03]
2023.03.03-01:14:02:354:[step-12800/88900: 14.40%]--[loss-3.227001: wl-3.791348, gl-2.279164]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:28:14]
2023.03.03-01:15:27:454:[step-12900/88900: 14.51%]--[loss-3.485087: wl-4.379562, gl-2.390196]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:04:38]
2023.03.03-01:16:53:554:[step-13000/88900: 14.62%]--[loss-3.503713: wl-4.645330, gl-2.342380]--[lr: pb-0.000050, pf-0.000050]--[ETA-20:50:46]
2023.03.03-01:18:20:654:[step-13100/88900: 14.74%]--[loss-3.666511: wl-4.619729, gl-2.511579]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:11:30]
2023.03.03-01:19:48:754:[step-13200/88900: 14.85%]--[loss-3.420137: wl-4.531516, gl-2.287258]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:55:57]
2023.03.03-01:21:12:854:[step-13300/88900: 14.96%]--[loss-3.237413: wl-4.258389, gl-2.172816]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:14:25]
End of epoch 15 / 100: train_loss: 3.364 	 time: 776 sec
Saving the model at the end of epoch 15, iters 13335
2023.03.03-01:22:47:65:[step-13400/88900: 15.07%]--[loss-3.143972: wl-3.876199, gl-2.174922]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:06:05]
2023.03.03-01:24:14:165:[step-13500/88900: 15.19%]--[loss-3.287582: wl-4.320375, gl-2.207489]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:22:58]
2023.03.03-01:25:41:265:[step-13600/88900: 15.30%]--[loss-3.317137: wl-3.909241, gl-2.339827]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:33:32]
2023.03.03-01:27:08:365:[step-13700/88900: 15.41%]--[loss-3.274769: wl-3.999181, gl-2.274974]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:50:03]
2023.03.03-01:28:33:465:[step-13800/88900: 15.52%]--[loss-3.154602: wl-4.028730, gl-2.147419]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:28:58]
2023.03.03-01:30:01:565:[step-13900/88900: 15.64%]--[loss-3.507464: wl-4.401031, gl-2.407206]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:34:45]
2023.03.03-01:31:27:665:[step-14000/88900: 15.75%]--[loss-3.171953: wl-4.125192, gl-2.140655]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:42:18]
2023.03.03-01:32:52:765:[step-14100/88900: 15.86%]--[loss-3.009245: wl-3.709489, gl-2.081872]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:28:57]
2023.03.03-01:34:17:865:[step-14200/88900: 15.97%]--[loss-2.858826: wl-3.553006, gl-1.970574]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:12:24]
End of epoch 16 / 100: train_loss: 3.352 	 time: 775 sec
Saving the model at the end of epoch 16, iters 14224
2023.03.03-01:35:51:76:[step-14300/88900: 16.09%]--[loss-3.432840: wl-4.851494, gl-2.219967]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:24:47]
2023.03.03-01:37:19:176:[step-14400/88900: 16.20%]--[loss-3.601631: wl-5.227143, gl-2.294846]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:43:23]
2023.03.03-01:38:45:276:[step-14500/88900: 16.31%]--[loss-3.259341: wl-4.102379, gl-2.233747]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:54:24]
2023.03.03-01:40:11:376:[step-14600/88900: 16.42%]--[loss-3.185229: wl-4.511762, gl-2.057289]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:05:40]
2023.03.03-01:41:37:476:[step-14700/88900: 16.54%]--[loss-3.407196: wl-4.550303, gl-2.269620]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:53:09]
2023.03.03-01:43:02:576:[step-14800/88900: 16.65%]--[loss-3.163852: wl-3.916510, gl-2.184724]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:05:29]
2023.03.03-01:44:28:676:[step-14900/88900: 16.76%]--[loss-3.566959: wl-5.296986, gl-2.242713]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:19:18]
2023.03.03-01:45:53:776:[step-15000/88900: 16.87%]--[loss-3.097656: wl-3.695454, gl-2.173793]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:45:08]
2023.03.03-01:47:18:876:[step-15100/88900: 16.99%]--[loss-3.140527: wl-3.981410, gl-2.145174]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:46:15]
End of epoch 17 / 100: train_loss: 3.328 	 time: 770 sec
Saving the model at the end of epoch 17, iters 15113
2023.03.03-01:48:51:87:[step-15200/88900: 17.10%]--[loss-3.286160: wl-4.353651, gl-2.197747]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:25:31]
2023.03.03-01:50:18:187:[step-15300/88900: 17.21%]--[loss-3.328574: wl-4.246044, gl-2.267063]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:10:07]
2023.03.03-01:51:44:287:[step-15400/88900: 17.32%]--[loss-3.233684: wl-4.247456, gl-2.171820]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:52:30]
2023.03.03-01:53:11:387:[step-15500/88900: 17.44%]--[loss-3.362558: wl-4.094164, gl-2.339017]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:48:10]
2023.03.03-01:54:38:487:[step-15600/88900: 17.55%]--[loss-3.033026: wl-3.923768, gl-2.052084]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:21:25]
2023.03.03-01:56:03:587:[step-15700/88900: 17.66%]--[loss-3.409389: wl-4.228948, gl-2.352152]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:36:46]
2023.03.03-01:57:29:687:[step-15800/88900: 17.77%]--[loss-3.605187: wl-4.232622, gl-2.547031]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:30:04]
2023.03.03-01:58:56:787:[step-15900/88900: 17.89%]--[loss-3.072790: wl-3.758383, gl-2.133194]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:05:19]
2023.03.03-02:00:21:887:[step-16000/88900: 18.00%]--[loss-3.119279: wl-3.987231, gl-2.122471]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:31:31]
End of epoch 18 / 100: train_loss: 3.313 	 time: 773 sec
Saving the model at the end of epoch 18, iters 16002
2023.03.03-02:01:56:98:[step-16100/88900: 18.11%]--[loss-3.400996: wl-4.698480, gl-2.226376]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:41:22]
2023.03.03-02:03:21:198:[step-16200/88900: 18.22%]--[loss-3.229660: wl-3.747772, gl-2.292717]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:19:30]
2023.03.03-02:04:48:298:[step-16300/88900: 18.34%]--[loss-3.281225: wl-4.252937, gl-2.217991]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:24:10]
2023.03.03-02:06:14:398:[step-16400/88900: 18.45%]--[loss-3.428873: wl-4.554964, gl-2.290132]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:08:01]
2023.03.03-02:07:40:498:[step-16500/88900: 18.56%]--[loss-3.013174: wl-3.721303, gl-2.082848]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:59:54]
2023.03.03-02:09:07:598:[step-16600/88900: 18.67%]--[loss-3.272854: wl-4.224141, gl-2.216819]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:48:56]
2023.03.03-02:10:32:698:[step-16700/88900: 18.79%]--[loss-3.401215: wl-4.391921, gl-2.303235]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:54:27]
2023.03.03-02:11:59:798:[step-16800/88900: 18.90%]--[loss-3.336928: wl-3.999483, gl-2.337057]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:06:44]
End of epoch 19 / 100: train_loss: 3.299 	 time: 774 sec
Saving the model at the end of epoch 19, iters 16891
2023.03.03-02:13:33:9:[step-16900/88900: 19.01%]--[loss-3.116412: wl-3.953502, gl-2.128036]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:12:58]
2023.03.03-02:15:01:109:[step-17000/88900: 19.12%]--[loss-2.927605: wl-3.819324, gl-1.972774]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:53:11]
2023.03.03-02:16:28:209:[step-17100/88900: 19.24%]--[loss-3.038213: wl-3.927552, gl-2.056325]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:03:06]
2023.03.03-02:17:54:309:[step-17200/88900: 19.35%]--[loss-3.358712: wl-4.448030, gl-2.246704]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:30:36]
2023.03.03-02:19:20:409:[step-17300/88900: 19.46%]--[loss-3.412690: wl-4.241367, gl-2.352348]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:25:27]
2023.03.03-02:20:47:509:[step-17400/88900: 19.57%]--[loss-3.461482: wl-4.592165, gl-2.313441]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:07:25]
2023.03.03-02:22:13:609:[step-17500/88900: 19.69%]--[loss-3.532950: wl-4.657126, gl-2.368669]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:53:44]
2023.03.03-02:23:41:709:[step-17600/88900: 19.80%]--[loss-3.133438: wl-3.735241, gl-2.199628]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:29:27]
2023.03.03-02:25:14:809:[step-17700/88900: 19.91%]--[loss-3.226704: wl-4.065493, gl-2.210330]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:57:13]
End of epoch 20 / 100: train_loss: 3.273 	 time: 785 sec
Saving the model at the end of epoch 20, iters 17780
2023.03.03-02:26:50:20:[step-17800/88900: 20.02%]--[loss-3.175589: wl-3.849092, gl-2.213316]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:00:34]
2023.03.03-02:28:23:120:[step-17900/88900: 20.13%]--[loss-3.337850: wl-4.337739, gl-2.253415]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:47:38]
2023.03.03-02:29:55:220:[step-18000/88900: 20.25%]--[loss-3.295739: wl-4.541181, gl-2.160444]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:25:16]
2023.03.03-02:31:26:320:[step-18100/88900: 20.36%]--[loss-3.228657: wl-4.026576, gl-2.222013]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:33:06]
2023.03.03-02:32:58:420:[step-18200/88900: 20.47%]--[loss-3.147512: wl-4.070036, gl-2.130003]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:54:18]
2023.03.03-02:34:30:520:[step-18300/88900: 20.58%]--[loss-3.331031: wl-4.441268, gl-2.220714]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:28:56]
2023.03.03-02:35:59:620:[step-18400/88900: 20.70%]--[loss-3.188529: wl-4.035196, gl-2.179729]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:01:36]
2023.03.03-02:37:21:720:[step-18500/88900: 20.81%]--[loss-3.089118: wl-4.068305, gl-2.072042]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:11:03]
2023.03.03-02:38:38:820:[step-18600/88900: 20.92%]--[loss-3.305604: wl-4.412328, gl-2.202522]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:06:37]
End of epoch 21 / 100: train_loss: 3.263 	 time: 787 sec
Saving the model at the end of epoch 21, iters 18669
2023.03.03-02:40:08:31:[step-18700/88900: 21.03%]--[loss-3.256509: wl-4.139912, gl-2.221531]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:27:33]
2023.03.03-02:41:35:131:[step-18800/88900: 21.15%]--[loss-3.334696: wl-4.385361, gl-2.238356]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:43:14]
2023.03.03-02:43:01:231:[step-18900/88900: 21.26%]--[loss-3.264152: wl-4.786687, gl-2.067480]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:00:08]
2023.03.03-02:44:27:331:[step-19000/88900: 21.37%]--[loss-3.066976: wl-3.915323, gl-2.088145]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:59:29]
2023.03.03-02:45:53:431:[step-19100/88900: 21.48%]--[loss-3.153093: wl-4.162498, gl-2.112468]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:27:16]
2023.03.03-02:47:18:531:[step-19200/88900: 21.60%]--[loss-3.442544: wl-4.683856, gl-2.271580]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:58:48]
2023.03.03-02:48:45:631:[step-19300/88900: 21.71%]--[loss-3.329348: wl-4.361392, gl-2.238999]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:11:19]
2023.03.03-02:50:10:731:[step-19400/88900: 21.82%]--[loss-3.355611: wl-4.630476, gl-2.197992]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:59:51]
2023.03.03-02:51:37:831:[step-19500/88900: 21.93%]--[loss-3.493737: wl-5.545465, gl-2.107371]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:54:21]
End of epoch 22 / 100: train_loss: 3.243 	 time: 773 sec
Saving the model at the end of epoch 22, iters 19558
2023.03.03-02:53:10:42:[step-19600/88900: 22.05%]--[loss-3.124379: wl-4.141558, gl-2.088990]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:39:34]
2023.03.03-02:54:35:142:[step-19700/88900: 22.16%]--[loss-3.243591: wl-4.538177, gl-2.109047]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:49:24]
2023.03.03-02:56:00:242:[step-19800/88900: 22.27%]--[loss-3.265163: wl-4.157777, gl-2.225718]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:32:17]
2023.03.03-02:57:27:342:[step-19900/88900: 22.38%]--[loss-3.278380: wl-4.047609, gl-2.266478]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:15:08]
2023.03.03-02:58:53:442:[step-20000/88900: 22.50%]--[loss-3.248608: wl-4.337975, gl-2.164114]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:57:43]
2023.03.03-03:00:20:542:[step-20100/88900: 22.61%]--[loss-3.090488: wl-4.091145, gl-2.067702]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:38:30]
2023.03.03-03:01:46:642:[step-20200/88900: 22.72%]--[loss-3.173605: wl-4.479767, gl-2.053663]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:26:32]
2023.03.03-03:03:12:742:[step-20300/88900: 22.83%]--[loss-3.329057: wl-4.924439, gl-2.097947]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:53:43]
2023.03.03-03:04:37:842:[step-20400/88900: 22.95%]--[loss-3.338419: wl-4.461640, gl-2.223009]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:10:42]
End of epoch 23 / 100: train_loss: 3.236 	 time: 769 sec
Saving the model at the end of epoch 23, iters 20447
2023.03.03-03:06:10:53:[step-20500/88900: 23.06%]--[loss-2.847354: wl-3.693525, gl-1.923973]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:37:24]
2023.03.03-03:07:37:153:[step-20600/88900: 23.17%]--[loss-3.293000: wl-4.706939, gl-2.116265]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:54:40]
2023.03.03-03:09:02:253:[step-20700/88900: 23.28%]--[loss-3.084197: wl-3.960700, gl-2.094022]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:43:45]
2023.03.03-03:10:29:353:[step-20800/88900: 23.40%]--[loss-3.001022: wl-4.141286, gl-1.965701]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:26:05]
2023.03.03-03:11:56:453:[step-20900/88900: 23.51%]--[loss-3.128680: wl-4.201877, gl-2.078211]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:32:04]
2023.03.03-03:13:23:553:[step-21000/88900: 23.62%]--[loss-3.240745: wl-4.198902, gl-2.191019]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:38:13]
2023.03.03-03:14:50:653:[step-21100/88900: 23.73%]--[loss-3.232523: wl-4.473355, gl-2.114185]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:42:12]
2023.03.03-03:16:16:753:[step-21200/88900: 23.85%]--[loss-2.986623: wl-3.884938, gl-2.015388]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:56:17]
2023.03.03-03:17:41:853:[step-21300/88900: 23.96%]--[loss-3.256711: wl-3.939079, gl-2.271941]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:33:42]
End of epoch 24 / 100: train_loss: 3.215 	 time: 775 sec
Saving the model at the end of epoch 24, iters 21336
2023.03.03-03:19:16:64:[step-21400/88900: 24.07%]--[loss-3.242411: wl-4.242862, gl-2.181695]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:14:29]
2023.03.03-03:20:41:164:[step-21500/88900: 24.18%]--[loss-2.851228: wl-3.487185, gl-1.979432]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:54:10]
2023.03.03-03:22:08:264:[step-21600/88900: 24.30%]--[loss-3.183788: wl-4.563953, gl-2.042799]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:16:16]
2023.03.03-03:23:33:364:[step-21700/88900: 24.41%]--[loss-3.282241: wl-4.130774, gl-2.249547]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:03:02]
2023.03.03-03:25:00:464:[step-21800/88900: 24.52%]--[loss-3.597343: wl-4.397025, gl-2.498087]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:30:23]
2023.03.03-03:26:25:564:[step-21900/88900: 24.63%]--[loss-3.268167: wl-3.961914, gl-2.277688]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:50:06]
2023.03.03-03:27:50:664:[step-22000/88900: 24.75%]--[loss-3.404528: wl-4.727158, gl-2.222739]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:33:39]
2023.03.03-03:29:16:764:[step-22100/88900: 24.86%]--[loss-3.154966: wl-3.951487, gl-2.167094]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:37:43]
2023.03.03-03:30:42:864:[step-22200/88900: 24.97%]--[loss-3.130102: wl-3.982678, gl-2.134433]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:49:13]
End of epoch 25 / 100: train_loss: 3.208 	 time: 770 sec
Saving the model at the end of epoch 25, iters 22225
2023.03.03-03:32:17:75:[step-22300/88900: 25.08%]--[loss-3.166829: wl-4.199910, gl-2.116851]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:04:05]
2023.03.03-03:33:44:175:[step-22400/88900: 25.20%]--[loss-3.257901: wl-4.069591, gl-2.240503]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:48:05]
2023.03.03-03:35:11:275:[step-22500/88900: 25.31%]--[loss-3.167906: wl-4.234694, gl-2.109232]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:32:52]
2023.03.03-03:36:36:375:[step-22600/88900: 25.42%]--[loss-3.056674: wl-4.087843, gl-2.034713]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:37:26]
2023.03.03-03:38:02:475:[step-22700/88900: 25.53%]--[loss-3.795690: wl-5.378585, gl-2.451044]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:59:08]
2023.03.03-03:39:27:575:[step-22800/88900: 25.65%]--[loss-3.030704: wl-3.693970, gl-2.107211]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:20:57]
2023.03.03-03:40:53:675:[step-22900/88900: 25.76%]--[loss-3.227886: wl-4.259065, gl-2.163120]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:13:25]
2023.03.03-03:42:19:775:[step-23000/88900: 25.87%]--[loss-3.285564: wl-4.684418, gl-2.114460]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:30:50]
2023.03.03-03:43:45:875:[step-23100/88900: 25.98%]--[loss-3.409749: wl-5.012446, gl-2.156637]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:15:45]
End of epoch 26 / 100: train_loss: 3.192 	 time: 773 sec
Saving the model at the end of epoch 26, iters 23114
2023.03.03-03:45:19:86:[step-23200/88900: 26.10%]--[loss-3.170856: wl-4.097239, gl-2.146547]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:56:25]
2023.03.03-03:46:46:186:[step-23300/88900: 26.21%]--[loss-3.253475: wl-4.359664, gl-2.163559]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:33:36]
2023.03.03-03:48:11:286:[step-23400/88900: 26.32%]--[loss-3.283427: wl-4.498775, gl-2.158733]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:04:41]
2023.03.03-03:49:38:386:[step-23500/88900: 26.43%]--[loss-3.123241: wl-4.256611, gl-2.059088]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:17:36]
2023.03.03-03:51:04:486:[step-23600/88900: 26.55%]--[loss-3.201320: wl-4.483727, gl-2.080388]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:31:07]
2023.03.03-03:52:29:586:[step-23700/88900: 26.66%]--[loss-3.216296: wl-4.109350, gl-2.188959]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:52:34]
2023.03.03-03:53:56:686:[step-23800/88900: 26.77%]--[loss-3.422093: wl-4.531384, gl-2.289247]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:42:45]
2023.03.03-03:55:21:786:[step-23900/88900: 26.88%]--[loss-3.276715: wl-4.757710, gl-2.087288]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:14:00]
2023.03.03-03:56:49:886:[step-24000/88900: 27.00%]--[loss-2.991265: wl-4.179297, gl-1.946441]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:37:14]
End of epoch 27 / 100: train_loss: 3.179 	 time: 774 sec
Saving the model at the end of epoch 27, iters 24003
2023.03.03-03:58:22:97:[step-24100/88900: 27.11%]--[loss-3.017698: wl-4.018356, gl-2.013109]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:04:20]
2023.03.03-03:59:48:197:[step-24200/88900: 27.22%]--[loss-3.010182: wl-3.890928, gl-2.037450]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:04:23]
2023.03.03-04:01:14:297:[step-24300/88900: 27.33%]--[loss-3.036751: wl-4.022087, gl-2.031229]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:24:41]
2023.03.03-04:02:41:397:[step-24400/88900: 27.45%]--[loss-3.083538: wl-3.961373, gl-2.093194]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:02:46]
2023.03.03-04:04:06:497:[step-24500/88900: 27.56%]--[loss-3.076548: wl-4.024732, gl-2.070365]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:25:42]
2023.03.03-04:05:32:597:[step-24600/88900: 27.67%]--[loss-3.296876: wl-4.456790, gl-2.182678]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:13:47]
2023.03.03-04:06:59:697:[step-24700/88900: 27.78%]--[loss-3.261580: wl-4.328312, gl-2.179502]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:08:55]
2023.03.03-04:08:24:797:[step-24800/88900: 27.90%]--[loss-3.139022: wl-4.189797, gl-2.091572]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:17:26]
End of epoch 28 / 100: train_loss: 3.165 	 time: 770 sec
Saving the model at the end of epoch 28, iters 24892
2023.03.03-04:09:58:8:[step-24900/88900: 28.01%]--[loss-3.024981: wl-4.112545, gl-1.996845]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:41:57]
2023.03.03-04:11:24:108:[step-25000/88900: 28.12%]--[loss-3.126385: wl-4.080264, gl-2.106319]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:18:31]
2023.03.03-04:12:49:208:[step-25100/88900: 28.23%]--[loss-3.416247: wl-4.508616, gl-2.289093]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:28:58]
2023.03.03-04:14:16:308:[step-25200/88900: 28.35%]--[loss-3.031559: wl-4.202985, gl-1.980812]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:55:09]
2023.03.03-04:15:40:408:[step-25300/88900: 28.46%]--[loss-3.104141: wl-4.276903, gl-2.034915]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:59:15]
2023.03.03-04:17:06:508:[step-25400/88900: 28.57%]--[loss-3.406374: wl-4.568457, gl-2.264260]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:31:29]
2023.03.03-04:18:31:608:[step-25500/88900: 28.68%]--[loss-3.120674: wl-4.011728, gl-2.117742]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:38:44]
2023.03.03-04:19:57:708:[step-25600/88900: 28.80%]--[loss-3.251435: wl-4.081983, gl-2.230939]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:56:10]
2023.03.03-04:21:23:808:[step-25700/88900: 28.91%]--[loss-3.167458: wl-4.205586, gl-2.116062]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:26:36]
End of epoch 29 / 100: train_loss: 3.159 	 time: 768 sec
Saving the model at the end of epoch 29, iters 25781
2023.03.03-04:22:57:19:[step-25800/88900: 29.02%]--[loss-3.179931: wl-4.399022, gl-2.080175]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:01:24]
2023.03.03-04:24:23:119:[step-25900/88900: 29.13%]--[loss-3.332734: wl-4.814616, gl-2.129080]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:32:23]
2023.03.03-04:25:56:219:[step-26000/88900: 29.25%]--[loss-2.914640: wl-3.976134, gl-1.920607]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:09:42]
2023.03.03-04:27:28:319:[step-26100/88900: 29.36%]--[loss-3.077711: wl-3.974180, gl-2.084166]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:25:48]
2023.03.03-04:28:59:419:[step-26200/88900: 29.47%]--[loss-2.928061: wl-4.012083, gl-1.925040]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:10:44]
2023.03.03-04:30:30:519:[step-26300/88900: 29.58%]--[loss-3.130873: wl-4.242330, gl-2.070291]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:51:23]
2023.03.03-04:32:01:619:[step-26400/88900: 29.70%]--[loss-3.014311: wl-4.044198, gl-2.003262]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:33:07]
2023.03.03-04:33:32:719:[step-26500/88900: 29.81%]--[loss-3.076244: wl-4.373213, gl-1.982941]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:48:23]
2023.03.03-04:35:03:819:[step-26600/88900: 29.92%]--[loss-3.136321: wl-4.202857, gl-2.085607]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:31:25]
End of epoch 30 / 100: train_loss: 3.144 	 time: 813 sec
Saving the model at the end of epoch 30, iters 26670
2023.03.03-04:36:42:30:[step-26700/88900: 30.03%]--[loss-3.142168: wl-4.352448, gl-2.054056]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:55:22]
2023.03.03-04:38:08:130:[step-26800/88900: 30.15%]--[loss-3.022188: wl-4.291374, gl-1.949345]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:59:50]
2023.03.03-04:39:25:230:[step-26900/88900: 30.26%]--[loss-3.025774: wl-4.034522, gl-2.017143]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:27:02]
2023.03.03-04:40:42:330:[step-27000/88900: 30.37%]--[loss-2.927581: wl-3.593731, gl-2.029148]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:05:30]
2023.03.03-04:42:08:430:[step-27100/88900: 30.48%]--[loss-3.032170: wl-4.162422, gl-1.991565]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:19:55]
2023.03.03-04:43:34:530:[step-27200/88900: 30.60%]--[loss-3.110799: wl-4.235168, gl-2.052007]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:10:45]
2023.03.03-04:44:59:630:[step-27300/88900: 30.71%]--[loss-3.344763: wl-5.226212, gl-2.038210]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:06:39]
2023.03.03-04:46:25:730:[step-27400/88900: 30.82%]--[loss-2.948832: wl-3.889476, gl-1.976463]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:30:06]
2023.03.03-04:47:50:830:[step-27500/88900: 30.93%]--[loss-2.896035: wl-3.746712, gl-1.959357]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:47:31]
End of epoch 31 / 100: train_loss: 3.136 	 time: 754 sec
Saving the model at the end of epoch 31, iters 27559
2023.03.03-04:49:25:41:[step-27600/88900: 31.05%]--[loss-3.041431: wl-4.014431, gl-2.037824]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:24:13]
2023.03.03-04:50:51:141:[step-27700/88900: 31.16%]--[loss-3.240252: wl-4.366119, gl-2.148722]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:49:38]
2023.03.03-04:52:16:241:[step-27800/88900: 31.27%]--[loss-3.210787: wl-4.205431, gl-2.159430]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:18:34]
2023.03.03-04:53:42:341:[step-27900/88900: 31.38%]--[loss-3.153362: wl-4.376068, gl-2.059345]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:28:23]
2023.03.03-04:55:08:441:[step-28000/88900: 31.50%]--[loss-3.017363: wl-3.990027, gl-2.019856]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:26:34]
2023.03.03-04:56:34:541:[step-28100/88900: 31.61%]--[loss-2.981342: wl-3.767419, gl-2.039487]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:50:01]
2023.03.03-04:58:00:641:[step-28200/88900: 31.72%]--[loss-3.149554: wl-4.087407, gl-2.127703]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:56:20]
2023.03.03-04:59:27:741:[step-28300/88900: 31.83%]--[loss-3.251465: wl-4.286091, gl-2.179942]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:17:35]
2023.03.03-05:00:53:841:[step-28400/88900: 31.95%]--[loss-2.954737: wl-4.000056, gl-1.954723]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:53:12]
End of epoch 32 / 100: train_loss: 3.124 	 time: 771 sec
Saving the model at the end of epoch 32, iters 28448
2023.03.03-05:02:26:52:[step-28500/88900: 32.06%]--[loss-3.196681: wl-4.791049, gl-1.998918]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:48:22]
2023.03.03-05:03:52:152:[step-28600/88900: 32.17%]--[loss-3.355239: wl-4.732139, gl-2.172204]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:08:30]
2023.03.03-05:05:19:252:[step-28700/88900: 32.28%]--[loss-3.004681: wl-3.732926, gl-2.071449]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:05:51]
2023.03.03-05:06:45:352:[step-28800/88900: 32.40%]--[loss-3.005242: wl-4.070450, gl-1.987629]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:28:01]
2023.03.03-05:08:11:452:[step-28900/88900: 32.51%]--[loss-2.928923: wl-3.836863, gl-1.969707]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:26:13]
2023.03.03-05:09:36:552:[step-29000/88900: 32.62%]--[loss-3.092422: wl-4.148629, gl-2.055265]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:20:41]
2023.03.03-05:11:02:652:[step-29100/88900: 32.73%]--[loss-3.277060: wl-4.369504, gl-2.184684]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:04:54]
2023.03.03-05:12:28:752:[step-29200/88900: 32.85%]--[loss-2.950637: wl-4.105580, gl-1.924242]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:02:49]
2023.03.03-05:13:54:852:[step-29300/88900: 32.96%]--[loss-3.156132: wl-4.741035, gl-1.970874]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:59:04]
End of epoch 33 / 100: train_loss: 3.113 	 time: 770 sec
Saving the model at the end of epoch 33, iters 29337
2023.03.03-05:15:27:63:[step-29400/88900: 33.07%]--[loss-3.077569: wl-3.892650, gl-2.104406]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:21:16]
2023.03.03-05:16:53:163:[step-29500/88900: 33.18%]--[loss-2.984081: wl-3.828344, gl-2.026995]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:28:12]
2023.03.03-05:18:19:263:[step-29600/88900: 33.30%]--[loss-2.918573: wl-3.782444, gl-1.972962]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:37:48]
2023.03.03-05:19:45:363:[step-29700/88900: 33.41%]--[loss-3.224864: wl-4.216040, gl-2.170855]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:26:34]
2023.03.03-05:21:11:463:[step-29800/88900: 33.52%]--[loss-2.971334: wl-3.890523, gl-1.998703]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:16:51]
2023.03.03-05:22:36:563:[step-29900/88900: 33.63%]--[loss-2.960399: wl-3.845829, gl-1.998942]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:21:50]
2023.03.03-05:24:02:663:[step-30000/88900: 33.75%]--[loss-3.336053: wl-4.456810, gl-2.221851]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:39:46]
2023.03.03-05:25:27:763:[step-30100/88900: 33.86%]--[loss-2.963861: wl-3.790957, gl-2.016122]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:29:10]
2023.03.03-05:26:53:863:[step-30200/88900: 33.97%]--[loss-2.968710: wl-4.137875, gl-1.934241]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:06:05]
End of epoch 34 / 100: train_loss: 3.103 	 time: 770 sec
Saving the model at the end of epoch 34, iters 30226
2023.03.03-05:28:27:74:[step-30300/88900: 34.08%]--[loss-3.079517: wl-4.097825, gl-2.055061]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:15:12]
2023.03.03-05:29:53:174:[step-30400/88900: 34.20%]--[loss-3.053335: wl-3.970527, gl-2.060703]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:30:37]
2023.03.03-05:31:19:274:[step-30500/88900: 34.31%]--[loss-3.336359: wl-4.572754, gl-2.193171]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:34:55]
2023.03.03-05:32:45:374:[step-30600/88900: 34.42%]--[loss-3.085656: wl-3.963186, gl-2.094860]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:31:26]
2023.03.03-05:34:11:474:[step-30700/88900: 34.53%]--[loss-2.979125: wl-3.679069, gl-2.059358]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:36:01]
2023.03.03-05:35:37:574:[step-30800/88900: 34.65%]--[loss-2.861358: wl-3.733457, gl-1.927994]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:52:01]
2023.03.03-05:37:04:674:[step-30900/88900: 34.76%]--[loss-2.894732: wl-3.785781, gl-1.948287]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:38:46]
2023.03.03-05:38:29:774:[step-31000/88900: 34.87%]--[loss-2.977329: wl-4.057151, gl-1.963042]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:41:09]
2023.03.03-05:39:55:874:[step-31100/88900: 34.98%]--[loss-3.100848: wl-4.017853, gl-2.096384]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:56:29]
End of epoch 35 / 100: train_loss: 3.096 	 time: 771 sec
Saving the model at the end of epoch 35, iters 31115
2023.03.03-05:41:29:85:[step-31200/88900: 35.10%]--[loss-2.927078: wl-3.905236, gl-1.950769]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:21:29]
2023.03.03-05:42:54:185:[step-31300/88900: 35.21%]--[loss-3.092505: wl-4.131649, gl-2.059593]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:38:24]
2023.03.03-05:44:19:285:[step-31400/88900: 35.32%]--[loss-3.098813: wl-4.151249, gl-2.061000]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:53:14]
2023.03.03-05:45:44:385:[step-31500/88900: 35.43%]--[loss-2.938591: wl-4.267971, gl-1.871598]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:26:26]
2023.03.03-05:47:10:485:[step-31600/88900: 35.55%]--[loss-3.027198: wl-3.834881, gl-2.068478]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:29:05]
2023.03.03-05:48:35:585:[step-31700/88900: 35.66%]--[loss-2.993698: wl-4.065824, gl-1.977242]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:01:43]
2023.03.03-05:50:01:685:[step-31800/88900: 35.77%]--[loss-3.124676: wl-4.351684, gl-2.036756]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:20:33]
2023.03.03-05:51:26:785:[step-31900/88900: 35.88%]--[loss-3.116512: wl-4.315706, gl-2.037585]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:28:25]
2023.03.03-05:52:51:885:[step-32000/88900: 36.00%]--[loss-2.785118: wl-3.559346, gl-1.895281]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:20:23]
End of epoch 36 / 100: train_loss: 3.083 	 time: 766 sec
Saving the model at the end of epoch 36, iters 32004
2023.03.03-05:54:25:96:[step-32100/88900: 36.11%]--[loss-3.168024: wl-4.453550, gl-2.054637]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:18:41]
2023.03.03-05:55:50:196:[step-32200/88900: 36.22%]--[loss-3.297443: wl-4.232913, gl-2.239214]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:45:55]
2023.03.03-05:57:16:296:[step-32300/88900: 36.33%]--[loss-2.948038: wl-3.913706, gl-1.969611]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:34:02]
2023.03.03-05:58:43:396:[step-32400/88900: 36.45%]--[loss-3.120564: wl-4.487771, gl-1.998621]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:56:45]
2023.03.03-06:00:08:496:[step-32500/88900: 36.56%]--[loss-3.247526: wl-4.818698, gl-2.042851]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:33:27]
2023.03.03-06:01:34:596:[step-32600/88900: 36.67%]--[loss-3.318976: wl-4.040747, gl-2.308789]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:07:01]
2023.03.03-06:03:00:696:[step-32700/88900: 36.78%]--[loss-3.137814: wl-4.071120, gl-2.120034]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:34:08]
2023.03.03-06:04:26:796:[step-32800/88900: 36.90%]--[loss-3.260926: wl-5.151002, gl-1.973176]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:59:42]
End of epoch 37 / 100: train_loss: 3.072 	 time: 770 sec
Saving the model at the end of epoch 37, iters 32893
2023.03.03-06:05:59:7:[step-32900/88900: 37.01%]--[loss-2.880068: wl-3.684930, gl-1.958836]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:04:44]
2023.03.03-06:07:25:107:[step-33000/88900: 37.12%]--[loss-2.933053: wl-3.964022, gl-1.942048]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:38:53]
2023.03.03-06:08:52:207:[step-33100/88900: 37.23%]--[loss-2.832110: wl-3.634578, gl-1.923465]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:28:38]
2023.03.03-06:10:16:307:[step-33200/88900: 37.35%]--[loss-3.012984: wl-3.913456, gl-2.034620]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:28:05]
2023.03.03-06:11:43:407:[step-33300/88900: 37.46%]--[loss-3.027962: wl-4.169292, gl-1.985639]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:36:46]
2023.03.03-06:13:09:507:[step-33400/88900: 37.57%]--[loss-3.025272: wl-3.991852, gl-2.027309]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:24:24]
2023.03.03-06:14:35:607:[step-33500/88900: 37.68%]--[loss-3.208113: wl-4.685611, gl-2.036710]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:35:50]
2023.03.03-06:16:02:707:[step-33600/88900: 37.80%]--[loss-3.091915: wl-4.188793, gl-2.044717]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:15:36]
2023.03.03-06:17:31:807:[step-33700/88900: 37.91%]--[loss-3.134937: wl-4.170597, gl-2.092288]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:38:34]
End of epoch 38 / 100: train_loss: 3.065 	 time: 775 sec
Saving the model at the end of epoch 38, iters 33782
2023.03.03-06:19:06:18:[step-33800/88900: 38.02%]--[loss-3.259419: wl-4.438904, gl-2.149693]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:35:25]
2023.03.03-06:20:34:118:[step-33900/88900: 38.13%]--[loss-3.080798: wl-4.233845, gl-2.022336]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:43:00]
2023.03.03-06:22:02:218:[step-34000/88900: 38.25%]--[loss-2.997964: wl-3.984329, gl-2.001882]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:20:06]
2023.03.03-06:23:31:318:[step-34100/88900: 38.36%]--[loss-2.922831: wl-3.931688, gl-1.939909]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:28:12]
2023.03.03-06:25:08:418:[step-34200/88900: 38.47%]--[loss-2.944915: wl-4.121695, gl-1.914491]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:41:23]
2023.03.03-06:26:40:518:[step-34300/88900: 38.58%]--[loss-2.973606: wl-4.041605, gl-1.963204]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:57:16]
2023.03.03-06:28:18:618:[step-34400/88900: 38.70%]--[loss-3.299457: wl-4.846086, gl-2.087935]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:16:18]
2023.03.03-06:29:53:718:[step-34500/88900: 38.81%]--[loss-3.036124: wl-4.281800, gl-1.965674]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:57:31]
2023.03.03-06:31:29:818:[step-34600/88900: 38.92%]--[loss-3.051500: wl-4.323119, gl-1.970720]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:28:46]
End of epoch 39 / 100: train_loss: 3.055 	 time: 830 sec
Saving the model at the end of epoch 39, iters 34671
2023.03.03-06:33:09:29:[step-34700/88900: 39.03%]--[loss-2.956859: wl-4.060826, gl-1.941653]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:13:07]
2023.03.03-06:34:37:129:[step-34800/88900: 39.15%]--[loss-3.027151: wl-4.447019, gl-1.915396]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:22:46]
2023.03.03-06:35:53:229:[step-34900/88900: 39.26%]--[loss-3.242429: wl-4.284103, gl-2.171403]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:23:37]
2023.03.03-06:37:09:329:[step-35000/88900: 39.37%]--[loss-2.735194: wl-3.669693, gl-1.817771]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:21:12]
2023.03.03-06:38:26:429:[step-35100/88900: 39.48%]--[loss-3.080721: wl-4.214765, gl-2.027030]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:29:16]
2023.03.03-06:39:42:529:[step-35200/88900: 39.60%]--[loss-2.979697: wl-4.303538, gl-1.903812]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:13:11]
2023.03.03-06:40:57:629:[step-35300/88900: 39.71%]--[loss-2.878014: wl-3.735029, gl-1.944257]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:11:53]
2023.03.03-06:42:14:729:[step-35400/88900: 39.82%]--[loss-3.078170: wl-4.320610, gl-1.998018]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:14:04]
2023.03.03-06:43:29:829:[step-35500/88900: 39.93%]--[loss-2.991439: wl-3.880266, gl-2.021373]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:10:19]
End of epoch 40 / 100: train_loss: 3.044 	 time: 701 sec
Saving the model at the end of epoch 40, iters 35560
2023.03.03-06:44:53:40:[step-35600/88900: 40.04%]--[loss-3.237183: wl-4.340651, gl-2.152020]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:07:29]
2023.03.03-06:46:08:140:[step-35700/88900: 40.16%]--[loss-2.804658: wl-3.681653, gl-1.884245]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:14:44]
2023.03.03-06:47:24:240:[step-35800/88900: 40.27%]--[loss-3.271177: wl-4.222753, gl-2.215489]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:05:54]
2023.03.03-06:48:40:340:[step-35900/88900: 40.38%]--[loss-3.057823: wl-4.472175, gl-1.939779]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:08:20]
2023.03.03-06:49:56:440:[step-36000/88900: 40.49%]--[loss-2.879326: wl-3.870899, gl-1.911601]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:14:38]
2023.03.03-06:51:11:540:[step-36100/88900: 40.61%]--[loss-2.999853: wl-4.008302, gl-1.997778]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:04:24]
2023.03.03-06:52:27:640:[step-36200/88900: 40.72%]--[loss-2.828739: wl-3.821582, gl-1.873344]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:01:42]
2023.03.03-06:53:43:740:[step-36300/88900: 40.83%]--[loss-2.882202: wl-3.890501, gl-1.909576]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:00:39]
2023.03.03-06:54:58:840:[step-36400/88900: 40.94%]--[loss-3.253719: wl-4.775059, gl-2.059955]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:58:03]
End of epoch 41 / 100: train_loss: 3.041 	 time: 679 sec
Saving the model at the end of epoch 41, iters 36449
2023.03.03-06:56:21:51:[step-36500/88900: 41.06%]--[loss-3.331433: wl-4.718977, gl-2.151689]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:59:57]
2023.03.03-06:57:37:151:[step-36600/88900: 41.17%]--[loss-3.182453: wl-4.588910, gl-2.035226]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:00:30]
2023.03.03-06:58:53:251:[step-36700/88900: 41.28%]--[loss-3.045152: wl-4.851498, gl-1.832278]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:57:20]
2023.03.03-07:00:08:351:[step-36800/88900: 41.39%]--[loss-2.945223: wl-3.914007, gl-1.966722]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:03:34]
2023.03.03-07:01:24:451:[step-36900/88900: 41.51%]--[loss-2.956050: wl-4.081938, gl-1.935565]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:00:14]
2023.03.03-07:02:40:551:[step-37000/88900: 41.62%]--[loss-3.047686: wl-4.357471, gl-1.958318]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:57:16]
2023.03.03-07:03:55:651:[step-37100/88900: 41.73%]--[loss-2.851704: wl-3.902389, gl-1.876107]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:55:52]
2023.03.03-07:05:11:751:[step-37200/88900: 41.84%]--[loss-3.113125: wl-4.741874, gl-1.927657]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:50:11]
2023.03.03-07:06:27:851:[step-37300/88900: 41.96%]--[loss-3.089791: wl-4.119480, gl-2.059921]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:49:18]
End of epoch 42 / 100: train_loss: 3.034 	 time: 680 sec
Saving the model at the end of epoch 42, iters 37338
2023.03.03-07:07:50:62:[step-37400/88900: 42.07%]--[loss-2.879811: wl-3.731484, gl-1.946940]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:44:42]
2023.03.03-07:09:06:162:[step-37500/88900: 42.18%]--[loss-3.078363: wl-4.042813, gl-2.067660]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:22:52]
2023.03.03-07:10:21:262:[step-37600/88900: 42.29%]--[loss-2.777047: wl-3.814421, gl-1.823442]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:50:31]
2023.03.03-07:11:37:362:[step-37700/88900: 42.41%]--[loss-3.041141: wl-4.584324, gl-1.895060]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:52:23]
2023.03.03-07:12:53:462:[step-37800/88900: 42.52%]--[loss-2.980502: wl-4.067577, gl-1.963607]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:47:47]
2023.03.03-07:14:09:562:[step-37900/88900: 42.63%]--[loss-2.930298: wl-4.103813, gl-1.904344]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:57:10]
2023.03.03-07:15:25:662:[step-38000/88900: 42.74%]--[loss-2.880732: wl-4.061883, gl-1.865261]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:41:43]
2023.03.03-07:16:40:762:[step-38100/88900: 42.86%]--[loss-2.925973: wl-3.931062, gl-1.943207]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:39:30]
2023.03.03-07:17:56:862:[step-38200/88900: 42.97%]--[loss-2.898833: wl-4.054018, gl-1.885329]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:40:49]
End of epoch 43 / 100: train_loss: 3.025 	 time: 680 sec
Saving the model at the end of epoch 43, iters 38227
2023.03.03-07:19:19:73:[step-38300/88900: 43.08%]--[loss-3.040149: wl-4.342614, gl-1.954496]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:36:39]
2023.03.03-07:20:35:173:[step-38400/88900: 43.19%]--[loss-3.162867: wl-4.580645, gl-2.017705]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:33:25]
2023.03.03-07:21:51:273:[step-38500/88900: 43.31%]--[loss-2.809654: wl-3.880192, gl-1.839606]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:37:13]
2023.03.03-07:23:07:373:[step-38600/88900: 43.42%]--[loss-3.223173: wl-4.489125, gl-2.100892]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:30:59]
2023.03.03-07:24:23:473:[step-38700/88900: 43.53%]--[loss-2.931951: wl-4.014997, gl-1.928201]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:31:15]
2023.03.03-07:25:39:573:[step-38800/88900: 43.64%]--[loss-3.164343: wl-4.365337, gl-2.073009]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:30:22]
2023.03.03-07:26:55:673:[step-38900/88900: 43.76%]--[loss-3.107417: wl-4.187237, gl-2.060608]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:30:15]
2023.03.03-07:28:11:773:[step-39000/88900: 43.87%]--[loss-2.974355: wl-3.931544, gl-1.991468]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:29:53]
2023.03.03-07:29:27:873:[step-39100/88900: 43.98%]--[loss-2.969944: wl-3.797887, gl-2.020473]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:26:36]
End of epoch 44 / 100: train_loss: 3.016 	 time: 681 sec
Saving the model at the end of epoch 44, iters 39116
2023.03.03-07:30:50:84:[step-39200/88900: 44.09%]--[loss-2.858765: wl-3.729095, gl-1.926491]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:30:41]
2023.03.03-07:32:06:184:[step-39300/88900: 44.21%]--[loss-2.882528: wl-3.817550, gl-1.928140]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:21:33]
2023.03.03-07:33:22:284:[step-39400/88900: 44.32%]--[loss-2.943380: wl-3.921345, gl-1.963043]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:28:38]
2023.03.03-07:34:38:384:[step-39500/88900: 44.43%]--[loss-3.126387: wl-4.150707, gl-2.088711]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:31:17]
2023.03.03-07:35:53:484:[step-39600/88900: 44.54%]--[loss-3.222487: wl-4.676470, gl-2.053370]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:20:53]
2023.03.03-07:37:09:584:[step-39700/88900: 44.66%]--[loss-3.039176: wl-4.483276, gl-1.918357]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:17:32]
2023.03.03-07:38:25:684:[step-39800/88900: 44.77%]--[loss-2.829974: wl-3.881424, gl-1.859618]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:24:40]
2023.03.03-07:39:40:784:[step-39900/88900: 44.88%]--[loss-2.989537: wl-4.376328, gl-1.895455]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:15:47]
2023.03.03-07:40:56:884:[step-40000/88900: 44.99%]--[loss-2.959891: wl-3.911111, gl-1.982113]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:13:11]
End of epoch 45 / 100: train_loss: 3.012 	 time: 680 sec
Saving the model at the end of epoch 45, iters 40005
2023.03.03-07:42:20:95:[step-40100/88900: 45.11%]--[loss-2.910204: wl-4.054706, gl-1.896528]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:22:09]
2023.03.03-07:43:36:195:[step-40200/88900: 45.22%]--[loss-2.911435: wl-4.037317, gl-1.902106]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:24:07]
2023.03.03-07:44:53:295:[step-40300/88900: 45.33%]--[loss-2.978106: wl-4.346446, gl-1.891495]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:14:59]
2023.03.03-07:46:09:395:[step-40400/88900: 45.44%]--[loss-2.895544: wl-3.744814, gl-1.959341]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:10:53]
2023.03.03-07:47:25:495:[step-40500/88900: 45.56%]--[loss-3.031615: wl-4.343366, gl-1.945773]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:12:15]
2023.03.03-07:48:41:595:[step-40600/88900: 45.67%]--[loss-3.051765: wl-4.131153, gl-2.018977]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:16:26]
2023.03.03-07:49:57:695:[step-40700/88900: 45.78%]--[loss-2.990329: wl-3.939613, gl-2.005426]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:10:30]
2023.03.03-07:51:13:795:[step-40800/88900: 45.89%]--[loss-3.086627: wl-4.444988, gl-1.975381]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:28:02]
End of epoch 46 / 100: train_loss: 3.007 	 time: 684 sec
Saving the model at the end of epoch 46, iters 40894
2023.03.03-07:52:37:6:[step-40900/88900: 46.01%]--[loss-2.777812: wl-3.834897, gl-1.819087]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:11:47]
2023.03.03-07:53:54:106:[step-41000/88900: 46.12%]--[loss-3.040926: wl-4.161324, gl-2.000595]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:24:33]
2023.03.03-07:55:12:206:[step-41100/88900: 46.23%]--[loss-2.738536: wl-3.732237, gl-1.805477]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:23:55]
2023.03.03-07:56:29:306:[step-41200/88900: 46.34%]--[loss-3.180732: wl-5.199723, gl-1.880801]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:12:21]
2023.03.03-07:57:47:406:[step-41300/88900: 46.46%]--[loss-2.872482: wl-3.657845, gl-1.958021]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:13:01]
2023.03.03-07:59:04:506:[step-41400/88900: 46.57%]--[loss-2.862612: wl-4.121921, gl-1.832132]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:04:09]
2023.03.03-08:00:22:606:[step-41500/88900: 46.68%]--[loss-2.849429: wl-3.813008, gl-1.896177]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:21:35]
2023.03.03-08:01:39:706:[step-41600/88900: 46.79%]--[loss-2.919775: wl-4.100672, gl-1.894607]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:15:46]
2023.03.03-08:02:57:806:[step-41700/88900: 46.91%]--[loss-3.149473: wl-4.953382, gl-1.911127]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:01:23]
End of epoch 47 / 100: train_loss: 2.993 	 time: 695 sec
Saving the model at the end of epoch 47, iters 41783
2023.03.03-08:04:22:17:[step-41800/88900: 47.02%]--[loss-2.808979: wl-3.917004, gl-1.829728]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:03:08]
2023.03.03-08:05:39:117:[step-41900/88900: 47.13%]--[loss-2.951643: wl-4.033315, gl-1.943315]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:03:50]
2023.03.03-08:06:57:217:[step-42000/88900: 47.24%]--[loss-2.853142: wl-3.962694, gl-1.862469]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:07:05]
2023.03.03-08:08:16:317:[step-42100/88900: 47.36%]--[loss-2.920913: wl-4.135795, gl-1.886964]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:06:59]
2023.03.03-08:09:35:417:[step-42200/88900: 47.47%]--[loss-2.834258: wl-3.758009, gl-1.894756]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:02:38]
2023.03.03-08:11:03:517:[step-42300/88900: 47.58%]--[loss-3.188632: wl-5.031361, gl-1.930792]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:02:33]
2023.03.03-08:12:21:617:[step-42400/88900: 47.69%]--[loss-3.030769: wl-4.243368, gl-1.969927]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:01:30]
2023.03.03-08:13:47:717:[step-42500/88900: 47.81%]--[loss-3.267525: wl-4.987825, gl-2.020569]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:00:07]
2023.03.03-08:15:06:817:[step-42600/88900: 47.92%]--[loss-2.888517: wl-4.267256, gl-1.821703]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:19:33]
End of epoch 48 / 100: train_loss: 2.987 	 time: 730 sec
Saving the model at the end of epoch 48, iters 42672
2023.03.03-08:16:43:28:[step-42700/88900: 48.03%]--[loss-3.243000: wl-4.843755, gl-2.032062]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:07:41]
2023.03.03-08:18:11:128:[step-42800/88900: 48.14%]--[loss-2.919499: wl-3.915335, gl-1.940665]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:52:00]
2023.03.03-08:19:29:228:[step-42900/88900: 48.26%]--[loss-3.251682: wl-5.040948, gl-1.991445]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:51:01]
2023.03.03-08:20:58:328:[step-43000/88900: 48.37%]--[loss-3.126750: wl-4.454595, gl-2.013101]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:03:16]
2023.03.03-08:22:16:428:[step-43100/88900: 48.48%]--[loss-2.913036: wl-3.780284, gl-1.967965]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:07:15]
2023.03.03-08:23:45:528:[step-43200/88900: 48.59%]--[loss-3.009504: wl-4.211237, gl-1.956694]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:44:20]
2023.03.03-08:25:04:628:[step-43300/88900: 48.71%]--[loss-3.023228: wl-4.578921, gl-1.878497]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:57:38]
2023.03.03-08:26:31:728:[step-43400/88900: 48.82%]--[loss-2.967484: wl-4.271869, gl-1.899517]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:46:57]
2023.03.03-08:27:49:828:[step-43500/88900: 48.93%]--[loss-2.990573: wl-4.004091, gl-1.989551]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:43:05]
End of epoch 49 / 100: train_loss: 2.978 	 time: 747 sec
Saving the model at the end of epoch 49, iters 43561
2023.03.03-08:29:21:39:[step-43600/88900: 49.04%]--[loss-3.285065: wl-4.521932, gl-2.154583]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:55:45]
2023.03.03-08:30:40:139:[step-43700/88900: 49.16%]--[loss-3.392183: wl-5.383217, gl-2.046379]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:37:47]
2023.03.03-08:32:08:239:[step-43800/88900: 49.27%]--[loss-3.160247: wl-3.985195, gl-2.163949]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:49:15]
2023.03.03-08:33:26:339:[step-43900/88900: 49.38%]--[loss-3.101291: wl-3.874769, gl-2.132599]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:43:36]
2023.03.03-08:34:56:439:[step-44000/88900: 49.49%]--[loss-3.313735: wl-4.286514, gl-2.242106]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:40:56]
2023.03.03-08:36:13:539:[step-44100/88900: 49.61%]--[loss-2.881185: wl-3.766189, gl-1.939637]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:53:30]
2023.03.03-08:37:31:639:[step-44200/88900: 49.72%]--[loss-3.175287: wl-4.064762, gl-2.159096]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:41:55]
2023.03.03-08:38:48:739:[step-44300/88900: 49.83%]--[loss-3.242095: wl-4.271732, gl-2.174162]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:48:41]
2023.03.03-08:40:06:839:[step-44400/88900: 49.94%]--[loss-2.906881: wl-3.616763, gl-2.002690]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:42:27]
End of epoch 50 / 100: train_loss: 3.189 	 time: 723 sec
Saving the model at the end of epoch 50, iters 44450
2023.03.03-08:41:31:50:[step-44500/88900: 50.06%]--[loss-3.125077: wl-3.889121, gl-2.152797]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:28:54]
2023.03.03-08:42:48:150:[step-44600/88900: 50.17%]--[loss-3.200990: wl-4.400804, gl-2.100789]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:26:47]
2023.03.03-08:44:06:250:[step-44700/88900: 50.28%]--[loss-2.968348: wl-3.778019, gl-2.023843]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:24:57]
2023.03.03-08:45:24:350:[step-44800/88900: 50.39%]--[loss-2.953286: wl-3.719770, gl-2.023343]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:44:42]
2023.03.03-08:46:42:450:[step-44900/88900: 50.51%]--[loss-3.236611: wl-4.385341, gl-2.140276]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:24:31]
2023.03.03-08:48:00:550:[step-45000/88900: 50.62%]--[loss-3.309605: wl-4.169496, gl-2.267231]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:28:45]
2023.03.03-08:49:17:650:[step-45100/88900: 50.73%]--[loss-3.192500: wl-4.749238, gl-2.005190]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:34:40]
2023.03.03-08:50:35:750:[step-45200/88900: 50.84%]--[loss-2.977221: wl-4.113265, gl-1.948904]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:25:02]
2023.03.03-08:51:52:850:[step-45300/88900: 50.96%]--[loss-2.963845: wl-4.057661, gl-1.949430]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:22:59]
End of epoch 51 / 100: train_loss: 3.172 	 time: 697 sec
Saving the model at the end of epoch 51, iters 45339
2023.03.03-08:53:18:61:[step-45400/88900: 51.07%]--[loss-3.183715: wl-4.411570, gl-2.080822]--[lr: pb-0.000050, pf-0.000049]--[ETA-9:31:45]
2023.03.03-08:54:36:161:[step-45500/88900: 51.18%]--[loss-2.924304: wl-3.885563, gl-1.952913]--[lr: pb-0.000050, pf-0.000049]--[ETA-9:16:00]
2023.03.03-08:55:54:261:[step-45600/88900: 51.29%]--[loss-2.974561: wl-4.286734, gl-1.902877]--[lr: pb-0.000050, pf-0.000049]--[ETA-9:07:39]
2023.03.03-08:57:11:361:[step-45700/88900: 51.41%]--[loss-3.147086: wl-3.983026, gl-2.151330]--[lr: pb-0.000050, pf-0.000049]--[ETA-9:11:40]
2023.03.03-08:58:30:461:[step-45800/88900: 51.52%]--[loss-2.870772: wl-3.822113, gl-1.915244]--[lr: pb-0.000050, pf-0.000049]--[ETA-9:15:51]
2023.03.03-08:59:48:561:[step-45900/88900: 51.63%]--[loss-3.250414: wl-4.758211, gl-2.060862]--[lr: pb-0.000050, pf-0.000049]--[ETA-9:11:42]
2023.03.03-09:01:05:661:[step-46000/88900: 51.74%]--[loss-3.139250: wl-4.269084, gl-2.071979]--[lr: pb-0.000050, pf-0.000049]--[ETA-9:16:31]
2023.03.03-09:02:23:761:[step-46100/88900: 51.86%]--[loss-3.125877: wl-4.297227, gl-2.051570]--[lr: pb-0.000050, pf-0.000049]--[ETA-9:06:27]
2023.03.03-09:03:40:861:[step-46200/88900: 51.97%]--[loss-2.984659: wl-3.924712, gl-2.003481]--[lr: pb-0.000050, pf-0.000049]--[ETA-9:09:52]
End of epoch 52 / 100: train_loss: 3.151 	 time: 698 sec
Saving the model at the end of epoch 52, iters 46228
2023.03.03-09:05:06:72:[step-46300/88900: 52.08%]--[loss-3.032388: wl-4.052390, gl-2.019290]--[lr: pb-0.000050, pf-0.000048]--[ETA-9:13:58]
2023.03.03-09:06:24:172:[step-46400/88900: 52.19%]--[loss-2.969346: wl-3.993432, gl-1.970988]--[lr: pb-0.000050, pf-0.000048]--[ETA-9:15:59]
2023.03.03-09:07:41:272:[step-46500/88900: 52.31%]--[loss-3.167699: wl-4.679436, gl-1.997840]--[lr: pb-0.000050, pf-0.000048]--[ETA-9:16:17]
2023.03.03-09:08:59:372:[step-46600/88900: 52.42%]--[loss-3.119666: wl-4.272432, gl-2.051558]--[lr: pb-0.000050, pf-0.000048]--[ETA-9:10:39]
2023.03.03-09:10:17:472:[step-46700/88900: 52.53%]--[loss-3.139728: wl-4.022195, gl-2.134179]--[lr: pb-0.000050, pf-0.000048]--[ETA-9:02:57]
2023.03.03-09:11:34:572:[step-46800/88900: 52.64%]--[loss-3.311784: wl-4.249990, gl-2.249286]--[lr: pb-0.000050, pf-0.000048]--[ETA-8:57:22]
2023.03.03-09:12:52:672:[step-46900/88900: 52.76%]--[loss-3.337961: wl-4.732726, gl-2.154779]--[lr: pb-0.000050, pf-0.000048]--[ETA-8:57:10]
2023.03.03-09:14:09:772:[step-47000/88900: 52.87%]--[loss-3.038081: wl-4.020514, gl-2.032953]--[lr: pb-0.000050, pf-0.000048]--[ETA-8:52:29]
2023.03.03-09:15:27:872:[step-47100/88900: 52.98%]--[loss-3.325908: wl-4.306316, gl-2.249329]--[lr: pb-0.000050, pf-0.000048]--[ETA-9:04:26]
End of epoch 53 / 100: train_loss: 3.143 	 time: 697 sec
Saving the model at the end of epoch 53, iters 47117
2023.03.03-09:16:53:83:[step-47200/88900: 53.09%]--[loss-2.977445: wl-4.081431, gl-1.957087]--[lr: pb-0.000050, pf-0.000047]--[ETA-8:53:57]
2023.03.03-09:18:10:183:[step-47300/88900: 53.21%]--[loss-3.209664: wl-4.562562, gl-2.069024]--[lr: pb-0.000050, pf-0.000047]--[ETA-8:51:58]
2023.03.03-09:19:28:283:[step-47400/88900: 53.32%]--[loss-3.387915: wl-4.420953, gl-2.282676]--[lr: pb-0.000050, pf-0.000047]--[ETA-8:53:34]
2023.03.03-09:20:46:383:[step-47500/88900: 53.43%]--[loss-3.237087: wl-4.245161, gl-2.175797]--[lr: pb-0.000050, pf-0.000047]--[ETA-8:48:36]
2023.03.03-09:22:03:483:[step-47600/88900: 53.54%]--[loss-3.061665: wl-4.478246, gl-1.942103]--[lr: pb-0.000050, pf-0.000047]--[ETA-8:46:23]
2023.03.03-09:23:20:583:[step-47700/88900: 53.66%]--[loss-3.098581: wl-4.161078, gl-2.058311]--[lr: pb-0.000050, pf-0.000047]--[ETA-9:00:41]
2023.03.03-09:24:38:683:[step-47800/88900: 53.77%]--[loss-2.849231: wl-3.783252, gl-1.903419]--[lr: pb-0.000050, pf-0.000047]--[ETA-8:51:00]
2023.03.03-09:25:56:783:[step-47900/88900: 53.88%]--[loss-3.326740: wl-4.519678, gl-2.196820]--[lr: pb-0.000050, pf-0.000047]--[ETA-8:56:41]
2023.03.03-09:27:14:883:[step-48000/88900: 53.99%]--[loss-3.020729: wl-4.114953, gl-1.991990]--[lr: pb-0.000050, pf-0.000047]--[ETA-8:47:11]
End of epoch 54 / 100: train_loss: 3.133 	 time: 697 sec
Saving the model at the end of epoch 54, iters 48006
2023.03.03-09:28:40:94:[step-48100/88900: 54.11%]--[loss-3.270710: wl-4.265646, gl-2.204298]--[lr: pb-0.000050, pf-0.000046]--[ETA-8:46:28]
2023.03.03-09:29:58:194:[step-48200/88900: 54.22%]--[loss-3.053406: wl-4.118961, gl-2.023665]--[lr: pb-0.000050, pf-0.000046]--[ETA-8:45:38]
2023.03.03-09:31:17:294:[step-48300/88900: 54.33%]--[loss-3.067239: wl-3.800784, gl-2.117043]--[lr: pb-0.000050, pf-0.000046]--[ETA-8:50:33]
2023.03.03-09:32:45:394:[step-48400/88900: 54.44%]--[loss-3.108040: wl-4.200103, gl-2.058014]--[lr: pb-0.000050, pf-0.000046]--[ETA-8:57:11]
2023.03.03-09:34:09:494:[step-48500/88900: 54.56%]--[loss-2.764876: wl-3.591125, gl-1.867095]--[lr: pb-0.000050, pf-0.000046]--[ETA-1 day, 10:30:50]
2023.03.03-09:35:33:594:[step-48600/88900: 54.67%]--[loss-2.892743: wl-3.684035, gl-1.971734]--[lr: pb-0.000050, pf-0.000046]--[ETA-8:35:35]
2023.03.03-09:37:01:694:[step-48700/88900: 54.78%]--[loss-3.274822: wl-4.585108, gl-2.128545]--[lr: pb-0.000050, pf-0.000046]--[ETA-8:30:13]
2023.03.03-09:38:20:794:[step-48800/88900: 54.89%]--[loss-3.032003: wl-4.056851, gl-2.017790]--[lr: pb-0.000050, pf-0.000046]--[ETA-8:57:09]
End of epoch 55 / 100: train_loss: 3.114 	 time: 748 sec
Saving the model at the end of epoch 55, iters 48895
2023.03.03-09:39:59:5:[step-48900/88900: 55.01%]--[loss-2.954244: wl-3.883785, gl-1.983298]--[lr: pb-0.000050, pf-0.000045]--[ETA-10:23:50]
2023.03.03-09:41:19:105:[step-49000/88900: 55.12%]--[loss-3.327898: wl-4.855917, gl-2.113919]--[lr: pb-0.000050, pf-0.000045]--[ETA-8:56:34]
2023.03.03-09:42:50:205:[step-49100/88900: 55.23%]--[loss-3.376108: wl-4.468108, gl-2.259081]--[lr: pb-0.000050, pf-0.000045]--[ETA-8:37:21]
2023.03.03-09:44:09:305:[step-49200/88900: 55.34%]--[loss-2.905969: wl-3.802615, gl-1.955315]--[lr: pb-0.000050, pf-0.000045]--[ETA-9:18:27]
2023.03.03-09:45:39:405:[step-49300/88900: 55.46%]--[loss-3.112466: wl-3.720590, gl-2.182318]--[lr: pb-0.000050, pf-0.000045]--[ETA-8:37:21]
2023.03.03-09:46:58:505:[step-49400/88900: 55.57%]--[loss-3.035245: wl-4.106868, gl-2.008528]--[lr: pb-0.000050, pf-0.000045]--[ETA-8:32:25]
2023.03.03-09:48:35:605:[step-49500/88900: 55.68%]--[loss-3.335936: wl-4.873173, gl-2.117642]--[lr: pb-0.000050, pf-0.000045]--[ETA-8:31:45]
2023.03.03-09:49:54:705:[step-49600/88900: 55.79%]--[loss-3.031792: wl-3.915641, gl-2.052882]--[lr: pb-0.000050, pf-0.000045]--[ETA-8:47:28]
2023.03.03-09:51:23:805:[step-49700/88900: 55.91%]--[loss-3.290379: wl-4.618523, gl-2.135748]--[lr: pb-0.000050, pf-0.000045]--[ETA-8:21:06]
End of epoch 56 / 100: train_loss: 3.109 	 time: 760 sec
Saving the model at the end of epoch 56, iters 49784
2023.03.03-09:52:49:16:[step-49800/88900: 56.02%]--[loss-2.882560: wl-3.896051, gl-1.908547]--[lr: pb-0.000050, pf-0.000044]--[ETA-8:21:21]
2023.03.03-09:54:18:116:[step-49900/88900: 56.13%]--[loss-3.322389: wl-5.279700, gl-2.002464]--[lr: pb-0.000050, pf-0.000044]--[ETA-8:42:23]
2023.03.03-09:55:37:216:[step-50000/88900: 56.24%]--[loss-3.194034: wl-4.203148, gl-2.143247]--[lr: pb-0.000050, pf-0.000044]--[ETA-8:22:35]
2023.03.03-09:56:54:316:[step-50100/88900: 56.36%]--[loss-2.996744: wl-4.061064, gl-1.981478]--[lr: pb-0.000050, pf-0.000044]--[ETA-8:18:14]
2023.03.03-09:58:12:416:[step-50200/88900: 56.47%]--[loss-3.121014: wl-4.236601, gl-2.061864]--[lr: pb-0.000050, pf-0.000044]--[ETA-8:11:45]
2023.03.03-09:59:29:516:[step-50300/88900: 56.58%]--[loss-3.082456: wl-4.208440, gl-2.030346]--[lr: pb-0.000050, pf-0.000044]--[ETA-8:51:29]
2023.03.03-10:00:47:616:[step-50400/88900: 56.69%]--[loss-2.929909: wl-4.127168, gl-1.898117]--[lr: pb-0.000050, pf-0.000044]--[ETA-8:20:32]
2023.03.03-10:02:05:716:[step-50500/88900: 56.81%]--[loss-3.219288: wl-4.348160, gl-2.132248]--[lr: pb-0.000050, pf-0.000044]--[ETA-8:09:13]
2023.03.03-10:03:23:816:[step-50600/88900: 56.92%]--[loss-3.048997: wl-3.900481, gl-2.073877]--[lr: pb-0.000050, pf-0.000044]--[ETA-8:12:11]
End of epoch 57 / 100: train_loss: 3.090 	 time: 711 sec
Saving the model at the end of epoch 57, iters 50673
2023.03.03-10:04:49:27:[step-50700/88900: 57.03%]--[loss-3.214497: wl-4.428519, gl-2.107368]--[lr: pb-0.000050, pf-0.000043]--[ETA-8:16:35]
2023.03.03-10:06:08:127:[step-50800/88900: 57.14%]--[loss-2.963239: wl-3.972437, gl-1.970129]--[lr: pb-0.000050, pf-0.000043]--[ETA-8:38:36]
2023.03.03-10:07:26:227:[step-50900/88900: 57.26%]--[loss-3.143426: wl-4.441335, gl-2.033092]--[lr: pb-0.000050, pf-0.000043]--[ETA-8:25:09]
2023.03.03-10:08:53:327:[step-51000/88900: 57.37%]--[loss-3.166782: wl-4.479327, gl-2.046950]--[lr: pb-0.000050, pf-0.000043]--[ETA-8:28:27]
2023.03.03-10:10:12:427:[step-51100/88900: 57.48%]--[loss-3.092405: wl-4.284803, gl-2.021204]--[lr: pb-0.000050, pf-0.000043]--[ETA-8:09:47]
2023.03.03-10:11:41:527:[step-51200/88900: 57.59%]--[loss-2.874902: wl-3.848495, gl-1.912778]--[lr: pb-0.000050, pf-0.000043]--[ETA-8:08:27]
2023.03.03-10:13:00:627:[step-51300/88900: 57.71%]--[loss-3.097056: wl-3.893585, gl-2.123660]--[lr: pb-0.000050, pf-0.000043]--[ETA-8:29:18]
2023.03.03-10:14:28:727:[step-51400/88900: 57.82%]--[loss-3.060420: wl-3.827306, gl-2.103594]--[lr: pb-0.000050, pf-0.000043]--[ETA-8:06:20]
2023.03.03-10:15:50:827:[step-51500/88900: 57.93%]--[loss-3.316270: wl-4.404599, gl-2.215121]--[lr: pb-0.000050, pf-0.000043]--[ETA-8:27:24]
End of epoch 58 / 100: train_loss: 3.084 	 time: 740 sec
Saving the model at the end of epoch 58, iters 51562
2023.03.03-10:17:38:38:[step-51600/88900: 58.04%]--[loss-2.963032: wl-4.455791, gl-1.849084]--[lr: pb-0.000050, pf-0.000042]--[ETA-8:34:52]
2023.03.03-10:18:56:138:[step-51700/88900: 58.16%]--[loss-2.817486: wl-3.685376, gl-1.896142]--[lr: pb-0.000050, pf-0.000042]--[ETA-7:55:28]
2023.03.03-10:20:23:238:[step-51800/88900: 58.27%]--[loss-3.213886: wl-4.419185, gl-2.109090]--[lr: pb-0.000050, pf-0.000042]--[ETA-8:02:30]
2023.03.03-10:21:40:338:[step-51900/88900: 58.38%]--[loss-2.989378: wl-3.988626, gl-1.992221]--[lr: pb-0.000050, pf-0.000042]--[ETA-7:51:03]
2023.03.03-10:23:06:438:[step-52000/88900: 58.49%]--[loss-3.174475: wl-4.198589, gl-2.124828]--[lr: pb-0.000050, pf-0.000042]--[ETA-8:30:18]
2023.03.03-10:24:23:538:[step-52100/88900: 58.61%]--[loss-3.173370: wl-4.479479, gl-2.053500]--[lr: pb-0.000050, pf-0.000042]--[ETA-7:43:22]
2023.03.03-10:25:48:638:[step-52200/88900: 58.72%]--[loss-3.027771: wl-3.923172, gl-2.046978]--[lr: pb-0.000050, pf-0.000042]--[ETA-7:51:04]
2023.03.03-10:27:13:738:[step-52300/88900: 58.83%]--[loss-3.002120: wl-4.284239, gl-1.931060]--[lr: pb-0.000050, pf-0.000042]--[ETA-7:43:54]
2023.03.03-10:28:30:838:[step-52400/88900: 58.94%]--[loss-3.245468: wl-4.842002, gl-2.034968]--[lr: pb-0.000050, pf-0.000042]--[ETA-7:42:17]
End of epoch 59 / 100: train_loss: 3.077 	 time: 748 sec
Saving the model at the end of epoch 59, iters 52451
2023.03.03-10:30:02:49:[step-52500/88900: 59.06%]--[loss-3.293143: wl-4.065725, gl-2.276711]--[lr: pb-0.000050, pf-0.000041]--[ETA-8:09:40]
2023.03.03-10:31:20:149:[step-52600/88900: 59.17%]--[loss-3.274224: wl-4.435894, gl-2.165251]--[lr: pb-0.000050, pf-0.000041]--[ETA-7:49:00]
2023.03.03-10:32:46:249:[step-52700/88900: 59.28%]--[loss-3.280649: wl-4.831209, gl-2.072847]--[lr: pb-0.000050, pf-0.000041]--[ETA-7:43:31]
2023.03.03-10:34:03:349:[step-52800/88900: 59.39%]--[loss-3.105996: wl-4.098674, gl-2.081328]--[lr: pb-0.000050, pf-0.000041]--[ETA-7:40:47]
2023.03.03-10:35:27:449:[step-52900/88900: 59.51%]--[loss-2.689736: wl-3.554654, gl-1.801073]--[lr: pb-0.000050, pf-0.000041]--[ETA-7:45:53]
2023.03.03-10:36:45:549:[step-53000/88900: 59.62%]--[loss-2.874822: wl-3.694973, gl-1.951079]--[lr: pb-0.000050, pf-0.000041]--[ETA-7:43:27]
2023.03.03-10:38:08:649:[step-53100/88900: 59.73%]--[loss-3.144287: wl-4.400709, gl-2.044109]--[lr: pb-0.000050, pf-0.000041]--[ETA-7:33:47]
2023.03.03-10:39:24:749:[step-53200/88900: 59.84%]--[loss-3.013400: wl-3.812925, gl-2.060169]--[lr: pb-0.000050, pf-0.000041]--[ETA-7:28:24]
2023.03.03-10:40:40:849:[step-53300/88900: 59.96%]--[loss-2.832473: wl-3.950186, gl-1.844926]--[lr: pb-0.000050, pf-0.000041]--[ETA-7:29:01]
End of epoch 60 / 100: train_loss: 3.063 	 time: 719 sec
Saving the model at the end of epoch 60, iters 53340
2023.03.03-10:42:03:60:[step-53400/88900: 60.07%]--[loss-2.994827: wl-4.026805, gl-1.988126]--[lr: pb-0.000050, pf-0.000040]--[ETA-7:27:03]
2023.03.03-10:43:19:160:[step-53500/88900: 60.18%]--[loss-2.964498: wl-3.641474, gl-2.054129]--[lr: pb-0.000050, pf-0.000040]--[ETA-7:27:11]
2023.03.03-10:44:35:260:[step-53600/88900: 60.29%]--[loss-3.113276: wl-4.018806, gl-2.108574]--[lr: pb-0.000050, pf-0.000040]--[ETA-7:26:50]
2023.03.03-10:45:52:360:[step-53700/88900: 60.40%]--[loss-2.929515: wl-3.661351, gl-2.014177]--[lr: pb-0.000050, pf-0.000040]--[ETA-7:26:35]
2023.03.03-10:47:07:460:[step-53800/88900: 60.52%]--[loss-2.908660: wl-4.025821, gl-1.902205]--[lr: pb-0.000050, pf-0.000040]--[ETA-7:20:34]
2023.03.03-10:48:23:560:[step-53900/88900: 60.63%]--[loss-3.204252: wl-4.493232, gl-2.080944]--[lr: pb-0.000050, pf-0.000040]--[ETA-7:23:48]
2023.03.03-10:49:39:660:[step-54000/88900: 60.74%]--[loss-3.051307: wl-4.216346, gl-1.997221]--[lr: pb-0.000050, pf-0.000040]--[ETA-7:19:55]
2023.03.03-10:50:55:760:[step-54100/88900: 60.85%]--[loss-2.884254: wl-4.004097, gl-1.883230]--[lr: pb-0.000050, pf-0.000040]--[ETA-7:25:48]
2023.03.03-10:52:12:860:[step-54200/88900: 60.97%]--[loss-2.966598: wl-3.740199, gl-2.031548]--[lr: pb-0.000050, pf-0.000040]--[ETA-7:15:44]
End of epoch 61 / 100: train_loss: 3.058 	 time: 683 sec
Saving the model at the end of epoch 61, iters 54229
2023.03.03-10:53:36:71:[step-54300/88900: 61.08%]--[loss-2.884549: wl-3.912787, gl-1.906352]--[lr: pb-0.000050, pf-0.000039]--[ETA-7:17:34]
2023.03.03-10:54:52:171:[step-54400/88900: 61.19%]--[loss-3.148840: wl-4.233939, gl-2.090356]--[lr: pb-0.000050, pf-0.000039]--[ETA-7:16:47]
2023.03.03-10:56:08:271:[step-54500/88900: 61.30%]--[loss-2.987010: wl-4.270961, gl-1.919270]--[lr: pb-0.000050, pf-0.000039]--[ETA-7:20:24]
2023.03.03-10:57:24:371:[step-54600/88900: 61.42%]--[loss-3.377219: wl-4.964926, gl-2.135988]--[lr: pb-0.000050, pf-0.000039]--[ETA-7:17:19]
2023.03.03-10:58:40:471:[step-54700/88900: 61.53%]--[loss-3.024940: wl-4.296958, gl-1.950701]--[lr: pb-0.000050, pf-0.000039]--[ETA-7:10:04]
2023.03.03-10:59:56:571:[step-54800/88900: 61.64%]--[loss-3.079239: wl-4.211847, gl-2.026277]--[lr: pb-0.000050, pf-0.000039]--[ETA-7:11:02]
2023.03.03-11:01:12:671:[step-54900/88900: 61.75%]--[loss-3.074877: wl-3.854699, gl-2.111202]--[lr: pb-0.000050, pf-0.000039]--[ETA-7:07:22]
2023.03.03-11:02:27:771:[step-55000/88900: 61.87%]--[loss-3.324556: wl-5.114769, gl-2.045864]--[lr: pb-0.000050, pf-0.000039]--[ETA-7:06:46]
2023.03.03-11:03:43:871:[step-55100/88900: 61.98%]--[loss-3.262439: wl-4.611450, gl-2.109576]--[lr: pb-0.000050, pf-0.000039]--[ETA-7:04:02]
End of epoch 62 / 100: train_loss: 3.047 	 time: 682 sec
Saving the model at the end of epoch 62, iters 55118
2023.03.03-11:05:07:82:[step-55200/88900: 62.09%]--[loss-3.010620: wl-4.014484, gl-2.006999]--[lr: pb-0.000050, pf-0.000038]--[ETA-7:03:08]
2023.03.03-11:06:23:182:[step-55300/88900: 62.20%]--[loss-2.809858: wl-3.769719, gl-1.867428]--[lr: pb-0.000050, pf-0.000038]--[ETA-7:07:22]
2023.03.03-11:07:38:282:[step-55400/88900: 62.32%]--[loss-3.149359: wl-4.194365, gl-2.100767]--[lr: pb-0.000050, pf-0.000038]--[ETA-7:01:18]
2023.03.03-11:08:54:382:[step-55500/88900: 62.43%]--[loss-3.030365: wl-3.816447, gl-2.076253]--[lr: pb-0.000050, pf-0.000038]--[ETA-6:57:38]
2023.03.03-11:10:10:482:[step-55600/88900: 62.54%]--[loss-3.124930: wl-4.444204, gl-2.013879]--[lr: pb-0.000050, pf-0.000038]--[ETA-6:58:52]
2023.03.03-11:11:26:582:[step-55700/88900: 62.65%]--[loss-3.227458: wl-4.508305, gl-2.100382]--[lr: pb-0.000050, pf-0.000038]--[ETA-7:01:09]
2023.03.03-11:12:41:682:[step-55800/88900: 62.77%]--[loss-3.060194: wl-4.564324, gl-1.919113]--[lr: pb-0.000050, pf-0.000038]--[ETA-6:55:48]
2023.03.03-11:13:57:782:[step-55900/88900: 62.88%]--[loss-3.427981: wl-5.560981, gl-2.037736]--[lr: pb-0.000050, pf-0.000038]--[ETA-7:01:34]
2023.03.03-11:15:13:882:[step-56000/88900: 62.99%]--[loss-3.145087: wl-4.454752, gl-2.031399]--[lr: pb-0.000050, pf-0.000038]--[ETA-6:52:54]
End of epoch 63 / 100: train_loss: 3.040 	 time: 680 sec
Saving the model at the end of epoch 63, iters 56007
2023.03.03-11:16:36:93:[step-56100/88900: 63.10%]--[loss-2.934966: wl-3.879652, gl-1.965053]--[lr: pb-0.000050, pf-0.000037]--[ETA-6:53:52]
2023.03.03-11:17:52:193:[step-56200/88900: 63.22%]--[loss-3.569778: wl-4.902453, gl-2.344165]--[lr: pb-0.000050, pf-0.000037]--[ETA-6:53:33]
2023.03.03-11:19:08:293:[step-56300/88900: 63.33%]--[loss-3.126435: wl-4.019625, gl-2.121529]--[lr: pb-0.000050, pf-0.000037]--[ETA-6:49:36]
2023.03.03-11:20:24:393:[step-56400/88900: 63.44%]--[loss-2.797246: wl-3.517121, gl-1.917966]--[lr: pb-0.000050, pf-0.000037]--[ETA-6:50:48]
2023.03.03-11:21:39:493:[step-56500/88900: 63.55%]--[loss-3.052398: wl-4.138258, gl-2.017834]--[lr: pb-0.000050, pf-0.000037]--[ETA-6:51:48]
2023.03.03-11:22:55:593:[step-56600/88900: 63.67%]--[loss-2.927133: wl-4.095024, gl-1.903377]--[lr: pb-0.000050, pf-0.000037]--[ETA-6:51:01]
2023.03.03-11:24:11:693:[step-56700/88900: 63.78%]--[loss-2.833080: wl-3.637616, gl-1.923676]--[lr: pb-0.000050, pf-0.000037]--[ETA-6:45:11]
2023.03.03-11:25:27:793:[step-56800/88900: 63.89%]--[loss-3.480847: wl-4.847570, gl-2.268955]--[lr: pb-0.000050, pf-0.000037]--[ETA-6:44:49]
End of epoch 64 / 100: train_loss: 3.031 	 time: 681 sec
Saving the model at the end of epoch 64, iters 56896
2023.03.03-11:26:51:4:[step-56900/88900: 64.00%]--[loss-3.032691: wl-3.893805, gl-2.059240]--[lr: pb-0.000050, pf-0.000036]--[ETA-6:57:32]
2023.03.03-11:28:07:104:[step-57000/88900: 64.12%]--[loss-2.856397: wl-3.939915, gl-1.871418]--[lr: pb-0.000050, pf-0.000036]--[ETA-6:44:07]
2023.03.03-11:29:23:204:[step-57100/88900: 64.23%]--[loss-2.785306: wl-3.817900, gl-1.830831]--[lr: pb-0.000050, pf-0.000036]--[ETA-6:46:46]
2023.03.03-11:30:39:304:[step-57200/88900: 64.34%]--[loss-3.035937: wl-4.236355, gl-1.976848]--[lr: pb-0.000050, pf-0.000036]--[ETA-6:38:46]
2023.03.03-11:31:54:404:[step-57300/88900: 64.45%]--[loss-2.977585: wl-3.714891, gl-2.048862]--[lr: pb-0.000050, pf-0.000036]--[ETA-6:36:29]
2023.03.03-11:33:10:504:[step-57400/88900: 64.57%]--[loss-3.165036: wl-4.084700, gl-2.143861]--[lr: pb-0.000050, pf-0.000036]--[ETA-6:42:50]
2023.03.03-11:34:26:604:[step-57500/88900: 64.68%]--[loss-2.889036: wl-4.309505, gl-1.811660]--[lr: pb-0.000050, pf-0.000036]--[ETA-6:34:38]
2023.03.03-11:35:42:704:[step-57600/88900: 64.79%]--[loss-2.928741: wl-3.864292, gl-1.962668]--[lr: pb-0.000050, pf-0.000036]--[ETA-6:33:52]
2023.03.03-11:36:58:804:[step-57700/88900: 64.90%]--[loss-3.071234: wl-4.163434, gl-2.030375]--[lr: pb-0.000050, pf-0.000036]--[ETA-6:35:56]
End of epoch 65 / 100: train_loss: 3.015 	 time: 681 sec
Saving the model at the end of epoch 65, iters 57785
2023.03.03-11:38:21:15:[step-57800/88900: 65.02%]--[loss-2.763055: wl-3.826437, gl-1.806446]--[lr: pb-0.000050, pf-0.000035]--[ETA-6:37:10]
2023.03.03-11:39:37:115:[step-57900/88900: 65.13%]--[loss-3.156598: wl-4.128534, gl-2.124464]--[lr: pb-0.000050, pf-0.000035]--[ETA-6:34:16]
2023.03.03-11:40:53:215:[step-58000/88900: 65.24%]--[loss-3.157825: wl-4.109779, gl-2.130381]--[lr: pb-0.000050, pf-0.000035]--[ETA-6:35:18]
2023.03.03-11:42:10:315:[step-58100/88900: 65.35%]--[loss-3.149421: wl-4.446267, gl-2.037854]--[lr: pb-0.000050, pf-0.000035]--[ETA-6:30:59]
2023.03.03-11:43:26:415:[step-58200/88900: 65.47%]--[loss-2.944385: wl-3.788278, gl-1.997315]--[lr: pb-0.000050, pf-0.000035]--[ETA-6:25:32]
2023.03.03-11:44:41:515:[step-58300/88900: 65.58%]--[loss-2.964792: wl-4.576150, gl-1.820754]--[lr: pb-0.000050, pf-0.000035]--[ETA-6:24:56]
2023.03.03-11:45:57:615:[step-58400/88900: 65.69%]--[loss-3.128563: wl-4.072956, gl-2.110324]--[lr: pb-0.000050, pf-0.000035]--[ETA-6:22:45]
2023.03.03-11:47:13:715:[step-58500/88900: 65.80%]--[loss-2.859930: wl-3.848395, gl-1.897831]--[lr: pb-0.000050, pf-0.000035]--[ETA-6:22:04]
2023.03.03-11:48:29:815:[step-58600/88900: 65.92%]--[loss-3.153658: wl-4.294550, gl-2.080021]--[lr: pb-0.000050, pf-0.000035]--[ETA-6:27:51]
End of epoch 66 / 100: train_loss: 3.010 	 time: 682 sec
Saving the model at the end of epoch 66, iters 58674
2023.03.03-11:49:53:26:[step-58700/88900: 66.03%]--[loss-3.123330: wl-3.892670, gl-2.150162]--[lr: pb-0.000050, pf-0.000034]--[ETA-6:18:00]
2023.03.03-11:51:08:126:[step-58800/88900: 66.14%]--[loss-2.837275: wl-3.888614, gl-1.865121]--[lr: pb-0.000050, pf-0.000034]--[ETA-6:19:27]
2023.03.03-11:52:24:226:[step-58900/88900: 66.25%]--[loss-3.087628: wl-4.758296, gl-1.898054]--[lr: pb-0.000050, pf-0.000034]--[ETA-6:16:43]
2023.03.03-11:53:40:326:[step-59000/88900: 66.37%]--[loss-2.853566: wl-3.743400, gl-1.917716]--[lr: pb-0.000050, pf-0.000034]--[ETA-6:20:40]
2023.03.03-11:54:57:426:[step-59100/88900: 66.48%]--[loss-2.935136: wl-4.078424, gl-1.915531]--[lr: pb-0.000050, pf-0.000034]--[ETA-6:16:35]
2023.03.03-11:56:13:526:[step-59200/88900: 66.59%]--[loss-2.880530: wl-3.874030, gl-1.912022]--[lr: pb-0.000050, pf-0.000034]--[ETA-6:15:25]
2023.03.03-11:57:29:626:[step-59300/88900: 66.70%]--[loss-3.117783: wl-3.997223, gl-2.118477]--[lr: pb-0.000050, pf-0.000034]--[ETA-6:29:16]
2023.03.03-11:58:45:726:[step-59400/88900: 66.82%]--[loss-3.030229: wl-3.886494, gl-2.058606]--[lr: pb-0.000050, pf-0.000034]--[ETA-6:11:23]
2023.03.03-12:00:01:826:[step-59500/88900: 66.93%]--[loss-3.067492: wl-4.150068, gl-2.029975]--[lr: pb-0.000050, pf-0.000034]--[ETA-6:12:36]
End of epoch 67 / 100: train_loss: 3.000 	 time: 683 sec
Saving the model at the end of epoch 67, iters 59563
2023.03.03-12:01:25:37:[step-59600/88900: 67.04%]--[loss-2.810339: wl-4.136092, gl-1.776316]--[lr: pb-0.000050, pf-0.000033]--[ETA-6:13:15]
2023.03.03-12:02:41:137:[step-59700/88900: 67.15%]--[loss-3.213628: wl-4.924297, gl-1.982554]--[lr: pb-0.000050, pf-0.000033]--[ETA-6:03:57]
2023.03.03-12:03:57:237:[step-59800/88900: 67.27%]--[loss-2.975707: wl-4.060945, gl-1.960471]--[lr: pb-0.000050, pf-0.000033]--[ETA-6:08:05]
2023.03.03-12:05:13:337:[step-59900/88900: 67.38%]--[loss-2.888254: wl-3.585646, gl-1.991842]--[lr: pb-0.000050, pf-0.000033]--[ETA-6:06:08]
2023.03.03-12:06:29:437:[step-60000/88900: 67.49%]--[loss-3.141238: wl-4.284260, gl-2.070173]--[lr: pb-0.000050, pf-0.000033]--[ETA-6:06:40]
2023.03.03-12:07:46:537:[step-60100/88900: 67.60%]--[loss-2.983542: wl-3.828100, gl-2.026517]--[lr: pb-0.000050, pf-0.000033]--[ETA-6:03:49]
2023.03.03-12:09:02:637:[step-60200/88900: 67.72%]--[loss-2.926659: wl-4.085120, gl-1.905379]--[lr: pb-0.000050, pf-0.000033]--[ETA-6:04:23]
2023.03.03-12:10:18:737:[step-60300/88900: 67.83%]--[loss-2.722048: wl-3.808690, gl-1.769875]--[lr: pb-0.000050, pf-0.000033]--[ETA-6:21:23]
2023.03.03-12:11:34:837:[step-60400/88900: 67.94%]--[loss-3.012825: wl-4.139799, gl-1.977875]--[lr: pb-0.000050, pf-0.000033]--[ETA-5:59:47]
End of epoch 68 / 100: train_loss: 2.994 	 time: 683 sec
Saving the model at the end of epoch 68, iters 60452
2023.03.03-12:12:58:48:[step-60500/88900: 68.05%]--[loss-2.863071: wl-4.064834, gl-1.846863]--[lr: pb-0.000050, pf-0.000032]--[ETA-5:58:51]
2023.03.03-12:14:14:148:[step-60600/88900: 68.17%]--[loss-2.927810: wl-3.947363, gl-1.940969]--[lr: pb-0.000050, pf-0.000032]--[ETA-5:59:21]
2023.03.03-12:15:30:248:[step-60700/88900: 68.28%]--[loss-3.006857: wl-4.144462, gl-1.970742]--[lr: pb-0.000050, pf-0.000032]--[ETA-5:54:58]
2023.03.03-12:16:47:348:[step-60800/88900: 68.39%]--[loss-3.005897: wl-4.131473, gl-1.973029]--[lr: pb-0.000050, pf-0.000032]--[ETA-5:53:41]
2023.03.03-12:18:02:448:[step-60900/88900: 68.50%]--[loss-3.222602: wl-4.114454, gl-2.193989]--[lr: pb-0.000050, pf-0.000032]--[ETA-5:54:35]
2023.03.03-12:19:18:548:[step-61000/88900: 68.62%]--[loss-2.860161: wl-3.673079, gl-1.941891]--[lr: pb-0.000050, pf-0.000032]--[ETA-5:53:00]
2023.03.03-12:20:34:648:[step-61100/88900: 68.73%]--[loss-2.865305: wl-3.879373, gl-1.895462]--[lr: pb-0.000050, pf-0.000032]--[ETA-5:50:09]
2023.03.03-12:21:50:748:[step-61200/88900: 68.84%]--[loss-2.856712: wl-3.958158, gl-1.867172]--[lr: pb-0.000050, pf-0.000032]--[ETA-5:48:41]
2023.03.03-12:23:31:848:[step-61300/88900: 68.95%]--[loss-2.892455: wl-3.672043, gl-1.974445]--[lr: pb-0.000050, pf-0.000032]--[ETA-5:51:08]
End of epoch 69 / 100: train_loss: 2.985 	 time: 707 sec
Saving the model at the end of epoch 69, iters 61341
2023.03.03-12:25:18:59:[step-61400/88900: 69.07%]--[loss-3.142801: wl-4.364581, gl-2.051655]--[lr: pb-0.000050, pf-0.000031]--[ETA-9:09:05]
2023.03.03-12:27:08:159:[step-61500/88900: 69.18%]--[loss-3.263097: wl-4.839516, gl-2.053218]--[lr: pb-0.000050, pf-0.000031]--[ETA-5:44:23]
2023.03.03-12:28:55:259:[step-61600/88900: 69.29%]--[loss-2.882236: wl-3.842497, gl-1.921612]--[lr: pb-0.000050, pf-0.000031]--[ETA-5:41:02]
2023.03.03-12:30:11:359:[step-61700/88900: 69.40%]--[loss-3.190403: wl-4.465883, gl-2.073933]--[lr: pb-0.000050, pf-0.000031]--[ETA-5:43:57]
2023.03.03-12:31:27:459:[step-61800/88900: 69.52%]--[loss-3.172105: wl-4.507973, gl-2.045112]--[lr: pb-0.000050, pf-0.000031]--[ETA-5:41:41]
2023.03.03-12:32:42:559:[step-61900/88900: 69.63%]--[loss-2.935367: wl-3.848261, gl-1.973302]--[lr: pb-0.000050, pf-0.000031]--[ETA-5:42:51]
2023.03.03-12:33:59:659:[step-62000/88900: 69.74%]--[loss-3.077846: wl-4.256505, gl-2.013720]--[lr: pb-0.000050, pf-0.000031]--[ETA-5:42:00]
2023.03.03-12:35:15:759:[step-62100/88900: 69.85%]--[loss-2.851345: wl-3.985414, gl-1.854991]--[lr: pb-0.000050, pf-0.000031]--[ETA-5:40:24]
2023.03.03-12:36:32:859:[step-62200/88900: 69.97%]--[loss-3.003894: wl-4.097502, gl-1.979519]--[lr: pb-0.000050, pf-0.000031]--[ETA-5:37:43]
End of epoch 70 / 100: train_loss: 2.975 	 time: 772 sec
Saving the model at the end of epoch 70, iters 62230
2023.03.03-12:37:56:70:[step-62300/88900: 70.08%]--[loss-3.382265: wl-4.809689, gl-2.179843]--[lr: pb-0.000050, pf-0.000030]--[ETA-5:40:30]
2023.03.03-12:39:12:170:[step-62400/88900: 70.19%]--[loss-2.860992: wl-3.960513, gl-1.870864]--[lr: pb-0.000050, pf-0.000030]--[ETA-5:35:31]
2023.03.03-12:40:28:270:[step-62500/88900: 70.30%]--[loss-3.053662: wl-4.177856, gl-2.009198]--[lr: pb-0.000050, pf-0.000030]--[ETA-5:37:20]
2023.03.03-12:41:44:370:[step-62600/88900: 70.42%]--[loss-3.135208: wl-4.299386, gl-2.060362]--[lr: pb-0.000050, pf-0.000030]--[ETA-5:39:38]
2023.03.03-12:43:01:470:[step-62700/88900: 70.53%]--[loss-2.680289: wl-3.671834, gl-1.762330]--[lr: pb-0.000050, pf-0.000030]--[ETA-5:36:21]
2023.03.03-12:44:18:570:[step-62800/88900: 70.64%]--[loss-3.045661: wl-4.260001, gl-1.980661]--[lr: pb-0.000050, pf-0.000030]--[ETA-5:36:31]
2023.03.03-12:45:41:670:[step-62900/88900: 70.75%]--[loss-2.983505: wl-4.245840, gl-1.922045]--[lr: pb-0.000050, pf-0.000030]--[ETA-5:31:19]
2023.03.03-12:46:58:770:[step-63000/88900: 70.87%]--[loss-2.984883: wl-4.284827, gl-1.913676]--[lr: pb-0.000050, pf-0.000030]--[ETA-5:32:56]
2023.03.03-12:48:15:870:[step-63100/88900: 70.98%]--[loss-3.000325: wl-4.423755, gl-1.894387]--[lr: pb-0.000050, pf-0.000030]--[ETA-5:31:07]
End of epoch 71 / 100: train_loss: 2.970 	 time: 694 sec
Saving the model at the end of epoch 71, iters 63119
2023.03.03-12:49:40:81:[step-63200/88900: 71.09%]--[loss-2.974174: wl-4.104994, gl-1.947926]--[lr: pb-0.000050, pf-0.000029]--[ETA-5:25:33]
2023.03.03-12:50:56:181:[step-63300/88900: 71.20%]--[loss-2.802094: wl-3.648185, gl-1.890047]--[lr: pb-0.000050, pf-0.000029]--[ETA-5:23:15]
2023.03.03-12:52:13:281:[step-63400/88900: 71.32%]--[loss-2.904670: wl-3.881926, gl-1.934188]--[lr: pb-0.000050, pf-0.000029]--[ETA-5:24:46]
2023.03.03-12:53:29:381:[step-63500/88900: 71.43%]--[loss-2.940912: wl-3.918305, gl-1.961336]--[lr: pb-0.000050, pf-0.000029]--[ETA-5:23:50]
2023.03.03-12:54:46:481:[step-63600/88900: 71.54%]--[loss-2.965892: wl-3.947113, gl-1.979114]--[lr: pb-0.000050, pf-0.000029]--[ETA-5:22:41]
2023.03.03-12:56:04:581:[step-63700/88900: 71.65%]--[loss-2.897945: wl-3.856125, gl-1.933914]--[lr: pb-0.000050, pf-0.000029]--[ETA-5:28:48]
2023.03.03-12:57:23:681:[step-63800/88900: 71.77%]--[loss-2.959007: wl-4.191888, gl-1.911035]--[lr: pb-0.000050, pf-0.000029]--[ETA-5:26:21]
2023.03.03-12:58:41:781:[step-63900/88900: 71.88%]--[loss-3.282818: wl-4.779318, gl-2.087988]--[lr: pb-0.000050, pf-0.000029]--[ETA-5:29:29]
2023.03.03-12:59:59:881:[step-64000/88900: 71.99%]--[loss-2.938597: wl-4.009438, gl-1.936237]--[lr: pb-0.000050, pf-0.000029]--[ETA-5:20:31]
End of epoch 72 / 100: train_loss: 2.957 	 time: 695 sec
Saving the model at the end of epoch 72, iters 64008
2023.03.03-13:01:26:92:[step-64100/88900: 72.10%]--[loss-2.911539: wl-4.303754, gl-1.835601]--[lr: pb-0.000050, pf-0.000028]--[ETA-5:23:07]
2023.03.03-13:02:44:192:[step-64200/88900: 72.22%]--[loss-2.986393: wl-3.929853, gl-2.003930]--[lr: pb-0.000050, pf-0.000028]--[ETA-5:21:42]
2023.03.03-13:04:16:292:[step-64300/88900: 72.33%]--[loss-3.086634: wl-4.211432, gl-2.033776]--[lr: pb-0.000050, pf-0.000028]--[ETA-5:24:20]
2023.03.03-13:05:34:392:[step-64400/88900: 72.44%]--[loss-2.612209: wl-3.576999, gl-1.717960]--[lr: pb-0.000050, pf-0.000028]--[ETA-5:18:53]
2023.03.03-13:06:53:492:[step-64500/88900: 72.55%]--[loss-2.779178: wl-3.871332, gl-1.811345]--[lr: pb-0.000050, pf-0.000028]--[ETA-5:25:54]
2023.03.03-13:08:14:592:[step-64600/88900: 72.67%]--[loss-2.817869: wl-4.101887, gl-1.792397]--[lr: pb-0.000050, pf-0.000028]--[ETA-5:30:32]
2023.03.03-13:09:48:692:[step-64700/88900: 72.78%]--[loss-3.460886: wl-5.344109, gl-2.124859]--[lr: pb-0.000050, pf-0.000028]--[ETA-5:16:54]
2023.03.03-13:11:08:792:[step-64800/88900: 72.89%]--[loss-2.865434: wl-3.985374, gl-1.869090]--[lr: pb-0.000050, pf-0.000028]--[ETA-5:33:35]
End of epoch 73 / 100: train_loss: 2.952 	 time: 739 sec
Saving the model at the end of epoch 73, iters 64897
2023.03.03-13:12:36:3:[step-64900/88900: 73.00%]--[loss-2.888622: wl-4.003007, gl-1.887870]--[lr: pb-0.000050, pf-0.000027]--[ETA-5:36:50]
2023.03.03-13:14:14:103:[step-65000/88900: 73.12%]--[loss-2.980364: wl-3.780336, gl-2.035280]--[lr: pb-0.000050, pf-0.000027]--[ETA-5:19:59]
2023.03.03-13:15:35:203:[step-65100/88900: 73.23%]--[loss-3.064752: wl-4.846036, gl-1.853243]--[lr: pb-0.000050, pf-0.000027]--[ETA-5:37:14]
2023.03.03-13:17:10:303:[step-65200/88900: 73.34%]--[loss-3.093426: wl-4.484620, gl-1.972271]--[lr: pb-0.000050, pf-0.000027]--[ETA-5:08:41]
2023.03.03-13:18:30:403:[step-65300/88900: 73.45%]--[loss-2.819911: wl-3.910179, gl-1.842366]--[lr: pb-0.000050, pf-0.000027]--[ETA-5:08:44]
2023.03.03-13:19:50:503:[step-65400/88900: 73.57%]--[loss-2.759299: wl-3.739748, gl-1.824362]--[lr: pb-0.000050, pf-0.000027]--[ETA-5:08:33]
2023.03.03-13:21:21:603:[step-65500/88900: 73.68%]--[loss-3.296279: wl-4.891145, gl-2.073493]--[lr: pb-0.000050, pf-0.000027]--[ETA-5:14:48]
2023.03.03-13:22:41:703:[step-65600/88900: 73.79%]--[loss-2.742032: wl-3.842858, gl-1.781317]--[lr: pb-0.000050, pf-0.000027]--[ETA-5:23:36]
2023.03.03-13:24:01:803:[step-65700/88900: 73.90%]--[loss-3.149579: wl-4.726476, gl-1.967960]--[lr: pb-0.000050, pf-0.000027]--[ETA-5:03:34]
End of epoch 74 / 100: train_loss: 2.938 	 time: 777 sec
Saving the model at the end of epoch 74, iters 65786
2023.03.03-13:25:44:14:[step-65800/88900: 74.02%]--[loss-2.932779: wl-3.989792, gl-1.935331]--[lr: pb-0.000050, pf-0.000026]--[ETA-5:10:45]
2023.03.03-13:27:04:114:[step-65900/88900: 74.13%]--[loss-3.007736: wl-4.155731, gl-1.968803]--[lr: pb-0.000050, pf-0.000026]--[ETA-5:03:10]
2023.03.03-13:28:38:214:[step-66000/88900: 74.24%]--[loss-2.796892: wl-4.094646, gl-1.773230]--[lr: pb-0.000050, pf-0.000026]--[ETA-5:02:39]
2023.03.03-13:29:58:314:[step-66100/88900: 74.35%]--[loss-3.141928: wl-4.401693, gl-2.041504]--[lr: pb-0.000050, pf-0.000026]--[ETA-4:59:36]
2023.03.03-13:31:18:414:[step-66200/88900: 74.47%]--[loss-2.836062: wl-3.831475, gl-1.878193]--[lr: pb-0.000050, pf-0.000026]--[ETA-4:55:43]
2023.03.03-13:32:50:514:[step-66300/88900: 74.58%]--[loss-2.817640: wl-3.879433, gl-1.847782]--[lr: pb-0.000050, pf-0.000026]--[ETA-4:59:18]
2023.03.03-13:34:11:614:[step-66400/88900: 74.69%]--[loss-2.927350: wl-3.780345, gl-1.982263]--[lr: pb-0.000050, pf-0.000026]--[ETA-4:55:32]
2023.03.03-13:35:45:714:[step-66500/88900: 74.80%]--[loss-2.755170: wl-3.497132, gl-1.880887]--[lr: pb-0.000050, pf-0.000026]--[ETA-5:13:04]
2023.03.03-13:37:05:814:[step-66600/88900: 74.92%]--[loss-2.748106: wl-3.521017, gl-1.867852]--[lr: pb-0.000050, pf-0.000026]--[ETA-4:57:50]
End of epoch 75 / 100: train_loss: 2.932 	 time: 761 sec
Saving the model at the end of epoch 75, iters 66675
2023.03.03-13:38:37:25:[step-66700/88900: 75.03%]--[loss-3.069391: wl-4.442059, gl-1.958877]--[lr: pb-0.000050, pf-0.000025]--[ETA-21:41:15]
2023.03.03-13:40:09:125:[step-66800/88900: 75.14%]--[loss-2.978684: wl-4.415482, gl-1.874814]--[lr: pb-0.000050, pf-0.000025]--[ETA-4:53:34]
2023.03.03-13:41:29:225:[step-66900/88900: 75.25%]--[loss-3.233607: wl-4.339144, gl-2.148821]--[lr: pb-0.000050, pf-0.000025]--[ETA-4:58:33]
2023.03.03-13:43:06:325:[step-67000/88900: 75.37%]--[loss-2.889234: wl-4.050954, gl-1.876495]--[lr: pb-0.000050, pf-0.000025]--[ETA-4:42:02]
2023.03.03-13:44:26:425:[step-67100/88900: 75.48%]--[loss-2.827739: wl-3.996060, gl-1.828724]--[lr: pb-0.000050, pf-0.000025]--[ETA-4:52:30]
2023.03.03-13:45:45:525:[step-67200/88900: 75.59%]--[loss-3.173900: wl-4.617791, gl-2.019453]--[lr: pb-0.000050, pf-0.000025]--[ETA-4:51:07]
2023.03.03-13:47:19:625:[step-67300/88900: 75.70%]--[loss-2.745636: wl-3.792792, gl-1.797438]--[lr: pb-0.000050, pf-0.000025]--[ETA-4:38:22]
2023.03.03-13:48:39:725:[step-67400/88900: 75.82%]--[loss-2.931445: wl-3.922220, gl-1.950890]--[lr: pb-0.000050, pf-0.000025]--[ETA-5:02:44]
2023.03.03-13:49:59:825:[step-67500/88900: 75.93%]--[loss-2.939330: wl-3.928504, gl-1.957204]--[lr: pb-0.000050, pf-0.000025]--[ETA-4:58:43]
End of epoch 76 / 100: train_loss: 2.926 	 time: 777 sec
Saving the model at the end of epoch 76, iters 67564
2023.03.03-13:51:40:36:[step-67600/88900: 76.04%]--[loss-2.811398: wl-3.963402, gl-1.820547]--[lr: pb-0.000050, pf-0.000024]--[ETA-4:54:17]
2023.03.03-13:53:00:136:[step-67700/88900: 76.15%]--[loss-3.099777: wl-4.053426, gl-2.086421]--[lr: pb-0.000050, pf-0.000024]--[ETA-4:43:23]
2023.03.03-13:54:20:236:[step-67800/88900: 76.27%]--[loss-3.155957: wl-4.303841, gl-2.079997]--[lr: pb-0.000050, pf-0.000024]--[ETA-4:39:41]
2023.03.03-13:55:54:336:[step-67900/88900: 76.38%]--[loss-2.739232: wl-3.865942, gl-1.772747]--[lr: pb-0.000050, pf-0.000024]--[ETA-4:46:40]
2023.03.03-13:57:15:436:[step-68000/88900: 76.49%]--[loss-2.743424: wl-3.496776, gl-1.869230]--[lr: pb-0.000050, pf-0.000024]--[ETA-4:42:12]
2023.03.03-13:58:46:536:[step-68100/88900: 76.60%]--[loss-3.113156: wl-4.253657, gl-2.049742]--[lr: pb-0.000050, pf-0.000024]--[ETA-4:34:25]
2023.03.03-14:00:07:636:[step-68200/88900: 76.72%]--[loss-3.163933: wl-4.751528, gl-1.976051]--[lr: pb-0.000050, pf-0.000024]--[ETA-4:53:36]
2023.03.03-14:01:27:736:[step-68300/88900: 76.83%]--[loss-2.482075: wl-3.178524, gl-1.687444]--[lr: pb-0.000050, pf-0.000024]--[ETA-4:28:20]
2023.03.03-14:03:01:836:[step-68400/88900: 76.94%]--[loss-2.783594: wl-3.993745, gl-1.785158]--[lr: pb-0.000050, pf-0.000024]--[ETA-4:29:51]
End of epoch 77 / 100: train_loss: 2.918 	 time: 758 sec
Saving the model at the end of epoch 77, iters 68453
2023.03.03-14:04:30:47:[step-68500/88900: 77.05%]--[loss-3.003572: wl-4.263911, gl-1.937595]--[lr: pb-0.000050, pf-0.000023]--[ETA-4:31:16]
2023.03.03-14:06:03:147:[step-68600/88900: 77.17%]--[loss-2.798211: wl-3.955017, gl-1.809457]--[lr: pb-0.000050, pf-0.000023]--[ETA-4:28:08]
2023.03.03-14:07:23:247:[step-68700/88900: 77.28%]--[loss-3.216540: wl-5.106520, gl-1.939910]--[lr: pb-0.000050, pf-0.000023]--[ETA-4:28:45]
2023.03.03-14:08:43:347:[step-68800/88900: 77.39%]--[loss-2.872684: wl-4.185989, gl-1.826186]--[lr: pb-0.000050, pf-0.000023]--[ETA-4:26:25]
2023.03.03-14:10:16:447:[step-68900/88900: 77.50%]--[loss-3.334655: wl-5.954897, gl-1.845931]--[lr: pb-0.000050, pf-0.000023]--[ETA-4:24:39]
2023.03.03-14:11:37:547:[step-69000/88900: 77.62%]--[loss-2.851695: wl-3.607661, gl-1.949779]--[lr: pb-0.000050, pf-0.000023]--[ETA-4:24:43]
2023.03.03-14:13:10:647:[step-69100/88900: 77.73%]--[loss-3.076587: wl-4.360123, gl-1.986556]--[lr: pb-0.000050, pf-0.000023]--[ETA-4:24:30]
2023.03.03-14:14:31:747:[step-69200/88900: 77.84%]--[loss-2.936842: wl-3.988273, gl-1.939774]--[lr: pb-0.000050, pf-0.000023]--[ETA-4:25:08]
2023.03.03-14:16:03:847:[step-69300/88900: 77.95%]--[loss-2.906797: wl-4.418990, gl-1.802049]--[lr: pb-0.000050, pf-0.000023]--[ETA-23:31:58]
End of epoch 78 / 100: train_loss: 2.909 	 time: 773 sec
Saving the model at the end of epoch 78, iters 69342
2023.03.03-14:17:32:58:[step-69400/88900: 78.07%]--[loss-2.870691: wl-4.090254, gl-1.848127]--[lr: pb-0.000050, pf-0.000022]--[ETA-4:18:55]
2023.03.03-14:18:52:158:[step-69500/88900: 78.18%]--[loss-2.773573: wl-3.918649, gl-1.793911]--[lr: pb-0.000050, pf-0.000022]--[ETA-4:24:05]
2023.03.03-14:20:27:258:[step-69600/88900: 78.29%]--[loss-2.862055: wl-4.024580, gl-1.855911]--[lr: pb-0.000050, pf-0.000022]--[ETA-4:10:45]
2023.03.03-14:21:44:358:[step-69700/88900: 78.40%]--[loss-2.967150: wl-4.351352, gl-1.879312]--[lr: pb-0.000050, pf-0.000022]--[ETA-4:13:14]
2023.03.03-14:23:02:458:[step-69800/88900: 78.52%]--[loss-2.891664: wl-4.141492, gl-1.856290]--[lr: pb-0.000050, pf-0.000022]--[ETA-4:07:43]
2023.03.03-14:24:20:558:[step-69900/88900: 78.63%]--[loss-2.779236: wl-3.690254, gl-1.856672]--[lr: pb-0.000050, pf-0.000022]--[ETA-4:05:14]
2023.03.03-14:25:38:658:[step-70000/88900: 78.74%]--[loss-2.892544: wl-3.995136, gl-1.893759]--[lr: pb-0.000050, pf-0.000022]--[ETA-4:05:50]
2023.03.03-14:26:56:758:[step-70100/88900: 78.85%]--[loss-2.729637: wl-3.473150, gl-1.861349]--[lr: pb-0.000050, pf-0.000022]--[ETA-4:02:42]
2023.03.03-14:28:16:858:[step-70200/88900: 78.97%]--[loss-2.806465: wl-3.628062, gl-1.899449]--[lr: pb-0.000050, pf-0.000022]--[ETA-4:15:48]
End of epoch 79 / 100: train_loss: 2.906 	 time: 722 sec
Saving the model at the end of epoch 79, iters 70231
2023.03.03-14:29:45:69:[step-70300/88900: 79.08%]--[loss-2.970953: wl-4.108440, gl-1.943843]--[lr: pb-0.000050, pf-0.000021]--[ETA-4:14:18]
2023.03.03-14:31:20:169:[step-70400/88900: 79.19%]--[loss-2.986671: wl-4.369889, gl-1.894199]--[lr: pb-0.000050, pf-0.000021]--[ETA-4:01:54]
2023.03.03-14:32:40:269:[step-70500/88900: 79.30%]--[loss-2.711412: wl-3.702130, gl-1.785880]--[lr: pb-0.000050, pf-0.000021]--[ETA-4:14:29]
2023.03.03-14:34:00:369:[step-70600/88900: 79.42%]--[loss-2.902649: wl-4.010431, gl-1.900041]--[lr: pb-0.000050, pf-0.000021]--[ETA-3:58:42]
2023.03.03-14:35:34:469:[step-70700/88900: 79.53%]--[loss-2.997234: wl-3.830735, gl-2.039550]--[lr: pb-0.000050, pf-0.000021]--[ETA-3:59:38]
2023.03.03-14:36:53:569:[step-70800/88900: 79.64%]--[loss-2.807072: wl-3.676949, gl-1.887835]--[lr: pb-0.000050, pf-0.000021]--[ETA-3:57:37]
2023.03.03-14:38:23:669:[step-70900/88900: 79.75%]--[loss-2.962613: wl-4.290550, gl-1.889976]--[lr: pb-0.000050, pf-0.000021]--[ETA-3:53:03]
2023.03.03-14:39:42:769:[step-71000/88900: 79.87%]--[loss-2.819599: wl-3.808066, gl-1.867583]--[lr: pb-0.000050, pf-0.000021]--[ETA-4:02:16]
2023.03.03-14:41:02:869:[step-71100/88900: 79.98%]--[loss-2.809175: wl-3.695876, gl-1.885206]--[lr: pb-0.000050, pf-0.000021]--[ETA-3:56:56]
End of epoch 80 / 100: train_loss: 2.896 	 time: 757 sec
Saving the model at the end of epoch 80, iters 71120
2023.03.03-14:42:45:80:[step-71200/88900: 80.09%]--[loss-2.901404: wl-4.061697, gl-1.885980]--[lr: pb-0.000050, pf-0.000020]--[ETA-3:54:03]
2023.03.03-14:44:04:180:[step-71300/88900: 80.20%]--[loss-2.774977: wl-3.824592, gl-1.818829]--[lr: pb-0.000050, pf-0.000020]--[ETA-3:57:45]
2023.03.03-14:45:35:280:[step-71400/88900: 80.31%]--[loss-2.960604: wl-4.172691, gl-1.917431]--[lr: pb-0.000050, pf-0.000020]--[ETA-3:45:08]
2023.03.03-14:46:55:380:[step-71500/88900: 80.43%]--[loss-3.093683: wl-4.443848, gl-1.982721]--[lr: pb-0.000050, pf-0.000020]--[ETA-3:50:19]
2023.03.03-14:48:14:480:[step-71600/88900: 80.54%]--[loss-2.801236: wl-3.930516, gl-1.818607]--[lr: pb-0.000050, pf-0.000020]--[ETA-3:43:30]
2023.03.03-14:49:46:580:[step-71700/88900: 80.65%]--[loss-2.881376: wl-3.962284, gl-1.890805]--[lr: pb-0.000050, pf-0.000020]--[ETA-3:43:44]
2023.03.03-14:51:05:680:[step-71800/88900: 80.76%]--[loss-2.802495: wl-3.928589, gl-1.820348]--[lr: pb-0.000050, pf-0.000020]--[ETA-3:54:54]
2023.03.03-14:52:37:780:[step-71900/88900: 80.88%]--[loss-2.726543: wl-3.831236, gl-1.768734]--[lr: pb-0.000050, pf-0.000020]--[ETA-3:46:20]
2023.03.03-14:53:56:880:[step-72000/88900: 80.99%]--[loss-2.908048: wl-4.100094, gl-1.883025]--[lr: pb-0.000050, pf-0.000020]--[ETA-3:38:39]
End of epoch 81 / 100: train_loss: 2.888 	 time: 764 sec
Saving the model at the end of epoch 81, iters 72009
2023.03.03-14:55:38:91:[step-72100/88900: 81.10%]--[loss-2.651324: wl-3.518448, gl-1.771712]--[lr: pb-0.000050, pf-0.000019]--[ETA-3:42:39]
2023.03.03-14:56:58:191:[step-72200/88900: 81.21%]--[loss-2.715379: wl-3.734133, gl-1.781846]--[lr: pb-0.000050, pf-0.000019]--[ETA-3:39:08]
2023.03.03-14:58:25:291:[step-72300/88900: 81.33%]--[loss-2.650417: wl-3.665956, gl-1.733928]--[lr: pb-0.000050, pf-0.000019]--[ETA-3:48:05]
2023.03.03-14:59:48:391:[step-72400/88900: 81.44%]--[loss-2.672950: wl-3.668078, gl-1.755930]--[lr: pb-0.000050, pf-0.000019]--[ETA-3:39:39]
2023.03.03-15:01:08:491:[step-72500/88900: 81.55%]--[loss-3.221503: wl-4.635105, gl-2.062727]--[lr: pb-0.000050, pf-0.000019]--[ETA-3:40:55]
2023.03.03-15:02:41:591:[step-72600/88900: 81.66%]--[loss-3.017056: wl-4.792795, gl-1.818857]--[lr: pb-0.000050, pf-0.000019]--[ETA-3:36:50]
2023.03.03-15:04:01:691:[step-72700/88900: 81.78%]--[loss-2.741385: wl-3.555700, gl-1.852460]--[lr: pb-0.000050, pf-0.000019]--[ETA-3:43:29]
2023.03.03-15:05:35:791:[step-72800/88900: 81.89%]--[loss-2.831461: wl-4.099475, gl-1.806592]--[lr: pb-0.000050, pf-0.000019]--[ETA-3:31:40]
End of epoch 82 / 100: train_loss: 2.881 	 time: 768 sec
Saving the model at the end of epoch 82, iters 72898
2023.03.03-15:07:03:2:[step-72900/88900: 82.00%]--[loss-2.714958: wl-3.819436, gl-1.760099]--[lr: pb-0.000050, pf-0.000018]--[ETA-4:00:06]
2023.03.03-15:08:35:102:[step-73000/88900: 82.11%]--[loss-2.934445: wl-3.994883, gl-1.935725]--[lr: pb-0.000050, pf-0.000018]--[ETA-3:33:22]
2023.03.03-15:09:55:202:[step-73100/88900: 82.23%]--[loss-2.681670: wl-3.657182, gl-1.767375]--[lr: pb-0.000050, pf-0.000018]--[ETA-3:28:21]
2023.03.03-15:11:15:302:[step-73200/88900: 82.34%]--[loss-2.594665: wl-3.465353, gl-1.728327]--[lr: pb-0.000050, pf-0.000018]--[ETA-3:35:08]
2023.03.03-15:12:49:402:[step-73300/88900: 82.45%]--[loss-2.741222: wl-3.829478, gl-1.783852]--[lr: pb-0.000050, pf-0.000018]--[ETA-3:27:43]
2023.03.03-15:14:09:502:[step-73400/88900: 82.56%]--[loss-3.107593: wl-4.642080, gl-1.947073]--[lr: pb-0.000050, pf-0.000018]--[ETA-3:22:26]
2023.03.03-15:15:41:602:[step-73500/88900: 82.68%]--[loss-2.740505: wl-3.886536, gl-1.768871]--[lr: pb-0.000050, pf-0.000018]--[ETA-3:22:41]
2023.03.03-15:17:01:702:[step-73600/88900: 82.79%]--[loss-2.864326: wl-4.050231, gl-1.851768]--[lr: pb-0.000050, pf-0.000018]--[ETA-3:23:06]
2023.03.03-15:18:33:802:[step-73700/88900: 82.90%]--[loss-3.007197: wl-4.372780, gl-1.914002]--[lr: pb-0.000050, pf-0.000018]--[ETA-3:17:06]
End of epoch 83 / 100: train_loss: 2.875 	 time: 769 sec
Saving the model at the end of epoch 83, iters 73787
2023.03.03-15:20:01:13:[step-73800/88900: 83.01%]--[loss-2.855478: wl-3.761688, gl-1.915055]--[lr: pb-0.000050, pf-0.000017]--[ETA-3:21:12]
2023.03.03-15:21:36:113:[step-73900/88900: 83.13%]--[loss-2.798724: wl-3.818919, gl-1.843994]--[lr: pb-0.000050, pf-0.000017]--[ETA-3:23:33]
2023.03.03-15:22:56:213:[step-74000/88900: 83.24%]--[loss-3.067327: wl-4.521507, gl-1.936951]--[lr: pb-0.000050, pf-0.000017]--[ETA-3:24:23]
2023.03.03-15:24:17:313:[step-74100/88900: 83.35%]--[loss-3.029931: wl-4.612539, gl-1.876796]--[lr: pb-0.000050, pf-0.000017]--[ETA-3:32:10]
2023.03.03-15:25:50:413:[step-74200/88900: 83.46%]--[loss-2.753294: wl-3.682526, gl-1.832663]--[lr: pb-0.000050, pf-0.000017]--[ETA-3:21:40]
2023.03.03-15:27:10:513:[step-74300/88900: 83.58%]--[loss-3.044914: wl-4.640308, gl-1.884837]--[lr: pb-0.000050, pf-0.000017]--[ETA-3:13:08]
2023.03.03-15:28:42:613:[step-74400/88900: 83.69%]--[loss-2.779563: wl-3.874259, gl-1.810998]--[lr: pb-0.000050, pf-0.000017]--[ETA-3:12:02]
2023.03.03-15:30:02:713:[step-74500/88900: 83.80%]--[loss-2.808690: wl-3.575009, gl-1.914938]--[lr: pb-0.000050, pf-0.000017]--[ETA-3:08:28]
2023.03.03-15:31:22:813:[step-74600/88900: 83.91%]--[loss-2.847121: wl-3.681697, gl-1.926696]--[lr: pb-0.000050, pf-0.000017]--[ETA-3:36:49]
End of epoch 84 / 100: train_loss: 2.867 	 time: 769 sec
Saving the model at the end of epoch 84, iters 74676
2023.03.03-15:33:01:24:[step-74700/88900: 84.03%]--[loss-2.958995: wl-3.988586, gl-1.961849]--[lr: pb-0.000050, pf-0.000016]--[ETA-3:03:15]
2023.03.03-15:34:27:124:[step-74800/88900: 84.14%]--[loss-2.788393: wl-3.953267, gl-1.800076]--[lr: pb-0.000050, pf-0.000016]--[ETA-10:11:12]
2023.03.03-15:35:49:224:[step-74900/88900: 84.25%]--[loss-2.847257: wl-3.847435, gl-1.885398]--[lr: pb-0.000050, pf-0.000016]--[ETA-3:00:25]
2023.03.03-15:37:07:324:[step-75000/88900: 84.36%]--[loss-2.783708: wl-4.086949, gl-1.761971]--[lr: pb-0.000050, pf-0.000016]--[ETA-3:01:05]
2023.03.03-15:38:25:424:[step-75100/88900: 84.48%]--[loss-2.970273: wl-4.301116, gl-1.894994]--[lr: pb-0.000050, pf-0.000016]--[ETA-3:04:28]
2023.03.03-15:39:43:524:[step-75200/88900: 84.59%]--[loss-2.808993: wl-4.088140, gl-1.786958]--[lr: pb-0.000050, pf-0.000016]--[ETA-2:58:20]
2023.03.03-15:41:01:624:[step-75300/88900: 84.70%]--[loss-2.745335: wl-3.816983, gl-1.791090]--[lr: pb-0.000050, pf-0.000016]--[ETA-2:55:33]
2023.03.03-15:42:19:724:[step-75400/88900: 84.81%]--[loss-2.850798: wl-4.207427, gl-1.798941]--[lr: pb-0.000050, pf-0.000016]--[ETA-2:54:45]
2023.03.03-15:43:37:824:[step-75500/88900: 84.93%]--[loss-2.838082: wl-4.093145, gl-1.814796]--[lr: pb-0.000050, pf-0.000016]--[ETA-2:55:31]
End of epoch 85 / 100: train_loss: 2.864 	 time: 713 sec
Saving the model at the end of epoch 85, iters 75565
2023.03.03-15:45:03:35:[step-75600/88900: 85.04%]--[loss-2.769074: wl-3.798881, gl-1.819354]--[lr: pb-0.000050, pf-0.000015]--[ETA-2:54:02]
2023.03.03-15:46:21:135:[step-75700/88900: 85.15%]--[loss-2.816679: wl-3.953539, gl-1.828295]--[lr: pb-0.000050, pf-0.000015]--[ETA-2:55:22]
2023.03.03-15:47:40:235:[step-75800/88900: 85.26%]--[loss-2.651561: wl-3.649240, gl-1.739251]--[lr: pb-0.000050, pf-0.000015]--[ETA-2:50:01]
2023.03.03-15:48:59:335:[step-75900/88900: 85.38%]--[loss-2.865980: wl-3.975238, gl-1.872171]--[lr: pb-0.000050, pf-0.000015]--[ETA-2:49:45]
2023.03.03-15:50:20:435:[step-76000/88900: 85.49%]--[loss-2.912183: wl-3.878104, gl-1.942657]--[lr: pb-0.000050, pf-0.000015]--[ETA-2:53:21]
2023.03.03-15:51:45:535:[step-76100/88900: 85.60%]--[loss-2.918480: wl-4.078037, gl-1.898970]--[lr: pb-0.000050, pf-0.000015]--[ETA-11:20:30]
2023.03.03-15:53:10:635:[step-76200/88900: 85.71%]--[loss-2.846171: wl-3.931912, gl-1.863193]--[lr: pb-0.000050, pf-0.000015]--[ETA-2:55:19]
2023.03.03-15:54:31:735:[step-76300/88900: 85.83%]--[loss-2.626616: wl-3.586004, gl-1.730115]--[lr: pb-0.000050, pf-0.000015]--[ETA-2:49:42]
2023.03.03-15:56:04:835:[step-76400/88900: 85.94%]--[loss-2.805306: wl-3.743915, gl-1.869327]--[lr: pb-0.000050, pf-0.000015]--[ETA-2:52:25]
End of epoch 86 / 100: train_loss: 2.852 	 time: 739 sec
Saving the model at the end of epoch 86, iters 76454
2023.03.03-15:57:32:46:[step-76500/88900: 86.05%]--[loss-2.621050: wl-3.391207, gl-1.773248]--[lr: pb-0.000050, pf-0.000014]--[ETA-2:46:56]
2023.03.03-15:59:04:146:[step-76600/88900: 86.16%]--[loss-2.938498: wl-4.136287, gl-1.904427]--[lr: pb-0.000050, pf-0.000014]--[ETA-2:41:54]
2023.03.03-16:00:24:246:[step-76700/88900: 86.28%]--[loss-3.093916: wl-4.315161, gl-2.015126]--[lr: pb-0.000050, pf-0.000014]--[ETA-2:44:43]
2023.03.03-16:01:59:346:[step-76800/88900: 86.39%]--[loss-2.780114: wl-3.729716, gl-1.847685]--[lr: pb-0.000050, pf-0.000014]--[ETA-2:36:22]
2023.03.03-16:03:19:446:[step-76900/88900: 86.50%]--[loss-2.814586: wl-3.930385, gl-1.831990]--[lr: pb-0.000050, pf-0.000014]--[ETA-2:38:59]
2023.03.03-16:04:39:546:[step-77000/88900: 86.61%]--[loss-2.908078: wl-4.139404, gl-1.873227]--[lr: pb-0.000050, pf-0.000014]--[ETA-2:43:24]
2023.03.03-16:06:09:646:[step-77100/88900: 86.73%]--[loss-2.955062: wl-4.115641, gl-1.926152]--[lr: pb-0.000050, pf-0.000014]--[ETA-2:43:28]
2023.03.03-16:07:30:746:[step-77200/88900: 86.84%]--[loss-2.576357: wl-3.566979, gl-1.684612]--[lr: pb-0.000050, pf-0.000014]--[ETA-2:40:08]
2023.03.03-16:09:07:846:[step-77300/88900: 86.95%]--[loss-2.863898: wl-4.498473, gl-1.739280]--[lr: pb-0.000050, pf-0.000014]--[ETA-2:39:45]
End of epoch 87 / 100: train_loss: 2.845 	 time: 773 sec
Saving the model at the end of epoch 87, iters 77343
2023.03.03-16:10:36:57:[step-77400/88900: 87.06%]--[loss-3.053867: wl-4.066248, gl-2.037305]--[lr: pb-0.000050, pf-0.000013]--[ETA-2:32:21]
2023.03.03-16:12:08:157:[step-77500/88900: 87.18%]--[loss-2.762589: wl-4.080330, gl-1.742506]--[lr: pb-0.000050, pf-0.000013]--[ETA-2:27:56]
2023.03.03-16:13:27:257:[step-77600/88900: 87.29%]--[loss-2.945153: wl-4.220013, gl-1.890149]--[lr: pb-0.000050, pf-0.000013]--[ETA-2:27:39]
2023.03.03-16:14:48:357:[step-77700/88900: 87.40%]--[loss-3.061468: wl-4.415132, gl-1.957685]--[lr: pb-0.000050, pf-0.000013]--[ETA-2:29:14]
2023.03.03-16:16:18:457:[step-77800/88900: 87.51%]--[loss-2.942662: wl-4.445059, gl-1.831397]--[lr: pb-0.000050, pf-0.000013]--[ETA-2:26:26]
2023.03.03-16:17:38:557:[step-77900/88900: 87.63%]--[loss-2.747475: wl-4.002795, gl-1.746777]--[lr: pb-0.000050, pf-0.000013]--[ETA-2:24:40]
2023.03.03-16:19:13:657:[step-78000/88900: 87.74%]--[loss-2.713700: wl-3.785287, gl-1.767379]--[lr: pb-0.000050, pf-0.000013]--[ETA-2:23:54]
2023.03.03-16:20:33:757:[step-78100/88900: 87.85%]--[loss-2.805326: wl-3.921346, gl-1.824989]--[lr: pb-0.000050, pf-0.000013]--[ETA-2:22:36]
2023.03.03-16:22:06:857:[step-78200/88900: 87.96%]--[loss-2.772930: wl-4.183079, gl-1.727160]--[lr: pb-0.000050, pf-0.000013]--[ETA-2:19:00]
End of epoch 88 / 100: train_loss: 2.840 	 time: 769 sec
Saving the model at the end of epoch 88, iters 78232
2023.03.03-16:23:33:68:[step-78300/88900: 88.08%]--[loss-2.567882: wl-3.443126, gl-1.707100]--[lr: pb-0.000050, pf-0.000012]--[ETA-2:21:46]
2023.03.03-16:24:52:168:[step-78400/88900: 88.19%]--[loss-2.871643: wl-4.009942, gl-1.869158]--[lr: pb-0.000050, pf-0.000012]--[ETA-2:17:05]
2023.03.03-16:26:10:268:[step-78500/88900: 88.30%]--[loss-2.790626: wl-3.748575, gl-1.853482]--[lr: pb-0.000050, pf-0.000012]--[ETA-2:15:22]
2023.03.03-16:27:28:368:[step-78600/88900: 88.41%]--[loss-2.887990: wl-4.209767, gl-1.835548]--[lr: pb-0.000050, pf-0.000012]--[ETA-2:14:58]
2023.03.03-16:28:47:468:[step-78700/88900: 88.53%]--[loss-2.484093: wl-3.381225, gl-1.638787]--[lr: pb-0.000050, pf-0.000012]--[ETA-2:11:48]
2023.03.03-16:30:05:568:[step-78800/88900: 88.64%]--[loss-2.685636: wl-4.009048, gl-1.683374]--[lr: pb-0.000050, pf-0.000012]--[ETA-2:10:37]
2023.03.03-16:31:23:668:[step-78900/88900: 88.75%]--[loss-2.913391: wl-4.233187, gl-1.855094]--[lr: pb-0.000050, pf-0.000012]--[ETA-2:09:23]
2023.03.03-16:32:40:768:[step-79000/88900: 88.86%]--[loss-2.772090: wl-3.760661, gl-1.831925]--[lr: pb-0.000050, pf-0.000012]--[ETA-2:07:32]
2023.03.03-16:33:58:868:[step-79100/88900: 88.98%]--[loss-2.791418: wl-3.786176, gl-1.844874]--[lr: pb-0.000050, pf-0.000012]--[ETA-2:07:58]
End of epoch 89 / 100: train_loss: 2.835 	 time: 703 sec
Saving the model at the end of epoch 89, iters 79121
2023.03.03-16:35:23:79:[step-79200/88900: 89.09%]--[loss-2.688054: wl-4.027209, gl-1.681251]--[lr: pb-0.000050, pf-0.000011]--[ETA-2:05:30]
2023.03.03-16:36:42:179:[step-79300/88900: 89.20%]--[loss-2.825166: wl-4.244934, gl-1.763932]--[lr: pb-0.000050, pf-0.000011]--[ETA-2:05:25]
2023.03.03-16:38:00:279:[step-79400/88900: 89.31%]--[loss-3.027962: wl-4.018328, gl-2.023380]--[lr: pb-0.000050, pf-0.000011]--[ETA-2:06:38]
2023.03.03-16:39:19:379:[step-79500/88900: 89.43%]--[loss-2.807839: wl-3.862103, gl-1.842313]--[lr: pb-0.000050, pf-0.000011]--[ETA-2:01:39]
2023.03.03-16:40:37:479:[step-79600/88900: 89.54%]--[loss-2.728306: wl-3.986904, gl-1.731580]--[lr: pb-0.000050, pf-0.000011]--[ETA-2:00:51]
2023.03.03-16:41:55:579:[step-79700/88900: 89.65%]--[loss-2.898540: wl-4.401297, gl-1.798216]--[lr: pb-0.000050, pf-0.000011]--[ETA-2:00:00]
2023.03.03-16:43:13:679:[step-79800/88900: 89.76%]--[loss-2.902708: wl-3.942171, gl-1.917165]--[lr: pb-0.000050, pf-0.000011]--[ETA-1:58:12]
2023.03.03-16:44:31:779:[step-79900/88900: 89.88%]--[loss-2.808177: wl-3.846483, gl-1.846557]--[lr: pb-0.000050, pf-0.000011]--[ETA-1:55:44]
2023.03.03-16:45:48:879:[step-80000/88900: 89.99%]--[loss-2.842264: wl-3.988681, gl-1.845094]--[lr: pb-0.000050, pf-0.000011]--[ETA-1:54:35]
End of epoch 90 / 100: train_loss: 2.830 	 time: 700 sec
Saving the model at the end of epoch 90, iters 80010
2023.03.03-16:47:14:90:[step-80100/88900: 90.10%]--[loss-2.825119: wl-3.979656, gl-1.830205]--[lr: pb-0.000050, pf-0.000010]--[ETA-1:53:44]
2023.03.03-16:48:32:190:[step-80200/88900: 90.21%]--[loss-3.027347: wl-4.052237, gl-2.014288]--[lr: pb-0.000050, pf-0.000010]--[ETA-1:53:10]
2023.03.03-16:49:50:290:[step-80300/88900: 90.33%]--[loss-2.834093: wl-4.038795, gl-1.824394]--[lr: pb-0.000050, pf-0.000010]--[ETA-1:51:16]
2023.03.03-16:51:08:390:[step-80400/88900: 90.44%]--[loss-2.904655: wl-4.368372, gl-1.812562]--[lr: pb-0.000050, pf-0.000010]--[ETA-1:50:54]
2023.03.03-16:52:26:490:[step-80500/88900: 90.55%]--[loss-2.852874: wl-4.044158, gl-1.841835]--[lr: pb-0.000050, pf-0.000010]--[ETA-1:48:56]
2023.03.03-16:53:44:590:[step-80600/88900: 90.66%]--[loss-3.069201: wl-4.440389, gl-1.959103]--[lr: pb-0.000050, pf-0.000010]--[ETA-1:47:41]
2023.03.03-16:55:02:690:[step-80700/88900: 90.78%]--[loss-2.743057: wl-3.809439, gl-1.790698]--[lr: pb-0.000050, pf-0.000010]--[ETA-1:48:41]
2023.03.03-16:56:20:790:[step-80800/88900: 90.89%]--[loss-3.062009: wl-4.321617, gl-1.981605]--[lr: pb-0.000050, pf-0.000010]--[ETA-1:45:57]
End of epoch 91 / 100: train_loss: 2.822 	 time: 700 sec
Saving the model at the end of epoch 91, iters 80899
2023.03.03-16:57:45:1:[step-80900/88900: 91.00%]--[loss-2.718132: wl-3.659310, gl-1.803304]--[lr: pb-0.000050, pf-0.000009]--[ETA-2:30:57]
2023.03.03-16:59:04:101:[step-81000/88900: 91.11%]--[loss-2.749092: wl-3.991551, gl-1.751205]--[lr: pb-0.000050, pf-0.000009]--[ETA-1:42:11]
2023.03.03-17:00:22:201:[step-81100/88900: 91.23%]--[loss-2.939446: wl-3.875817, gl-1.970492]--[lr: pb-0.000050, pf-0.000009]--[ETA-1:40:29]
2023.03.03-17:01:41:301:[step-81200/88900: 91.34%]--[loss-2.799174: wl-4.304776, gl-1.722980]--[lr: pb-0.000050, pf-0.000009]--[ETA-1:39:56]
2023.03.03-17:02:59:401:[step-81300/88900: 91.45%]--[loss-2.868066: wl-3.885964, gl-1.896575]--[lr: pb-0.000050, pf-0.000009]--[ETA-1:39:25]
2023.03.03-17:04:17:501:[step-81400/88900: 91.56%]--[loss-2.788870: wl-3.851993, gl-1.825872]--[lr: pb-0.000050, pf-0.000009]--[ETA-1:36:59]
2023.03.03-17:05:35:601:[step-81500/88900: 91.68%]--[loss-2.762856: wl-3.984242, gl-1.766796]--[lr: pb-0.000050, pf-0.000009]--[ETA-1:36:13]
2023.03.03-17:06:53:701:[step-81600/88900: 91.79%]--[loss-2.843362: wl-4.047678, gl-1.831443]--[lr: pb-0.000050, pf-0.000009]--[ETA-1:33:16]
2023.03.03-17:08:11:801:[step-81700/88900: 91.90%]--[loss-2.774691: wl-3.783595, gl-1.828792]--[lr: pb-0.000050, pf-0.000009]--[ETA-1:33:15]
End of epoch 92 / 100: train_loss: 2.817 	 time: 701 sec
Saving the model at the end of epoch 92, iters 81788
2023.03.03-17:09:36:12:[step-81800/88900: 92.01%]--[loss-3.107099: wl-4.743288, gl-1.921277]--[lr: pb-0.000050, pf-0.000008]--[ETA-1:33:03]
2023.03.03-17:10:55:112:[step-81900/88900: 92.13%]--[loss-2.759642: wl-4.165253, gl-1.718329]--[lr: pb-0.000050, pf-0.000008]--[ETA-1:30:31]
2023.03.03-17:12:13:212:[step-82000/88900: 92.24%]--[loss-2.916864: wl-4.077404, gl-1.897513]--[lr: pb-0.000050, pf-0.000008]--[ETA-1:29:17]
2023.03.03-17:13:31:312:[step-82100/88900: 92.35%]--[loss-2.524193: wl-3.264187, gl-1.708146]--[lr: pb-0.000050, pf-0.000008]--[ETA-1:27:42]
2023.03.03-17:14:50:412:[step-82200/88900: 92.46%]--[loss-2.691047: wl-3.739765, gl-1.756105]--[lr: pb-0.000050, pf-0.000008]--[ETA-1:26:21]
2023.03.03-17:16:08:512:[step-82300/88900: 92.58%]--[loss-2.707576: wl-3.847054, gl-1.745812]--[lr: pb-0.000050, pf-0.000008]--[ETA-1:26:15]
2023.03.03-17:17:27:612:[step-82400/88900: 92.69%]--[loss-2.683873: wl-3.833496, gl-1.725499]--[lr: pb-0.000050, pf-0.000008]--[ETA-1:24:29]
2023.03.03-17:18:49:712:[step-82500/88900: 92.80%]--[loss-2.719010: wl-3.958133, gl-1.729477]--[lr: pb-0.000050, pf-0.000008]--[ETA-1:28:51]
2023.03.03-17:20:09:812:[step-82600/88900: 92.91%]--[loss-2.687739: wl-3.728663, gl-1.755573]--[lr: pb-0.000050, pf-0.000008]--[ETA-1:22:08]
End of epoch 93 / 100: train_loss: 2.811 	 time: 710 sec
Saving the model at the end of epoch 93, iters 82677
2023.03.03-17:21:37:23:[step-82700/88900: 93.03%]--[loss-2.784925: wl-3.901779, gl-1.809480]--[lr: pb-0.000050, pf-0.000007]--[ETA-1:21:33]
2023.03.03-17:23:10:123:[step-82800/88900: 93.14%]--[loss-2.815689: wl-4.162285, gl-1.775117]--[lr: pb-0.000050, pf-0.000007]--[ETA-1:23:41]
2023.03.03-17:24:30:223:[step-82900/88900: 93.25%]--[loss-2.889455: wl-4.064125, gl-1.873424]--[lr: pb-0.000050, pf-0.000007]--[ETA-1:20:02]
2023.03.03-17:25:50:323:[step-83000/88900: 93.36%]--[loss-3.122405: wl-4.203147, gl-2.071619]--[lr: pb-0.000050, pf-0.000007]--[ETA-1:20:45]
2023.03.03-17:27:26:423:[step-83100/88900: 93.48%]--[loss-2.685854: wl-3.705832, gl-1.759396]--[lr: pb-0.000050, pf-0.000007]--[ETA-1:18:08]
2023.03.03-17:28:47:523:[step-83200/88900: 93.59%]--[loss-2.703315: wl-3.460362, gl-1.838225]--[lr: pb-0.000050, pf-0.000007]--[ETA-1:15:07]
2023.03.03-17:30:07:623:[step-83300/88900: 93.70%]--[loss-3.245957: wl-4.340536, gl-2.160823]--[lr: pb-0.000050, pf-0.000007]--[ETA-1:15:51]
2023.03.03-17:31:44:723:[step-83400/88900: 93.81%]--[loss-2.847404: wl-4.086374, gl-1.825810]--[lr: pb-0.000050, pf-0.000007]--[ETA-1:13:18]
2023.03.03-17:33:04:823:[step-83500/88900: 93.93%]--[loss-3.021993: wl-4.952973, gl-1.783750]--[lr: pb-0.000050, pf-0.000007]--[ETA-1:11:38]
End of epoch 94 / 100: train_loss: 2.808 	 time: 765 sec
Saving the model at the end of epoch 94, iters 83566
2023.03.03-17:34:33:34:[step-83600/88900: 94.04%]--[loss-2.709934: wl-3.669922, gl-1.792453]--[lr: pb-0.000050, pf-0.000006]--[ETA-1:09:12]
2023.03.03-17:36:05:134:[step-83700/88900: 94.15%]--[loss-2.617717: wl-3.780614, gl-1.672564]--[lr: pb-0.000050, pf-0.000006]--[ETA-1:09:17]
2023.03.03-17:37:25:234:[step-83800/88900: 94.26%]--[loss-2.929042: wl-4.339459, gl-1.844177]--[lr: pb-0.000050, pf-0.000006]--[ETA-1:08:28]
2023.03.03-17:38:55:334:[step-83900/88900: 94.38%]--[loss-2.595597: wl-3.712468, gl-1.667480]--[lr: pb-0.000050, pf-0.000006]--[ETA-1:07:58]
2023.03.03-17:40:14:434:[step-84000/88900: 94.49%]--[loss-2.914925: wl-4.259261, gl-1.850110]--[lr: pb-0.000050, pf-0.000006]--[ETA-1:05:54]
2023.03.03-17:41:33:534:[step-84100/88900: 94.60%]--[loss-2.924036: wl-4.230563, gl-1.866395]--[lr: pb-0.000050, pf-0.000006]--[ETA-1:02:35]
2023.03.03-17:43:02:634:[step-84200/88900: 94.71%]--[loss-2.541371: wl-3.345850, gl-1.704908]--[lr: pb-0.000050, pf-0.000006]--[ETA-1:01:34]
2023.03.03-17:44:22:734:[step-84300/88900: 94.83%]--[loss-2.452051: wl-3.316898, gl-1.622826]--[lr: pb-0.000050, pf-0.000006]--[ETA-1:00:37]
2023.03.03-17:45:54:834:[step-84400/88900: 94.94%]--[loss-2.875697: wl-3.995664, gl-1.876781]--[lr: pb-0.000050, pf-0.000006]--[ETA-0:58:25]
End of epoch 95 / 100: train_loss: 2.806 	 time: 761 sec
Saving the model at the end of epoch 95, iters 84455
2023.03.03-17:47:24:45:[step-84500/88900: 95.05%]--[loss-2.908186: wl-4.172016, gl-1.865182]--[lr: pb-0.000050, pf-0.000005]--[ETA-0:59:37]
2023.03.03-17:48:44:145:[step-84600/88900: 95.16%]--[loss-2.716051: wl-3.693272, gl-1.792733]--[lr: pb-0.000050, pf-0.000005]--[ETA-1:00:00]
2023.03.03-17:50:16:245:[step-84700/88900: 95.28%]--[loss-2.775371: wl-3.739529, gl-1.840489]--[lr: pb-0.000050, pf-0.000005]--[ETA-0:54:51]
2023.03.03-17:51:36:345:[step-84800/88900: 95.39%]--[loss-2.560133: wl-3.539736, gl-1.675199]--[lr: pb-0.000050, pf-0.000005]--[ETA-0:54:38]
2023.03.03-17:53:07:445:[step-84900/88900: 95.50%]--[loss-2.914253: wl-4.651454, gl-1.751390]--[lr: pb-0.000050, pf-0.000005]--[ETA-0:51:40]
2023.03.03-17:54:26:545:[step-85000/88900: 95.61%]--[loss-3.075501: wl-5.223328, gl-1.769669]--[lr: pb-0.000050, pf-0.000005]--[ETA-0:51:43]
2023.03.03-17:55:46:645:[step-85100/88900: 95.73%]--[loss-2.935969: wl-4.731328, gl-1.753137]--[lr: pb-0.000050, pf-0.000005]--[ETA-0:51:37]
2023.03.03-17:57:18:745:[step-85200/88900: 95.84%]--[loss-2.893119: wl-4.566772, gl-1.751426]--[lr: pb-0.000050, pf-0.000005]--[ETA-0:47:24]
2023.03.03-17:58:35:845:[step-85300/88900: 95.95%]--[loss-2.740749: wl-3.710985, gl-1.813003]--[lr: pb-0.000050, pf-0.000005]--[ETA-0:46:26]
End of epoch 96 / 100: train_loss: 2.798 	 time: 750 sec
Saving the model at the end of epoch 96, iters 85344
2023.03.03-18:00:02:56:[step-85400/88900: 96.06%]--[loss-2.832399: wl-4.144395, gl-1.796300]--[lr: pb-0.000050, pf-0.000004]--[ETA-0:45:40]
2023.03.03-18:01:21:156:[step-85500/88900: 96.18%]--[loss-2.643254: wl-3.574313, gl-1.749676]--[lr: pb-0.000050, pf-0.000004]--[ETA-0:43:58]
2023.03.03-18:02:38:256:[step-85600/88900: 96.29%]--[loss-2.638945: wl-3.521926, gl-1.758464]--[lr: pb-0.000050, pf-0.000004]--[ETA-0:42:26]
2023.03.03-18:03:56:356:[step-85700/88900: 96.40%]--[loss-2.896859: wl-3.748712, gl-1.959681]--[lr: pb-0.000050, pf-0.000004]--[ETA-0:41:16]
2023.03.03-18:05:14:456:[step-85800/88900: 96.51%]--[loss-2.910852: wl-4.540702, gl-1.775676]--[lr: pb-0.000050, pf-0.000004]--[ETA-0:40:16]
2023.03.03-18:06:31:556:[step-85900/88900: 96.63%]--[loss-2.916656: wl-4.254823, gl-1.852950]--[lr: pb-0.000050, pf-0.000004]--[ETA-0:38:35]
2023.03.03-18:07:48:656:[step-86000/88900: 96.74%]--[loss-2.748564: wl-4.022018, gl-1.743059]--[lr: pb-0.000050, pf-0.000004]--[ETA-0:37:24]
2023.03.03-18:09:06:756:[step-86100/88900: 96.85%]--[loss-2.537085: wl-3.273362, gl-1.718744]--[lr: pb-0.000050, pf-0.000004]--[ETA-0:35:50]
2023.03.03-18:10:23:856:[step-86200/88900: 96.96%]--[loss-2.618353: wl-3.778582, gl-1.673707]--[lr: pb-0.000050, pf-0.000004]--[ETA-0:34:48]
End of epoch 97 / 100: train_loss: 2.792 	 time: 698 sec
Saving the model at the end of epoch 97, iters 86233
2023.03.03-18:11:48:67:[step-86300/88900: 97.08%]--[loss-2.714448: wl-3.680438, gl-1.794339]--[lr: pb-0.000050, pf-0.000003]--[ETA-0:33:33]
2023.03.03-18:13:06:167:[step-86400/88900: 97.19%]--[loss-3.001696: wl-4.675544, gl-1.832810]--[lr: pb-0.000050, pf-0.000003]--[ETA-0:32:55]
2023.03.03-18:14:24:267:[step-86500/88900: 97.30%]--[loss-2.759684: wl-3.895969, gl-1.785692]--[lr: pb-0.000050, pf-0.000003]--[ETA-0:31:00]
2023.03.03-18:15:42:367:[step-86600/88900: 97.41%]--[loss-2.743248: wl-3.897955, gl-1.768760]--[lr: pb-0.000050, pf-0.000003]--[ETA-0:30:01]
2023.03.03-18:17:00:467:[step-86700/88900: 97.53%]--[loss-3.004470: wl-4.295683, gl-1.930549]--[lr: pb-0.000050, pf-0.000003]--[ETA-0:28:28]
2023.03.03-18:18:18:567:[step-86800/88900: 97.64%]--[loss-2.748450: wl-4.171605, gl-1.705549]--[lr: pb-0.000050, pf-0.000003]--[ETA-0:27:23]
2023.03.03-18:19:37:667:[step-86900/88900: 97.75%]--[loss-2.702099: wl-3.982285, gl-1.706528]--[lr: pb-0.000050, pf-0.000003]--[ETA-0:26:05]
2023.03.03-18:20:55:767:[step-87000/88900: 97.86%]--[loss-2.599867: wl-3.466113, gl-1.733339]--[lr: pb-0.000050, pf-0.000003]--[ETA-0:25:03]
2023.03.03-18:22:13:867:[step-87100/88900: 97.98%]--[loss-2.970476: wl-3.983436, gl-1.974617]--[lr: pb-0.000050, pf-0.000003]--[ETA-0:23:15]
End of epoch 98 / 100: train_loss: 2.793 	 time: 701 sec
Saving the model at the end of epoch 98, iters 87122
2023.03.03-18:23:39:78:[step-87200/88900: 98.09%]--[loss-3.009081: wl-4.118580, gl-1.979436]--[lr: pb-0.000050, pf-0.000002]--[ETA-0:22:10]
2023.03.03-18:24:57:178:[step-87300/88900: 98.20%]--[loss-2.759037: wl-3.972977, gl-1.765793]--[lr: pb-0.000050, pf-0.000002]--[ETA-0:20:33]
2023.03.03-18:26:15:278:[step-87400/88900: 98.31%]--[loss-2.837339: wl-4.010157, gl-1.834800]--[lr: pb-0.000050, pf-0.000002]--[ETA-0:19:31]
2023.03.03-18:27:33:378:[step-87500/88900: 98.43%]--[loss-2.918420: wl-4.117191, gl-1.889122]--[lr: pb-0.000050, pf-0.000002]--[ETA-0:17:53]
2023.03.03-18:28:52:478:[step-87600/88900: 98.54%]--[loss-2.743396: wl-3.728574, gl-1.811252]--[lr: pb-0.000050, pf-0.000002]--[ETA-0:17:27]
2023.03.03-18:30:12:578:[step-87700/88900: 98.65%]--[loss-2.979766: wl-4.480988, gl-1.859519]--[lr: pb-0.000050, pf-0.000002]--[ETA-0:15:50]
2023.03.03-18:31:32:678:[step-87800/88900: 98.76%]--[loss-2.838559: wl-3.924094, gl-1.857535]--[lr: pb-0.000050, pf-0.000002]--[ETA-0:14:16]
2023.03.03-18:33:06:778:[step-87900/88900: 98.88%]--[loss-2.853576: wl-3.801440, gl-1.903216]--[lr: pb-0.000050, pf-0.000002]--[ETA-0:13:27]
2023.03.03-18:34:26:878:[step-88000/88900: 98.99%]--[loss-2.883299: wl-4.366187, gl-1.791752]--[lr: pb-0.000050, pf-0.000002]--[ETA-0:11:59]
End of epoch 99 / 100: train_loss: 2.787 	 time: 724 sec
Saving the model at the end of epoch 99, iters 88011
2023.03.03-18:36:11:89:[step-88100/88900: 99.10%]--[loss-2.684448: wl-4.011701, gl-1.681523]--[lr: pb-0.000050, pf-0.000001]--[ETA-0:10:30]
2023.03.03-18:37:33:189:[step-88200/88900: 99.21%]--[loss-2.810876: wl-3.954853, gl-1.822163]--[lr: pb-0.000050, pf-0.000001]--[ETA-0:09:13]
2023.03.03-18:38:53:289:[step-88300/88900: 99.33%]--[loss-2.739565: wl-3.674773, gl-1.820872]--[lr: pb-0.000050, pf-0.000001]--[ETA-0:07:53]
2023.03.03-18:40:30:389:[step-88400/88900: 99.44%]--[loss-2.750638: wl-4.046966, gl-1.738897]--[lr: pb-0.000050, pf-0.000001]--[ETA-0:06:45]
2023.03.03-18:41:50:489:[step-88500/88900: 99.55%]--[loss-3.026455: wl-4.390851, gl-1.928742]--[lr: pb-0.000050, pf-0.000001]--[ETA-0:05:18]
2023.03.03-18:43:26:589:[step-88600/88900: 99.66%]--[loss-2.947342: wl-4.313231, gl-1.869034]--[lr: pb-0.000050, pf-0.000001]--[ETA-0:03:53]
2023.03.03-18:44:44:689:[step-88700/88900: 99.78%]--[loss-2.691608: wl-3.689198, gl-1.769308]--[lr: pb-0.000050, pf-0.000001]--[ETA-0:02:39]
2023.03.03-18:46:04:789:[step-88800/88900: 99.89%]--[loss-2.836604: wl-4.018201, gl-1.832054]--[lr: pb-0.000050, pf-0.000001]--[ETA-0:01:20]
2023.03.03-18:47:31:889:[step-88900/88900: 100.00%]--[loss-2.949035: wl-4.533550, gl-1.815648]--[lr: pb-0.000050, pf-0.000001]--[ETA-0:49:19]
End of epoch 100 / 100: train_loss: 2.784 	 time: 776 sec
Saving the model at the end of epoch 100, iters 88900
