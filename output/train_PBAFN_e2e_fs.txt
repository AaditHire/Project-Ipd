------------ Options -------------
PBAFN_gen_checkpoint: None
PBAFN_warp_checkpoint: checkpoints_fs/PBAFN_stage1_fs/PBAFN_warp_epoch_201.pth
PFAFN_gen_checkpoint: None
PFAFN_warp_checkpoint: None
batchSize: 32
beta1: 0.5
checkpoints_dir: checkpoints_fs
continue_train: False
data_type: 32
dataroot: ../dataset/Flow-Style-VTON/VITON_traindata
debug: False
display_freq: 100
display_winsize: 512
fineSize: 512
gpu_ids: [0]
input_nc: 3
isTrain: True
label_nc: 14
lambda_feat: 10.0
launcher: pytorch
loadSize: 512
load_pretrain: 
local_rank: 0
lr: 5e-05
max_dataset_size: inf
nThreads: 1
n_blocks_global: 4
n_blocks_local: 3
n_downsample_global: 4
n_layers_D: 3
n_local_enhancers: 1
name: PBAFN_e2e_fs
ndf: 64
netG: global
ngf: 64
niter: 50
niter_decay: 150
niter_fix_global: 0
no_flip: False
no_ganFeat_loss: False
no_html: False
no_lsgan: False
no_vgg_loss: False
norm: instance
num_D: 2
num_gpus: 1
output_nc: 3
phase: train
pool_size: 0
print_freq: 100
resize_or_crop: None
save_epoch_freq: 5
save_latest_freq: 1000
serial_batches: False
tf_log: False
tv_weight: 0.1
use_dropout: False
verbose: False
which_epoch: latest
-------------- End ----------------
------------ Options -------------
PBAFN_gen_checkpoint: None
PBAFN_warp_checkpoint: checkpoints_fs/PBAFN_stage1_fs/PBAFN_warp_epoch_201.pth
PFAFN_gen_checkpoint: None
PFAFN_warp_checkpoint: None
batchSize: 32
beta1: 0.5
checkpoints_dir: checkpoints_fs
continue_train: False
data_type: 32
dataroot: ../dataset/Flow-Style-VTON/VITON_traindata
debug: False
display_freq: 100
display_winsize: 512
fineSize: 512
gpu_ids: [0]
input_nc: 3
isTrain: True
label_nc: 14
lambda_feat: 10.0
launcher: pytorch
loadSize: 512
load_pretrain: 
local_rank: 0
lr: 5e-05
max_dataset_size: inf
nThreads: 1
n_blocks_global: 4
n_blocks_local: 3
n_downsample_global: 4
n_layers_D: 3
n_local_enhancers: 1
name: PBAFN_e2e_fs
ndf: 64
netG: global
ngf: 64
niter: 50
niter_decay: 150
niter_fix_global: 0
no_flip: False
no_ganFeat_loss: False
no_html: False
no_lsgan: False
no_vgg_loss: False
norm: instance
num_D: 2
num_gpus: 1
output_nc: 3
phase: train
pool_size: 0
print_freq: 100
resize_or_crop: None
save_epoch_freq: 5
save_latest_freq: 1000
serial_batches: False
tf_log: False
tv_weight: 0.1
use_dropout: False
verbose: False
which_epoch: latest
-------------- End ----------------
------------ Options -------------
PBAFN_gen_checkpoint: None
PBAFN_warp_checkpoint: checkpoints_fs/PBAFN_stage1_fs/PBAFN_warp_epoch_201.pth
PFAFN_gen_checkpoint: None
PFAFN_warp_checkpoint: None
batchSize: 32
beta1: 0.5
checkpoints_dir: checkpoints_fs
continue_train: False
data_type: 32
dataroot: ../dataset/Flow-Style-VTON/VITON_traindata
debug: False
display_freq: 100
display_winsize: 512
fineSize: 512
gpu_ids: [0]
input_nc: 3
isTrain: True
label_nc: 14
lambda_feat: 10.0
launcher: pytorch
loadSize: 512
load_pretrain: 
local_rank: 0
lr: 5e-05
max_dataset_size: inf
nThreads: 1
n_blocks_global: 4
n_blocks_local: 3
n_downsample_global: 4
n_layers_D: 3
n_local_enhancers: 1
name: PBAFN_e2e_fs
ndf: 64
netG: global
ngf: 64
niter: 50
niter_decay: 150
niter_fix_global: 0
no_flip: False
no_ganFeat_loss: False
no_html: False
no_lsgan: False
no_vgg_loss: False
norm: instance
num_D: 2
num_gpus: 1
output_nc: 3
phase: train
pool_size: 0
print_freq: 100
resize_or_crop: None
save_epoch_freq: 5
save_latest_freq: 1000
serial_batches: False
tf_log: False
tv_weight: 0.1
use_dropout: False
verbose: False
which_epoch: latest
-------------- End ----------------
dataset [AlignedDataset] was created
../dataset/Flow-Style-VTON/VITON_traindata/train_label label
../dataset/Flow-Style-VTON/VITON_traindata/train_img img
../dataset/Flow-Style-VTON/VITON_traindata/train_edge edge
../dataset/Flow-Style-VTON/VITON_traindata/train_color color
AFWM(
  (image_features): FeatureEncoder(
    (encoders): ModuleList(
      (0): Sequential(
        (0): DownSample(
          (block): Sequential(
            (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          )
        )
        (1): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (2): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
      )
      (1): Sequential(
        (0): DownSample(
          (block): Sequential(
            (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          )
        )
        (1): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (2): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
      )
      (2): Sequential(
        (0): DownSample(
          (block): Sequential(
            (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          )
        )
        (1): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (2): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
      )
      (3): Sequential(
        (0): DownSample(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          )
        )
        (1): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (2): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
      )
      (4): Sequential(
        (0): DownSample(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          )
        )
        (1): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (2): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
      )
    )
  )
  (cond_features): FeatureEncoder(
    (encoders): ModuleList(
      (0): Sequential(
        (0): DownSample(
          (block): Sequential(
            (0): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(45, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          )
        )
        (1): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (2): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
      )
      (1): Sequential(
        (0): DownSample(
          (block): Sequential(
            (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          )
        )
        (1): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (2): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
      )
      (2): Sequential(
        (0): DownSample(
          (block): Sequential(
            (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          )
        )
        (1): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (2): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
      )
      (3): Sequential(
        (0): DownSample(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          )
        )
        (1): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (2): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
      )
      (4): Sequential(
        (0): DownSample(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          )
        )
        (1): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (2): ResBlock(
          (block): Sequential(
            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU(inplace=True)
            (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU(inplace=True)
            (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
      )
    )
  )
  (image_FPN): RefinePyramid(
    (adaptive): ModuleList(
      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
      (4): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    )
    (smooth): ModuleList(
      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (cond_FPN): RefinePyramid(
    (adaptive): ModuleList(
      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
      (4): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    )
    (smooth): ModuleList(
      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (aflow_net): AFlowNet(
    (netRefine): ModuleList(
      (0): Sequential(
        (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.1)
        (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): LeakyReLU(negative_slope=0.1)
        (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): LeakyReLU(negative_slope=0.1)
        (6): Conv2d(32, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (1): Sequential(
        (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.1)
        (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): LeakyReLU(negative_slope=0.1)
        (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): LeakyReLU(negative_slope=0.1)
        (6): Conv2d(32, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (2): Sequential(
        (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.1)
        (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): LeakyReLU(negative_slope=0.1)
        (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): LeakyReLU(negative_slope=0.1)
        (6): Conv2d(32, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (3): Sequential(
        (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.1)
        (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): LeakyReLU(negative_slope=0.1)
        (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): LeakyReLU(negative_slope=0.1)
        (6): Conv2d(32, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (4): Sequential(
        (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): LeakyReLU(negative_slope=0.1)
        (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): LeakyReLU(negative_slope=0.1)
        (4): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): LeakyReLU(negative_slope=0.1)
        (6): Conv2d(32, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
    (netStyle): ModuleList(
      (0): StyledConvBlock(
        (conv0): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=256, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn0): LeakyReLU(negative_slope=0.2, inplace=True)
        (conv1): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=49, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn1): LeakyReLU(negative_slope=0.2, inplace=True)
      )
      (1): StyledConvBlock(
        (conv0): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=256, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn0): LeakyReLU(negative_slope=0.2, inplace=True)
        (conv1): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=49, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn1): LeakyReLU(negative_slope=0.2, inplace=True)
      )
      (2): StyledConvBlock(
        (conv0): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=256, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn0): LeakyReLU(negative_slope=0.2, inplace=True)
        (conv1): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=49, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn1): LeakyReLU(negative_slope=0.2, inplace=True)
      )
      (3): StyledConvBlock(
        (conv0): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=256, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn0): LeakyReLU(negative_slope=0.2, inplace=True)
        (conv1): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=49, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn1): LeakyReLU(negative_slope=0.2, inplace=True)
      )
      (4): StyledConvBlock(
        (conv0): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=256, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn0): LeakyReLU(negative_slope=0.2, inplace=True)
        (conv1): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=49, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn1): LeakyReLU(negative_slope=0.2, inplace=True)
      )
    )
    (netF): ModuleList(
      (0): Styled_F_ConvBlock(
        (conv0): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=49, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn0): LeakyReLU(negative_slope=0.2, inplace=True)
        (conv1): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=128, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
      )
      (1): Styled_F_ConvBlock(
        (conv0): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=49, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn0): LeakyReLU(negative_slope=0.2, inplace=True)
        (conv1): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=128, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
      )
      (2): Styled_F_ConvBlock(
        (conv0): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=49, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn0): LeakyReLU(negative_slope=0.2, inplace=True)
        (conv1): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=128, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
      )
      (3): Styled_F_ConvBlock(
        (conv0): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=49, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn0): LeakyReLU(negative_slope=0.2, inplace=True)
        (conv1): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=128, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
      )
      (4): Styled_F_ConvBlock(
        (conv0): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=49, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
        (actvn0): LeakyReLU(negative_slope=0.2, inplace=True)
        (conv1): ModulatedConv2d(
          (mlp_class_std): EqualLinear(
            (linear): Linear(in_features=256, out_features=128, bias=True)
          )
          (padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)
        )
      )
    )
    (cond_style): Sequential(
      (0): Conv2d(256, 128, kernel_size=(8, 6), stride=(1, 1))
      (1): LeakyReLU(negative_slope=0.1)
    )
    (image_style): Sequential(
      (0): Conv2d(256, 128, kernel_size=(8, 6), stride=(1, 1))
      (1): LeakyReLU(negative_slope=0.1)
    )
  )
)
ResUnetGenerator(
  (model): ResUnetSkipConnectionBlock(
    (model): Sequential(
      (0): Conv2d(8, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): ReLU(inplace=True)
      (2): ResidualBlock(
        (relu): ReLU(inplace=True)
        (block): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (3): ResidualBlock(
        (relu): ReLU(inplace=True)
        (block): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (4): ResUnetSkipConnectionBlock(
        (model): Sequential(
          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): ResidualBlock(
            (relu): ReLU(inplace=True)
            (block): Sequential(
              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (4): ResidualBlock(
            (relu): ReLU(inplace=True)
            (block): Sequential(
              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (5): ResUnetSkipConnectionBlock(
            (model): Sequential(
              (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
              (3): ResidualBlock(
                (relu): ReLU(inplace=True)
                (block): Sequential(
                  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (2): ReLU(inplace=True)
                  (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                )
              )
              (4): ResidualBlock(
                (relu): ReLU(inplace=True)
                (block): Sequential(
                  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (2): ReLU(inplace=True)
                  (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                )
              )
              (5): ResUnetSkipConnectionBlock(
                (model): Sequential(
                  (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (2): ReLU(inplace=True)
                  (3): ResidualBlock(
                    (relu): ReLU(inplace=True)
                    (block): Sequential(
                      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (2): ReLU(inplace=True)
                      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    )
                  )
                  (4): ResidualBlock(
                    (relu): ReLU(inplace=True)
                    (block): Sequential(
                      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (2): ReLU(inplace=True)
                      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    )
                  )
                  (5): ResUnetSkipConnectionBlock(
                    (model): Sequential(
                      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                      (1): ReLU(inplace=True)
                      (2): ResidualBlock(
                        (relu): ReLU(inplace=True)
                        (block): Sequential(
                          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                          (2): ReLU(inplace=True)
                          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                        )
                      )
                      (3): ResidualBlock(
                        (relu): ReLU(inplace=True)
                        (block): Sequential(
                          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                          (2): ReLU(inplace=True)
                          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                        )
                      )
                      (4): Upsample(scale_factor=2.0, mode=nearest)
                      (5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (7): ReLU(inplace=True)
                      (8): ResidualBlock(
                        (relu): ReLU(inplace=True)
                        (block): Sequential(
                          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                          (2): ReLU(inplace=True)
                          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                        )
                      )
                      (9): ResidualBlock(
                        (relu): ReLU(inplace=True)
                        (block): Sequential(
                          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                          (2): ReLU(inplace=True)
                          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                        )
                      )
                    )
                  )
                  (6): Upsample(scale_factor=2.0, mode=nearest)
                  (7): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (9): ReLU(inplace=True)
                  (10): ResidualBlock(
                    (relu): ReLU(inplace=True)
                    (block): Sequential(
                      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (2): ReLU(inplace=True)
                      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    )
                  )
                  (11): ResidualBlock(
                    (relu): ReLU(inplace=True)
                    (block): Sequential(
                      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (2): ReLU(inplace=True)
                      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    )
                  )
                )
              )
              (6): Upsample(scale_factor=2.0, mode=nearest)
              (7): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (9): ReLU(inplace=True)
              (10): ResidualBlock(
                (relu): ReLU(inplace=True)
                (block): Sequential(
                  (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (2): ReLU(inplace=True)
                  (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                )
              )
              (11): ResidualBlock(
                (relu): ReLU(inplace=True)
                (block): Sequential(
                  (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (2): ReLU(inplace=True)
                  (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                )
              )
            )
          )
          (6): Upsample(scale_factor=2.0, mode=nearest)
          (7): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (9): ReLU(inplace=True)
          (10): ResidualBlock(
            (relu): ReLU(inplace=True)
            (block): Sequential(
              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (11): ResidualBlock(
            (relu): ReLU(inplace=True)
            (block): Sequential(
              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
        )
      )
      (5): Upsample(scale_factor=2.0, mode=nearest)
      (6): Conv2d(128, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
  )
)
train_PBAFN_e2e_fs.py:105: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  t_mask = torch.FloatTensor((data['label'].cpu().numpy()==7).astype(np.float))
train_PBAFN_e2e_fs.py:108: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  pre_clothes_edge = torch.FloatTensor((edge.detach().numpy() > 0.5).astype(np.int))
train_PBAFN_e2e_fs.py:111: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  person_clothes_edge = torch.FloatTensor((data['label'].cpu().numpy()==4).astype(np.int))
train_PBAFN_e2e_fs.py:120: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  face_mask = torch.FloatTensor((data['label'].cpu().numpy()==1).astype(np.int))+torch.FloatTensor((data['label'].cpu().numpy()==12).astype(np.int))
train_PBAFN_e2e_fs.py:123: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  + torch.FloatTensor((data['label'].cpu().numpy()==10).astype(np.int))
train_PBAFN_e2e_fs.py:129: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  arm_mask = torch.FloatTensor((data['label'].cpu().numpy()==11).astype(np.float)) + torch.FloatTensor((data['label'].cpu().numpy()==13).astype(np.float))
train_PBAFN_e2e_fs.py:130: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  hand_mask = torch.FloatTensor((data['densepose'].cpu().numpy()==3).astype(np.int)) + torch.FloatTensor((data['densepose'].cpu().numpy()==4).astype(np.int))
train_PBAFN_e2e_fs.py:136: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  +torch.FloatTensor((data['densepose'].cpu().numpy()==21).astype(np.int))+torch.FloatTensor((data['densepose'].cpu().numpy()==22))
/opt/conda/lib/python3.7/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272168290/work/aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:4004: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  "Default grid_sample and affine_grid behavior has changed "
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:3635: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  "See the documentation of nn.Upsample for details.".format(mode)
/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  "The default behavior for interpolate/upsample with float scale_factor changed "
2023.01.21-11:36:31:100:[step-100/88800: 0.11%]--[loss-4.836401: wl-3.127961, gl-3.272420]--[lr-0.000050]--[ETA-1 day, 7:14:24]
2023.01.21-11:38:33:200:[step-200/88800: 0.23%]--[loss-4.467239: wl-2.974073, gl-2.980203]--[lr-0.000050]--[ETA-1 day, 2:22:23]
2023.01.21-11:40:19:300:[step-300/88800: 0.34%]--[loss-4.580407: wl-3.229565, gl-2.965624]--[lr-0.000050]--[ETA-1 day, 1:32:39]
2023.01.21-11:42:02:400:[step-400/88800: 0.45%]--[loss-4.526783: wl-3.289284, gl-2.882141]--[lr-0.000050]--[ETA-1 day, 2:03:10]
End of epoch 1 / 200 	 Time Taken: 488 sec
2023.01.21-11:43:47:56:[step-500/88800: 0.56%]--[loss-4.292839: wl-3.135666, gl-2.725005]--[lr-0.000050]--[ETA-1 day, 1:32:34]
2023.01.21-11:45:30:156:[step-600/88800: 0.68%]--[loss-4.150365: wl-3.048216, gl-2.626257]--[lr-0.000050]--[ETA-1 day, 1:24:59]
2023.01.21-11:47:13:256:[step-700/88800: 0.79%]--[loss-4.393799: wl-3.152073, gl-2.817762]--[lr-0.000050]--[ETA-1 day, 0:51:40]
2023.01.21-11:48:57:356:[step-800/88800: 0.90%]--[loss-4.180222: wl-3.204768, gl-2.577837]--[lr-0.000050]--[ETA-1 day, 0:29:25]
End of epoch 2 / 200 	 Time Taken: 460 sec
2023.01.21-11:50:41:12:[step-900/88800: 1.01%]--[loss-4.228087: wl-3.097998, gl-2.679089]--[lr-0.000050]--[ETA-1 day, 1:12:38]
2023.01.21-11:52:25:112:[step-1000/88800: 1.13%]--[loss-4.540784: wl-3.383666, gl-2.848951]--[lr-0.000050]--[ETA-1 day, 1:05:18]
2023.01.21-11:54:08:212:[step-1100/88800: 1.24%]--[loss-3.968502: wl-2.963376, gl-2.486814]--[lr-0.000050]--[ETA-1 day, 1:04:34]
2023.01.21-11:55:52:312:[step-1200/88800: 1.35%]--[loss-4.372665: wl-3.460144, gl-2.642593]--[lr-0.000050]--[ETA-1 day, 2:36:56]
2023.01.21-11:57:36:412:[step-1300/88800: 1.46%]--[loss-4.113129: wl-3.293962, gl-2.466148]--[lr-0.000050]--[ETA-1 day, 1:28:16]
End of epoch 3 / 200 	 Time Taken: 461 sec
2023.01.21-11:59:21:68:[step-1400/88800: 1.58%]--[loss-4.077056: wl-3.219543, gl-2.467284]--[lr-0.000050]--[ETA-1 day, 0:27:56]
2023.01.21-12:01:05:168:[step-1500/88800: 1.69%]--[loss-4.536547: wl-3.584838, gl-2.744128]--[lr-0.000050]--[ETA-1 day, 0:41:32]
2023.01.21-12:02:47:268:[step-1600/88800: 1.80%]--[loss-4.047204: wl-3.190787, gl-2.451811]--[lr-0.000050]--[ETA-1 day, 0:42:50]
2023.01.21-12:04:30:368:[step-1700/88800: 1.91%]--[loss-4.034704: wl-3.142259, gl-2.463574]--[lr-0.000050]--[ETA-1 day, 2:06:38]
End of epoch 4 / 200 	 Time Taken: 459 sec
2023.01.21-12:06:15:24:[step-1800/88800: 2.03%]--[loss-4.133995: wl-3.391578, gl-2.438206]--[lr-0.000050]--[ETA-1 day, 0:29:12]
2023.01.21-12:07:58:124:[step-1900/88800: 2.14%]--[loss-4.011333: wl-3.200542, gl-2.411062]--[lr-0.000050]--[ETA-1 day, 0:23:54]
2023.01.21-12:09:42:224:[step-2000/88800: 2.25%]--[loss-3.805173: wl-2.934551, gl-2.337898]--[lr-0.000050]--[ETA-1 day, 0:03:33]
2023.01.21-12:11:26:324:[step-2100/88800: 2.36%]--[loss-4.211880: wl-3.347796, gl-2.537982]--[lr-0.000050]--[ETA-1 day, 1:21:54]
2023.01.21-12:13:09:424:[step-2200/88800: 2.48%]--[loss-4.162967: wl-3.456100, gl-2.434916]--[lr-0.000050]--[ETA-1 day, 1:28:25]
End of epoch 5 / 200 	 Time Taken: 461 sec
saving the model at the end of epoch 5, iters 2220
2023.01.21-12:14:55:80:[step-2300/88800: 2.59%]--[loss-4.173967: wl-3.636912, gl-2.355512]--[lr-0.000050]--[ETA-1 day, 0:22:25]
2023.01.21-12:16:39:180:[step-2400/88800: 2.70%]--[loss-4.246848: wl-3.523563, gl-2.485066]--[lr-0.000050]--[ETA-1 day, 0:28:01]
2023.01.21-12:18:23:280:[step-2500/88800: 2.82%]--[loss-4.433174: wl-3.702722, gl-2.581813]--[lr-0.000050]--[ETA-23:41:53]
2023.01.21-12:20:06:380:[step-2600/88800: 2.93%]--[loss-3.830071: wl-3.114298, gl-2.272922]--[lr-0.000050]--[ETA-1 day, 0:21:40]
End of epoch 6 / 200 	 Time Taken: 462 sec
2023.01.21-12:21:52:36:[step-2700/88800: 3.04%]--[loss-3.995570: wl-3.418869, gl-2.286136]--[lr-0.000050]--[ETA-1 day, 0:13:38]
2023.01.21-12:23:35:136:[step-2800/88800: 3.15%]--[loss-3.901446: wl-3.189750, gl-2.306571]--[lr-0.000050]--[ETA-1 day, 1:17:25]
2023.01.21-12:25:19:236:[step-2900/88800: 3.27%]--[loss-3.929218: wl-3.268846, gl-2.294795]--[lr-0.000050]--[ETA-1 day, 0:11:57]
2023.01.21-12:27:02:336:[step-3000/88800: 3.38%]--[loss-4.077965: wl-3.462700, gl-2.346615]--[lr-0.000050]--[ETA-1 day, 0:56:40]
2023.01.21-12:28:45:436:[step-3100/88800: 3.49%]--[loss-3.961926: wl-3.492686, gl-2.215583]--[lr-0.000050]--[ETA-1 day, 0:06:12]
End of epoch 7 / 200 	 Time Taken: 460 sec
2023.01.21-12:30:31:92:[step-3200/88800: 3.60%]--[loss-4.528068: wl-4.278605, gl-2.388765]--[lr-0.000050]--[ETA-1 day, 0:20:18]
2023.01.21-12:32:14:192:[step-3300/88800: 3.72%]--[loss-3.996669: wl-3.419803, gl-2.286767]--[lr-0.000050]--[ETA-23:36:03]
2023.01.21-12:33:58:292:[step-3400/88800: 3.83%]--[loss-3.675277: wl-2.904274, gl-2.223140]--[lr-0.000050]--[ETA-23:46:20]
2023.01.21-12:35:41:392:[step-3500/88800: 3.94%]--[loss-3.829619: wl-3.197852, gl-2.230693]--[lr-0.000050]--[ETA-1 day, 0:02:54]
End of epoch 8 / 200 	 Time Taken: 461 sec
2023.01.21-12:37:26:48:[step-3600/88800: 4.05%]--[loss-3.906327: wl-3.412318, gl-2.200168]--[lr-0.000050]--[ETA-1 day, 0:08:52]
2023.01.21-12:39:10:148:[step-3700/88800: 4.17%]--[loss-3.943609: wl-3.262228, gl-2.312495]--[lr-0.000050]--[ETA-1 day, 1:17:49]
2023.01.21-12:40:53:248:[step-3800/88800: 4.28%]--[loss-3.813699: wl-3.204879, gl-2.211259]--[lr-0.000050]--[ETA-1 day, 0:38:30]
2023.01.21-12:42:37:348:[step-3900/88800: 4.39%]--[loss-3.949519: wl-3.423243, gl-2.237898]--[lr-0.000050]--[ETA-1 day, 0:14:00]
End of epoch 9 / 200 	 Time Taken: 460 sec
2023.01.21-12:44:21:4:[step-4000/88800: 4.50%]--[loss-4.145039: wl-3.869740, gl-2.210169]--[lr-0.000050]--[ETA-1 day, 3:38:47]
2023.01.21-12:46:05:104:[step-4100/88800: 4.62%]--[loss-4.317076: wl-4.071827, gl-2.281162]--[lr-0.000050]--[ETA-1 day, 0:14:01]
2023.01.21-12:47:49:204:[step-4200/88800: 4.73%]--[loss-3.757901: wl-3.262311, gl-2.126745]--[lr-0.000050]--[ETA-1 day, 0:03:37]
2023.01.21-12:49:31:304:[step-4300/88800: 4.84%]--[loss-4.100165: wl-3.572076, gl-2.314127]--[lr-0.000050]--[ETA-1 day, 0:16:03]
2023.01.21-12:51:14:404:[step-4400/88800: 4.95%]--[loss-3.904926: wl-3.484450, gl-2.162701]--[lr-0.000050]--[ETA-23:59:41]
End of epoch 10 / 200 	 Time Taken: 459 sec
saving the model at the end of epoch 10, iters 4440
2023.01.21-12:52:59:60:[step-4500/88800: 5.07%]--[loss-3.950920: wl-3.502436, gl-2.199702]--[lr-0.000050]--[ETA-1 day, 0:07:51]
2023.01.21-12:54:42:160:[step-4600/88800: 5.18%]--[loss-3.833939: wl-3.249796, gl-2.209041]--[lr-0.000050]--[ETA-23:44:57]
2023.01.21-12:56:25:260:[step-4700/88800: 5.29%]--[loss-3.797306: wl-3.230853, gl-2.181880]--[lr-0.000050]--[ETA-1 day, 0:11:59]
2023.01.21-12:58:08:360:[step-4800/88800: 5.41%]--[loss-3.813200: wl-3.178783, gl-2.223809]--[lr-0.000050]--[ETA-23:03:25]
End of epoch 11 / 200 	 Time Taken: 459 sec
2023.01.21-12:59:53:16:[step-4900/88800: 5.52%]--[loss-3.941496: wl-3.561957, gl-2.160517]--[lr-0.000050]--[ETA-23:13:26]
2023.01.21-13:01:36:116:[step-5000/88800: 5.63%]--[loss-3.629211: wl-3.147854, gl-2.055284]--[lr-0.000050]--[ETA-23:52:12]
2023.01.21-13:03:19:216:[step-5100/88800: 5.74%]--[loss-3.918479: wl-3.395109, gl-2.220925]--[lr-0.000050]--[ETA-23:05:12]
2023.01.21-13:05:02:316:[step-5200/88800: 5.86%]--[loss-3.803947: wl-3.355284, gl-2.126305]--[lr-0.000050]--[ETA-22:59:51]
2023.01.21-13:06:46:416:[step-5300/88800: 5.97%]--[loss-3.878597: wl-3.460330, gl-2.148432]--[lr-0.000050]--[ETA-1 day, 0:07:46]
End of epoch 12 / 200 	 Time Taken: 459 sec
2023.01.21-13:08:31:72:[step-5400/88800: 6.08%]--[loss-3.490815: wl-2.813630, gl-2.084001]--[lr-0.000050]--[ETA-1 day, 0:26:06]
2023.01.21-13:10:14:172:[step-5500/88800: 6.19%]--[loss-3.728412: wl-3.310504, gl-2.073160]--[lr-0.000050]--[ETA-23:38:27]
2023.01.21-13:11:57:272:[step-5600/88800: 6.31%]--[loss-3.698023: wl-3.209294, gl-2.093377]--[lr-0.000050]--[ETA-23:21:57]
2023.01.21-13:13:40:372:[step-5700/88800: 6.42%]--[loss-3.651739: wl-3.120745, gl-2.091367]--[lr-0.000050]--[ETA-1 day, 0:13:06]
End of epoch 13 / 200 	 Time Taken: 459 sec
2023.01.21-13:15:25:28:[step-5800/88800: 6.53%]--[loss-3.574800: wl-3.111334, gl-2.019134]--[lr-0.000050]--[ETA-1 day, 0:09:43]
2023.01.21-13:17:08:128:[step-5900/88800: 6.64%]--[loss-3.678650: wl-3.174633, gl-2.091333]--[lr-0.000050]--[ETA-22:37:56]
2023.01.21-13:18:51:228:[step-6000/88800: 6.76%]--[loss-3.657370: wl-3.225583, gl-2.044579]--[lr-0.000050]--[ETA-23:43:02]
2023.01.21-13:20:34:328:[step-6100/88800: 6.87%]--[loss-3.691925: wl-3.272846, gl-2.055502]--[lr-0.000050]--[ETA-23:33:15]
2023.01.21-13:22:17:428:[step-6200/88800: 6.98%]--[loss-3.501389: wl-3.005682, gl-1.998548]--[lr-0.000050]--[ETA-23:57:43]
End of epoch 14 / 200 	 Time Taken: 458 sec
2023.01.21-13:24:01:84:[step-6300/88800: 7.09%]--[loss-3.520973: wl-3.132455, gl-1.954745]--[lr-0.000050]--[ETA-1 day, 0:02:51]
2023.01.21-13:25:44:184:[step-6400/88800: 7.21%]--[loss-3.667614: wl-3.255181, gl-2.040024]--[lr-0.000050]--[ETA-23:19:46]
2023.01.21-13:27:27:284:[step-6500/88800: 7.32%]--[loss-3.675884: wl-3.302288, gl-2.024740]--[lr-0.000050]--[ETA-23:43:38]
2023.01.21-13:29:10:384:[step-6600/88800: 7.43%]--[loss-4.040061: wl-3.727046, gl-2.176538]--[lr-0.000050]--[ETA-1 day, 1:01:24]
End of epoch 15 / 200 	 Time Taken: 458 sec
saving the model at the end of epoch 15, iters 6660
2023.01.21-13:30:55:40:[step-6700/88800: 7.55%]--[loss-3.668205: wl-3.364734, gl-1.985838]--[lr-0.000050]--[ETA-23:52:43]
2023.01.21-13:32:38:140:[step-6800/88800: 7.66%]--[loss-3.978837: wl-3.985816, gl-1.985929]--[lr-0.000050]--[ETA-22:55:09]
2023.01.21-13:34:20:240:[step-6900/88800: 7.77%]--[loss-3.523022: wl-3.113273, gl-1.966385]--[lr-0.000050]--[ETA-23:11:29]
2023.01.21-13:36:03:340:[step-7000/88800: 7.88%]--[loss-3.631156: wl-3.258832, gl-2.001740]--[lr-0.000050]--[ETA-23:25:38]
2023.01.21-13:37:46:440:[step-7100/88800: 8.00%]--[loss-3.544295: wl-3.198286, gl-1.945152]--[lr-0.000050]--[ETA-22:49:20]
End of epoch 16 / 200 	 Time Taken: 457 sec
2023.01.21-13:39:31:96:[step-7200/88800: 8.11%]--[loss-3.516641: wl-3.100904, gl-1.966188]--[lr-0.000050]--[ETA-22:35:04]
2023.01.21-13:41:14:196:[step-7300/88800: 8.22%]--[loss-3.616881: wl-3.153307, gl-2.040227]--[lr-0.000050]--[ETA-22:58:56]
2023.01.21-13:42:58:296:[step-7400/88800: 8.33%]--[loss-3.667483: wl-3.461045, gl-1.936961]--[lr-0.000050]--[ETA-23:44:41]
2023.01.21-13:44:42:396:[step-7500/88800: 8.45%]--[loss-3.510447: wl-3.209544, gl-1.905675]--[lr-0.000050]--[ETA-1 day, 0:19:41]
End of epoch 17 / 200 	 Time Taken: 461 sec
2023.01.21-13:46:27:52:[step-7600/88800: 8.56%]--[loss-3.714799: wl-3.402177, gl-2.013711]--[lr-0.000050]--[ETA-23:07:01]
2023.01.21-13:48:10:152:[step-7700/88800: 8.67%]--[loss-3.588933: wl-3.265097, gl-1.956385]--[lr-0.000050]--[ETA-23:20:33]
2023.01.21-13:49:52:252:[step-7800/88800: 8.78%]--[loss-3.543082: wl-3.272956, gl-1.906604]--[lr-0.000050]--[ETA-23:49:45]
2023.01.21-13:51:35:352:[step-7900/88800: 8.90%]--[loss-3.560021: wl-3.196634, gl-1.961704]--[lr-0.000050]--[ETA-23:47:42]
End of epoch 18 / 200 	 Time Taken: 458 sec
2023.01.21-13:53:19:8:[step-8000/88800: 9.01%]--[loss-3.602905: wl-3.276437, gl-1.964687]--[lr-0.000050]--[ETA-23:14:57]
2023.01.21-13:55:03:108:[step-8100/88800: 9.12%]--[loss-3.564852: wl-3.262190, gl-1.933757]--[lr-0.000050]--[ETA-23:42:51]
2023.01.21-13:56:47:208:[step-8200/88800: 9.23%]--[loss-3.742383: wl-3.359478, gl-2.062644]--[lr-0.000050]--[ETA-23:48:22]
2023.01.21-13:58:30:308:[step-8300/88800: 9.35%]--[loss-3.764367: wl-3.377470, gl-2.075632]--[lr-0.000050]--[ETA-22:14:34]
2023.01.21-14:00:14:408:[step-8400/88800: 9.46%]--[loss-3.484725: wl-3.115228, gl-1.927111]--[lr-0.000050]--[ETA-22:22:13]
End of epoch 19 / 200 	 Time Taken: 461 sec
2023.01.21-14:01:59:64:[step-8500/88800: 9.57%]--[loss-3.775228: wl-3.441159, gl-2.054648]--[lr-0.000050]--[ETA-1 day, 0:14:16]
2023.01.21-14:03:43:164:[step-8600/88800: 9.68%]--[loss-3.540935: wl-3.267703, gl-1.907083]--[lr-0.000050]--[ETA-22:47:22]
2023.01.21-14:05:26:264:[step-8700/88800: 9.80%]--[loss-3.595323: wl-3.322974, gl-1.933835]--[lr-0.000050]--[ETA-22:19:13]
2023.01.21-14:07:09:364:[step-8800/88800: 9.91%]--[loss-3.769416: wl-3.391683, gl-2.073575]--[lr-0.000050]--[ETA-22:50:39]
End of epoch 20 / 200 	 Time Taken: 461 sec
saving the model at the end of epoch 20, iters 8880
2023.01.21-14:08:55:20:[step-8900/88800: 10.02%]--[loss-3.914219: wl-3.822771, gl-2.002834]--[lr-0.000050]--[ETA-22:39:39]
2023.01.21-14:10:38:120:[step-9000/88800: 10.14%]--[loss-3.842134: wl-3.761263, gl-1.961503]--[lr-0.000050]--[ETA-22:11:47]
2023.01.21-14:12:22:220:[step-9100/88800: 10.25%]--[loss-3.540962: wl-3.239606, gl-1.921159]--[lr-0.000050]--[ETA-1 day, 0:19:15]
2023.01.21-14:14:07:320:[step-9200/88800: 10.36%]--[loss-4.065585: wl-4.080019, gl-2.025575]--[lr-0.000050]--[ETA-23:56:01]
2023.01.21-14:15:51:420:[step-9300/88800: 10.47%]--[loss-3.797099: wl-3.481843, gl-2.056178]--[lr-0.000050]--[ETA-22:45:34]
End of epoch 21 / 200 	 Time Taken: 463 sec
2023.01.21-14:17:37:76:[step-9400/88800: 10.59%]--[loss-3.761059: wl-3.443447, gl-2.039335]--[lr-0.000050]--[ETA-21:57:28]
2023.01.21-14:19:20:176:[step-9500/88800: 10.70%]--[loss-3.662564: wl-3.384039, gl-1.970545]--[lr-0.000050]--[ETA-22:45:02]
2023.01.21-14:21:03:276:[step-9600/88800: 10.81%]--[loss-3.674184: wl-3.539282, gl-1.904543]--[lr-0.000050]--[ETA-22:27:55]
2023.01.21-14:22:48:376:[step-9700/88800: 10.92%]--[loss-3.739551: wl-3.563488, gl-1.957807]--[lr-0.000050]--[ETA-23:52:27]
End of epoch 22 / 200 	 Time Taken: 462 sec
2023.01.21-14:24:33:32:[step-9800/88800: 11.04%]--[loss-3.893335: wl-3.607605, gl-2.089532]--[lr-0.000050]--[ETA-22:22:42]
2023.01.21-14:26:16:132:[step-9900/88800: 11.15%]--[loss-3.646192: wl-3.484980, gl-1.903702]--[lr-0.000050]--[ETA-22:38:44]
2023.01.21-14:27:59:232:[step-10000/88800: 11.26%]--[loss-3.770599: wl-3.694153, gl-1.923522]--[lr-0.000050]--[ETA-22:14:04]
2023.01.21-14:29:44:332:[step-10100/88800: 11.37%]--[loss-3.449273: wl-3.175903, gl-1.861322]--[lr-0.000050]--[ETA-22:30:22]
2023.01.21-14:31:28:432:[step-10200/88800: 11.49%]--[loss-3.998366: wl-4.029154, gl-1.983789]--[lr-0.000050]--[ETA-22:16:35]
End of epoch 23 / 200 	 Time Taken: 461 sec
2023.01.21-14:33:14:88:[step-10300/88800: 11.60%]--[loss-3.716438: wl-3.444988, gl-1.993944]--[lr-0.000050]--[ETA-22:12:49]
2023.01.21-14:34:58:188:[step-10400/88800: 11.71%]--[loss-3.836839: wl-3.709140, gl-1.982269]--[lr-0.000050]--[ETA-22:00:22]
2023.01.21-14:36:42:288:[step-10500/88800: 11.82%]--[loss-3.584343: wl-3.388089, gl-1.890299]--[lr-0.000050]--[ETA-22:18:04]
2023.01.21-14:38:27:388:[step-10600/88800: 11.94%]--[loss-3.651714: wl-3.459554, gl-1.921937]--[lr-0.000050]--[ETA-23:33:16]
End of epoch 24 / 200 	 Time Taken: 464 sec
2023.01.21-14:40:12:44:[step-10700/88800: 12.05%]--[loss-3.507431: wl-3.232523, gl-1.891169]--[lr-0.000050]--[ETA-23:36:12]
2023.01.21-14:41:56:144:[step-10800/88800: 12.16%]--[loss-4.013575: wl-4.033705, gl-1.996722]--[lr-0.000050]--[ETA-1 day, 0:07:59]
2023.01.21-14:43:40:244:[step-10900/88800: 12.27%]--[loss-3.592448: wl-3.335674, gl-1.924611]--[lr-0.000050]--[ETA-22:28:05]
2023.01.21-14:45:24:344:[step-11000/88800: 12.39%]--[loss-3.510965: wl-3.203084, gl-1.909423]--[lr-0.000050]--[ETA-23:53:11]
2023.01.21-14:47:08:444:[step-11100/88800: 12.50%]--[loss-3.559152: wl-3.376136, gl-1.871084]--[lr-0.000050]--[ETA-22:06:50]
End of epoch 25 / 200 	 Time Taken: 463 sec
saving the model at the end of epoch 25, iters 11100
2023.01.21-14:48:56:100:[step-11200/88800: 12.61%]--[loss-3.478829: wl-3.187131, gl-1.885263]--[lr-0.000050]--[ETA-23:29:15]
2023.01.21-14:50:40:200:[step-11300/88800: 12.73%]--[loss-3.565624: wl-3.254760, gl-1.938244]--[lr-0.000050]--[ETA-22:02:02]
2023.01.21-14:52:24:300:[step-11400/88800: 12.84%]--[loss-3.695084: wl-3.644252, gl-1.872958]--[lr-0.000050]--[ETA-23:00:00]
2023.01.21-14:54:08:400:[step-11500/88800: 12.95%]--[loss-3.462094: wl-3.184349, gl-1.869920]--[lr-0.000050]--[ETA-21:24:41]
End of epoch 26 / 200 	 Time Taken: 464 sec
2023.01.21-14:55:53:56:[step-11600/88800: 13.06%]--[loss-3.629302: wl-3.526086, gl-1.866259]--[lr-0.000050]--[ETA-22:39:59]
2023.01.21-14:57:36:156:[step-11700/88800: 13.18%]--[loss-3.631820: wl-3.410656, gl-1.926492]--[lr-0.000050]--[ETA-21:56:54]
2023.01.21-14:59:19:256:[step-11800/88800: 13.29%]--[loss-3.714493: wl-3.455167, gl-1.986910]--[lr-0.000050]--[ETA-22:18:05]
2023.01.21-15:01:02:356:[step-11900/88800: 13.40%]--[loss-3.459259: wl-3.281448, gl-1.818534]--[lr-0.000050]--[ETA-21:46:22]
End of epoch 27 / 200 	 Time Taken: 459 sec
2023.01.21-15:02:47:12:[step-12000/88800: 13.51%]--[loss-3.600762: wl-3.452619, gl-1.874452]--[lr-0.000050]--[ETA-22:23:33]
2023.01.21-15:04:31:112:[step-12100/88800: 13.63%]--[loss-3.492871: wl-3.231042, gl-1.877349]--[lr-0.000050]--[ETA-21:48:18]
2023.01.21-15:06:14:212:[step-12200/88800: 13.74%]--[loss-3.507579: wl-3.419525, gl-1.797817]--[lr-0.000050]--[ETA-22:17:57]
2023.01.21-15:07:58:312:[step-12300/88800: 13.85%]--[loss-3.475440: wl-3.205304, gl-1.872788]--[lr-0.000050]--[ETA-21:15:02]
2023.01.21-15:09:42:412:[step-12400/88800: 13.96%]--[loss-3.298737: wl-2.946552, gl-1.825461]--[lr-0.000050]--[ETA-22:33:33]
End of epoch 28 / 200 	 Time Taken: 462 sec
2023.01.21-15:11:27:68:[step-12500/88800: 14.08%]--[loss-3.254363: wl-2.994039, gl-1.757344]--[lr-0.000050]--[ETA-21:08:24]
2023.01.21-15:13:11:168:[step-12600/88800: 14.19%]--[loss-3.488340: wl-3.214482, gl-1.881099]--[lr-0.000050]--[ETA-22:00:42]
2023.01.21-15:14:55:268:[step-12700/88800: 14.30%]--[loss-3.635243: wl-3.387033, gl-1.941727]--[lr-0.000050]--[ETA-22:10:42]
2023.01.21-15:16:38:368:[step-12800/88800: 14.41%]--[loss-3.313678: wl-3.020308, gl-1.803524]--[lr-0.000050]--[ETA-21:34:03]
End of epoch 29 / 200 	 Time Taken: 460 sec
2023.01.21-15:18:23:24:[step-12900/88800: 14.53%]--[loss-3.414800: wl-3.137203, gl-1.846199]--[lr-0.000050]--[ETA-21:02:50]
2023.01.21-15:20:07:124:[step-13000/88800: 14.64%]--[loss-3.758355: wl-3.678302, gl-1.919204]--[lr-0.000050]--[ETA-21:49:24]
2023.01.21-15:21:51:224:[step-13100/88800: 14.75%]--[loss-3.611094: wl-3.443613, gl-1.889287]--[lr-0.000050]--[ETA-22:15:30]
2023.01.21-15:23:35:324:[step-13200/88800: 14.86%]--[loss-3.584192: wl-3.383156, gl-1.892614]--[lr-0.000050]--[ETA-22:31:47]
2023.01.21-15:25:18:424:[step-13300/88800: 14.98%]--[loss-3.377255: wl-3.143596, gl-1.805457]--[lr-0.000050]--[ETA-21:49:49]
End of epoch 30 / 200 	 Time Taken: 462 sec
saving the model at the end of epoch 30, iters 13320
2023.01.21-15:27:04:80:[step-13400/88800: 15.09%]--[loss-3.638734: wl-3.558305, gl-1.859582]--[lr-0.000050]--[ETA-21:12:12]
2023.01.21-15:28:48:180:[step-13500/88800: 15.20%]--[loss-3.533554: wl-3.381649, gl-1.842730]--[lr-0.000050]--[ETA-21:15:08]
2023.01.21-15:30:31:280:[step-13600/88800: 15.32%]--[loss-3.665650: wl-3.578587, gl-1.876357]--[lr-0.000050]--[ETA-22:38:31]
2023.01.21-15:32:15:380:[step-13700/88800: 15.43%]--[loss-3.443080: wl-3.222097, gl-1.832031]--[lr-0.000050]--[ETA-21:18:32]
End of epoch 31 / 200 	 Time Taken: 461 sec
2023.01.21-15:34:00:36:[step-13800/88800: 15.54%]--[loss-3.676372: wl-3.457330, gl-1.947707]--[lr-0.000050]--[ETA-21:58:53]
2023.01.21-15:35:43:136:[step-13900/88800: 15.65%]--[loss-3.783463: wl-3.750612, gl-1.908157]--[lr-0.000050]--[ETA-21:12:10]
2023.01.21-15:37:26:236:[step-14000/88800: 15.77%]--[loss-3.450697: wl-3.340971, gl-1.780212]--[lr-0.000050]--[ETA-20:48:54]
2023.01.21-15:39:09:336:[step-14100/88800: 15.88%]--[loss-4.051340: wl-4.254443, gl-1.924119]--[lr-0.000050]--[ETA-22:32:05]
2023.01.21-15:40:52:436:[step-14200/88800: 15.99%]--[loss-3.582239: wl-3.436405, gl-1.864037]--[lr-0.000050]--[ETA-21:27:23]
End of epoch 32 / 200 	 Time Taken: 459 sec
2023.01.21-15:42:37:92:[step-14300/88800: 16.10%]--[loss-4.016795: wl-3.962369, gl-2.035611]--[lr-0.000050]--[ETA-21:33:26]
2023.01.21-15:44:21:192:[step-14400/88800: 16.22%]--[loss-3.662135: wl-3.594818, gl-1.864726]--[lr-0.000050]--[ETA-21:21:36]
2023.01.21-15:46:03:292:[step-14500/88800: 16.33%]--[loss-3.409571: wl-3.185090, gl-1.817026]--[lr-0.000050]--[ETA-21:32:18]
2023.01.21-15:47:47:392:[step-14600/88800: 16.44%]--[loss-3.591424: wl-3.445834, gl-1.868507]--[lr-0.000050]--[ETA-20:51:18]
End of epoch 33 / 200 	 Time Taken: 459 sec
2023.01.21-15:49:32:48:[step-14700/88800: 16.55%]--[loss-3.749251: wl-3.604846, gl-1.946828]--[lr-0.000050]--[ETA-21:10:25]
2023.01.21-15:51:15:148:[step-14800/88800: 16.67%]--[loss-3.452034: wl-3.127970, gl-1.888049]--[lr-0.000050]--[ETA-21:18:37]
2023.01.21-15:52:59:248:[step-14900/88800: 16.78%]--[loss-3.616075: wl-3.653213, gl-1.789469]--[lr-0.000050]--[ETA-21:26:46]
2023.01.21-15:54:43:348:[step-15000/88800: 16.89%]--[loss-3.515963: wl-3.254220, gl-1.888853]--[lr-0.000050]--[ETA-21:01:17]
End of epoch 34 / 200 	 Time Taken: 461 sec
2023.01.21-15:56:28:4:[step-15100/88800: 17.00%]--[loss-3.469935: wl-3.294707, gl-1.822581]--[lr-0.000050]--[ETA-21:23:24]
2023.01.21-15:58:11:104:[step-15200/88800: 17.12%]--[loss-3.324978: wl-3.042438, gl-1.803759]--[lr-0.000050]--[ETA-20:46:38]
2023.01.21-15:59:55:204:[step-15300/88800: 17.23%]--[loss-3.551599: wl-3.404344, gl-1.849427]--[lr-0.000050]--[ETA-20:59:50]
2023.01.21-16:01:39:304:[step-15400/88800: 17.34%]--[loss-3.381581: wl-3.059315, gl-1.851924]--[lr-0.000050]--[ETA-20:28:35]
2023.01.21-16:03:22:404:[step-15500/88800: 17.45%]--[loss-3.475756: wl-3.326174, gl-1.812669]--[lr-0.000050]--[ETA-20:17:32]
End of epoch 35 / 200 	 Time Taken: 461 sec
saving the model at the end of epoch 35, iters 15540
2023.01.21-16:05:08:60:[step-15600/88800: 17.57%]--[loss-3.506361: wl-3.387674, gl-1.812524]--[lr-0.000050]--[ETA-20:20:00]
2023.01.21-16:06:51:160:[step-15700/88800: 17.68%]--[loss-3.455135: wl-3.189235, gl-1.860518]--[lr-0.000050]--[ETA-20:45:10]
2023.01.21-16:08:35:260:[step-15800/88800: 17.79%]--[loss-3.486829: wl-3.431202, gl-1.771228]--[lr-0.000050]--[ETA-21:15:51]
2023.01.21-16:10:18:360:[step-15900/88800: 17.91%]--[loss-3.471539: wl-3.316427, gl-1.813326]--[lr-0.000050]--[ETA-20:43:04]
End of epoch 36 / 200 	 Time Taken: 461 sec
2023.01.21-16:12:04:16:[step-16000/88800: 18.02%]--[loss-3.960166: wl-4.044103, gl-1.938114]--[lr-0.000050]--[ETA-20:36:24]
2023.01.21-16:13:48:116:[step-16100/88800: 18.13%]--[loss-3.368988: wl-3.285720, gl-1.726127]--[lr-0.000050]--[ETA-20:57:19]
2023.01.21-16:15:31:216:[step-16200/88800: 18.24%]--[loss-3.349448: wl-3.261744, gl-1.718576]--[lr-0.000050]--[ETA-20:51:10]
2023.01.21-16:17:14:316:[step-16300/88800: 18.36%]--[loss-3.438038: wl-3.246690, gl-1.814694]--[lr-0.000050]--[ETA-20:23:21]
2023.01.21-16:18:57:416:[step-16400/88800: 18.47%]--[loss-3.594511: wl-3.585115, gl-1.801953]--[lr-0.000050]--[ETA-21:07:23]
End of epoch 37 / 200 	 Time Taken: 460 sec
2023.01.21-16:20:42:72:[step-16500/88800: 18.58%]--[loss-3.553486: wl-3.374422, gl-1.866275]--[lr-0.000050]--[ETA-20:54:58]
2023.01.21-16:22:26:172:[step-16600/88800: 18.69%]--[loss-3.383659: wl-3.244052, gl-1.761633]--[lr-0.000050]--[ETA-20:18:24]
2023.01.21-16:24:09:272:[step-16700/88800: 18.81%]--[loss-3.452945: wl-3.330405, gl-1.787743]--[lr-0.000050]--[ETA-19:46:04]
2023.01.21-16:25:53:372:[step-16800/88800: 18.92%]--[loss-3.527175: wl-3.312467, gl-1.870942]--[lr-0.000050]--[ETA-20:56:35]
End of epoch 38 / 200 	 Time Taken: 462 sec
2023.01.21-16:27:38:28:[step-16900/88800: 19.03%]--[loss-3.785503: wl-3.889183, gl-1.840911]--[lr-0.000050]--[ETA-20:42:26]
2023.01.21-16:29:22:128:[step-17000/88800: 19.14%]--[loss-3.638037: wl-3.390066, gl-1.943004]--[lr-0.000050]--[ETA-20:55:06]
2023.01.21-16:31:06:228:[step-17100/88800: 19.26%]--[loss-3.390698: wl-3.290630, gl-1.745383]--[lr-0.000050]--[ETA-20:47:27]
2023.01.21-16:32:49:328:[step-17200/88800: 19.37%]--[loss-3.408551: wl-3.238934, gl-1.789084]--[lr-0.000050]--[ETA-20:10:06]
2023.01.21-16:34:32:428:[step-17300/88800: 19.48%]--[loss-3.387184: wl-3.375662, gl-1.699353]--[lr-0.000050]--[ETA-20:49:10]
End of epoch 39 / 200 	 Time Taken: 461 sec
2023.01.21-16:36:18:84:[step-17400/88800: 19.59%]--[loss-3.271932: wl-3.111757, gl-1.716054]--[lr-0.000050]--[ETA-20:46:16]
2023.01.21-16:38:02:184:[step-17500/88800: 19.71%]--[loss-3.456746: wl-3.234043, gl-1.839724]--[lr-0.000050]--[ETA-22:11:37]
2023.01.21-16:39:46:284:[step-17600/88800: 19.82%]--[loss-3.490326: wl-3.529986, gl-1.725333]--[lr-0.000050]--[ETA-20:18:20]
2023.01.21-16:41:29:384:[step-17700/88800: 19.93%]--[loss-3.489393: wl-3.450271, gl-1.764257]--[lr-0.000050]--[ETA-19:45:03]
End of epoch 40 / 200 	 Time Taken: 462 sec
saving the model at the end of epoch 40, iters 17760
2023.01.21-16:43:14:40:[step-17800/88800: 20.05%]--[loss-3.531438: wl-3.543584, gl-1.759647]--[lr-0.000050]--[ETA-20:28:53]
2023.01.21-16:44:57:140:[step-17900/88800: 20.16%]--[loss-3.316878: wl-3.131490, gl-1.751133]--[lr-0.000050]--[ETA-19:55:41]
2023.01.21-16:46:40:240:[step-18000/88800: 20.27%]--[loss-3.613692: wl-3.618979, gl-1.804202]--[lr-0.000050]--[ETA-20:33:25]
2023.01.21-16:48:23:340:[step-18100/88800: 20.38%]--[loss-3.485196: wl-3.477600, gl-1.746396]--[lr-0.000050]--[ETA-20:57:08]
2023.01.21-16:50:05:440:[step-18200/88800: 20.50%]--[loss-3.394714: wl-3.224199, gl-1.782614]--[lr-0.000050]--[ETA-19:25:24]
End of epoch 41 / 200 	 Time Taken: 458 sec
2023.01.21-16:51:50:96:[step-18300/88800: 20.61%]--[loss-3.549096: wl-3.523690, gl-1.787251]--[lr-0.000050]--[ETA-20:25:40]
2023.01.21-16:53:34:196:[step-18400/88800: 20.72%]--[loss-3.424845: wl-3.356587, gl-1.746552]--[lr-0.000050]--[ETA-20:20:17]
2023.01.21-16:55:18:296:[step-18500/88800: 20.83%]--[loss-3.422151: wl-3.400229, gl-1.722036]--[lr-0.000050]--[ETA-20:58:26]
2023.01.21-16:57:01:396:[step-18600/88800: 20.95%]--[loss-3.546845: wl-3.522871, gl-1.785409]--[lr-0.000050]--[ETA-20:53:12]
End of epoch 42 / 200 	 Time Taken: 460 sec
2023.01.21-16:58:46:52:[step-18700/88800: 21.06%]--[loss-3.584105: wl-3.566083, gl-1.801063]--[lr-0.000050]--[ETA-20:24:11]
2023.01.21-17:00:30:152:[step-18800/88800: 21.17%]--[loss-3.456840: wl-3.314095, gl-1.799792]--[lr-0.000050]--[ETA-20:21:03]
2023.01.21-17:02:14:252:[step-18900/88800: 21.28%]--[loss-3.286515: wl-3.118839, gl-1.727096]--[lr-0.000050]--[ETA-20:02:30]
2023.01.21-17:03:57:352:[step-19000/88800: 21.40%]--[loss-3.481110: wl-3.404660, gl-1.778780]--[lr-0.000050]--[ETA-20:51:02]
End of epoch 43 / 200 	 Time Taken: 462 sec
2023.01.21-17:05:43:8:[step-19100/88800: 21.51%]--[loss-3.412371: wl-3.307668, gl-1.758537]--[lr-0.000050]--[ETA-20:11:14]
2023.01.21-17:07:26:108:[step-19200/88800: 21.62%]--[loss-3.945136: wl-4.288657, gl-1.800808]--[lr-0.000050]--[ETA-19:20:11]
2023.01.21-17:09:09:208:[step-19300/88800: 21.73%]--[loss-3.317301: wl-3.160679, gl-1.736961]--[lr-0.000050]--[ETA-20:00:38]
2023.01.21-17:10:52:308:[step-19400/88800: 21.85%]--[loss-3.285783: wl-3.143920, gl-1.713823]--[lr-0.000050]--[ETA-19:03:11]
2023.01.21-17:12:35:408:[step-19500/88800: 21.96%]--[loss-3.670493: wl-3.741062, gl-1.799962]--[lr-0.000050]--[ETA-19:45:38]
End of epoch 44 / 200 	 Time Taken: 459 sec
2023.01.21-17:14:20:64:[step-19600/88800: 22.07%]--[loss-3.291657: wl-3.120853, gl-1.731230]--[lr-0.000050]--[ETA-19:32:19]
2023.01.21-17:16:03:164:[step-19700/88800: 22.18%]--[loss-3.425687: wl-3.254362, gl-1.798506]--[lr-0.000050]--[ETA-19:57:24]
2023.01.21-17:17:46:264:[step-19800/88800: 22.30%]--[loss-3.318828: wl-3.201097, gl-1.718279]--[lr-0.000050]--[ETA-19:33:43]
2023.01.21-17:19:28:364:[step-19900/88800: 22.41%]--[loss-3.360391: wl-3.225138, gl-1.747822]--[lr-0.000050]--[ETA-19:34:40]
End of epoch 45 / 200 	 Time Taken: 457 sec
saving the model at the end of epoch 45, iters 19980
2023.01.21-17:21:12:20:[step-20000/88800: 22.52%]--[loss-3.401671: wl-3.343871, gl-1.729736]--[lr-0.000050]--[ETA-19:46:43]
2023.01.21-17:22:56:120:[step-20100/88800: 22.64%]--[loss-3.352862: wl-3.253276, gl-1.726224]--[lr-0.000050]--[ETA-19:03:55]
2023.01.21-17:24:38:220:[step-20200/88800: 22.75%]--[loss-3.457027: wl-3.393917, gl-1.760069]--[lr-0.000050]--[ETA-18:53:28]
2023.01.21-17:26:21:320:[step-20300/88800: 22.86%]--[loss-3.095739: wl-2.924259, gl-1.633609]--[lr-0.000050]--[ETA-19:33:09]
2023.01.21-17:28:03:420:[step-20400/88800: 22.97%]--[loss-3.581362: wl-3.658668, gl-1.752028]--[lr-0.000050]--[ETA-19:48:18]
End of epoch 46 / 200 	 Time Taken: 457 sec
2023.01.21-17:29:48:76:[step-20500/88800: 23.09%]--[loss-3.415366: wl-3.284288, gl-1.773222]--[lr-0.000050]--[ETA-20:28:05]
2023.01.21-17:31:31:176:[step-20600/88800: 23.20%]--[loss-3.373302: wl-3.274366, gl-1.736119]--[lr-0.000050]--[ETA-19:42:10]
2023.01.21-17:33:15:276:[step-20700/88800: 23.31%]--[loss-3.366334: wl-3.216311, gl-1.758179]--[lr-0.000050]--[ETA-19:18:53]
2023.01.21-17:34:58:376:[step-20800/88800: 23.42%]--[loss-3.621376: wl-3.830685, gl-1.706033]--[lr-0.000050]--[ETA-19:30:28]
End of epoch 47 / 200 	 Time Taken: 460 sec
2023.01.21-17:36:43:32:[step-20900/88800: 23.54%]--[loss-3.282210: wl-3.104907, gl-1.729757]--[lr-0.000050]--[ETA-20:04:07]
2023.01.21-17:38:27:132:[step-21000/88800: 23.65%]--[loss-3.091633: wl-2.922529, gl-1.630369]--[lr-0.000050]--[ETA-19:56:09]
2023.01.21-17:40:10:232:[step-21100/88800: 23.76%]--[loss-3.372250: wl-3.272733, gl-1.735883]--[lr-0.000050]--[ETA-19:07:41]
2023.01.21-17:41:53:332:[step-21200/88800: 23.87%]--[loss-3.442453: wl-3.479504, gl-1.702701]--[lr-0.000050]--[ETA-19:15:05]
2023.01.21-17:43:35:432:[step-21300/88800: 23.99%]--[loss-3.538313: wl-3.590593, gl-1.743017]--[lr-0.000050]--[ETA-19:13:57]
End of epoch 48 / 200 	 Time Taken: 459 sec
2023.01.21-17:45:20:88:[step-21400/88800: 24.10%]--[loss-3.558506: wl-3.628637, gl-1.744188]--[lr-0.000050]--[ETA-19:13:51]
2023.01.21-17:47:03:188:[step-21500/88800: 24.21%]--[loss-3.456971: wl-3.457557, gl-1.728192]--[lr-0.000050]--[ETA-18:40:20]
2023.01.21-17:48:47:288:[step-21600/88800: 24.32%]--[loss-3.360027: wl-3.261529, gl-1.729263]--[lr-0.000050]--[ETA-18:42:40]
2023.01.21-17:50:30:388:[step-21700/88800: 24.44%]--[loss-3.489747: wl-3.452821, gl-1.763336]--[lr-0.000050]--[ETA-19:02:12]
End of epoch 49 / 200 	 Time Taken: 460 sec
2023.01.21-17:52:15:44:[step-21800/88800: 24.55%]--[loss-3.487498: wl-3.438680, gl-1.768158]--[lr-0.000050]--[ETA-19:50:10]
2023.01.21-17:53:58:144:[step-21900/88800: 24.66%]--[loss-3.140141: wl-2.996612, gl-1.641834]--[lr-0.000050]--[ETA-18:48:02]
2023.01.21-17:55:42:244:[step-22000/88800: 24.77%]--[loss-3.616879: wl-3.713357, gl-1.760200]--[lr-0.000050]--[ETA-19:22:19]
2023.01.21-17:57:25:344:[step-22100/88800: 24.89%]--[loss-3.385670: wl-3.383230, gl-1.694055]--[lr-0.000050]--[ETA-19:43:57]
2023.01.21-17:59:08:444:[step-22200/88800: 25.00%]--[loss-3.705024: wl-3.781901, gl-1.814074]--[lr-0.000050]--[ETA-18:57:06]
End of epoch 50 / 200 	 Time Taken: 460 sec
saving the model at the end of epoch 50, iters 22200
2023.01.21-18:00:55:100:[step-22300/88800: 25.11%]--[loss-3.376065: wl-3.341899, gl-1.705115]--[lr-0.000050]--[ETA-19:59:53]
2023.01.21-18:02:38:200:[step-22400/88800: 25.23%]--[loss-3.228383: wl-3.179583, gl-1.638591]--[lr-0.000050]--[ETA-19:10:15]
2023.01.21-18:04:22:300:[step-22500/88800: 25.34%]--[loss-3.281871: wl-3.194615, gl-1.684563]--[lr-0.000050]--[ETA-19:11:34]
2023.01.21-18:06:05:400:[step-22600/88800: 25.45%]--[loss-3.442543: wl-3.508337, gl-1.688374]--[lr-0.000050]--[ETA-19:15:15]
End of epoch 51 / 200 	 Time Taken: 461 sec
2023.01.21-18:07:50:56:[step-22700/88800: 25.56%]--[loss-3.551159: wl-3.549373, gl-1.776473]--[lr-0.000050]--[ETA-18:52:36]
2023.01.21-18:09:33:156:[step-22800/88800: 25.68%]--[loss-3.639070: wl-3.700388, gl-1.788876]--[lr-0.000050]--[ETA-18:59:39]
2023.01.21-18:11:16:256:[step-22900/88800: 25.79%]--[loss-3.299733: wl-3.312919, gl-1.643274]--[lr-0.000050]--[ETA-18:44:08]
2023.01.21-18:12:59:356:[step-23000/88800: 25.90%]--[loss-3.535868: wl-3.423518, gl-1.824109]--[lr-0.000050]--[ETA-19:32:14]
End of epoch 52 / 200 	 Time Taken: 459 sec
2023.01.21-18:14:44:12:[step-23100/88800: 26.01%]--[loss-3.338862: wl-3.255618, gl-1.711053]--[lr-0.000049]--[ETA-18:46:42]
2023.01.21-18:16:28:112:[step-23200/88800: 26.13%]--[loss-3.654106: wl-3.875554, gl-1.716329]--[lr-0.000049]--[ETA-18:52:03]
2023.01.21-18:18:10:212:[step-23300/88800: 26.24%]--[loss-3.368819: wl-3.228712, gl-1.754464]--[lr-0.000049]--[ETA-18:39:22]
2023.01.21-18:19:53:312:[step-23400/88800: 26.35%]--[loss-3.255244: wl-3.164587, gl-1.672950]--[lr-0.000049]--[ETA-18:24:10]
2023.01.21-18:21:36:412:[step-23500/88800: 26.46%]--[loss-3.649188: wl-3.870899, gl-1.713739]--[lr-0.000049]--[ETA-19:28:32]
End of epoch 53 / 200 	 Time Taken: 458 sec
2023.01.21-18:23:21:68:[step-23600/88800: 26.58%]--[loss-3.308027: wl-3.170886, gl-1.722584]--[lr-0.000049]--[ETA-18:37:29]
2023.01.21-18:25:03:168:[step-23700/88800: 26.69%]--[loss-3.438505: wl-3.394090, gl-1.741460]--[lr-0.000049]--[ETA-18:33:31]
2023.01.21-18:26:45:268:[step-23800/88800: 26.80%]--[loss-3.540316: wl-3.561478, gl-1.759577]--[lr-0.000049]--[ETA-18:24:17]
2023.01.21-18:28:28:368:[step-23900/88800: 26.91%]--[loss-3.468647: wl-3.338295, gl-1.799500]--[lr-0.000049]--[ETA-18:10:30]
End of epoch 54 / 200 	 Time Taken: 456 sec
2023.01.21-18:30:12:24:[step-24000/88800: 27.03%]--[loss-3.144480: wl-2.976511, gl-1.656224]--[lr-0.000049]--[ETA-18:04:09]
2023.01.21-18:31:56:124:[step-24100/88800: 27.14%]--[loss-3.319752: wl-3.226044, gl-1.706730]--[lr-0.000049]--[ETA-19:03:44]
2023.01.21-18:33:38:224:[step-24200/88800: 27.25%]--[loss-3.333859: wl-3.271214, gl-1.698252]--[lr-0.000049]--[ETA-18:26:01]
2023.01.21-18:35:21:324:[step-24300/88800: 27.36%]--[loss-3.269395: wl-3.268017, gl-1.635387]--[lr-0.000049]--[ETA-18:28:09]
2023.01.21-18:37:04:424:[step-24400/88800: 27.48%]--[loss-3.578804: wl-3.653000, gl-1.752305]--[lr-0.000049]--[ETA-17:49:49]
End of epoch 55 / 200 	 Time Taken: 459 sec
saving the model at the end of epoch 55, iters 24420
2023.01.21-18:38:50:80:[step-24500/88800: 27.59%]--[loss-3.439167: wl-3.514288, gl-1.682022]--[lr-0.000048]--[ETA-19:27:46]
2023.01.21-18:40:33:180:[step-24600/88800: 27.70%]--[loss-3.391188: wl-3.419178, gl-1.681599]--[lr-0.000048]--[ETA-18:12:15]
2023.01.21-18:42:16:280:[step-24700/88800: 27.82%]--[loss-3.290637: wl-3.241561, gl-1.669857]--[lr-0.000048]--[ETA-18:22:07]
2023.01.21-18:43:59:380:[step-24800/88800: 27.93%]--[loss-3.293861: wl-3.105653, gl-1.741035]--[lr-0.000048]--[ETA-18:03:32]
End of epoch 56 / 200 	 Time Taken: 458 sec
2023.01.21-18:45:43:36:[step-24900/88800: 28.04%]--[loss-3.452168: wl-3.443248, gl-1.730544]--[lr-0.000048]--[ETA-17:51:01]
2023.01.21-18:47:25:136:[step-25000/88800: 28.15%]--[loss-3.221484: wl-3.205892, gl-1.618538]--[lr-0.000048]--[ETA-17:39:01]
2023.01.21-18:49:08:236:[step-25100/88800: 28.27%]--[loss-3.413596: wl-3.427377, gl-1.699908]--[lr-0.000048]--[ETA-17:58:01]
2023.01.21-18:50:51:336:[step-25200/88800: 28.38%]--[loss-3.393119: wl-3.339772, gl-1.723233]--[lr-0.000048]--[ETA-18:38:20]
2023.01.21-18:52:35:436:[step-25300/88800: 28.49%]--[loss-3.165029: wl-3.076802, gl-1.626628]--[lr-0.000048]--[ETA-17:24:15]
End of epoch 57 / 200 	 Time Taken: 458 sec
2023.01.21-18:54:20:92:[step-25400/88800: 28.60%]--[loss-3.209542: wl-3.178358, gl-1.620363]--[lr-0.000048]--[ETA-18:25:04]
2023.01.21-18:56:03:192:[step-25500/88800: 28.72%]--[loss-3.188247: wl-3.057681, gl-1.659407]--[lr-0.000048]--[ETA-17:34:34]
2023.01.21-18:57:47:292:[step-25600/88800: 28.83%]--[loss-3.072949: wl-2.931098, gl-1.607400]--[lr-0.000048]--[ETA-19:08:05]
2023.01.21-18:59:30:392:[step-25700/88800: 28.94%]--[loss-3.305997: wl-3.258347, gl-1.676824]--[lr-0.000048]--[ETA-17:22:35]
End of epoch 58 / 200 	 Time Taken: 461 sec
2023.01.21-19:01:15:48:[step-25800/88800: 29.05%]--[loss-3.268333: wl-3.216855, gl-1.659906]--[lr-0.000047]--[ETA-17:51:26]
2023.01.21-19:02:58:148:[step-25900/88800: 29.17%]--[loss-3.237463: wl-3.114036, gl-1.680445]--[lr-0.000047]--[ETA-18:01:38]
2023.01.21-19:04:42:248:[step-26000/88800: 29.28%]--[loss-3.251587: wl-3.199001, gl-1.652086]--[lr-0.000047]--[ETA-17:40:50]
2023.01.21-19:06:25:348:[step-26100/88800: 29.39%]--[loss-3.661873: wl-3.833605, gl-1.745071]--[lr-0.000047]--[ETA-18:08:08]
End of epoch 59 / 200 	 Time Taken: 459 sec
2023.01.21-19:08:09:4:[step-26200/88800: 29.50%]--[loss-3.464756: wl-3.566001, gl-1.681755]--[lr-0.000047]--[ETA-17:19:53]
2023.01.21-19:09:52:104:[step-26300/88800: 29.62%]--[loss-3.281434: wl-3.270410, gl-1.646229]--[lr-0.000047]--[ETA-17:44:20]
2023.01.21-19:11:35:204:[step-26400/88800: 29.73%]--[loss-3.220162: wl-3.026559, gl-1.706883]--[lr-0.000047]--[ETA-18:00:15]
2023.01.21-19:13:18:304:[step-26500/88800: 29.84%]--[loss-3.297403: wl-3.261910, gl-1.666448]--[lr-0.000047]--[ETA-17:13:10]
2023.01.21-19:15:02:404:[step-26600/88800: 29.95%]--[loss-3.451629: wl-3.453570, gl-1.724844]--[lr-0.000047]--[ETA-17:57:05]
End of epoch 60 / 200 	 Time Taken: 459 sec
saving the model at the end of epoch 60, iters 26640
2023.01.21-19:16:47:60:[step-26700/88800: 30.07%]--[loss-3.198488: wl-3.045040, gl-1.675968]--[lr-0.000047]--[ETA-17:55:48]
2023.01.21-19:18:30:160:[step-26800/88800: 30.18%]--[loss-3.305451: wl-3.225620, gl-1.692641]--[lr-0.000047]--[ETA-17:41:48]
2023.01.21-19:20:13:260:[step-26900/88800: 30.29%]--[loss-3.453830: wl-3.328525, gl-1.789567]--[lr-0.000047]--[ETA-18:20:47]
2023.01.21-19:21:56:360:[step-27000/88800: 30.41%]--[loss-3.241776: wl-3.116783, gl-1.683385]--[lr-0.000047]--[ETA-18:01:18]
End of epoch 61 / 200 	 Time Taken: 459 sec
2023.01.21-19:23:41:16:[step-27100/88800: 30.52%]--[loss-3.433663: wl-3.507483, gl-1.679922]--[lr-0.000046]--[ETA-16:50:46]
2023.01.21-19:25:25:116:[step-27200/88800: 30.63%]--[loss-3.237168: wl-3.137922, gl-1.668207]--[lr-0.000046]--[ETA-17:01:37]
2023.01.21-19:27:08:216:[step-27300/88800: 30.74%]--[loss-3.401158: wl-3.470749, gl-1.665783]--[lr-0.000046]--[ETA-17:45:17]
2023.01.21-19:28:52:316:[step-27400/88800: 30.86%]--[loss-3.606311: wl-3.748308, gl-1.732157]--[lr-0.000046]--[ETA-17:35:02]
2023.01.21-19:30:35:416:[step-27500/88800: 30.97%]--[loss-3.490180: wl-3.522302, gl-1.729029]--[lr-0.000046]--[ETA-17:33:44]
End of epoch 62 / 200 	 Time Taken: 461 sec
2023.01.21-19:32:20:72:[step-27600/88800: 31.08%]--[loss-3.603246: wl-3.683717, gl-1.761387]--[lr-0.000046]--[ETA-17:44:49]
2023.01.21-19:34:04:172:[step-27700/88800: 31.19%]--[loss-3.263028: wl-3.154020, gl-1.686018]--[lr-0.000046]--[ETA-16:51:26]
2023.01.21-19:35:47:272:[step-27800/88800: 31.31%]--[loss-3.632184: wl-3.810116, gl-1.727126]--[lr-0.000046]--[ETA-17:01:47]
2023.01.21-19:37:30:372:[step-27900/88800: 31.42%]--[loss-3.133542: wl-3.102637, gl-1.582223]--[lr-0.000046]--[ETA-17:51:44]
End of epoch 63 / 200 	 Time Taken: 460 sec
2023.01.21-19:39:15:28:[step-28000/88800: 31.53%]--[loss-3.293727: wl-3.293158, gl-1.647148]--[lr-0.000046]--[ETA-17:06:09]
2023.01.21-19:40:58:128:[step-28100/88800: 31.64%]--[loss-3.314397: wl-3.420087, gl-1.604354]--[lr-0.000046]--[ETA-18:29:28]
2023.01.21-19:42:42:228:[step-28200/88800: 31.76%]--[loss-3.330677: wl-3.346262, gl-1.657546]--[lr-0.000046]--[ETA-17:01:45]
2023.01.21-19:44:24:328:[step-28300/88800: 31.87%]--[loss-3.170648: wl-3.131000, gl-1.605147]--[lr-0.000046]--[ETA-17:03:28]
2023.01.21-19:46:07:428:[step-28400/88800: 31.98%]--[loss-3.255148: wl-3.200099, gl-1.655099]--[lr-0.000046]--[ETA-17:35:35]
End of epoch 64 / 200 	 Time Taken: 459 sec
2023.01.21-19:47:53:84:[step-28500/88800: 32.09%]--[loss-3.451242: wl-3.571043, gl-1.665720]--[lr-0.000045]--[ETA-16:49:21]
2023.01.21-19:49:36:184:[step-28600/88800: 32.21%]--[loss-3.425834: wl-3.624524, gl-1.613572]--[lr-0.000045]--[ETA-17:27:32]
2023.01.21-19:51:19:284:[step-28700/88800: 32.32%]--[loss-3.225237: wl-3.229776, gl-1.610349]--[lr-0.000045]--[ETA-17:34:46]
2023.01.21-19:53:03:384:[step-28800/88800: 32.43%]--[loss-3.180869: wl-3.077089, gl-1.642324]--[lr-0.000045]--[ETA-17:19:29]
End of epoch 65 / 200 	 Time Taken: 460 sec
saving the model at the end of epoch 65, iters 28860
2023.01.21-19:54:48:40:[step-28900/88800: 32.55%]--[loss-3.433290: wl-3.526410, gl-1.670085]--[lr-0.000045]--[ETA-17:15:19]
2023.01.21-19:56:32:140:[step-29000/88800: 32.66%]--[loss-3.296216: wl-3.279119, gl-1.656656]--[lr-0.000045]--[ETA-17:28:39]
2023.01.21-19:58:15:240:[step-29100/88800: 32.77%]--[loss-3.418134: wl-3.572290, gl-1.631989]--[lr-0.000045]--[ETA-16:29:44]
2023.01.21-19:59:57:340:[step-29200/88800: 32.88%]--[loss-3.386804: wl-3.372051, gl-1.700779]--[lr-0.000045]--[ETA-17:43:14]
2023.01.21-20:01:40:440:[step-29300/88800: 33.00%]--[loss-3.670975: wl-4.067422, gl-1.637264]--[lr-0.000045]--[ETA-16:37:07]
End of epoch 66 / 200 	 Time Taken: 459 sec
2023.01.21-20:03:26:96:[step-29400/88800: 33.11%]--[loss-3.333251: wl-3.487237, gl-1.589632]--[lr-0.000045]--[ETA-17:05:15]
2023.01.21-20:05:08:196:[step-29500/88800: 33.22%]--[loss-3.171086: wl-3.141558, gl-1.600307]--[lr-0.000045]--[ETA-17:16:36]
2023.01.21-20:06:52:296:[step-29600/88800: 33.33%]--[loss-3.320501: wl-3.333577, gl-1.653712]--[lr-0.000045]--[ETA-17:07:53]
2023.01.21-20:08:35:396:[step-29700/88800: 33.45%]--[loss-3.581098: wl-3.812054, gl-1.675071]--[lr-0.000045]--[ETA-17:02:03]
End of epoch 67 / 200 	 Time Taken: 460 sec
2023.01.21-20:10:21:52:[step-29800/88800: 33.56%]--[loss-3.307333: wl-3.343416, gl-1.635625]--[lr-0.000044]--[ETA-17:05:47]
2023.01.21-20:12:04:152:[step-29900/88800: 33.67%]--[loss-3.271472: wl-3.270146, gl-1.636399]--[lr-0.000044]--[ETA-17:01:12]
2023.01.21-20:13:48:252:[step-30000/88800: 33.78%]--[loss-3.272502: wl-3.369199, gl-1.587903]--[lr-0.000044]--[ETA-17:37:52]
2023.01.21-20:15:31:352:[step-30100/88800: 33.90%]--[loss-3.407074: wl-3.544007, gl-1.635071]--[lr-0.000044]--[ETA-16:44:01]
End of epoch 68 / 200 	 Time Taken: 461 sec
2023.01.21-20:17:16:8:[step-30200/88800: 34.01%]--[loss-3.292207: wl-3.321868, gl-1.631273]--[lr-0.000044]--[ETA-16:42:17]
2023.01.21-20:18:59:108:[step-30300/88800: 34.12%]--[loss-3.185091: wl-3.125764, gl-1.622208]--[lr-0.000044]--[ETA-17:06:44]
2023.01.21-20:20:42:208:[step-30400/88800: 34.23%]--[loss-3.379100: wl-3.463338, gl-1.647431]--[lr-0.000044]--[ETA-16:35:50]
2023.01.21-20:22:26:308:[step-30500/88800: 34.35%]--[loss-3.318092: wl-3.373638, gl-1.631273]--[lr-0.000044]--[ETA-16:55:17]
2023.01.21-20:24:10:408:[step-30600/88800: 34.46%]--[loss-3.364044: wl-3.374989, gl-1.676550]--[lr-0.000044]--[ETA-16:05:41]
End of epoch 69 / 200 	 Time Taken: 460 sec
2023.01.21-20:25:55:64:[step-30700/88800: 34.57%]--[loss-3.321036: wl-3.464571, gl-1.588751]--[lr-0.000044]--[ETA-16:32:27]
2023.01.21-20:27:37:164:[step-30800/88800: 34.68%]--[loss-3.482224: wl-3.568768, gl-1.697840]--[lr-0.000044]--[ETA-16:46:20]
2023.01.21-20:29:21:264:[step-30900/88800: 34.80%]--[loss-2.996521: wl-2.914233, gl-1.539405]--[lr-0.000044]--[ETA-16:43:32]
2023.01.21-20:31:04:364:[step-31000/88800: 34.91%]--[loss-3.249452: wl-3.227131, gl-1.635886]--[lr-0.000044]--[ETA-16:00:06]
End of epoch 70 / 200 	 Time Taken: 460 sec
saving the model at the end of epoch 70, iters 31080
2023.01.21-20:32:50:20:[step-31100/88800: 35.02%]--[loss-3.413555: wl-3.623316, gl-1.601897]--[lr-0.000043]--[ETA-16:53:04]
2023.01.21-20:34:33:120:[step-31200/88800: 35.14%]--[loss-3.327329: wl-3.390020, gl-1.632319]--[lr-0.000043]--[ETA-17:09:26]
2023.01.21-20:36:16:220:[step-31300/88800: 35.25%]--[loss-3.151671: wl-3.118352, gl-1.592495]--[lr-0.000043]--[ETA-17:01:09]
2023.01.21-20:37:59:320:[step-31400/88800: 35.36%]--[loss-3.350511: wl-3.542965, gl-1.579029]--[lr-0.000043]--[ETA-16:43:09]
2023.01.21-20:39:42:420:[step-31500/88800: 35.47%]--[loss-3.297395: wl-3.288556, gl-1.653117]--[lr-0.000043]--[ETA-17:08:54]
End of epoch 71 / 200 	 Time Taken: 459 sec
2023.01.21-20:41:28:76:[step-31600/88800: 35.59%]--[loss-3.216982: wl-3.250502, gl-1.591731]--[lr-0.000043]--[ETA-16:09:26]
2023.01.21-20:43:11:176:[step-31700/88800: 35.70%]--[loss-3.210295: wl-3.181275, gl-1.619657]--[lr-0.000043]--[ETA-16:21:46]
2023.01.21-20:44:54:276:[step-31800/88800: 35.81%]--[loss-3.154680: wl-3.015070, gl-1.647145]--[lr-0.000043]--[ETA-15:57:49]
2023.01.21-20:46:38:376:[step-31900/88800: 35.92%]--[loss-3.308428: wl-3.489719, gl-1.563569]--[lr-0.000043]--[ETA-16:07:06]
End of epoch 72 / 200 	 Time Taken: 460 sec
2023.01.21-20:48:23:32:[step-32000/88800: 36.04%]--[loss-3.355589: wl-3.383507, gl-1.663836]--[lr-0.000043]--[ETA-17:07:21]
2023.01.21-20:50:06:132:[step-32100/88800: 36.15%]--[loss-3.243533: wl-3.193820, gl-1.646623]--[lr-0.000043]--[ETA-16:41:38]
2023.01.21-20:51:49:232:[step-32200/88800: 36.26%]--[loss-3.228554: wl-3.229417, gl-1.613845]--[lr-0.000043]--[ETA-16:05:20]
2023.01.21-20:53:33:332:[step-32300/88800: 36.37%]--[loss-3.446675: wl-3.621027, gl-1.636162]--[lr-0.000043]--[ETA-16:10:36]
2023.01.21-20:55:15:432:[step-32400/88800: 36.49%]--[loss-3.164930: wl-3.095152, gl-1.617354]--[lr-0.000043]--[ETA-16:09:15]
End of epoch 73 / 200 	 Time Taken: 459 sec
2023.01.21-20:57:00:88:[step-32500/88800: 36.60%]--[loss-3.288321: wl-3.380131, gl-1.598256]--[lr-0.000042]--[ETA-16:18:48]
2023.01.21-20:58:43:188:[step-32600/88800: 36.71%]--[loss-3.418982: wl-3.465157, gl-1.686403]--[lr-0.000042]--[ETA-15:57:08]
2023.01.21-21:00:27:288:[step-32700/88800: 36.82%]--[loss-3.308490: wl-3.330882, gl-1.643049]--[lr-0.000042]--[ETA-16:04:39]
2023.01.21-21:02:10:388:[step-32800/88800: 36.94%]--[loss-3.238849: wl-3.202727, gl-1.637486]--[lr-0.000042]--[ETA-17:03:26]
End of epoch 74 / 200 	 Time Taken: 459 sec
2023.01.21-21:03:55:44:[step-32900/88800: 37.05%]--[loss-3.401206: wl-3.581768, gl-1.610322]--[lr-0.000042]--[ETA-15:26:46]
2023.01.21-21:05:38:144:[step-33000/88800: 37.16%]--[loss-3.106382: wl-3.032802, gl-1.589981]--[lr-0.000042]--[ETA-16:05:02]
2023.01.21-21:07:21:244:[step-33100/88800: 37.27%]--[loss-3.081886: wl-2.988339, gl-1.587716]--[lr-0.000042]--[ETA-16:06:41]
2023.01.21-21:09:04:344:[step-33200/88800: 37.39%]--[loss-3.378240: wl-3.362105, gl-1.697187]--[lr-0.000042]--[ETA-15:09:29]
2023.01.21-21:10:46:444:[step-33300/88800: 37.50%]--[loss-3.165309: wl-3.150730, gl-1.589944]--[lr-0.000042]--[ETA-15:22:49]
End of epoch 75 / 200 	 Time Taken: 459 sec
saving the model at the end of epoch 75, iters 33300
2023.01.21-21:12:31:100:[step-33400/88800: 37.61%]--[loss-3.223781: wl-3.230268, gl-1.608647]--[lr-0.000042]--[ETA-15:53:55]
2023.01.21-21:14:14:200:[step-33500/88800: 37.73%]--[loss-3.281053: wl-3.311857, gl-1.625124]--[lr-0.000042]--[ETA-15:22:42]
2023.01.21-21:15:56:300:[step-33600/88800: 37.84%]--[loss-3.262335: wl-3.245141, gl-1.639764]--[lr-0.000042]--[ETA-15:29:05]
2023.01.21-21:17:38:400:[step-33700/88800: 37.95%]--[loss-3.213774: wl-3.212546, gl-1.607502]--[lr-0.000042]--[ETA-15:35:25]
End of epoch 76 / 200 	 Time Taken: 457 sec
2023.01.21-21:19:23:56:[step-33800/88800: 38.06%]--[loss-3.319315: wl-3.318654, gl-1.659989]--[lr-0.000041]--[ETA-15:24:13]
2023.01.21-21:21:06:156:[step-33900/88800: 38.18%]--[loss-3.275691: wl-3.373211, gl-1.589086]--[lr-0.000041]--[ETA-15:41:06]
2023.01.21-21:22:49:256:[step-34000/88800: 38.29%]--[loss-3.210435: wl-3.223210, gl-1.598830]--[lr-0.000041]--[ETA-15:34:52]
2023.01.21-21:24:32:356:[step-34100/88800: 38.40%]--[loss-3.326706: wl-3.477056, gl-1.588178]--[lr-0.000041]--[ETA-16:36:13]
End of epoch 77 / 200 	 Time Taken: 458 sec
2023.01.21-21:26:17:12:[step-34200/88800: 38.51%]--[loss-3.373414: wl-3.446578, gl-1.650125]--[lr-0.000041]--[ETA-15:37:53]
2023.01.21-21:28:01:112:[step-34300/88800: 38.63%]--[loss-3.657229: wl-3.951565, gl-1.681446]--[lr-0.000041]--[ETA-15:16:22]
2023.01.21-21:29:44:212:[step-34400/88800: 38.74%]--[loss-3.382150: wl-3.461199, gl-1.651550]--[lr-0.000041]--[ETA-15:29:17]
2023.01.21-21:31:27:312:[step-34500/88800: 38.85%]--[loss-3.394313: wl-3.476928, gl-1.655849]--[lr-0.000041]--[ETA-15:41:06]
2023.01.21-21:33:10:412:[step-34600/88800: 38.96%]--[loss-3.252262: wl-3.307276, gl-1.598624]--[lr-0.000041]--[ETA-14:50:47]
End of epoch 78 / 200 	 Time Taken: 459 sec
2023.01.21-21:34:55:68:[step-34700/88800: 39.08%]--[loss-3.435646: wl-3.619109, gl-1.626091]--[lr-0.000041]--[ETA-15:28:14]
2023.01.21-21:36:38:168:[step-34800/88800: 39.19%]--[loss-3.278564: wl-3.364704, gl-1.596213]--[lr-0.000041]--[ETA-14:59:28]
2023.01.21-21:38:21:268:[step-34900/88800: 39.30%]--[loss-3.337399: wl-3.351637, gl-1.661580]--[lr-0.000041]--[ETA-15:14:20]
2023.01.21-21:40:04:368:[step-35000/88800: 39.41%]--[loss-3.447339: wl-3.506523, gl-1.694077]--[lr-0.000041]--[ETA-15:34:56]
End of epoch 79 / 200 	 Time Taken: 459 sec
2023.01.21-21:41:49:24:[step-35100/88800: 39.53%]--[loss-3.294523: wl-3.379540, gl-1.604753]--[lr-0.000040]--[ETA-15:17:06]
2023.01.21-21:43:32:124:[step-35200/88800: 39.64%]--[loss-3.363751: wl-3.359146, gl-1.684178]--[lr-0.000040]--[ETA-15:03:46]
2023.01.21-21:45:16:224:[step-35300/88800: 39.75%]--[loss-3.055817: wl-3.015593, gl-1.548020]--[lr-0.000040]--[ETA-15:26:33]
2023.01.21-21:46:58:324:[step-35400/88800: 39.86%]--[loss-3.253502: wl-3.226181, gl-1.640412]--[lr-0.000040]--[ETA-16:10:22]
2023.01.21-21:48:42:424:[step-35500/88800: 39.98%]--[loss-3.269893: wl-3.393149, gl-1.573318]--[lr-0.000040]--[ETA-15:01:13]
End of epoch 80 / 200 	 Time Taken: 459 sec
saving the model at the end of epoch 80, iters 35520
2023.01.21-21:50:27:80:[step-35600/88800: 40.09%]--[loss-3.575868: wl-3.830848, gl-1.660444]--[lr-0.000040]--[ETA-15:19:50]
2023.01.21-21:52:10:180:[step-35700/88800: 40.20%]--[loss-3.137025: wl-3.097408, gl-1.588322]--[lr-0.000040]--[ETA-15:18:12]
2023.01.21-21:53:53:280:[step-35800/88800: 40.32%]--[loss-3.285209: wl-3.310587, gl-1.629916]--[lr-0.000040]--[ETA-15:10:32]
2023.01.21-21:55:37:380:[step-35900/88800: 40.43%]--[loss-3.251221: wl-3.194657, gl-1.653893]--[lr-0.000040]--[ETA-15:15:19]
End of epoch 81 / 200 	 Time Taken: 460 sec
2023.01.21-21:57:21:36:[step-36000/88800: 40.54%]--[loss-3.506453: wl-3.469196, gl-1.771855]--[lr-0.000040]--[ETA-15:17:18]
2023.01.21-21:59:05:136:[step-36100/88800: 40.65%]--[loss-3.298224: wl-3.301435, gl-1.647507]--[lr-0.000040]--[ETA-15:09:33]
2023.01.21-22:00:48:236:[step-36200/88800: 40.77%]--[loss-3.223562: wl-3.249487, gl-1.598819]--[lr-0.000040]--[ETA-14:59:19]
2023.01.21-22:02:31:336:[step-36300/88800: 40.88%]--[loss-3.139743: wl-3.183008, gl-1.548239]--[lr-0.000040]--[ETA-15:06:08]
2023.01.21-22:04:14:436:[step-36400/88800: 40.99%]--[loss-3.511289: wl-3.764879, gl-1.628850]--[lr-0.000040]--[ETA-15:13:31]
End of epoch 82 / 200 	 Time Taken: 459 sec
2023.01.21-22:05:59:92:[step-36500/88800: 41.10%]--[loss-3.200760: wl-3.264391, gl-1.568564]--[lr-0.000039]--[ETA-15:12:32]
2023.01.21-22:07:42:192:[step-36600/88800: 41.22%]--[loss-3.253397: wl-3.326285, gl-1.590255]--[lr-0.000039]--[ETA-15:05:23]
2023.01.21-22:09:26:292:[step-36700/88800: 41.33%]--[loss-3.127393: wl-3.219630, gl-1.517579]--[lr-0.000039]--[ETA-14:36:27]
2023.01.21-22:11:09:392:[step-36800/88800: 41.44%]--[loss-3.203346: wl-3.239694, gl-1.583500]--[lr-0.000039]--[ETA-14:15:00]
End of epoch 83 / 200 	 Time Taken: 460 sec
2023.01.21-22:12:54:48:[step-36900/88800: 41.55%]--[loss-3.146832: wl-3.067222, gl-1.613221]--[lr-0.000039]--[ETA-14:21:58]
2023.01.21-22:14:37:148:[step-37000/88800: 41.67%]--[loss-3.086227: wl-3.071337, gl-1.550558]--[lr-0.000039]--[ETA-14:35:55]
2023.01.21-22:16:21:248:[step-37100/88800: 41.78%]--[loss-3.385408: wl-3.606116, gl-1.582350]--[lr-0.000039]--[ETA-14:54:29]
2023.01.21-22:18:04:348:[step-37200/88800: 41.89%]--[loss-3.125397: wl-3.161867, gl-1.544463]--[lr-0.000039]--[ETA-14:49:05]
End of epoch 84 / 200 	 Time Taken: 459 sec
2023.01.21-22:19:49:4:[step-37300/88800: 42.00%]--[loss-3.092409: wl-3.041605, gl-1.571607]--[lr-0.000039]--[ETA-14:13:25]
2023.01.21-22:21:32:104:[step-37400/88800: 42.12%]--[loss-3.127754: wl-3.165873, gl-1.544818]--[lr-0.000039]--[ETA-14:15:44]
2023.01.21-22:23:15:204:[step-37500/88800: 42.23%]--[loss-3.146503: wl-3.057347, gl-1.617830]--[lr-0.000039]--[ETA-15:46:14]
2023.01.21-22:24:58:304:[step-37600/88800: 42.34%]--[loss-3.196567: wl-3.150196, gl-1.621469]--[lr-0.000039]--[ETA-14:18:33]
2023.01.21-22:26:42:404:[step-37700/88800: 42.45%]--[loss-3.289975: wl-3.345376, gl-1.617287]--[lr-0.000039]--[ETA-14:12:22]
End of epoch 85 / 200 	 Time Taken: 459 sec
saving the model at the end of epoch 85, iters 37740
2023.01.21-22:28:26:60:[step-37800/88800: 42.57%]--[loss-3.349096: wl-3.405063, gl-1.646564]--[lr-0.000038]--[ETA-14:12:18]
2023.01.21-22:30:10:160:[step-37900/88800: 42.68%]--[loss-3.441849: wl-3.762173, gl-1.560763]--[lr-0.000038]--[ETA-14:01:46]
2023.01.21-22:31:52:260:[step-38000/88800: 42.79%]--[loss-3.241992: wl-3.243975, gl-1.620005]--[lr-0.000038]--[ETA-14:43:09]
2023.01.21-22:33:36:360:[step-38100/88800: 42.91%]--[loss-3.301155: wl-3.458403, gl-1.571954]--[lr-0.000038]--[ETA-14:16:55]
End of epoch 86 / 200 	 Time Taken: 458 sec
2023.01.21-22:35:21:16:[step-38200/88800: 43.02%]--[loss-3.155973: wl-3.152981, gl-1.579483]--[lr-0.000038]--[ETA-14:46:29]
2023.01.21-22:37:04:116:[step-38300/88800: 43.13%]--[loss-3.180456: wl-3.184698, gl-1.588107]--[lr-0.000038]--[ETA-14:27:06]
2023.01.21-22:38:48:216:[step-38400/88800: 43.24%]--[loss-3.284251: wl-3.396224, gl-1.586139]--[lr-0.000038]--[ETA-14:23:39]
2023.01.21-22:40:32:316:[step-38500/88800: 43.36%]--[loss-3.287466: wl-3.426960, gl-1.573986]--[lr-0.000038]--[ETA-14:07:03]
2023.01.21-22:42:15:416:[step-38600/88800: 43.47%]--[loss-3.369908: wl-3.524553, gl-1.607632]--[lr-0.000038]--[ETA-14:04:44]
End of epoch 87 / 200 	 Time Taken: 461 sec
2023.01.21-22:44:00:72:[step-38700/88800: 43.58%]--[loss-3.670841: wl-4.166391, gl-1.587646]--[lr-0.000038]--[ETA-14:03:00]
2023.01.21-22:45:43:172:[step-38800/88800: 43.69%]--[loss-3.352198: wl-3.517195, gl-1.593601]--[lr-0.000038]--[ETA-14:12:19]
2023.01.21-22:47:26:272:[step-38900/88800: 43.81%]--[loss-3.235081: wl-3.334151, gl-1.568005]--[lr-0.000038]--[ETA-14:29:36]
2023.01.21-22:49:09:372:[step-39000/88800: 43.92%]--[loss-3.105366: wl-3.086306, gl-1.562213]--[lr-0.000038]--[ETA-13:59:20]
End of epoch 88 / 200 	 Time Taken: 459 sec
2023.01.21-22:50:54:28:[step-39100/88800: 44.03%]--[loss-3.140334: wl-3.089757, gl-1.595455]--[lr-0.000037]--[ETA-14:28:42]
2023.01.21-22:52:37:128:[step-39200/88800: 44.14%]--[loss-3.261451: wl-3.368776, gl-1.577063]--[lr-0.000037]--[ETA-13:38:28]
2023.01.21-22:54:21:228:[step-39300/88800: 44.26%]--[loss-3.159703: wl-3.234721, gl-1.542343]--[lr-0.000037]--[ETA-13:57:56]
2023.01.21-22:56:04:328:[step-39400/88800: 44.37%]--[loss-3.325596: wl-3.539959, gl-1.555616]--[lr-0.000037]--[ETA-14:18:27]
2023.01.21-22:57:48:428:[step-39500/88800: 44.48%]--[loss-3.484365: wl-3.594270, gl-1.687229]--[lr-0.000037]--[ETA-14:17:24]
End of epoch 89 / 200 	 Time Taken: 460 sec
2023.01.21-22:59:32:84:[step-39600/88800: 44.59%]--[loss-3.210972: wl-3.223414, gl-1.599265]--[lr-0.000037]--[ETA-13:46:12]
2023.01.21-23:01:15:184:[step-39700/88800: 44.71%]--[loss-3.033793: wl-2.980520, gl-1.543533]--[lr-0.000037]--[ETA-14:22:19]
2023.01.21-23:02:59:284:[step-39800/88800: 44.82%]--[loss-3.127554: wl-3.111034, gl-1.572037]--[lr-0.000037]--[ETA-14:17:12]
2023.01.21-23:04:42:384:[step-39900/88800: 44.93%]--[loss-3.135475: wl-3.194951, gl-1.538000]--[lr-0.000037]--[ETA-14:18:37]
End of epoch 90 / 200 	 Time Taken: 460 sec
saving the model at the end of epoch 90, iters 39960
2023.01.21-23:06:28:40:[step-40000/88800: 45.05%]--[loss-2.905029: wl-2.833509, gl-1.488275]--[lr-0.000037]--[ETA-14:34:14]
2023.01.21-23:08:11:140:[step-40100/88800: 45.16%]--[loss-3.148930: wl-3.180518, gl-1.558671]--[lr-0.000037]--[ETA-13:46:48]
2023.01.21-23:09:53:240:[step-40200/88800: 45.27%]--[loss-3.031301: wl-2.964744, gl-1.548930]--[lr-0.000037]--[ETA-13:46:29]
2023.01.21-23:11:36:340:[step-40300/88800: 45.38%]--[loss-3.381027: wl-3.614983, gl-1.573535]--[lr-0.000037]--[ETA-13:40:27]
2023.01.21-23:13:19:440:[step-40400/88800: 45.50%]--[loss-3.153368: wl-3.174146, gl-1.566295]--[lr-0.000037]--[ETA-13:38:01]
End of epoch 91 / 200 	 Time Taken: 458 sec
2023.01.21-23:15:04:96:[step-40500/88800: 45.61%]--[loss-3.008691: wl-3.055315, gl-1.481034]--[lr-0.000036]--[ETA-13:55:31]
2023.01.21-23:16:48:196:[step-40600/88800: 45.72%]--[loss-3.270807: wl-3.444636, gl-1.548489]--[lr-0.000036]--[ETA-14:03:40]
2023.01.21-23:18:32:296:[step-40700/88800: 45.83%]--[loss-3.078488: wl-3.074599, gl-1.541188]--[lr-0.000036]--[ETA-13:21:28]
2023.01.21-23:20:17:396:[step-40800/88800: 45.95%]--[loss-3.018118: wl-3.009012, gl-1.513612]--[lr-0.000036]--[ETA-13:28:36]
End of epoch 92 / 200 	 Time Taken: 463 sec
2023.01.21-23:22:04:52:[step-40900/88800: 46.06%]--[loss-3.058876: wl-3.048781, gl-1.534485]--[lr-0.000036]--[ETA-15:16:08]
2023.01.21-23:23:47:152:[step-41000/88800: 46.17%]--[loss-3.289166: wl-3.405502, gl-1.586415]--[lr-0.000036]--[ETA-13:29:30]
2023.01.21-23:25:33:252:[step-41100/88800: 46.28%]--[loss-3.257548: wl-3.410460, gl-1.552318]--[lr-0.000036]--[ETA-13:43:50]
2023.01.21-23:27:19:352:[step-41200/88800: 46.40%]--[loss-3.121840: wl-3.143793, gl-1.549943]--[lr-0.000036]--[ETA-13:37:59]
End of epoch 93 / 200 	 Time Taken: 469 sec
2023.01.21-23:29:06:8:[step-41300/88800: 46.51%]--[loss-3.293278: wl-3.442693, gl-1.571931]--[lr-0.000036]--[ETA-13:38:53]
2023.01.21-23:30:53:108:[step-41400/88800: 46.62%]--[loss-3.073941: wl-3.040374, gl-1.553753]--[lr-0.000036]--[ETA-14:18:02]
2023.01.21-23:32:40:208:[step-41500/88800: 46.73%]--[loss-3.135741: wl-3.147241, gl-1.562120]--[lr-0.000036]--[ETA-13:44:32]
2023.01.21-23:34:27:308:[step-41600/88800: 46.85%]--[loss-3.197243: wl-3.293443, gl-1.550521]--[lr-0.000036]--[ETA-14:44:39]
2023.01.21-23:36:13:408:[step-41700/88800: 46.96%]--[loss-3.376246: wl-3.715829, gl-1.518331]--[lr-0.000036]--[ETA-14:10:34]
End of epoch 94 / 200 	 Time Taken: 475 sec
2023.01.21-23:38:02:64:[step-41800/88800: 47.07%]--[loss-3.196561: wl-3.249025, gl-1.572048]--[lr-0.000035]--[ETA-13:35:42]
2023.01.21-23:39:49:164:[step-41900/88800: 47.18%]--[loss-3.310983: wl-3.500420, gl-1.560773]--[lr-0.000035]--[ETA-13:34:17]
2023.01.21-23:41:36:264:[step-42000/88800: 47.30%]--[loss-3.162879: wl-3.264330, gl-1.530714]--[lr-0.000035]--[ETA-14:00:48]
2023.01.21-23:43:23:364:[step-42100/88800: 47.41%]--[loss-3.104871: wl-3.198419, gl-1.505661]--[lr-0.000035]--[ETA-14:09:08]
End of epoch 95 / 200 	 Time Taken: 476 sec
saving the model at the end of epoch 95, iters 42180
2023.01.21-23:45:12:20:[step-42200/88800: 47.52%]--[loss-3.127018: wl-3.240201, gl-1.506918]--[lr-0.000035]--[ETA-14:27:46]
2023.01.21-23:46:58:120:[step-42300/88800: 47.64%]--[loss-3.088336: wl-3.077532, gl-1.549570]--[lr-0.000035]--[ETA-13:52:48]
2023.01.21-23:48:45:220:[step-42400/88800: 47.75%]--[loss-3.084763: wl-3.109703, gl-1.529911]--[lr-0.000035]--[ETA-13:50:34]
2023.01.21-23:50:31:320:[step-42500/88800: 47.86%]--[loss-3.144372: wl-3.181368, gl-1.553688]--[lr-0.000035]--[ETA-13:29:09]
2023.01.21-23:52:18:420:[step-42600/88800: 47.97%]--[loss-3.410720: wl-3.665989, gl-1.577725]--[lr-0.000035]--[ETA-13:35:43]
End of epoch 96 / 200 	 Time Taken: 474 sec
2023.01.21-23:54:06:76:[step-42700/88800: 48.09%]--[loss-3.137088: wl-3.119296, gl-1.577440]--[lr-0.000035]--[ETA-13:21:49]
2023.01.21-23:55:52:176:[step-42800/88800: 48.20%]--[loss-3.213189: wl-3.296629, gl-1.564874]--[lr-0.000035]--[ETA-13:45:31]
2023.01.21-23:57:38:276:[step-42900/88800: 48.31%]--[loss-3.295023: wl-3.373728, gl-1.608159]--[lr-0.000035]--[ETA-13:24:03]
2023.01.21-23:59:24:376:[step-43000/88800: 48.42%]--[loss-3.190555: wl-3.321222, gl-1.529944]--[lr-0.000035]--[ETA-13:07:49]
End of epoch 97 / 200 	 Time Taken: 472 sec
2023.01.22-00:01:11:32:[step-43100/88800: 48.54%]--[loss-3.313423: wl-3.511270, gl-1.557788]--[lr-0.000034]--[ETA-13:14:44]
2023.01.22-00:02:58:132:[step-43200/88800: 48.65%]--[loss-3.105382: wl-3.180706, gl-1.515029]--[lr-0.000034]--[ETA-13:17:06]
2023.01.22-00:04:44:232:[step-43300/88800: 48.76%]--[loss-3.069869: wl-3.170983, gl-1.484377]--[lr-0.000034]--[ETA-13:50:31]
2023.01.22-00:06:29:332:[step-43400/88800: 48.87%]--[loss-3.087017: wl-3.058574, gl-1.557729]--[lr-0.000034]--[ETA-13:30:20]
2023.01.22-00:08:15:432:[step-43500/88800: 48.99%]--[loss-3.158309: wl-3.319638, gl-1.498489]--[lr-0.000034]--[ETA-14:01:59]
End of epoch 98 / 200 	 Time Taken: 472 sec
2023.01.22-00:10:03:88:[step-43600/88800: 49.10%]--[loss-3.348826: wl-3.600283, gl-1.548684]--[lr-0.000034]--[ETA-13:23:28]
2023.01.22-00:11:49:188:[step-43700/88800: 49.21%]--[loss-3.372354: wl-3.468074, gl-1.638317]--[lr-0.000034]--[ETA-12:37:44]
2023.01.22-00:13:35:288:[step-43800/88800: 49.32%]--[loss-3.029488: wl-3.033637, gl-1.512670]--[lr-0.000034]--[ETA-12:55:49]
2023.01.22-00:15:20:388:[step-43900/88800: 49.44%]--[loss-3.162292: wl-3.286720, gl-1.518932]--[lr-0.000034]--[ETA-12:43:40]
End of epoch 99 / 200 	 Time Taken: 471 sec
2023.01.22-00:17:08:44:[step-44000/88800: 49.55%]--[loss-2.911229: wl-2.897717, gl-1.462370]--[lr-0.000034]--[ETA-12:49:05]
2023.01.22-00:18:53:144:[step-44100/88800: 49.66%]--[loss-3.207563: wl-3.248216, gl-1.583455]--[lr-0.000034]--[ETA-12:51:38]
2023.01.22-00:20:39:244:[step-44200/88800: 49.77%]--[loss-3.244553: wl-3.390415, gl-1.549345]--[lr-0.000034]--[ETA-12:37:28]
2023.01.22-00:22:25:344:[step-44300/88800: 49.89%]--[loss-3.376428: wl-3.546213, gl-1.603322]--[lr-0.000034]--[ETA-12:59:07]
2023.01.22-00:24:11:444:[step-44400/88800: 50.00%]--[loss-3.371331: wl-3.659341, gl-1.541661]--[lr-0.000034]--[ETA-12:33:25]
End of epoch 100 / 200 	 Time Taken: 471 sec
saving the model at the end of epoch 100, iters 44400
2023.01.22-00:25:59:100:[step-44500/88800: 50.11%]--[loss-3.082971: wl-3.090801, gl-1.537571]--[lr-0.000033]--[ETA-12:48:02]
2023.01.22-00:27:44:200:[step-44600/88800: 50.23%]--[loss-3.684789: wl-4.163518, gl-1.603030]--[lr-0.000033]--[ETA-12:52:29]
2023.01.22-00:29:30:300:[step-44700/88800: 50.34%]--[loss-3.223946: wl-3.417557, gl-1.515167]--[lr-0.000033]--[ETA-12:36:55]
2023.01.22-00:31:16:400:[step-44800/88800: 50.45%]--[loss-3.053002: wl-3.155335, gl-1.475335]--[lr-0.000033]--[ETA-12:39:43]
End of epoch 101 / 200 	 Time Taken: 470 sec
2023.01.22-00:33:03:56:[step-44900/88800: 50.56%]--[loss-3.278718: wl-3.506492, gl-1.525472]--[lr-0.000033]--[ETA-12:35:09]
2023.01.22-00:34:49:156:[step-45000/88800: 50.68%]--[loss-3.021713: wl-3.097169, gl-1.473128]--[lr-0.000033]--[ETA-13:41:12]
2023.01.22-00:36:35:256:[step-45100/88800: 50.79%]--[loss-3.428225: wl-3.843658, gl-1.506396]--[lr-0.000033]--[ETA-13:10:42]
2023.01.22-00:38:20:356:[step-45200/88800: 50.90%]--[loss-3.205550: wl-3.254798, gl-1.578151]--[lr-0.000033]--[ETA-13:03:07]
End of epoch 102 / 200 	 Time Taken: 469 sec
2023.01.22-00:40:06:12:[step-45300/88800: 51.01%]--[loss-3.483522: wl-3.771780, gl-1.597632]--[lr-0.000033]--[ETA-12:02:16]
2023.01.22-00:41:51:112:[step-45400/88800: 51.13%]--[loss-3.300848: wl-3.581350, gl-1.510172]--[lr-0.000033]--[ETA-12:39:31]
2023.01.22-00:43:37:212:[step-45500/88800: 51.24%]--[loss-3.413143: wl-3.607020, gl-1.609633]--[lr-0.000033]--[ETA-12:33:25]
2023.01.22-00:45:23:312:[step-45600/88800: 51.35%]--[loss-3.084677: wl-3.058964, gl-1.555194]--[lr-0.000033]--[ETA-12:36:58]
2023.01.22-00:47:09:412:[step-45700/88800: 51.46%]--[loss-3.198840: wl-3.333119, gl-1.532281]--[lr-0.000033]--[ETA-12:58:23]
End of epoch 103 / 200 	 Time Taken: 470 sec
2023.01.22-00:48:56:68:[step-45800/88800: 51.58%]--[loss-3.240255: wl-3.291531, gl-1.594489]--[lr-0.000032]--[ETA-12:27:01]
2023.01.22-00:50:42:168:[step-45900/88800: 51.69%]--[loss-3.179436: wl-3.269327, gl-1.544773]--[lr-0.000032]--[ETA-12:36:27]
2023.01.22-00:52:28:268:[step-46000/88800: 51.80%]--[loss-3.083050: wl-3.185412, gl-1.490345]--[lr-0.000032]--[ETA-12:40:53]
2023.01.22-00:54:13:368:[step-46100/88800: 51.91%]--[loss-2.953805: wl-2.913117, gl-1.497247]--[lr-0.000032]--[ETA-12:43:18]
End of epoch 104 / 200 	 Time Taken: 471 sec
2023.01.22-00:56:01:24:[step-46200/88800: 52.03%]--[loss-3.000798: wl-2.967107, gl-1.517244]--[lr-0.000032]--[ETA-12:09:22]
2023.01.22-00:57:47:124:[step-46300/88800: 52.14%]--[loss-3.119336: wl-3.199617, gl-1.519528]--[lr-0.000032]--[ETA-12:30:29]
2023.01.22-00:59:33:224:[step-46400/88800: 52.25%]--[loss-3.039612: wl-3.129230, gl-1.474997]--[lr-0.000032]--[ETA-12:30:40]
2023.01.22-01:01:19:324:[step-46500/88800: 52.36%]--[loss-3.223616: wl-3.289417, gl-1.578907]--[lr-0.000032]--[ETA-12:16:40]
2023.01.22-01:03:05:424:[step-46600/88800: 52.48%]--[loss-3.031337: wl-3.128597, gl-1.467038]--[lr-0.000032]--[ETA-12:46:37]
End of epoch 105 / 200 	 Time Taken: 471 sec
saving the model at the end of epoch 105, iters 46620
2023.01.22-01:04:52:80:[step-46700/88800: 52.59%]--[loss-3.114548: wl-3.144257, gl-1.542420]--[lr-0.000032]--[ETA-12:25:56]
2023.01.22-01:06:38:180:[step-46800/88800: 52.70%]--[loss-3.228816: wl-3.429588, gl-1.514022]--[lr-0.000032]--[ETA-12:54:06]
2023.01.22-01:08:24:280:[step-46900/88800: 52.82%]--[loss-3.164911: wl-3.292622, gl-1.518600]--[lr-0.000032]--[ETA-12:14:11]
2023.01.22-01:10:09:380:[step-47000/88800: 52.93%]--[loss-3.227523: wl-3.372929, gl-1.541058]--[lr-0.000032]--[ETA-12:56:39]
End of epoch 106 / 200 	 Time Taken: 470 sec
2023.01.22-01:11:56:36:[step-47100/88800: 53.04%]--[loss-3.137885: wl-3.252767, gl-1.511501]--[lr-0.000031]--[ETA-11:54:45]
2023.01.22-01:13:42:136:[step-47200/88800: 53.15%]--[loss-3.232500: wl-3.304247, gl-1.580377]--[lr-0.000031]--[ETA-12:04:00]
2023.01.22-01:15:28:236:[step-47300/88800: 53.27%]--[loss-3.079811: wl-3.082399, gl-1.538611]--[lr-0.000031]--[ETA-12:35:20]
2023.01.22-01:17:13:336:[step-47400/88800: 53.38%]--[loss-3.152795: wl-3.246749, gl-1.529421]--[lr-0.000031]--[ETA-11:38:04]
2023.01.22-01:18:58:436:[step-47500/88800: 53.49%]--[loss-3.441199: wl-3.856035, gl-1.513181]--[lr-0.000031]--[ETA-11:57:26]
End of epoch 107 / 200 	 Time Taken: 470 sec
2023.01.22-01:20:45:92:[step-47600/88800: 53.60%]--[loss-3.285980: wl-3.406100, gl-1.582930]--[lr-0.000031]--[ETA-12:11:05]
2023.01.22-01:22:31:192:[step-47700/88800: 53.72%]--[loss-3.228823: wl-3.273003, gl-1.592321]--[lr-0.000031]--[ETA-12:14:29]
2023.01.22-01:24:18:292:[step-47800/88800: 53.83%]--[loss-3.344385: wl-3.513284, gl-1.587743]--[lr-0.000031]--[ETA-12:33:36]
2023.01.22-01:26:04:392:[step-47900/88800: 53.94%]--[loss-3.196635: wl-3.259333, gl-1.566969]--[lr-0.000031]--[ETA-12:04:49]
End of epoch 108 / 200 	 Time Taken: 472 sec
2023.01.22-01:27:53:48:[step-48000/88800: 54.05%]--[loss-3.072165: wl-3.115245, gl-1.514542]--[lr-0.000031]--[ETA-13:29:46]
2023.01.22-01:29:40:148:[step-48100/88800: 54.17%]--[loss-3.458949: wl-3.659409, gl-1.629244]--[lr-0.000031]--[ETA-12:28:58]
2023.01.22-01:31:27:248:[step-48200/88800: 54.28%]--[loss-3.277831: wl-3.320650, gl-1.617506]--[lr-0.000031]--[ETA-11:54:12]
2023.01.22-01:33:14:348:[step-48300/88800: 54.39%]--[loss-3.468851: wl-3.779302, gl-1.579200]--[lr-0.000031]--[ETA-11:49:54]
End of epoch 109 / 200 	 Time Taken: 473 sec
2023.01.22-01:34:59:4:[step-48400/88800: 54.50%]--[loss-3.202883: wl-3.258018, gl-1.573873]--[lr-0.000030]--[ETA-11:47:44]
2023.01.22-01:36:45:104:[step-48500/88800: 54.62%]--[loss-3.286403: wl-3.407053, gl-1.582877]--[lr-0.000030]--[ETA-11:51:55]
2023.01.22-01:38:30:204:[step-48600/88800: 54.73%]--[loss-3.158652: wl-3.252151, gl-1.532576]--[lr-0.000030]--[ETA-11:11:18]
2023.01.22-01:40:16:304:[step-48700/88800: 54.84%]--[loss-3.290112: wl-3.574332, gl-1.502946]--[lr-0.000030]--[ETA-12:02:44]
2023.01.22-01:42:01:404:[step-48800/88800: 54.95%]--[loss-3.308211: wl-3.660560, gl-1.477931]--[lr-0.000030]--[ETA-11:47:17]
End of epoch 110 / 200 	 Time Taken: 470 sec
saving the model at the end of epoch 110, iters 48840
2023.01.22-01:43:49:60:[step-48900/88800: 55.07%]--[loss-3.065559: wl-3.137807, gl-1.496656]--[lr-0.000030]--[ETA-11:52:22]
2023.01.22-01:45:35:160:[step-49000/88800: 55.18%]--[loss-3.168063: wl-3.204061, gl-1.566033]--[lr-0.000030]--[ETA-11:41:38]
2023.01.22-01:47:20:260:[step-49100/88800: 55.29%]--[loss-3.076238: wl-3.123326, gl-1.514575]--[lr-0.000030]--[ETA-12:00:29]
2023.01.22-01:49:06:360:[step-49200/88800: 55.41%]--[loss-2.938079: wl-2.961209, gl-1.457474]--[lr-0.000030]--[ETA-11:45:28]
End of epoch 111 / 200 	 Time Taken: 470 sec
2023.01.22-01:50:53:16:[step-49300/88800: 55.52%]--[loss-3.095700: wl-3.180907, gl-1.505247]--[lr-0.000030]--[ETA-11:38:00]
2023.01.22-01:52:39:116:[step-49400/88800: 55.63%]--[loss-3.304697: wl-3.479421, gl-1.564986]--[lr-0.000030]--[ETA-11:31:45]
2023.01.22-01:54:25:216:[step-49500/88800: 55.74%]--[loss-2.964385: wl-2.972316, gl-1.478227]--[lr-0.000030]--[ETA-11:24:21]
2023.01.22-01:56:10:316:[step-49600/88800: 55.86%]--[loss-3.209935: wl-3.322351, gl-1.548760]--[lr-0.000030]--[ETA-11:28:06]
2023.01.22-01:57:56:416:[step-49700/88800: 55.97%]--[loss-3.300443: wl-3.439092, gl-1.580897]--[lr-0.000030]--[ETA-11:41:31]
End of epoch 112 / 200 	 Time Taken: 470 sec
2023.01.22-01:59:43:72:[step-49800/88800: 56.08%]--[loss-3.118453: wl-3.187382, gl-1.524761]--[lr-0.000029]--[ETA-12:03:10]
2023.01.22-02:01:29:172:[step-49900/88800: 56.19%]--[loss-3.196834: wl-3.370880, gl-1.511394]--[lr-0.000029]--[ETA-11:22:40]
2023.01.22-02:03:15:272:[step-50000/88800: 56.31%]--[loss-3.115061: wl-3.311349, gl-1.459386]--[lr-0.000029]--[ETA-11:54:31]
2023.01.22-02:05:00:372:[step-50100/88800: 56.42%]--[loss-3.169186: wl-3.267846, gl-1.535263]--[lr-0.000029]--[ETA-11:12:20]
End of epoch 113 / 200 	 Time Taken: 470 sec
2023.01.22-02:06:47:28:[step-50200/88800: 56.53%]--[loss-3.350092: wl-3.648576, gl-1.525805]--[lr-0.000029]--[ETA-11:27:26]
2023.01.22-02:08:32:128:[step-50300/88800: 56.64%]--[loss-3.142634: wl-3.269375, gl-1.507946]--[lr-0.000029]--[ETA-11:37:54]
2023.01.22-02:10:18:228:[step-50400/88800: 56.76%]--[loss-3.058288: wl-3.057478, gl-1.529549]--[lr-0.000029]--[ETA-11:46:28]
2023.01.22-02:12:04:328:[step-50500/88800: 56.87%]--[loss-3.177449: wl-3.224727, gl-1.565086]--[lr-0.000029]--[ETA-11:12:08]
2023.01.22-02:13:49:428:[step-50600/88800: 56.98%]--[loss-3.080768: wl-3.132976, gl-1.514280]--[lr-0.000029]--[ETA-11:36:57]
End of epoch 114 / 200 	 Time Taken: 470 sec
2023.01.22-02:15:36:84:[step-50700/88800: 57.09%]--[loss-3.239586: wl-3.333438, gl-1.572867]--[lr-0.000029]--[ETA-11:11:12]
2023.01.22-02:17:22:184:[step-50800/88800: 57.21%]--[loss-3.106075: wl-3.188961, gl-1.511594]--[lr-0.000029]--[ETA-10:44:44]
2023.01.22-02:19:08:284:[step-50900/88800: 57.32%]--[loss-3.186210: wl-3.348622, gl-1.511899]--[lr-0.000029]--[ETA-10:57:16]
2023.01.22-02:20:53:384:[step-51000/88800: 57.43%]--[loss-3.271034: wl-3.422625, gl-1.559721]--[lr-0.000029]--[ETA-10:52:36]
End of epoch 115 / 200 	 Time Taken: 470 sec
saving the model at the end of epoch 115, iters 51060
2023.01.22-02:22:41:40:[step-51100/88800: 57.55%]--[loss-3.102409: wl-3.256273, gl-1.474273]--[lr-0.000028]--[ETA-10:49:54]
2023.01.22-02:24:26:140:[step-51200/88800: 57.66%]--[loss-3.318051: wl-3.632844, gl-1.501629]--[lr-0.000028]--[ETA-10:24:28]
2023.01.22-02:26:12:240:[step-51300/88800: 57.77%]--[loss-3.151915: wl-3.214594, gl-1.544618]--[lr-0.000028]--[ETA-10:47:46]
2023.01.22-02:27:57:340:[step-51400/88800: 57.88%]--[loss-3.094584: wl-3.214621, gl-1.487273]--[lr-0.000028]--[ETA-10:31:30]
2023.01.22-02:29:42:440:[step-51500/88800: 58.00%]--[loss-3.175118: wl-3.249721, gl-1.550258]--[lr-0.000028]--[ETA-10:28:21]
End of epoch 116 / 200 	 Time Taken: 469 sec
2023.01.22-02:31:29:96:[step-51600/88800: 58.11%]--[loss-3.126042: wl-3.292009, gl-1.480037]--[lr-0.000028]--[ETA-10:54:26]
2023.01.22-02:33:15:196:[step-51700/88800: 58.22%]--[loss-3.259305: wl-3.392329, gl-1.563141]--[lr-0.000028]--[ETA-10:40:50]
2023.01.22-02:35:00:296:[step-51800/88800: 58.33%]--[loss-3.177975: wl-3.308225, gl-1.523862]--[lr-0.000028]--[ETA-11:10:29]
2023.01.22-02:36:45:396:[step-51900/88800: 58.45%]--[loss-3.014350: wl-3.150252, gl-1.439224]--[lr-0.000028]--[ETA-10:42:04]
End of epoch 117 / 200 	 Time Taken: 469 sec
2023.01.22-02:38:32:52:[step-52000/88800: 58.56%]--[loss-3.286578: wl-3.607031, gl-1.483063]--[lr-0.000028]--[ETA-10:41:46]
2023.01.22-02:40:18:152:[step-52100/88800: 58.67%]--[loss-3.102838: wl-3.198339, gl-1.503669]--[lr-0.000028]--[ETA-10:40:12]
2023.01.22-02:42:03:252:[step-52200/88800: 58.78%]--[loss-3.641478: wl-4.022053, gl-1.630451]--[lr-0.000028]--[ETA-10:17:31]
2023.01.22-02:43:48:352:[step-52300/88800: 58.90%]--[loss-3.210108: wl-3.486322, gl-1.466947]--[lr-0.000028]--[ETA-10:50:12]
End of epoch 118 / 200 	 Time Taken: 469 sec
2023.01.22-02:45:35:8:[step-52400/88800: 59.01%]--[loss-2.921398: wl-2.942497, gl-1.450149]--[lr-0.000027]--[ETA-10:33:18]
2023.01.22-02:47:21:108:[step-52500/88800: 59.12%]--[loss-2.948889: wl-2.974797, gl-1.461490]--[lr-0.000027]--[ETA-10:11:30]
2023.01.22-02:49:06:208:[step-52600/88800: 59.23%]--[loss-3.136741: wl-3.134159, gl-1.569662]--[lr-0.000027]--[ETA-10:10:37]
2023.01.22-02:50:51:308:[step-52700/88800: 59.35%]--[loss-3.115698: wl-3.219837, gl-1.505780]--[lr-0.000027]--[ETA-11:04:52]
2023.01.22-02:52:36:408:[step-52800/88800: 59.46%]--[loss-3.225881: wl-3.400691, gl-1.525535]--[lr-0.000027]--[ETA-10:30:13]
End of epoch 119 / 200 	 Time Taken: 468 sec
2023.01.22-02:54:23:64:[step-52900/88800: 59.57%]--[loss-3.202985: wl-3.324175, gl-1.540897]--[lr-0.000027]--[ETA-10:20:15]
2023.01.22-02:56:08:164:[step-53000/88800: 59.68%]--[loss-3.048319: wl-3.174146, gl-1.461246]--[lr-0.000027]--[ETA-11:03:25]
2023.01.22-02:57:54:264:[step-53100/88800: 59.80%]--[loss-3.083164: wl-3.153146, gl-1.506591]--[lr-0.000027]--[ETA-10:18:53]
2023.01.22-02:59:39:364:[step-53200/88800: 59.91%]--[loss-3.263094: wl-3.585514, gl-1.470337]--[lr-0.000027]--[ETA-10:08:11]
End of epoch 120 / 200 	 Time Taken: 469 sec
saving the model at the end of epoch 120, iters 53280
2023.01.22-03:01:27:20:[step-53300/88800: 60.02%]--[loss-3.303566: wl-3.521386, gl-1.542873]--[lr-0.000027]--[ETA-10:00:01]
2023.01.22-03:03:11:120:[step-53400/88800: 60.14%]--[loss-3.439345: wl-3.779420, gl-1.549635]--[lr-0.000027]--[ETA-10:41:26]
2023.01.22-03:04:56:220:[step-53500/88800: 60.25%]--[loss-3.341421: wl-3.628801, gl-1.527021]--[lr-0.000027]--[ETA-10:13:10]
2023.01.22-03:06:42:320:[step-53600/88800: 60.36%]--[loss-3.066523: wl-3.169647, gl-1.481699]--[lr-0.000027]--[ETA-10:27:28]
2023.01.22-03:08:27:420:[step-53700/88800: 60.47%]--[loss-3.061560: wl-3.125137, gl-1.498991]--[lr-0.000027]--[ETA-9:45:18]
End of epoch 121 / 200 	 Time Taken: 468 sec
2023.01.22-03:10:14:76:[step-53800/88800: 60.59%]--[loss-3.140943: wl-3.293461, gl-1.494213]--[lr-0.000026]--[ETA-10:09:06]
2023.01.22-03:11:59:176:[step-53900/88800: 60.70%]--[loss-2.993687: wl-3.077640, gl-1.454867]--[lr-0.000026]--[ETA-10:10:03]
2023.01.22-03:13:45:276:[step-54000/88800: 60.81%]--[loss-3.421974: wl-3.751557, gl-1.546195]--[lr-0.000026]--[ETA-9:56:01]
2023.01.22-03:15:30:376:[step-54100/88800: 60.92%]--[loss-3.003864: wl-3.031276, gl-1.488226]--[lr-0.000026]--[ETA-10:27:25]
End of epoch 122 / 200 	 Time Taken: 468 sec
2023.01.22-03:17:17:32:[step-54200/88800: 61.04%]--[loss-3.861078: wl-4.663111, gl-1.529523]--[lr-0.000026]--[ETA-10:07:39]
2023.01.22-03:19:02:132:[step-54300/88800: 61.15%]--[loss-3.408371: wl-3.843340, gl-1.486701]--[lr-0.000026]--[ETA-10:22:50]
2023.01.22-03:20:47:232:[step-54400/88800: 61.26%]--[loss-3.405277: wl-3.724835, gl-1.542860]--[lr-0.000026]--[ETA-10:20:22]
2023.01.22-03:22:32:332:[step-54500/88800: 61.37%]--[loss-3.146604: wl-3.265794, gl-1.513707]--[lr-0.000026]--[ETA-10:01:35]
2023.01.22-03:24:18:432:[step-54600/88800: 61.49%]--[loss-2.944044: wl-3.023939, gl-1.432075]--[lr-0.000026]--[ETA-10:30:31]
End of epoch 123 / 200 	 Time Taken: 469 sec
2023.01.22-03:26:05:88:[step-54700/88800: 61.60%]--[loss-3.157141: wl-3.382448, gl-1.465917]--[lr-0.000026]--[ETA-9:56:48]
2023.01.22-03:27:50:188:[step-54800/88800: 61.71%]--[loss-3.178762: wl-3.296040, gl-1.530742]--[lr-0.000026]--[ETA-10:02:04]
2023.01.22-03:29:35:288:[step-54900/88800: 61.82%]--[loss-3.303457: wl-3.632044, gl-1.487435]--[lr-0.000026]--[ETA-9:57:20]
2023.01.22-03:31:21:388:[step-55000/88800: 61.94%]--[loss-3.144720: wl-3.293387, gl-1.498026]--[lr-0.000026]--[ETA-9:37:16]
End of epoch 124 / 200 	 Time Taken: 470 sec
2023.01.22-03:33:09:44:[step-55100/88800: 62.05%]--[loss-3.166660: wl-3.296241, gl-1.518540]--[lr-0.000025]--[ETA-9:49:45]
2023.01.22-03:34:54:144:[step-55200/88800: 62.16%]--[loss-3.450665: wl-3.903549, gl-1.498890]--[lr-0.000025]--[ETA-9:34:58]
2023.01.22-03:36:41:244:[step-55300/88800: 62.27%]--[loss-2.946593: wl-2.953282, gl-1.469952]--[lr-0.000025]--[ETA-9:44:41]
2023.01.22-03:38:28:344:[step-55400/88800: 62.39%]--[loss-3.002247: wl-3.115400, gl-1.444547]--[lr-0.000025]--[ETA-9:53:13]
2023.01.22-03:40:15:444:[step-55500/88800: 62.50%]--[loss-2.912008: wl-2.912571, gl-1.455722]--[lr-0.000025]--[ETA-9:34:49]
End of epoch 125 / 200 	 Time Taken: 474 sec
saving the model at the end of epoch 125, iters 55500
2023.01.22-03:42:04:100:[step-55600/88800: 62.61%]--[loss-3.022127: wl-3.095364, gl-1.474445]--[lr-0.000025]--[ETA-10:12:32]
2023.01.22-03:43:47:200:[step-55700/88800: 62.73%]--[loss-3.285263: wl-3.529993, gl-1.520267]--[lr-0.000025]--[ETA-9:19:52]
2023.01.22-03:45:32:300:[step-55800/88800: 62.84%]--[loss-3.517451: wl-3.864473, gl-1.585215]--[lr-0.000025]--[ETA-9:38:55]
2023.01.22-03:47:17:400:[step-55900/88800: 62.95%]--[loss-3.087201: wl-3.257062, gl-1.458670]--[lr-0.000025]--[ETA-9:47:40]
End of epoch 126 / 200 	 Time Taken: 467 sec
2023.01.22-03:49:04:56:[step-56000/88800: 63.06%]--[loss-3.508879: wl-3.866728, gl-1.575515]--[lr-0.000025]--[ETA-9:30:59]
2023.01.22-03:50:49:156:[step-56100/88800: 63.18%]--[loss-2.928354: wl-2.957636, gl-1.449536]--[lr-0.000025]--[ETA-9:22:29]
2023.01.22-03:52:34:256:[step-56200/88800: 63.29%]--[loss-3.017853: wl-3.102967, gl-1.466370]--[lr-0.000025]--[ETA-9:25:02]
2023.01.22-03:54:20:356:[step-56300/88800: 63.40%]--[loss-3.097287: wl-3.198094, gl-1.498240]--[lr-0.000025]--[ETA-9:58:06]
End of epoch 127 / 200 	 Time Taken: 468 sec
2023.01.22-03:56:06:12:[step-56400/88800: 63.51%]--[loss-3.109575: wl-3.235738, gl-1.491706]--[lr-0.000024]--[ETA-9:33:34]
2023.01.22-03:57:51:112:[step-56500/88800: 63.63%]--[loss-3.179202: wl-3.267238, gl-1.545583]--[lr-0.000024]--[ETA-9:20:17]
2023.01.22-03:59:37:212:[step-56600/88800: 63.74%]--[loss-3.053219: wl-3.178425, gl-1.464007]--[lr-0.000024]--[ETA-9:05:18]
2023.01.22-04:01:22:312:[step-56700/88800: 63.85%]--[loss-3.190402: wl-3.375334, gl-1.502735]--[lr-0.000024]--[ETA-9:21:33]
2023.01.22-04:03:07:412:[step-56800/88800: 63.96%]--[loss-2.960150: wl-3.039139, gl-1.440580]--[lr-0.000024]--[ETA-8:48:16]
End of epoch 128 / 200 	 Time Taken: 468 sec
2023.01.22-04:04:54:68:[step-56900/88800: 64.08%]--[loss-2.992139: wl-3.054749, gl-1.464764]--[lr-0.000024]--[ETA-9:26:42]
2023.01.22-04:06:39:168:[step-57000/88800: 64.19%]--[loss-3.192299: wl-3.283089, gl-1.550754]--[lr-0.000024]--[ETA-9:30:01]
2023.01.22-04:08:25:268:[step-57100/88800: 64.30%]--[loss-3.029679: wl-3.082695, gl-1.488332]--[lr-0.000024]--[ETA-8:59:45]
2023.01.22-04:10:11:368:[step-57200/88800: 64.41%]--[loss-3.085215: wl-3.285157, gl-1.442637]--[lr-0.000024]--[ETA-9:14:58]
End of epoch 129 / 200 	 Time Taken: 470 sec
2023.01.22-04:11:58:24:[step-57300/88800: 64.53%]--[loss-2.978259: wl-3.018939, gl-1.468789]--[lr-0.000024]--[ETA-9:49:22]
2023.01.22-04:13:43:124:[step-57400/88800: 64.64%]--[loss-3.124710: wl-3.343005, gl-1.453207]--[lr-0.000024]--[ETA-9:08:37]
2023.01.22-04:15:29:224:[step-57500/88800: 64.75%]--[loss-2.937042: wl-3.014655, gl-1.429714]--[lr-0.000024]--[ETA-9:11:23]
2023.01.22-04:17:14:324:[step-57600/88800: 64.86%]--[loss-3.112785: wl-3.237800, gl-1.493885]--[lr-0.000024]--[ETA-9:30:21]
2023.01.22-04:18:59:424:[step-57700/88800: 64.98%]--[loss-2.878184: wl-2.895785, gl-1.430292]--[lr-0.000024]--[ETA-8:47:03]
End of epoch 130 / 200 	 Time Taken: 469 sec
saving the model at the end of epoch 130, iters 57720
2023.01.22-04:20:46:80:[step-57800/88800: 65.09%]--[loss-3.443470: wl-3.905405, gl-1.490768]--[lr-0.000023]--[ETA-8:39:45]
2023.01.22-04:22:31:180:[step-57900/88800: 65.20%]--[loss-3.125467: wl-3.235383, gl-1.507776]--[lr-0.000023]--[ETA-9:00:41]
2023.01.22-04:24:17:280:[step-58000/88800: 65.32%]--[loss-3.059991: wl-3.209273, gl-1.455355]--[lr-0.000023]--[ETA-8:46:24]
2023.01.22-04:26:02:380:[step-58100/88800: 65.43%]--[loss-2.989260: wl-3.071288, gl-1.453616]--[lr-0.000023]--[ETA-8:48:12]
End of epoch 131 / 200 	 Time Taken: 469 sec
2023.01.22-04:27:49:36:[step-58200/88800: 65.54%]--[loss-3.277907: wl-3.687080, gl-1.434367]--[lr-0.000023]--[ETA-9:09:20]
2023.01.22-04:29:34:136:[step-58300/88800: 65.65%]--[loss-3.100996: wl-3.167998, gl-1.516996]--[lr-0.000023]--[ETA-9:14:42]
2023.01.22-04:31:19:236:[step-58400/88800: 65.77%]--[loss-2.939335: wl-2.995578, gl-1.441545]--[lr-0.000023]--[ETA-8:52:46]
2023.01.22-04:33:04:336:[step-58500/88800: 65.88%]--[loss-2.911740: wl-2.930525, gl-1.446477]--[lr-0.000023]--[ETA-9:08:26]
2023.01.22-04:34:50:436:[step-58600/88800: 65.99%]--[loss-3.341641: wl-3.729431, gl-1.476925]--[lr-0.000023]--[ETA-8:48:29]
End of epoch 132 / 200 	 Time Taken: 468 sec
2023.01.22-04:36:37:92:[step-58700/88800: 66.10%]--[loss-3.145099: wl-3.340837, gl-1.474681]--[lr-0.000023]--[ETA-9:15:43]
2023.01.22-04:38:22:192:[step-58800/88800: 66.22%]--[loss-3.205190: wl-3.419330, gl-1.495525]--[lr-0.000023]--[ETA-8:45:11]
2023.01.22-04:40:08:292:[step-58900/88800: 66.33%]--[loss-2.997358: wl-3.106899, gl-1.443908]--[lr-0.000023]--[ETA-8:58:31]
2023.01.22-04:41:53:392:[step-59000/88800: 66.44%]--[loss-3.196008: wl-3.497834, gl-1.447091]--[lr-0.000023]--[ETA-9:02:01]
End of epoch 133 / 200 	 Time Taken: 470 sec
2023.01.22-04:43:40:48:[step-59100/88800: 66.55%]--[loss-3.205940: wl-3.381840, gl-1.515021]--[lr-0.000022]--[ETA-8:18:03]
2023.01.22-04:45:26:148:[step-59200/88800: 66.67%]--[loss-3.120203: wl-3.240635, gl-1.499885]--[lr-0.000022]--[ETA-8:55:04]
2023.01.22-04:47:11:248:[step-59300/88800: 66.78%]--[loss-3.306400: wl-3.534151, gl-1.539324]--[lr-0.000022]--[ETA-8:35:04]
2023.01.22-04:48:57:348:[step-59400/88800: 66.89%]--[loss-2.999824: wl-3.059836, gl-1.469906]--[lr-0.000022]--[ETA-8:30:59]
End of epoch 134 / 200 	 Time Taken: 470 sec
2023.01.22-04:50:44:4:[step-59500/88800: 67.00%]--[loss-3.293330: wl-3.644648, gl-1.471006]--[lr-0.000022]--[ETA-8:51:29]
2023.01.22-04:52:30:104:[step-59600/88800: 67.12%]--[loss-3.093258: wl-3.155252, gl-1.515632]--[lr-0.000022]--[ETA-8:42:42]
2023.01.22-04:54:15:204:[step-59700/88800: 67.23%]--[loss-3.054376: wl-3.195255, gl-1.456748]--[lr-0.000022]--[ETA-8:35:06]
2023.01.22-04:56:01:304:[step-59800/88800: 67.34%]--[loss-3.049181: wl-3.194013, gl-1.452175]--[lr-0.000022]--[ETA-8:19:04]
2023.01.22-04:57:46:404:[step-59900/88800: 67.45%]--[loss-3.209633: wl-3.417283, gl-1.500991]--[lr-0.000022]--[ETA-8:29:58]
End of epoch 135 / 200 	 Time Taken: 470 sec
saving the model at the end of epoch 135, iters 59940
2023.01.22-04:59:34:60:[step-60000/88800: 67.57%]--[loss-3.617387: wl-4.149284, gl-1.542745]--[lr-0.000022]--[ETA-8:24:38]
2023.01.22-05:01:19:160:[step-60100/88800: 67.68%]--[loss-3.448772: wl-3.740082, gl-1.578731]--[lr-0.000022]--[ETA-8:26:58]
2023.01.22-05:03:05:260:[step-60200/88800: 67.79%]--[loss-3.169474: wl-3.343070, gl-1.497939]--[lr-0.000022]--[ETA-8:19:17]
2023.01.22-05:04:50:360:[step-60300/88800: 67.91%]--[loss-3.214446: wl-3.472859, gl-1.478017]--[lr-0.000022]--[ETA-8:05:35]
End of epoch 136 / 200 	 Time Taken: 470 sec
2023.01.22-05:06:38:16:[step-60400/88800: 68.02%]--[loss-3.064539: wl-3.248353, gl-1.440363]--[lr-0.000021]--[ETA-8:44:32]
2023.01.22-05:08:23:116:[step-60500/88800: 68.13%]--[loss-3.192202: wl-3.331716, gl-1.526345]--[lr-0.000021]--[ETA-8:07:27]
2023.01.22-05:10:08:216:[step-60600/88800: 68.24%]--[loss-3.135097: wl-3.376735, gl-1.446729]--[lr-0.000021]--[ETA-7:51:53]
2023.01.22-05:11:54:316:[step-60700/88800: 68.36%]--[loss-2.881618: wl-2.907306, gl-1.427964]--[lr-0.000021]--[ETA-8:31:37]
2023.01.22-05:13:40:416:[step-60800/88800: 68.47%]--[loss-3.288582: wl-3.539332, gl-1.518916]--[lr-0.000021]--[ETA-8:25:40]
End of epoch 137 / 200 	 Time Taken: 470 sec
2023.01.22-05:15:27:72:[step-60900/88800: 68.58%]--[loss-3.085248: wl-3.194177, gl-1.488159]--[lr-0.000021]--[ETA-7:55:07]
2023.01.22-05:17:12:172:[step-61000/88800: 68.69%]--[loss-3.070712: wl-3.201005, gl-1.470209]--[lr-0.000021]--[ETA-8:10:40]
2023.01.22-05:18:58:272:[step-61100/88800: 68.81%]--[loss-3.149119: wl-3.344489, gl-1.476875]--[lr-0.000021]--[ETA-8:08:38]
2023.01.22-05:20:43:372:[step-61200/88800: 68.92%]--[loss-3.338973: wl-3.720750, gl-1.478598]--[lr-0.000021]--[ETA-8:18:44]
End of epoch 138 / 200 	 Time Taken: 467 sec
2023.01.22-05:22:28:28:[step-61300/88800: 69.03%]--[loss-3.100286: wl-3.354327, gl-1.423123]--[lr-0.000021]--[ETA-7:51:27]
2023.01.22-05:24:13:128:[step-61400/88800: 69.14%]--[loss-3.221995: wl-3.526271, gl-1.458859]--[lr-0.000021]--[ETA-8:24:18]
2023.01.22-05:25:59:228:[step-61500/88800: 69.26%]--[loss-3.489821: wl-3.981242, gl-1.499201]--[lr-0.000021]--[ETA-7:59:51]
2023.01.22-05:27:45:328:[step-61600/88800: 69.37%]--[loss-2.994528: wl-3.135798, gl-1.426629]--[lr-0.000021]--[ETA-7:50:43]
2023.01.22-05:29:31:428:[step-61700/88800: 69.48%]--[loss-3.047357: wl-3.197056, gl-1.448829]--[lr-0.000021]--[ETA-7:37:47]
End of epoch 139 / 200 	 Time Taken: 470 sec
2023.01.22-05:31:18:84:[step-61800/88800: 69.59%]--[loss-2.895690: wl-2.949864, gl-1.420758]--[lr-0.000020]--[ETA-7:27:29]
2023.01.22-05:33:03:184:[step-61900/88800: 69.71%]--[loss-3.236238: wl-3.500807, gl-1.485834]--[lr-0.000020]--[ETA-7:29:05]
2023.01.22-05:34:49:284:[step-62000/88800: 69.82%]--[loss-3.148299: wl-3.277561, gl-1.509518]--[lr-0.000020]--[ETA-7:46:27]
2023.01.22-05:36:34:384:[step-62100/88800: 69.93%]--[loss-3.067530: wl-3.221093, gl-1.456984]--[lr-0.000020]--[ETA-7:41:59]
End of epoch 140 / 200 	 Time Taken: 469 sec
saving the model at the end of epoch 140, iters 62160
2023.01.22-05:38:21:40:[step-62200/88800: 70.05%]--[loss-3.092977: wl-3.260478, gl-1.462738]--[lr-0.000020]--[ETA-7:53:40]
2023.01.22-05:40:06:140:[step-62300/88800: 70.16%]--[loss-3.265277: wl-3.460933, gl-1.534810]--[lr-0.000020]--[ETA-7:58:41]
2023.01.22-05:41:52:240:[step-62400/88800: 70.27%]--[loss-2.993273: wl-3.141010, gl-1.422768]--[lr-0.000020]--[ETA-7:39:15]
2023.01.22-05:43:38:340:[step-62500/88800: 70.38%]--[loss-3.241358: wl-3.532874, gl-1.474921]--[lr-0.000020]--[ETA-7:53:44]
2023.01.22-05:45:24:440:[step-62600/88800: 70.50%]--[loss-2.940363: wl-3.027230, gl-1.426748]--[lr-0.000020]--[ETA-7:26:52]
End of epoch 141 / 200 	 Time Taken: 470 sec
2023.01.22-05:47:12:96:[step-62700/88800: 70.61%]--[loss-3.050803: wl-3.220366, gl-1.440620]--[lr-0.000020]--[ETA-7:44:03]
2023.01.22-05:48:59:196:[step-62800/88800: 70.72%]--[loss-3.012932: wl-3.074833, gl-1.475516]--[lr-0.000020]--[ETA-7:33:50]
2023.01.22-05:50:46:296:[step-62900/88800: 70.83%]--[loss-3.253425: wl-3.415532, gl-1.545659]--[lr-0.000020]--[ETA-7:40:29]
2023.01.22-05:52:30:396:[step-63000/88800: 70.95%]--[loss-3.230414: wl-3.414504, gl-1.523162]--[lr-0.000020]--[ETA-7:15:06]
End of epoch 142 / 200 	 Time Taken: 471 sec
2023.01.22-05:54:16:52:[step-63100/88800: 71.06%]--[loss-3.181135: wl-3.409250, gl-1.476510]--[lr-0.000019]--[ETA-7:23:45]
2023.01.22-05:56:01:152:[step-63200/88800: 71.17%]--[loss-3.147211: wl-3.361639, gl-1.466391]--[lr-0.000019]--[ETA-7:29:38]
2023.01.22-05:57:46:252:[step-63300/88800: 71.28%]--[loss-3.223881: wl-3.494271, gl-1.476746]--[lr-0.000019]--[ETA-7:36:53]
2023.01.22-05:59:32:352:[step-63400/88800: 71.40%]--[loss-3.366156: wl-3.792593, gl-1.469859]--[lr-0.000019]--[ETA-7:21:47]
End of epoch 143 / 200 	 Time Taken: 469 sec
2023.01.22-06:01:19:8:[step-63500/88800: 71.51%]--[loss-2.971927: wl-3.126796, gl-1.408529]--[lr-0.000019]--[ETA-7:27:27]
2023.01.22-06:03:05:108:[step-63600/88800: 71.62%]--[loss-2.997903: wl-3.150295, gl-1.422755]--[lr-0.000019]--[ETA-7:37:40]
2023.01.22-06:04:50:208:[step-63700/88800: 71.73%]--[loss-2.915922: wl-2.946531, gl-1.442657]--[lr-0.000019]--[ETA-6:56:14]
2023.01.22-06:06:36:308:[step-63800/88800: 71.85%]--[loss-3.106394: wl-3.304224, gl-1.454282]--[lr-0.000019]--[ETA-7:23:20]
2023.01.22-06:08:21:408:[step-63900/88800: 71.96%]--[loss-3.280091: wl-3.523481, gl-1.518351]--[lr-0.000019]--[ETA-6:59:59]
End of epoch 144 / 200 	 Time Taken: 470 sec
2023.01.22-06:10:08:64:[step-64000/88800: 72.07%]--[loss-3.115164: wl-3.248074, gl-1.491127]--[lr-0.000019]--[ETA-6:57:21]
2023.01.22-06:11:54:164:[step-64100/88800: 72.18%]--[loss-3.090885: wl-3.246177, gl-1.467796]--[lr-0.000019]--[ETA-7:09:42]
2023.01.22-06:13:39:264:[step-64200/88800: 72.30%]--[loss-3.038395: wl-3.181055, gl-1.447867]--[lr-0.000019]--[ETA-7:05:05]
2023.01.22-06:15:24:364:[step-64300/88800: 72.41%]--[loss-3.157242: wl-3.319530, gl-1.497477]--[lr-0.000019]--[ETA-7:22:48]
End of epoch 145 / 200 	 Time Taken: 468 sec
saving the model at the end of epoch 145, iters 64380
2023.01.22-06:17:11:20:[step-64400/88800: 72.52%]--[loss-2.835341: wl-2.865133, gl-1.402774]--[lr-0.000018]--[ETA-7:23:42]
2023.01.22-06:18:56:120:[step-64500/88800: 72.64%]--[loss-3.110165: wl-3.257110, gl-1.481611]--[lr-0.000018]--[ETA-7:06:24]
2023.01.22-06:20:41:220:[step-64600/88800: 72.75%]--[loss-2.983959: wl-3.123153, gl-1.422383]--[lr-0.000018]--[ETA-7:11:01]
2023.01.22-06:22:26:320:[step-64700/88800: 72.86%]--[loss-2.944875: wl-3.053628, gl-1.418061]--[lr-0.000018]--[ETA-7:05:52]
2023.01.22-06:24:12:420:[step-64800/88800: 72.97%]--[loss-3.248784: wl-3.583905, gl-1.456832]--[lr-0.000018]--[ETA-7:22:39]
End of epoch 146 / 200 	 Time Taken: 468 sec
2023.01.22-06:25:58:76:[step-64900/88800: 73.09%]--[loss-3.167388: wl-3.473187, gl-1.430794]--[lr-0.000018]--[ETA-7:05:39]
2023.01.22-06:27:43:176:[step-65000/88800: 73.20%]--[loss-2.953547: wl-3.052947, gl-1.427073]--[lr-0.000018]--[ETA-7:10:22]
2023.01.22-06:29:28:276:[step-65100/88800: 73.31%]--[loss-3.054525: wl-3.230869, gl-1.439091]--[lr-0.000018]--[ETA-6:36:05]
2023.01.22-06:31:14:376:[step-65200/88800: 73.42%]--[loss-2.963295: wl-3.094829, gl-1.415881]--[lr-0.000018]--[ETA-7:02:32]
End of epoch 147 / 200 	 Time Taken: 468 sec
2023.01.22-06:33:01:32:[step-65300/88800: 73.54%]--[loss-3.253726: wl-3.521352, gl-1.493049]--[lr-0.000018]--[ETA-6:41:53]
2023.01.22-06:34:46:132:[step-65400/88800: 73.65%]--[loss-3.215250: wl-3.459586, gl-1.485458]--[lr-0.000018]--[ETA-6:43:12]
2023.01.22-06:36:31:232:[step-65500/88800: 73.76%]--[loss-2.975070: wl-3.108868, gl-1.420636]--[lr-0.000018]--[ETA-6:41:00]
2023.01.22-06:38:17:332:[step-65600/88800: 73.87%]--[loss-3.084543: wl-3.303525, gl-1.432780]--[lr-0.000018]--[ETA-6:24:05]
2023.01.22-06:40:02:432:[step-65700/88800: 73.99%]--[loss-3.077274: wl-3.301503, gl-1.426523]--[lr-0.000018]--[ETA-7:00:35]
End of epoch 148 / 200 	 Time Taken: 468 sec
2023.01.22-06:41:49:88:[step-65800/88800: 74.10%]--[loss-3.310465: wl-3.663571, gl-1.478679]--[lr-0.000017]--[ETA-6:26:20]
2023.01.22-06:43:34:188:[step-65900/88800: 74.21%]--[loss-2.981316: wl-3.039896, gl-1.461368]--[lr-0.000017]--[ETA-6:24:17]
2023.01.22-06:45:19:288:[step-66000/88800: 74.32%]--[loss-2.925428: wl-2.993398, gl-1.428729]--[lr-0.000017]--[ETA-6:36:04]
2023.01.22-06:47:05:388:[step-66100/88800: 74.44%]--[loss-3.022880: wl-3.184173, gl-1.430793]--[lr-0.000017]--[ETA-6:36:24]
End of epoch 149 / 200 	 Time Taken: 469 sec
2023.01.22-06:48:52:44:[step-66200/88800: 74.55%]--[loss-3.212233: wl-3.391118, gl-1.516674]--[lr-0.000017]--[ETA-6:32:08]
2023.01.22-06:50:37:144:[step-66300/88800: 74.66%]--[loss-3.069097: wl-3.239595, gl-1.449299]--[lr-0.000017]--[ETA-6:28:42]
2023.01.22-06:52:23:244:[step-66400/88800: 74.77%]--[loss-3.020163: wl-3.170258, gl-1.435034]--[lr-0.000017]--[ETA-6:12:22]
2023.01.22-06:54:08:344:[step-66500/88800: 74.89%]--[loss-3.327447: wl-3.715419, gl-1.469738]--[lr-0.000017]--[ETA-6:19:52]
2023.01.22-06:55:53:444:[step-66600/88800: 75.00%]--[loss-3.207964: wl-3.500112, gl-1.457908]--[lr-0.000017]--[ETA-6:22:06]
End of epoch 150 / 200 	 Time Taken: 469 sec
saving the model at the end of epoch 150, iters 66600
2023.01.22-06:57:40:100:[step-66700/88800: 75.11%]--[loss-3.096298: wl-3.208467, gl-1.492065]--[lr-0.000017]--[ETA-6:27:24]
2023.01.22-06:59:25:200:[step-66800/88800: 75.23%]--[loss-2.969308: wl-3.071812, gl-1.433402]--[lr-0.000017]--[ETA-6:34:11]
2023.01.22-07:01:11:300:[step-66900/88800: 75.34%]--[loss-3.041052: wl-3.219140, gl-1.431482]--[lr-0.000017]--[ETA-6:29:30]
2023.01.22-07:02:56:400:[step-67000/88800: 75.45%]--[loss-2.906685: wl-3.038494, gl-1.387438]--[lr-0.000017]--[ETA-6:11:56]
End of epoch 151 / 200 	 Time Taken: 468 sec
2023.01.22-07:04:43:56:[step-67100/88800: 75.56%]--[loss-3.315931: wl-3.764758, gl-1.433552]--[lr-0.000016]--[ETA-6:28:25]
2023.01.22-07:06:28:156:[step-67200/88800: 75.68%]--[loss-3.298803: wl-3.672115, gl-1.462746]--[lr-0.000016]--[ETA-6:01:26]
2023.01.22-07:08:14:256:[step-67300/88800: 75.79%]--[loss-3.072427: wl-3.330969, gl-1.406943]--[lr-0.000016]--[ETA-6:22:15]
2023.01.22-07:09:59:356:[step-67400/88800: 75.90%]--[loss-3.293586: wl-3.645010, gl-1.471081]--[lr-0.000016]--[ETA-6:00:38]
End of epoch 152 / 200 	 Time Taken: 469 sec
2023.01.22-07:11:46:12:[step-67500/88800: 76.01%]--[loss-3.142131: wl-3.342753, gl-1.470754]--[lr-0.000016]--[ETA-6:34:59]
2023.01.22-07:13:31:112:[step-67600/88800: 76.13%]--[loss-3.050982: wl-3.198665, gl-1.451649]--[lr-0.000016]--[ETA-6:16:19]
2023.01.22-07:15:16:212:[step-67700/88800: 76.24%]--[loss-3.184880: wl-3.373725, gl-1.498018]--[lr-0.000016]--[ETA-5:59:48]
2023.01.22-07:17:02:312:[step-67800/88800: 76.35%]--[loss-3.028607: wl-3.102435, gl-1.477390]--[lr-0.000016]--[ETA-6:07:52]
2023.01.22-07:18:47:412:[step-67900/88800: 76.46%]--[loss-3.007846: wl-3.165946, gl-1.424874]--[lr-0.000016]--[ETA-5:48:10]
End of epoch 153 / 200 	 Time Taken: 469 sec
2023.01.22-07:20:34:68:[step-68000/88800: 76.58%]--[loss-3.079679: wl-3.222453, gl-1.468453]--[lr-0.000016]--[ETA-5:59:45]
2023.01.22-07:22:20:168:[step-68100/88800: 76.69%]--[loss-3.290182: wl-3.632580, gl-1.473892]--[lr-0.000016]--[ETA-6:02:40]
2023.01.22-07:24:05:268:[step-68200/88800: 76.80%]--[loss-3.132900: wl-3.378479, gl-1.443660]--[lr-0.000016]--[ETA-6:23:21]
2023.01.22-07:25:51:368:[step-68300/88800: 76.91%]--[loss-2.950201: wl-3.103540, gl-1.398431]--[lr-0.000016]--[ETA-6:14:39]
End of epoch 154 / 200 	 Time Taken: 470 sec
2023.01.22-07:27:38:24:[step-68400/88800: 77.03%]--[loss-3.108620: wl-3.281333, gl-1.467953]--[lr-0.000015]--[ETA-5:49:47]
2023.01.22-07:29:23:124:[step-68500/88800: 77.14%]--[loss-3.217663: wl-3.558799, gl-1.438264]--[lr-0.000015]--[ETA-6:09:02]
2023.01.22-07:31:09:224:[step-68600/88800: 77.25%]--[loss-2.936739: wl-3.015756, gl-1.428861]--[lr-0.000015]--[ETA-5:49:23]
2023.01.22-07:32:54:324:[step-68700/88800: 77.36%]--[loss-2.978474: wl-3.069127, gl-1.443910]--[lr-0.000015]--[ETA-5:59:42]
2023.01.22-07:34:40:424:[step-68800/88800: 77.48%]--[loss-3.188001: wl-3.608449, gl-1.383776]--[lr-0.000015]--[ETA-5:43:21]
End of epoch 155 / 200 	 Time Taken: 469 sec
saving the model at the end of epoch 155, iters 68820
2023.01.22-07:36:27:80:[step-68900/88800: 77.59%]--[loss-3.161963: wl-3.324085, gl-1.499921]--[lr-0.000015]--[ETA-5:58:16]
2023.01.22-07:38:12:180:[step-69000/88800: 77.70%]--[loss-3.219190: wl-3.521224, gl-1.458578]--[lr-0.000015]--[ETA-5:50:03]
2023.01.22-07:39:57:280:[step-69100/88800: 77.82%]--[loss-3.099364: wl-3.271459, gl-1.463634]--[lr-0.000015]--[ETA-5:37:48]
2023.01.22-07:41:43:380:[step-69200/88800: 77.93%]--[loss-3.033855: wl-3.131109, gl-1.468301]--[lr-0.000015]--[ETA-5:39:11]
End of epoch 156 / 200 	 Time Taken: 468 sec
2023.01.22-07:43:30:36:[step-69300/88800: 78.04%]--[loss-3.000774: wl-3.116761, gl-1.442394]--[lr-0.000015]--[ETA-6:05:44]
2023.01.22-07:45:15:136:[step-69400/88800: 78.15%]--[loss-3.057060: wl-3.243509, gl-1.435306]--[lr-0.000015]--[ETA-5:29:13]
2023.01.22-07:47:00:236:[step-69500/88800: 78.27%]--[loss-3.077189: wl-3.177063, gl-1.488658]--[lr-0.000015]--[ETA-5:43:18]
2023.01.22-07:48:46:336:[step-69600/88800: 78.38%]--[loss-3.068123: wl-3.294672, gl-1.420786]--[lr-0.000015]--[ETA-5:43:41]
2023.01.22-07:50:31:436:[step-69700/88800: 78.49%]--[loss-3.259738: wl-3.507528, gl-1.505974]--[lr-0.000015]--[ETA-5:38:54]
End of epoch 157 / 200 	 Time Taken: 468 sec
2023.01.22-07:52:18:92:[step-69800/88800: 78.60%]--[loss-3.143771: wl-3.504503, gl-1.391519]--[lr-0.000014]--[ETA-5:29:59]
2023.01.22-07:54:04:192:[step-69900/88800: 78.72%]--[loss-3.018760: wl-3.265726, gl-1.385896]--[lr-0.000014]--[ETA-5:20:42]
2023.01.22-07:55:51:292:[step-70000/88800: 78.83%]--[loss-3.021103: wl-3.088980, gl-1.476613]--[lr-0.000014]--[ETA-5:28:39]
2023.01.22-07:57:39:392:[step-70100/88800: 78.94%]--[loss-2.974956: wl-3.222054, gl-1.363928]--[lr-0.000014]--[ETA-5:22:53]
End of epoch 158 / 200 	 Time Taken: 475 sec
2023.01.22-07:59:28:48:[step-70200/88800: 79.05%]--[loss-3.032452: wl-3.191483, gl-1.436710]--[lr-0.000014]--[ETA-5:31:34]
2023.01.22-08:01:12:148:[step-70300/88800: 79.17%]--[loss-3.099806: wl-3.393322, gl-1.403145]--[lr-0.000014]--[ETA-5:17:13]
2023.01.22-08:02:56:248:[step-70400/88800: 79.28%]--[loss-3.313671: wl-3.746781, gl-1.440280]--[lr-0.000014]--[ETA-5:25:53]
2023.01.22-08:04:39:348:[step-70500/88800: 79.39%]--[loss-2.995939: wl-3.187162, gl-1.402358]--[lr-0.000014]--[ETA-5:12:45]
End of epoch 159 / 200 	 Time Taken: 465 sec
2023.01.22-08:06:25:4:[step-70600/88800: 79.50%]--[loss-3.086457: wl-3.334576, gl-1.419169]--[lr-0.000014]--[ETA-5:37:24]
2023.01.22-08:08:10:104:[step-70700/88800: 79.62%]--[loss-3.141665: wl-3.389095, gl-1.447117]--[lr-0.000014]--[ETA-5:03:44]
2023.01.22-08:09:54:204:[step-70800/88800: 79.73%]--[loss-3.254039: wl-3.680571, gl-1.413753]--[lr-0.000014]--[ETA-4:58:35]
2023.01.22-08:11:38:304:[step-70900/88800: 79.84%]--[loss-2.964200: wl-3.014249, gl-1.457075]--[lr-0.000014]--[ETA-5:12:26]
2023.01.22-08:13:22:404:[step-71000/88800: 79.95%]--[loss-3.085141: wl-3.357678, gl-1.406302]--[lr-0.000014]--[ETA-5:18:26]
End of epoch 160 / 200 	 Time Taken: 464 sec
saving the model at the end of epoch 160, iters 71040
2023.01.22-08:15:08:60:[step-71100/88800: 80.07%]--[loss-3.053954: wl-3.158566, gl-1.474671]--[lr-0.000013]--[ETA-5:08:33]
2023.01.22-08:16:51:160:[step-71200/88800: 80.18%]--[loss-2.954507: wl-3.117856, gl-1.395579]--[lr-0.000013]--[ETA-4:59:36]
2023.01.22-08:18:35:260:[step-71300/88800: 80.29%]--[loss-2.952824: wl-3.109535, gl-1.398056]--[lr-0.000013]--[ETA-5:05:44]
2023.01.22-08:20:19:360:[step-71400/88800: 80.41%]--[loss-2.869273: wl-2.923966, gl-1.407290]--[lr-0.000013]--[ETA-5:15:42]
End of epoch 161 / 200 	 Time Taken: 462 sec
2023.01.22-08:22:05:16:[step-71500/88800: 80.52%]--[loss-3.385404: wl-3.737596, gl-1.516606]--[lr-0.000013]--[ETA-4:59:48]
2023.01.22-08:23:49:116:[step-71600/88800: 80.63%]--[loss-3.081028: wl-3.270477, gl-1.445790]--[lr-0.000013]--[ETA-4:56:00]
2023.01.22-08:25:33:216:[step-71700/88800: 80.74%]--[loss-3.050424: wl-3.197813, gl-1.451517]--[lr-0.000013]--[ETA-5:08:16]
2023.01.22-08:27:17:316:[step-71800/88800: 80.86%]--[loss-3.041876: wl-3.252629, gl-1.415561]--[lr-0.000013]--[ETA-4:49:22]
2023.01.22-08:29:02:416:[step-71900/88800: 80.97%]--[loss-3.065769: wl-3.298592, gl-1.416473]--[lr-0.000013]--[ETA-4:52:53]
End of epoch 162 / 200 	 Time Taken: 464 sec
2023.01.22-08:30:48:72:[step-72000/88800: 81.08%]--[loss-3.142385: wl-3.441377, gl-1.421697]--[lr-0.000013]--[ETA-4:38:25]
2023.01.22-08:32:31:172:[step-72100/88800: 81.19%]--[loss-2.833387: wl-2.864810, gl-1.400982]--[lr-0.000013]--[ETA-5:14:38]
2023.01.22-08:34:15:272:[step-72200/88800: 81.31%]--[loss-3.128453: wl-3.376647, gl-1.440130]--[lr-0.000013]--[ETA-4:44:05]
2023.01.22-08:35:58:372:[step-72300/88800: 81.42%]--[loss-2.912014: wl-3.061130, gl-1.381449]--[lr-0.000013]--[ETA-4:52:17]
End of epoch 163 / 200 	 Time Taken: 462 sec
2023.01.22-08:37:44:28:[step-72400/88800: 81.53%]--[loss-3.280242: wl-3.626602, gl-1.466941]--[lr-0.000012]--[ETA-4:51:51]
2023.01.22-08:39:28:128:[step-72500/88800: 81.64%]--[loss-3.048112: wl-3.244517, gl-1.425853]--[lr-0.000012]--[ETA-4:38:26]
2023.01.22-08:41:12:228:[step-72600/88800: 81.76%]--[loss-3.142324: wl-3.491845, gl-1.396401]--[lr-0.000012]--[ETA-4:35:29]
2023.01.22-08:42:55:328:[step-72700/88800: 81.87%]--[loss-3.446623: wl-3.896350, gl-1.498448]--[lr-0.000012]--[ETA-4:44:37]
2023.01.22-08:44:39:428:[step-72800/88800: 81.98%]--[loss-3.000331: wl-3.095675, gl-1.452493]--[lr-0.000012]--[ETA-4:39:42]
End of epoch 164 / 200 	 Time Taken: 462 sec
2023.01.22-08:46:25:84:[step-72900/88800: 82.09%]--[loss-3.215961: wl-3.527694, gl-1.452114]--[lr-0.000012]--[ETA-4:48:45]
2023.01.22-08:48:09:184:[step-73000/88800: 82.21%]--[loss-3.004061: wl-3.242338, gl-1.382892]--[lr-0.000012]--[ETA-4:37:53]
2023.01.22-08:49:52:284:[step-73100/88800: 82.32%]--[loss-3.176916: wl-3.529653, gl-1.412090]--[lr-0.000012]--[ETA-4:28:07]
2023.01.22-08:51:36:384:[step-73200/88800: 82.43%]--[loss-3.011527: wl-3.188571, gl-1.417241]--[lr-0.000012]--[ETA-4:29:01]
End of epoch 165 / 200 	 Time Taken: 462 sec
saving the model at the end of epoch 165, iters 73260
2023.01.22-08:53:21:40:[step-73300/88800: 82.55%]--[loss-2.906385: wl-3.052034, gl-1.380369]--[lr-0.000012]--[ETA-4:24:33]
2023.01.22-08:55:04:140:[step-73400/88800: 82.66%]--[loss-3.216791: wl-3.513419, gl-1.460082]--[lr-0.000012]--[ETA-4:20:53]
2023.01.22-08:56:48:240:[step-73500/88800: 82.77%]--[loss-2.827600: wl-2.919796, gl-1.367702]--[lr-0.000012]--[ETA-4:35:49]
2023.01.22-08:58:32:340:[step-73600/88800: 82.88%]--[loss-2.868048: wl-2.943424, gl-1.396336]--[lr-0.000012]--[ETA-4:16:00]
2023.01.22-09:00:15:440:[step-73700/88800: 83.00%]--[loss-3.077412: wl-3.224584, gl-1.465120]--[lr-0.000012]--[ETA-4:12:35]
End of epoch 166 / 200 	 Time Taken: 461 sec
2023.01.22-09:02:01:96:[step-73800/88800: 83.11%]--[loss-2.972368: wl-3.129858, gl-1.407438]--[lr-0.000011]--[ETA-4:14:44]
2023.01.22-09:03:43:196:[step-73900/88800: 83.22%]--[loss-3.114176: wl-3.383501, gl-1.422426]--[lr-0.000011]--[ETA-4:14:49]
2023.01.22-09:05:26:296:[step-74000/88800: 83.33%]--[loss-3.060730: wl-3.262439, gl-1.429511]--[lr-0.000011]--[ETA-4:14:57]
2023.01.22-09:07:09:396:[step-74100/88800: 83.45%]--[loss-3.835563: wl-4.662369, gl-1.504379]--[lr-0.000011]--[ETA-4:10:45]
End of epoch 167 / 200 	 Time Taken: 458 sec
2023.01.22-09:08:54:52:[step-74200/88800: 83.56%]--[loss-2.834753: wl-2.933779, gl-1.367863]--[lr-0.000011]--[ETA-4:04:12]
2023.01.22-09:10:37:152:[step-74300/88800: 83.67%]--[loss-3.079726: wl-3.177044, gl-1.491204]--[lr-0.000011]--[ETA-3:58:49]
2023.01.22-09:12:21:252:[step-74400/88800: 83.78%]--[loss-2.966531: wl-3.111944, gl-1.410559]--[lr-0.000011]--[ETA-4:01:41]
2023.01.22-09:14:04:352:[step-74500/88800: 83.90%]--[loss-3.007874: wl-3.184507, gl-1.415620]--[lr-0.000011]--[ETA-3:56:09]
End of epoch 168 / 200 	 Time Taken: 460 sec
2023.01.22-09:15:49:8:[step-74600/88800: 84.01%]--[loss-3.050419: wl-3.232285, gl-1.434277]--[lr-0.000011]--[ETA-4:04:53]
2023.01.22-09:17:32:108:[step-74700/88800: 84.12%]--[loss-3.293415: wl-3.649780, gl-1.468525]--[lr-0.000011]--[ETA-4:01:48]
2023.01.22-09:19:15:208:[step-74800/88800: 84.23%]--[loss-3.103441: wl-3.368626, gl-1.419128]--[lr-0.000011]--[ETA-4:01:25]
2023.01.22-09:20:58:308:[step-74900/88800: 84.35%]--[loss-3.106964: wl-3.389559, gl-1.412185]--[lr-0.000011]--[ETA-3:49:59]
2023.01.22-09:22:41:408:[step-75000/88800: 84.46%]--[loss-2.937659: wl-3.019338, gl-1.427989]--[lr-0.000011]--[ETA-4:00:39]
End of epoch 169 / 200 	 Time Taken: 459 sec
2023.01.22-09:24:26:64:[step-75100/88800: 84.57%]--[loss-3.216017: wl-3.578578, gl-1.426728]--[lr-0.000010]--[ETA-3:52:10]
2023.01.22-09:26:08:164:[step-75200/88800: 84.68%]--[loss-3.360138: wl-3.914286, gl-1.402995]--[lr-0.000010]--[ETA-3:48:55]
2023.01.22-09:27:51:264:[step-75300/88800: 84.80%]--[loss-3.170917: wl-3.504337, gl-1.418748]--[lr-0.000010]--[ETA-3:53:46]
2023.01.22-09:29:34:364:[step-75400/88800: 84.91%]--[loss-3.067122: wl-3.287076, gl-1.423584]--[lr-0.000010]--[ETA-3:56:00]
End of epoch 170 / 200 	 Time Taken: 458 sec
saving the model at the end of epoch 170, iters 75480
2023.01.22-09:31:19:20:[step-75500/88800: 85.02%]--[loss-3.219059: wl-3.441169, gl-1.498474]--[lr-0.000010]--[ETA-3:42:48]
2023.01.22-09:33:02:120:[step-75600/88800: 85.14%]--[loss-3.025733: wl-3.129423, gl-1.461022]--[lr-0.000010]--[ETA-3:38:22]
2023.01.22-09:34:45:220:[step-75700/88800: 85.25%]--[loss-3.282418: wl-3.709761, gl-1.427538]--[lr-0.000010]--[ETA-3:49:46]
2023.01.22-09:36:28:320:[step-75800/88800: 85.36%]--[loss-3.177879: wl-3.541378, gl-1.407190]--[lr-0.000010]--[ETA-3:53:26]
2023.01.22-09:38:11:420:[step-75900/88800: 85.47%]--[loss-2.971246: wl-3.129621, gl-1.406436]--[lr-0.000010]--[ETA-3:33:46]
End of epoch 171 / 200 	 Time Taken: 459 sec
2023.01.22-09:39:56:76:[step-76000/88800: 85.59%]--[loss-2.939006: wl-3.092136, gl-1.392938]--[lr-0.000010]--[ETA-3:36:26]
2023.01.22-09:41:39:176:[step-76100/88800: 85.70%]--[loss-2.918507: wl-3.110343, gl-1.363335]--[lr-0.000010]--[ETA-3:38:26]
2023.01.22-09:43:23:276:[step-76200/88800: 85.81%]--[loss-3.024014: wl-3.284486, gl-1.381771]--[lr-0.000010]--[ETA-3:37:13]
2023.01.22-09:45:06:376:[step-76300/88800: 85.92%]--[loss-3.051973: wl-3.346316, gl-1.378815]--[lr-0.000010]--[ETA-3:39:50]
End of epoch 172 / 200 	 Time Taken: 460 sec
2023.01.22-09:46:51:32:[step-76400/88800: 86.04%]--[loss-3.174700: wl-3.435301, gl-1.457050]--[lr-0.000009]--[ETA-3:35:14]
2023.01.22-09:48:34:132:[step-76500/88800: 86.15%]--[loss-3.027471: wl-3.359622, gl-1.347660]--[lr-0.000009]--[ETA-3:32:22]
2023.01.22-09:50:17:232:[step-76600/88800: 86.26%]--[loss-3.008664: wl-3.187589, gl-1.414870]--[lr-0.000009]--[ETA-3:36:58]
2023.01.22-09:51:59:332:[step-76700/88800: 86.37%]--[loss-3.110074: wl-3.443431, gl-1.388358]--[lr-0.000009]--[ETA-3:37:20]
2023.01.22-09:53:42:432:[step-76800/88800: 86.49%]--[loss-3.159686: wl-3.522588, gl-1.398392]--[lr-0.000009]--[ETA-3:34:07]
End of epoch 173 / 200 	 Time Taken: 458 sec
2023.01.22-09:55:27:88:[step-76900/88800: 86.60%]--[loss-2.956184: wl-3.070195, gl-1.421086]--[lr-0.000009]--[ETA-3:18:14]
2023.01.22-09:57:10:188:[step-77000/88800: 86.71%]--[loss-2.984807: wl-3.142570, gl-1.413522]--[lr-0.000009]--[ETA-3:23:34]
2023.01.22-09:58:53:288:[step-77100/88800: 86.82%]--[loss-3.168252: wl-3.397734, gl-1.469385]--[lr-0.000009]--[ETA-3:14:51]
2023.01.22-10:00:36:388:[step-77200/88800: 86.94%]--[loss-3.187110: wl-3.611418, gl-1.381401]--[lr-0.000009]--[ETA-3:19:18]
End of epoch 174 / 200 	 Time Taken: 458 sec
2023.01.22-10:02:20:44:[step-77300/88800: 87.05%]--[loss-3.250236: wl-3.645260, gl-1.427606]--[lr-0.000009]--[ETA-3:20:57]
2023.01.22-10:04:04:144:[step-77400/88800: 87.16%]--[loss-3.071008: wl-3.285792, gl-1.428112]--[lr-0.000009]--[ETA-3:14:12]
2023.01.22-10:05:47:244:[step-77500/88800: 87.27%]--[loss-3.172132: wl-3.480183, gl-1.432040]--[lr-0.000009]--[ETA-3:17:47]
2023.01.22-10:07:30:344:[step-77600/88800: 87.39%]--[loss-2.910121: wl-3.009618, gl-1.405312]--[lr-0.000009]--[ETA-3:15:12]
2023.01.22-10:09:13:444:[step-77700/88800: 87.50%]--[loss-2.937375: wl-3.087763, gl-1.393494]--[lr-0.000009]--[ETA-3:14:22]
End of epoch 175 / 200 	 Time Taken: 460 sec
saving the model at the end of epoch 175, iters 77700
2023.01.22-10:10:59:100:[step-77800/88800: 87.61%]--[loss-3.015819: wl-3.210168, gl-1.410735]--[lr-0.000008]--[ETA-3:06:39]
2023.01.22-10:12:42:200:[step-77900/88800: 87.73%]--[loss-3.039146: wl-3.269357, gl-1.404467]--[lr-0.000008]--[ETA-3:05:52]
2023.01.22-10:14:25:300:[step-78000/88800: 87.84%]--[loss-3.001818: wl-3.141593, gl-1.431021]--[lr-0.000008]--[ETA-3:07:27]
2023.01.22-10:16:09:400:[step-78100/88800: 87.95%]--[loss-2.971402: wl-3.099304, gl-1.421750]--[lr-0.000008]--[ETA-3:02:24]
End of epoch 176 / 200 	 Time Taken: 460 sec
2023.01.22-10:17:54:56:[step-78200/88800: 88.06%]--[loss-3.441563: wl-3.880309, gl-1.501409]--[lr-0.000008]--[ETA-2:57:27]
2023.01.22-10:19:38:156:[step-78300/88800: 88.18%]--[loss-3.174173: wl-3.426887, gl-1.460729]--[lr-0.000008]--[ETA-3:01:42]
2023.01.22-10:21:20:256:[step-78400/88800: 88.29%]--[loss-3.115576: wl-3.363565, gl-1.433793]--[lr-0.000008]--[ETA-2:52:25]
2023.01.22-10:23:03:356:[step-78500/88800: 88.40%]--[loss-3.069981: wl-3.311352, gl-1.414305]--[lr-0.000008]--[ETA-3:06:12]
End of epoch 177 / 200 	 Time Taken: 459 sec
2023.01.22-10:24:48:12:[step-78600/88800: 88.51%]--[loss-2.848593: wl-2.927974, gl-1.384606]--[lr-0.000008]--[ETA-2:48:09]
2023.01.22-10:26:31:112:[step-78700/88800: 88.63%]--[loss-3.134736: wl-3.352955, gl-1.458258]--[lr-0.000008]--[ETA-2:55:00]
2023.01.22-10:28:14:212:[step-78800/88800: 88.74%]--[loss-3.067899: wl-3.337741, gl-1.399029]--[lr-0.000008]--[ETA-2:59:55]
2023.01.22-10:29:57:312:[step-78900/88800: 88.85%]--[loss-2.855018: wl-2.960811, gl-1.374613]--[lr-0.000008]--[ETA-2:48:58]
2023.01.22-10:31:40:412:[step-79000/88800: 88.96%]--[loss-3.298672: wl-3.740904, gl-1.428220]--[lr-0.000008]--[ETA-2:42:39]
End of epoch 178 / 200 	 Time Taken: 459 sec
2023.01.22-10:33:26:68:[step-79100/88800: 89.08%]--[loss-2.969220: wl-3.099633, gl-1.419403]--[lr-0.000007]--[ETA-2:51:08]
2023.01.22-10:35:10:168:[step-79200/88800: 89.19%]--[loss-2.969305: wl-3.205934, gl-1.366338]--[lr-0.000007]--[ETA-2:48:54]
2023.01.22-10:36:53:268:[step-79300/88800: 89.30%]--[loss-3.166034: wl-3.546202, gl-1.392932]--[lr-0.000007]--[ETA-2:46:02]
2023.01.22-10:38:37:368:[step-79400/88800: 89.41%]--[loss-3.477557: wl-3.809778, gl-1.572668]--[lr-0.000007]--[ETA-2:46:14]
End of epoch 179 / 200 	 Time Taken: 462 sec
2023.01.22-10:40:22:24:[step-79500/88800: 89.53%]--[loss-3.110066: wl-3.440179, gl-1.389977]--[lr-0.000007]--[ETA-2:38:31]
2023.01.22-10:42:06:124:[step-79600/88800: 89.64%]--[loss-2.868134: wl-3.016999, gl-1.359635]--[lr-0.000007]--[ETA-2:40:08]
2023.01.22-10:43:50:224:[step-79700/88800: 89.75%]--[loss-2.941897: wl-3.138138, gl-1.372828]--[lr-0.000007]--[ETA-2:39:12]
2023.01.22-10:45:33:324:[step-79800/88800: 89.86%]--[loss-3.279112: wl-3.707939, gl-1.425143]--[lr-0.000007]--[ETA-2:29:53]
2023.01.22-10:47:17:424:[step-79900/88800: 89.98%]--[loss-3.217449: wl-3.573771, gl-1.430563]--[lr-0.000007]--[ETA-2:29:30]
End of epoch 180 / 200 	 Time Taken: 461 sec
saving the model at the end of epoch 180, iters 79920
2023.01.22-10:49:03:80:[step-80000/88800: 90.09%]--[loss-3.111173: wl-3.409800, gl-1.406273]--[lr-0.000007]--[ETA-2:24:07]
2023.01.22-10:50:47:180:[step-80100/88800: 90.20%]--[loss-3.035364: wl-3.194304, gl-1.438213]--[lr-0.000007]--[ETA-2:33:28]
2023.01.22-10:52:31:280:[step-80200/88800: 90.32%]--[loss-3.136782: wl-3.411664, gl-1.430950]--[lr-0.000007]--[ETA-2:32:41]
2023.01.22-10:54:14:380:[step-80300/88800: 90.43%]--[loss-2.942199: wl-3.144608, gl-1.369895]--[lr-0.000007]--[ETA-2:23:07]
End of epoch 181 / 200 	 Time Taken: 461 sec
2023.01.22-10:55:59:36:[step-80400/88800: 90.54%]--[loss-3.230125: wl-3.542873, gl-1.458688]--[lr-0.000006]--[ETA-2:31:02]
2023.01.22-10:57:43:136:[step-80500/88800: 90.65%]--[loss-3.216826: wl-3.598516, gl-1.417568]--[lr-0.000006]--[ETA-2:19:06]
2023.01.22-10:59:26:236:[step-80600/88800: 90.77%]--[loss-2.967187: wl-3.086808, gl-1.423783]--[lr-0.000006]--[ETA-2:19:30]
2023.01.22-11:01:09:336:[step-80700/88800: 90.88%]--[loss-3.079695: wl-3.397991, gl-1.380699]--[lr-0.000006]--[ETA-2:18:03]
2023.01.22-11:02:51:436:[step-80800/88800: 90.99%]--[loss-3.031929: wl-3.238238, gl-1.412810]--[lr-0.000006]--[ETA-2:17:35]
End of epoch 182 / 200 	 Time Taken: 459 sec
2023.01.22-11:04:36:92:[step-80900/88800: 91.10%]--[loss-3.114094: wl-3.481878, gl-1.373155]--[lr-0.000006]--[ETA-2:13:00]
2023.01.22-11:06:19:192:[step-81000/88800: 91.22%]--[loss-3.112039: wl-3.325425, gl-1.449326]--[lr-0.000006]--[ETA-2:14:04]
2023.01.22-11:08:02:292:[step-81100/88800: 91.33%]--[loss-2.839686: wl-2.877729, gl-1.400822]--[lr-0.000006]--[ETA-2:10:06]
2023.01.22-11:09:44:392:[step-81200/88800: 91.44%]--[loss-3.079960: wl-3.260200, gl-1.449860]--[lr-0.000006]--[ETA-2:07:48]
End of epoch 183 / 200 	 Time Taken: 458 sec
2023.01.22-11:11:29:48:[step-81300/88800: 91.55%]--[loss-2.892982: wl-3.122768, gl-1.331598]--[lr-0.000006]--[ETA-2:07:53]
2023.01.22-11:13:12:148:[step-81400/88800: 91.67%]--[loss-2.936916: wl-3.183589, gl-1.345122]--[lr-0.000006]--[ETA-2:07:18]
2023.01.22-11:14:56:248:[step-81500/88800: 91.78%]--[loss-3.203523: wl-3.495410, gl-1.455818]--[lr-0.000006]--[ETA-2:03:04]
2023.01.22-11:16:39:348:[step-81600/88800: 91.89%]--[loss-2.917970: wl-3.062916, gl-1.386511]--[lr-0.000006]--[ETA-2:04:26]
End of epoch 184 / 200 	 Time Taken: 460 sec
2023.01.22-11:18:25:4:[step-81700/88800: 92.00%]--[loss-2.900990: wl-3.092177, gl-1.354902]--[lr-0.000005]--[ETA-2:03:10]
2023.01.22-11:20:08:104:[step-81800/88800: 92.12%]--[loss-2.981473: wl-3.065923, gl-1.448511]--[lr-0.000005]--[ETA-2:01:15]
2023.01.22-11:21:51:204:[step-81900/88800: 92.23%]--[loss-3.007731: wl-3.282213, gl-1.366625]--[lr-0.000005]--[ETA-2:01:53]
2023.01.22-11:23:35:304:[step-82000/88800: 92.34%]--[loss-3.154375: wl-3.477592, gl-1.415580]--[lr-0.000005]--[ETA-2:04:11]
2023.01.22-11:25:19:404:[step-82100/88800: 92.45%]--[loss-3.086843: wl-3.195204, gl-1.489240]--[lr-0.000005]--[ETA-1:57:38]
End of epoch 185 / 200 	 Time Taken: 461 sec
saving the model at the end of epoch 185, iters 82140
2023.01.22-11:27:05:60:[step-82200/88800: 92.57%]--[loss-2.952591: wl-3.166809, gl-1.369187]--[lr-0.000005]--[ETA-1:52:54]
2023.01.22-11:28:48:160:[step-82300/88800: 92.68%]--[loss-3.072451: wl-3.378451, gl-1.383226]--[lr-0.000005]--[ETA-1:53:41]
2023.01.22-11:30:31:260:[step-82400/88800: 92.79%]--[loss-2.808937: wl-2.927782, gl-1.345046]--[lr-0.000005]--[ETA-1:46:12]
2023.01.22-11:32:14:360:[step-82500/88800: 92.91%]--[loss-3.126710: wl-3.391114, gl-1.431153]--[lr-0.000005]--[ETA-1:54:44]
End of epoch 186 / 200 	 Time Taken: 460 sec
2023.01.22-11:33:59:16:[step-82600/88800: 93.02%]--[loss-2.879647: wl-3.031944, gl-1.363675]--[lr-0.000005]--[ETA-1:42:48]
2023.01.22-11:35:42:116:[step-82700/88800: 93.13%]--[loss-3.024469: wl-3.320690, gl-1.364124]--[lr-0.000005]--[ETA-1:46:07]
2023.01.22-11:37:26:216:[step-82800/88800: 93.24%]--[loss-2.953148: wl-3.141845, gl-1.382225]--[lr-0.000005]--[ETA-1:45:35]
2023.01.22-11:39:09:316:[step-82900/88800: 93.36%]--[loss-3.069972: wl-3.366882, gl-1.386531]--[lr-0.000005]--[ETA-1:38:01]
2023.01.22-11:40:52:416:[step-83000/88800: 93.47%]--[loss-2.962165: wl-3.131645, gl-1.396342]--[lr-0.000005]--[ETA-1:36:48]
End of epoch 187 / 200 	 Time Taken: 460 sec
2023.01.22-11:42:37:72:[step-83100/88800: 93.58%]--[loss-3.126495: wl-3.458233, gl-1.397379]--[lr-0.000004]--[ETA-1:37:51]
2023.01.22-11:44:20:172:[step-83200/88800: 93.69%]--[loss-3.168525: wl-3.380228, gl-1.478411]--[lr-0.000004]--[ETA-1:33:05]
2023.01.22-11:46:03:272:[step-83300/88800: 93.81%]--[loss-3.094749: wl-3.449588, gl-1.369955]--[lr-0.000004]--[ETA-1:31:51]
2023.01.22-11:47:45:372:[step-83400/88800: 93.92%]--[loss-3.175736: wl-3.413436, gl-1.469018]--[lr-0.000004]--[ETA-1:31:41]
End of epoch 188 / 200 	 Time Taken: 456 sec
2023.01.22-11:49:29:28:[step-83500/88800: 94.03%]--[loss-3.068827: wl-3.206348, gl-1.465652]--[lr-0.000004]--[ETA-1:31:43]
2023.01.22-11:51:12:128:[step-83600/88800: 94.14%]--[loss-2.872965: wl-3.030061, gl-1.357935]--[lr-0.000004]--[ETA-1:30:28]
2023.01.22-11:52:55:228:[step-83700/88800: 94.26%]--[loss-3.175983: wl-3.491630, gl-1.430169]--[lr-0.000004]--[ETA-1:27:33]
2023.01.22-11:54:38:328:[step-83800/88800: 94.37%]--[loss-3.256052: wl-3.658598, gl-1.426753]--[lr-0.000004]--[ETA-1:26:31]
2023.01.22-11:56:22:428:[step-83900/88800: 94.48%]--[loss-3.067237: wl-3.286603, gl-1.423936]--[lr-0.000004]--[ETA-1:20:41]
End of epoch 189 / 200 	 Time Taken: 459 sec
2023.01.22-11:58:07:84:[step-84000/88800: 94.59%]--[loss-2.993388: wl-3.237262, gl-1.374757]--[lr-0.000004]--[ETA-1:25:57]
2023.01.22-11:59:48:184:[step-84100/88800: 94.71%]--[loss-3.000841: wl-3.171283, gl-1.415200]--[lr-0.000004]--[ETA-1:22:21]
2023.01.22-12:01:32:284:[step-84200/88800: 94.82%]--[loss-3.149887: wl-3.395858, gl-1.451958]--[lr-0.000004]--[ETA-1:20:42]
2023.01.22-12:03:15:384:[step-84300/88800: 94.93%]--[loss-3.390256: wl-3.848772, gl-1.465870]--[lr-0.000004]--[ETA-1:18:42]
End of epoch 190 / 200 	 Time Taken: 458 sec
saving the model at the end of epoch 190, iters 84360
2023.01.22-12:05:00:40:[step-84400/88800: 95.05%]--[loss-3.262588: wl-3.599788, gl-1.462694]--[lr-0.000003]--[ETA-1:16:14]
2023.01.22-12:06:43:140:[step-84500/88800: 95.16%]--[loss-2.951313: wl-3.101024, gl-1.400802]--[lr-0.000003]--[ETA-1:14:01]
2023.01.22-12:08:27:240:[step-84600/88800: 95.27%]--[loss-3.287763: wl-3.705912, gl-1.434807]--[lr-0.000003]--[ETA-1:12:00]
2023.01.22-12:10:10:340:[step-84700/88800: 95.38%]--[loss-3.054664: wl-3.287477, gl-1.410925]--[lr-0.000003]--[ETA-1:10:20]
2023.01.22-12:11:53:440:[step-84800/88800: 95.50%]--[loss-3.148972: wl-3.504656, gl-1.396644]--[lr-0.000003]--[ETA-1:06:17]
End of epoch 191 / 200 	 Time Taken: 460 sec
2023.01.22-12:13:38:96:[step-84900/88800: 95.61%]--[loss-2.932086: wl-3.060760, gl-1.401706]--[lr-0.000003]--[ETA-1:08:35]
2023.01.22-12:15:22:196:[step-85000/88800: 95.72%]--[loss-3.058237: wl-3.323248, gl-1.396613]--[lr-0.000003]--[ETA-1:07:19]
2023.01.22-12:17:06:296:[step-85100/88800: 95.83%]--[loss-2.970945: wl-3.218625, gl-1.361632]--[lr-0.000003]--[ETA-1:03:36]
2023.01.22-12:18:49:396:[step-85200/88800: 95.95%]--[loss-3.009018: wl-3.224021, gl-1.397008]--[lr-0.000003]--[ETA-1:03:07]
End of epoch 192 / 200 	 Time Taken: 462 sec
2023.01.22-12:20:35:52:[step-85300/88800: 96.06%]--[loss-2.908131: wl-3.032734, gl-1.391764]--[lr-0.000003]--[ETA-1:01:08]
2023.01.22-12:22:19:152:[step-85400/88800: 96.17%]--[loss-2.874099: wl-3.017205, gl-1.365496]--[lr-0.000003]--[ETA-0:59:45]
2023.01.22-12:24:03:252:[step-85500/88800: 96.28%]--[loss-2.965503: wl-3.222372, gl-1.354316]--[lr-0.000003]--[ETA-0:55:03]
2023.01.22-12:25:47:352:[step-85600/88800: 96.40%]--[loss-2.914956: wl-3.137935, gl-1.345989]--[lr-0.000003]--[ETA-0:58:05]
End of epoch 193 / 200 	 Time Taken: 462 sec
2023.01.22-12:27:32:8:[step-85700/88800: 96.51%]--[loss-3.087835: wl-3.388026, gl-1.393822]--[lr-0.000002]--[ETA-0:51:38]
2023.01.22-12:29:17:108:[step-85800/88800: 96.62%]--[loss-3.021195: wl-3.182335, gl-1.430028]--[lr-0.000002]--[ETA-0:49:45]
2023.01.22-12:31:01:208:[step-85900/88800: 96.73%]--[loss-2.909084: wl-3.086954, gl-1.365607]--[lr-0.000002]--[ETA-0:49:45]
2023.01.22-12:32:44:308:[step-86000/88800: 96.85%]--[loss-3.016614: wl-3.240943, gl-1.396143]--[lr-0.000002]--[ETA-0:47:41]
2023.01.22-12:34:28:408:[step-86100/88800: 96.96%]--[loss-2.897151: wl-3.111241, gl-1.341530]--[lr-0.000002]--[ETA-0:48:16]
End of epoch 194 / 200 	 Time Taken: 463 sec
2023.01.22-12:36:14:64:[step-86200/88800: 97.07%]--[loss-2.949037: wl-3.147375, gl-1.375349]--[lr-0.000002]--[ETA-0:44:08]
2023.01.22-12:37:58:164:[step-86300/88800: 97.18%]--[loss-2.864995: wl-3.003775, gl-1.363107]--[lr-0.000002]--[ETA-0:43:01]
2023.01.22-12:39:42:264:[step-86400/88800: 97.30%]--[loss-2.979008: wl-3.154097, gl-1.401959]--[lr-0.000002]--[ETA-0:42:02]
2023.01.22-12:41:26:364:[step-86500/88800: 97.41%]--[loss-3.490035: wl-3.909456, gl-1.535307]--[lr-0.000002]--[ETA-0:39:24]
End of epoch 195 / 200 	 Time Taken: 462 sec
saving the model at the end of epoch 195, iters 86580
2023.01.22-12:43:11:20:[step-86600/88800: 97.52%]--[loss-2.850129: wl-2.970657, gl-1.364800]--[lr-0.000002]--[ETA-0:37:56]
2023.01.22-12:44:54:120:[step-86700/88800: 97.64%]--[loss-3.017525: wl-3.271121, gl-1.381965]--[lr-0.000002]--[ETA-0:36:45]
2023.01.22-12:46:38:220:[step-86800/88800: 97.75%]--[loss-3.226267: wl-3.412071, gl-1.520231]--[lr-0.000002]--[ETA-0:35:19]
2023.01.22-12:48:22:320:[step-86900/88800: 97.86%]--[loss-2.939517: wl-3.118844, gl-1.380095]--[lr-0.000002]--[ETA-0:34:05]
2023.01.22-12:50:05:420:[step-87000/88800: 97.97%]--[loss-3.324004: wl-3.880574, gl-1.383717]--[lr-0.000002]--[ETA-0:29:53]
End of epoch 196 / 200 	 Time Taken: 461 sec
2023.01.22-12:51:51:76:[step-87100/88800: 98.09%]--[loss-3.047605: wl-3.363317, gl-1.365946]--[lr-0.000001]--[ETA-0:28:02]
2023.01.22-12:53:34:176:[step-87200/88800: 98.20%]--[loss-2.971907: wl-3.181430, gl-1.381192]--[lr-0.000001]--[ETA-0:26:34]
2023.01.22-12:55:17:276:[step-87300/88800: 98.31%]--[loss-3.197900: wl-3.621708, gl-1.387046]--[lr-0.000001]--[ETA-0:25:02]
2023.01.22-12:57:01:376:[step-87400/88800: 98.42%]--[loss-2.946709: wl-3.137943, gl-1.377737]--[lr-0.000001]--[ETA-0:24:09]
End of epoch 197 / 200 	 Time Taken: 461 sec
2023.01.22-12:58:47:32:[step-87500/88800: 98.54%]--[loss-2.988833: wl-3.219180, gl-1.379243]--[lr-0.000001]--[ETA-0:21:50]
2023.01.22-13:00:31:132:[step-87600/88800: 98.65%]--[loss-3.173680: wl-3.483725, gl-1.431818]--[lr-0.000001]--[ETA-0:20:04]
2023.01.22-13:02:15:232:[step-87700/88800: 98.76%]--[loss-3.093442: wl-3.393249, gl-1.396817]--[lr-0.000001]--[ETA-0:19:16]
2023.01.22-13:03:59:332:[step-87800/88800: 98.87%]--[loss-3.070689: wl-3.325798, gl-1.407790]--[lr-0.000001]--[ETA-0:17:07]
2023.01.22-13:05:42:432:[step-87900/88800: 98.99%]--[loss-3.068038: wl-3.330292, gl-1.402892]--[lr-0.000001]--[ETA-0:14:55]
End of epoch 198 / 200 	 Time Taken: 462 sec
2023.01.22-13:07:27:88:[step-88000/88800: 99.10%]--[loss-2.994939: wl-3.186582, gl-1.401648]--[lr-0.000001]--[ETA-0:14:20]
2023.01.22-13:09:13:188:[step-88100/88800: 99.21%]--[loss-3.090714: wl-3.352484, gl-1.414473]--[lr-0.000001]--[ETA-0:11:55]
2023.01.22-13:10:58:288:[step-88200/88800: 99.32%]--[loss-3.016971: wl-3.173616, gl-1.430163]--[lr-0.000001]--[ETA-0:10:14]
2023.01.22-13:12:44:388:[step-88300/88800: 99.44%]--[loss-3.146320: wl-3.328324, gl-1.482159]--[lr-0.000001]--[ETA-0:08:46]
End of epoch 199 / 200 	 Time Taken: 468 sec
2023.01.22-13:14:31:44:[step-88400/88800: 99.55%]--[loss-3.164742: wl-3.519764, gl-1.404860]--[lr-0.000000]--[ETA-0:07:02]
2023.01.22-13:16:17:144:[step-88500/88800: 99.66%]--[loss-3.070307: wl-3.269131, gl-1.435742]--[lr-0.000000]--[ETA-0:05:02]
2023.01.22-13:18:03:244:[step-88600/88800: 99.77%]--[loss-2.906816: wl-3.050107, gl-1.381763]--[lr-0.000000]--[ETA-0:03:25]
2023.01.22-13:19:49:344:[step-88700/88800: 99.89%]--[loss-3.035752: wl-3.343196, gl-1.364154]--[lr-0.000000]--[ETA-0:01:46]
2023.01.22-13:21:34:444:[step-88800/88800: 100.00%]--[loss-3.137503: wl-3.478450, gl-1.398278]--[lr-0.000000]--[ETA-0:07:35]
End of epoch 200 / 200 	 Time Taken: 471 sec
saving the model at the end of epoch 200, iters 88800
/opt/conda/lib/python3.7/site-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
