/opt/conda/envs/srmgn/lib/python3.10/site-packages/torch/distributed/launch.py:180: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
------------ Options -------------
PBAFN_gen_checkpoint: checkpoints/SRMGN_PB_e2e_align_200/PBAFN_gen_epoch_201.pth
PBAFN_warp_checkpoint: checkpoints/SRMGN_PB_e2e_align_200/PBAFN_warp_epoch_201.pth
PFAFN_gen_checkpoint: None
PFAFN_warp_checkpoint: checkpoints/SRMGN_PF_stage1_align_100/PFAFN_warp_epoch_101.pth
align_corners: True
batchSize: 16
beta1: 0.5
checkpoints_dir: checkpoints
continue_train: False
data_type: 32
dataroot: ../dataset/Flow-Style-VTON/VITON_traindata
debug: False
display_freq: 100
display_winsize: 512
fineSize: 512
gpu_ids: [2]
input_nc: 3
isTrain: True
label_nc: 14
lambda_feat: 10.0
launcher: pytorch
loadSize: 512
load_pretrain: 
local_rank: 0
lr: 5e-05
max_dataset_size: inf
nThreads: 1
n_blocks_global: 4
n_blocks_local: 3
n_downsample_global: 4
n_layers_D: 3
n_local_enhancers: 1
name: FS_PF_e2e_align_100
ndf: 64
netG: global
ngf: 64
niter: 50
niter_decay: 50
niter_fix_global: 0
no_flip: False
no_ganFeat_loss: False
no_html: False
no_lsgan: False
no_vgg_loss: False
norm: instance
num_D: 2
num_gpus: 1
output_nc: 3
phase: train
pool_size: 0
print_freq: 100
resize_or_crop: None
save_epoch_freq: 1
save_latest_freq: 1000
serial_batches: False
tf_log: False
tv_weight: 0.1
use_dropout: False
valroot: dataset/VITON_valdata/
verbose: False
which_epoch: latest
-------------- End ----------------
------------ Options -------------
PBAFN_gen_checkpoint: checkpoints/SRMGN_PB_e2e_align_200/PBAFN_gen_epoch_201.pth
PBAFN_warp_checkpoint: checkpoints/SRMGN_PB_e2e_align_200/PBAFN_warp_epoch_201.pth
PFAFN_gen_checkpoint: None
PFAFN_warp_checkpoint: checkpoints/SRMGN_PF_stage1_align_100/PFAFN_warp_epoch_101.pth
align_corners: True
batchSize: 16
beta1: 0.5
checkpoints_dir: checkpoints
continue_train: False
data_type: 32
dataroot: ../dataset/Flow-Style-VTON/VITON_traindata
debug: False
display_freq: 100
display_winsize: 512
fineSize: 512
gpu_ids: [2]
input_nc: 3
isTrain: True
label_nc: 14
lambda_feat: 10.0
launcher: pytorch
loadSize: 512
load_pretrain: 
local_rank: 0
lr: 5e-05
max_dataset_size: inf
nThreads: 1
n_blocks_global: 4
n_blocks_local: 3
n_downsample_global: 4
n_layers_D: 3
n_local_enhancers: 1
name: FS_PF_e2e_align_100
ndf: 64
netG: global
ngf: 64
niter: 50
niter_decay: 50
niter_fix_global: 0
no_flip: False
no_ganFeat_loss: False
no_html: False
no_lsgan: False
no_vgg_loss: False
norm: instance
num_D: 2
num_gpus: 1
output_nc: 3
phase: train
pool_size: 0
print_freq: 100
resize_or_crop: None
save_epoch_freq: 1
save_latest_freq: 1000
serial_batches: False
tf_log: False
tv_weight: 0.1
use_dropout: False
valroot: dataset/VITON_valdata/
verbose: False
which_epoch: latest
-------------- End ----------------
------------ Options -------------
PBAFN_gen_checkpoint: checkpoints/SRMGN_PB_e2e_align_200/PBAFN_gen_epoch_201.pth
PBAFN_warp_checkpoint: checkpoints/SRMGN_PB_e2e_align_200/PBAFN_warp_epoch_201.pth
PFAFN_gen_checkpoint: None
PFAFN_warp_checkpoint: checkpoints/SRMGN_PF_stage1_align_100/PFAFN_warp_epoch_101.pth
align_corners: True
batchSize: 16
beta1: 0.5
checkpoints_dir: checkpoints
continue_train: False
data_type: 32
dataroot: ../dataset/Flow-Style-VTON/VITON_traindata
debug: False
display_freq: 100
display_winsize: 512
fineSize: 512
gpu_ids: [2]
input_nc: 3
isTrain: True
label_nc: 14
lambda_feat: 10.0
launcher: pytorch
loadSize: 512
load_pretrain: 
local_rank: 0
lr: 5e-05
max_dataset_size: inf
nThreads: 1
n_blocks_global: 4
n_blocks_local: 3
n_downsample_global: 4
n_layers_D: 3
n_local_enhancers: 1
name: FS_PF_e2e_align_100
ndf: 64
netG: global
ngf: 64
niter: 50
niter_decay: 50
niter_fix_global: 0
no_flip: False
no_ganFeat_loss: False
no_html: False
no_lsgan: False
no_vgg_loss: False
norm: instance
num_D: 2
num_gpus: 1
output_nc: 3
phase: train
pool_size: 0
print_freq: 100
resize_or_crop: None
save_epoch_freq: 1
save_latest_freq: 1000
serial_batches: False
tf_log: False
tv_weight: 0.1
use_dropout: False
valroot: dataset/VITON_valdata/
verbose: False
which_epoch: latest
-------------- End ----------------
dataset [AlignedDataset] was created
../dataset/Flow-Style-VTON/VITON_traindata/train_label label
../dataset/Flow-Style-VTON/VITON_traindata/train_img img
../dataset/Flow-Style-VTON/VITON_traindata/train_edge edge
../dataset/Flow-Style-VTON/VITON_traindata/train_color color
/opt/conda/envs/srmgn/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.
  warnings.warn(warning.format(ret))
/opt/conda/envs/srmgn/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
2023.03.02-01:37:54:100:[step-100/88900: 0.11%]--[loss-4.935372: wl-4.169621, gl-3.892967]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:29:14]
2023.03.02-01:39:14:200:[step-200/88900: 0.22%]--[loss-4.019953: wl-3.760969, gl-3.079710]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:14:26]
2023.03.02-01:40:34:300:[step-300/88900: 0.34%]--[loss-4.375240: wl-4.572837, gl-3.232031]--[lr: pb-0.000050, pf-0.000050]--[ETA-20:46:40]
2023.03.02-01:41:54:400:[step-400/88900: 0.45%]--[loss-4.497082: wl-4.218449, gl-3.442469]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:32:42]
2023.03.02-01:43:15:500:[step-500/88900: 0.56%]--[loss-4.250845: wl-3.961777, gl-3.260401]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:00:13]
2023.03.02-01:44:33:600:[step-600/88900: 0.67%]--[loss-4.553857: wl-5.023582, gl-3.297962]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 2:19:12]
2023.03.02-01:45:52:700:[step-700/88900: 0.79%]--[loss-3.750593: wl-4.144843, gl-2.714382]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:30:31]
2023.03.02-01:47:13:800:[step-800/88900: 0.90%]--[loss-4.209229: wl-4.364711, gl-3.118052]--[lr: pb-0.000050, pf-0.000050]--[ETA-20:26:34]
End of epoch 1 / 100: train_loss: 4.285 	 time: 715 sec
Saving the model at the end of epoch 1, iters 889
2023.03.02-01:48:42:11:[step-900/88900: 1.01%]--[loss-3.818530: wl-3.970798, gl-2.825830]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:03:09]
2023.03.02-01:50:03:111:[step-1000/88900: 1.12%]--[loss-3.940212: wl-4.216465, gl-2.886096]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:22:21]
2023.03.02-01:51:25:211:[step-1100/88900: 1.24%]--[loss-3.730433: wl-3.979180, gl-2.735638]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:26:04]
2023.03.02-01:52:46:311:[step-1200/88900: 1.35%]--[loss-3.962751: wl-4.211073, gl-2.909983]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:35:11]
2023.03.02-01:54:07:411:[step-1300/88900: 1.46%]--[loss-3.775797: wl-4.185553, gl-2.729409]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:54:12]
2023.03.02-01:55:27:511:[step-1400/88900: 1.57%]--[loss-4.180665: wl-4.448073, gl-3.068647]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:09:27]
2023.03.02-01:56:49:611:[step-1500/88900: 1.69%]--[loss-3.769051: wl-3.897746, gl-2.794614]--[lr: pb-0.000050, pf-0.000050]--[ETA-22:24:19]
2023.03.02-01:58:10:711:[step-1600/88900: 1.80%]--[loss-3.937472: wl-4.218565, gl-2.882831]--[lr: pb-0.000050, pf-0.000050]--[ETA-21:18:52]
2023.03.02-01:59:28:811:[step-1700/88900: 1.91%]--[loss-3.623616: wl-4.476326, gl-2.504534]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:03:12]
End of epoch 2 / 100: train_loss: 3.837 	 time: 727 sec
Saving the model at the end of epoch 2, iters 1778
2023.03.02-02:01:00:22:[step-1800/88900: 2.02%]--[loss-3.804290: wl-4.047523, gl-2.792409]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:24:25]
2023.03.02-02:02:21:122:[step-1900/88900: 2.14%]--[loss-3.881648: wl-4.458502, gl-2.767022]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:00:49]
2023.03.02-02:03:42:222:[step-2000/88900: 2.25%]--[loss-4.191425: wl-4.484308, gl-3.070348]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:15:49]
2023.03.02-02:05:03:322:[step-2100/88900: 2.36%]--[loss-4.231071: wl-5.142216, gl-2.945517]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:26:28]
2023.03.02-02:06:25:422:[step-2200/88900: 2.47%]--[loss-3.652487: wl-4.347860, gl-2.565522]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:04:23]
2023.03.02-02:07:46:522:[step-2300/88900: 2.59%]--[loss-3.622297: wl-3.996140, gl-2.623262]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:23:41]
2023.03.02-02:09:06:622:[step-2400/88900: 2.70%]--[loss-3.356869: wl-3.848641, gl-2.394708]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:10:00]
2023.03.02-02:10:27:722:[step-2500/88900: 2.81%]--[loss-3.987170: wl-5.660597, gl-2.572021]--[lr: pb-0.000050, pf-0.000050]--[ETA-20:25:00]
2023.03.02-02:11:48:822:[step-2600/88900: 2.92%]--[loss-3.947563: wl-4.027454, gl-2.940700]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:53:40]
End of epoch 3 / 100: train_loss: 3.730 	 time: 728 sec
Saving the model at the end of epoch 3, iters 2667
2023.03.02-02:13:19:33:[step-2700/88900: 3.04%]--[loss-3.991683: wl-4.485940, gl-2.870198]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 1:43:14]
2023.03.02-02:14:37:133:[step-2800/88900: 3.15%]--[loss-3.496237: wl-3.810841, gl-2.543527]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:17:09]
2023.03.02-02:15:58:233:[step-2900/88900: 3.26%]--[loss-3.832169: wl-4.076962, gl-2.812928]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:16:39]
2023.03.02-02:17:19:333:[step-3000/88900: 3.37%]--[loss-3.611080: wl-3.947294, gl-2.624257]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 0:39:07]
2023.03.02-02:18:39:433:[step-3100/88900: 3.49%]--[loss-4.020306: wl-4.274014, gl-2.951802]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:16:35]
2023.03.02-02:20:00:533:[step-3200/88900: 3.60%]--[loss-3.309101: wl-3.712873, gl-2.380883]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:50:22]
2023.03.02-02:21:20:633:[step-3300/88900: 3.71%]--[loss-3.504605: wl-4.007596, gl-2.502706]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:47:43]
2023.03.02-02:22:40:733:[step-3400/88900: 3.82%]--[loss-3.655458: wl-4.210357, gl-2.602869]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:15:12]
2023.03.02-02:24:00:833:[step-3500/88900: 3.94%]--[loss-3.663109: wl-4.188076, gl-2.616090]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:56:15]
End of epoch 4 / 100: train_loss: 3.681 	 time: 723 sec
Saving the model at the end of epoch 4, iters 3556
2023.03.02-02:25:32:44:[step-3600/88900: 4.05%]--[loss-3.393495: wl-3.806626, gl-2.441839]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:29:00]
2023.03.02-02:26:51:144:[step-3700/88900: 4.16%]--[loss-3.754020: wl-4.491623, gl-2.631114]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 1:42:18]
2023.03.02-02:28:12:244:[step-3800/88900: 4.27%]--[loss-3.540489: wl-3.976838, gl-2.546279]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:47:06]
2023.03.02-02:29:31:344:[step-3900/88900: 4.39%]--[loss-3.567784: wl-4.011890, gl-2.564812]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:53:53]
2023.03.02-02:30:51:444:[step-4000/88900: 4.50%]--[loss-3.577115: wl-4.395351, gl-2.478277]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:17:08]
2023.03.02-02:32:11:544:[step-4100/88900: 4.61%]--[loss-3.798544: wl-4.434918, gl-2.689815]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:09:41]
2023.03.02-02:33:32:644:[step-4200/88900: 4.72%]--[loss-3.606942: wl-4.540742, gl-2.471756]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:34:33]
2023.03.02-02:34:53:744:[step-4300/88900: 4.84%]--[loss-3.959300: wl-4.913645, gl-2.730888]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:16:23]
2023.03.02-02:36:13:844:[step-4400/88900: 4.95%]--[loss-3.454763: wl-3.787916, gl-2.507784]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:58:56]
End of epoch 5 / 100: train_loss: 3.640 	 time: 724 sec
Saving the model at the end of epoch 5, iters 4445
2023.03.02-02:37:47:55:[step-4500/88900: 5.06%]--[loss-3.602870: wl-4.102727, gl-2.577188]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:36:21]
2023.03.02-02:39:07:155:[step-4600/88900: 5.17%]--[loss-3.515222: wl-4.120555, gl-2.485083]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 7:13:04]
2023.03.02-02:40:28:255:[step-4700/88900: 5.29%]--[loss-3.659706: wl-4.129067, gl-2.627439]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:37:26]
2023.03.02-02:41:48:355:[step-4800/88900: 5.40%]--[loss-3.361076: wl-3.654642, gl-2.447416]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:07:08]
2023.03.02-02:43:08:455:[step-4900/88900: 5.51%]--[loss-3.786158: wl-4.828907, gl-2.578931]--[lr: pb-0.000050, pf-0.000050]--[ETA-20:24:54]
2023.03.02-02:44:28:555:[step-5000/88900: 5.62%]--[loss-3.431144: wl-4.260988, gl-2.365896]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:20:51]
2023.03.02-02:45:49:655:[step-5100/88900: 5.74%]--[loss-3.506244: wl-4.158320, gl-2.466664]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:45:03]
2023.03.02-02:47:09:755:[step-5200/88900: 5.85%]--[loss-3.610524: wl-3.821729, gl-2.655092]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:33:31]
2023.03.02-02:48:29:855:[step-5300/88900: 5.96%]--[loss-3.571835: wl-4.224777, gl-2.515640]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:39:53]
End of epoch 6 / 100: train_loss: 3.612 	 time: 727 sec
Saving the model at the end of epoch 6, iters 5334
2023.03.02-02:50:02:66:[step-5400/88900: 6.07%]--[loss-3.431892: wl-4.142045, gl-2.396380]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:49:54]
2023.03.02-02:51:23:166:[step-5500/88900: 6.19%]--[loss-4.021348: wl-5.144074, gl-2.735330]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:31:47]
2023.03.02-02:52:42:266:[step-5600/88900: 6.30%]--[loss-3.604990: wl-4.358296, gl-2.515416]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:07:07]
2023.03.02-02:54:02:366:[step-5700/88900: 6.41%]--[loss-3.726056: wl-4.734224, gl-2.542500]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:19:02]
2023.03.02-02:55:23:466:[step-5800/88900: 6.52%]--[loss-3.136511: wl-3.784301, gl-2.190436]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:35:07]
2023.03.02-02:56:43:566:[step-5900/88900: 6.64%]--[loss-3.249074: wl-3.843485, gl-2.288203]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:31:34]
2023.03.02-02:58:02:666:[step-6000/88900: 6.75%]--[loss-3.380038: wl-3.993233, gl-2.381730]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:23:57]
2023.03.02-02:59:23:766:[step-6100/88900: 6.86%]--[loss-3.454858: wl-4.218002, gl-2.400357]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:59:14]
2023.03.02-03:00:43:866:[step-6200/88900: 6.97%]--[loss-3.601497: wl-4.173120, gl-2.558218]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:08:31]
End of epoch 7 / 100: train_loss: 3.582 	 time: 725 sec
Saving the model at the end of epoch 7, iters 6223
2023.03.02-03:02:18:77:[step-6300/88900: 7.09%]--[loss-3.604684: wl-4.131980, gl-2.571689]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:27:27]
2023.03.02-03:03:39:177:[step-6400/88900: 7.20%]--[loss-3.527029: wl-4.160382, gl-2.486933]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:30:40]
2023.03.02-03:04:59:277:[step-6500/88900: 7.31%]--[loss-3.447657: wl-4.101388, gl-2.422310]--[lr: pb-0.000050, pf-0.000050]--[ETA-20:49:17]
2023.03.02-03:06:19:377:[step-6600/88900: 7.42%]--[loss-3.913360: wl-5.301170, gl-2.588067]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:27:04]
2023.03.02-03:07:39:477:[step-6700/88900: 7.54%]--[loss-3.710475: wl-4.585365, gl-2.564133]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:18:48]
2023.03.02-03:09:00:577:[step-6800/88900: 7.65%]--[loss-3.739405: wl-4.261236, gl-2.674096]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:04:50]
2023.03.02-03:10:21:677:[step-6900/88900: 7.76%]--[loss-3.511220: wl-4.516975, gl-2.381976]--[lr: pb-0.000050, pf-0.000050]--[ETA-20:42:05]
2023.03.02-03:11:41:777:[step-7000/88900: 7.87%]--[loss-3.717905: wl-4.263709, gl-2.651978]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:56:59]
2023.03.02-03:13:01:877:[step-7100/88900: 7.99%]--[loss-3.690733: wl-4.565151, gl-2.549446]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:13:26]
End of epoch 8 / 100: train_loss: 3.555 	 time: 729 sec
Saving the model at the end of epoch 8, iters 7112
2023.03.02-03:14:35:88:[step-7200/88900: 8.10%]--[loss-3.596482: wl-5.093832, gl-2.323024]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:11:39]
2023.03.02-03:15:55:188:[step-7300/88900: 8.21%]--[loss-3.291329: wl-3.810727, gl-2.338648]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:04:01]
2023.03.02-03:17:17:288:[step-7400/88900: 8.32%]--[loss-3.677577: wl-4.420042, gl-2.572567]--[lr: pb-0.000050, pf-0.000050]--[ETA-21:29:41]
2023.03.02-03:18:36:388:[step-7500/88900: 8.44%]--[loss-3.613468: wl-4.438473, gl-2.503850]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:02:59]
2023.03.02-03:19:56:488:[step-7600/88900: 8.55%]--[loss-3.624275: wl-4.293942, gl-2.550789]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:08:24]
2023.03.02-03:21:16:588:[step-7700/88900: 8.66%]--[loss-3.507716: wl-4.339162, gl-2.422925]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:22:16]
2023.03.02-03:22:36:688:[step-7800/88900: 8.77%]--[loss-3.681130: wl-4.899793, gl-2.456182]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:33:44]
2023.03.02-03:23:56:788:[step-7900/88900: 8.89%]--[loss-3.973084: wl-4.722098, gl-2.792560]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:35:10]
2023.03.02-03:25:16:888:[step-8000/88900: 9.00%]--[loss-3.600302: wl-4.369843, gl-2.507841]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:20:36]
End of epoch 9 / 100: train_loss: 3.530 	 time: 725 sec
Saving the model at the end of epoch 9, iters 8001
2023.03.02-03:26:52:99:[step-8100/88900: 9.11%]--[loss-3.713899: wl-4.387593, gl-2.617001]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:08:27]
2023.03.02-03:28:13:199:[step-8200/88900: 9.22%]--[loss-3.521228: wl-4.356622, gl-2.432072]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:58:08]
2023.03.02-03:29:33:299:[step-8300/88900: 9.34%]--[loss-3.557086: wl-4.339232, gl-2.472278]--[lr: pb-0.000050, pf-0.000050]--[ETA-21:21:42]
2023.03.02-03:30:54:399:[step-8400/88900: 9.45%]--[loss-3.966770: wl-5.104637, gl-2.690611]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:53:12]
2023.03.02-03:32:15:499:[step-8500/88900: 9.56%]--[loss-3.721487: wl-4.980136, gl-2.476453]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:04:09]
2023.03.02-03:33:36:599:[step-8600/88900: 9.67%]--[loss-3.851634: wl-4.954403, gl-2.613033]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:07:21]
2023.03.02-03:34:57:699:[step-8700/88900: 9.79%]--[loss-3.460513: wl-4.250007, gl-2.398011]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:37:18]
2023.03.02-03:36:18:799:[step-8800/88900: 9.90%]--[loss-3.621034: wl-4.518560, gl-2.491394]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:27:34]
End of epoch 10 / 100: train_loss: 3.514 	 time: 733 sec
Saving the model at the end of epoch 10, iters 8890
2023.03.02-03:37:53:10:[step-8900/88900: 10.01%]--[loss-3.742006: wl-4.614472, gl-2.588388]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:30:04]
2023.03.02-03:39:14:110:[step-9000/88900: 10.12%]--[loss-3.605715: wl-4.460193, gl-2.490666]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:43:45]
2023.03.02-03:40:36:210:[step-9100/88900: 10.24%]--[loss-3.382692: wl-3.897772, gl-2.408249]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:45:46]
2023.03.02-03:41:57:310:[step-9200/88900: 10.35%]--[loss-3.465880: wl-4.306752, gl-2.389193]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:21:11]
2023.03.02-03:43:18:410:[step-9300/88900: 10.46%]--[loss-3.716351: wl-4.801854, gl-2.515887]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:03:09]
2023.03.02-03:44:37:510:[step-9400/88900: 10.57%]--[loss-3.777046: wl-4.854615, gl-2.563392]--[lr: pb-0.000050, pf-0.000050]--[ETA-20:28:01]
2023.03.02-03:45:57:610:[step-9500/88900: 10.69%]--[loss-3.687318: wl-4.788358, gl-2.490229]--[lr: pb-0.000050, pf-0.000050]--[ETA-23:50:39]
2023.03.02-03:47:18:710:[step-9600/88900: 10.80%]--[loss-3.415133: wl-4.163608, gl-2.374231]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:59:12]
2023.03.02-03:48:38:810:[step-9700/88900: 10.91%]--[loss-3.713748: wl-4.751423, gl-2.525892]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:47:12]
End of epoch 11 / 100: train_loss: 3.500 	 time: 731 sec
Saving the model at the end of epoch 11, iters 9779
2023.03.02-03:50:14:21:[step-9800/88900: 11.02%]--[loss-3.634037: wl-4.627474, gl-2.477168]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:12:59]
2023.03.02-03:51:33:121:[step-9900/88900: 11.14%]--[loss-3.785708: wl-4.899307, gl-2.560881]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:34:23]
2023.03.02-03:52:54:221:[step-10000/88900: 11.25%]--[loss-3.361535: wl-4.012690, gl-2.358363]--[lr: pb-0.000050, pf-0.000050]--[ETA-21:31:35]
2023.03.02-03:54:16:321:[step-10100/88900: 11.36%]--[loss-3.391128: wl-4.360493, gl-2.301004]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:51:13]
2023.03.02-03:55:37:421:[step-10200/88900: 11.47%]--[loss-3.440797: wl-4.370627, gl-2.348141]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:11:06]
2023.03.02-03:56:59:521:[step-10300/88900: 11.59%]--[loss-3.537757: wl-4.037282, gl-2.528436]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:36:32]
2023.03.02-03:58:19:621:[step-10400/88900: 11.70%]--[loss-3.556236: wl-3.976816, gl-2.562032]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 2:32:20]
2023.03.02-03:59:39:721:[step-10500/88900: 11.81%]--[loss-3.855010: wl-4.933157, gl-2.621721]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:07:06]
2023.03.02-04:00:59:821:[step-10600/88900: 11.92%]--[loss-3.682286: wl-4.651609, gl-2.519384]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:31:02]
End of epoch 12 / 100: train_loss: 3.481 	 time: 731 sec
Saving the model at the end of epoch 12, iters 10668
2023.03.02-04:02:34:32:[step-10700/88900: 12.04%]--[loss-3.368512: wl-3.977745, gl-2.374076]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:11:34]
2023.03.02-04:03:56:132:[step-10800/88900: 12.15%]--[loss-3.504789: wl-4.167751, gl-2.462851]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:58:09]
2023.03.02-04:05:17:232:[step-10900/88900: 12.26%]--[loss-3.465737: wl-4.360826, gl-2.375530]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:45:43]
2023.03.02-04:06:37:332:[step-11000/88900: 12.37%]--[loss-3.706664: wl-4.397636, gl-2.607255]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:26:13]
2023.03.02-04:07:58:432:[step-11100/88900: 12.49%]--[loss-3.508126: wl-4.285630, gl-2.436719]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:57:18]
2023.03.02-04:09:19:532:[step-11200/88900: 12.60%]--[loss-3.181428: wl-4.033643, gl-2.173018]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:55:52]
2023.03.02-04:10:39:632:[step-11300/88900: 12.71%]--[loss-3.319928: wl-3.830213, gl-2.362375]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:16:51]
2023.03.02-04:11:59:732:[step-11400/88900: 12.82%]--[loss-3.326456: wl-3.968719, gl-2.334276]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:56:08]
2023.03.02-04:13:18:832:[step-11500/88900: 12.94%]--[loss-3.426283: wl-4.061376, gl-2.410939]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:50:10]
End of epoch 13 / 100: train_loss: 3.466 	 time: 728 sec
Saving the model at the end of epoch 13, iters 11557
2023.03.02-04:14:53:43:[step-11600/88900: 13.05%]--[loss-3.440306: wl-4.518193, gl-2.310758]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:58:02]
2023.03.02-04:16:14:143:[step-11700/88900: 13.16%]--[loss-3.360445: wl-4.295861, gl-2.286480]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:21:00]
2023.03.02-04:17:35:243:[step-11800/88900: 13.27%]--[loss-3.693252: wl-4.774425, gl-2.499646]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:51:08]
2023.03.02-04:18:56:343:[step-11900/88900: 13.39%]--[loss-3.118356: wl-3.636474, gl-2.209238]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:36:19]
2023.03.02-04:20:16:443:[step-12000/88900: 13.50%]--[loss-3.371625: wl-4.216618, gl-2.317470]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:52:26]
2023.03.02-04:21:36:543:[step-12100/88900: 13.61%]--[loss-3.346397: wl-4.169595, gl-2.303998]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:47:51]
2023.03.02-04:22:58:643:[step-12200/88900: 13.72%]--[loss-3.244003: wl-3.933537, gl-2.260619]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:02:21]
2023.03.02-04:24:19:743:[step-12300/88900: 13.84%]--[loss-3.398766: wl-4.193045, gl-2.350505]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:29:41]
2023.03.02-04:25:39:843:[step-12400/88900: 13.95%]--[loss-3.906389: wl-5.325789, gl-2.574942]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:53:30]
End of epoch 14 / 100: train_loss: 3.457 	 time: 731 sec
Saving the model at the end of epoch 14, iters 12446
2023.03.02-04:27:14:54:[step-12500/88900: 14.06%]--[loss-3.159986: wl-3.781564, gl-2.214595]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:59:53]
2023.03.02-04:28:32:154:[step-12600/88900: 14.17%]--[loss-3.308810: wl-4.022209, gl-2.303258]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:43:08]
2023.03.02-04:29:53:254:[step-12700/88900: 14.29%]--[loss-3.429307: wl-4.290732, gl-2.356624]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:32:34]
2023.03.02-04:31:14:354:[step-12800/88900: 14.40%]--[loss-3.414106: wl-3.829924, gl-2.456625]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:17:15]
2023.03.02-04:32:35:454:[step-12900/88900: 14.51%]--[loss-3.530926: wl-4.434167, gl-2.422384]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:57:42]
2023.03.02-04:33:56:554:[step-13000/88900: 14.62%]--[loss-3.614520: wl-4.719055, gl-2.434756]--[lr: pb-0.000050, pf-0.000050]--[ETA-20:17:23]
2023.03.02-04:35:16:654:[step-13100/88900: 14.74%]--[loss-3.847116: wl-4.730178, gl-2.664571]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:13:24]
2023.03.02-04:36:38:754:[step-13200/88900: 14.85%]--[loss-3.520084: wl-4.525449, gl-2.388721]--[lr: pb-0.000050, pf-0.000050]--[ETA-21:01:36]
2023.03.02-04:37:59:854:[step-13300/88900: 14.96%]--[loss-3.316235: wl-4.114800, gl-2.287535]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:24:10]
End of epoch 15 / 100: train_loss: 3.444 	 time: 730 sec
Saving the model at the end of epoch 15, iters 13335
2023.03.02-04:39:36:65:[step-13400/88900: 15.07%]--[loss-3.172477: wl-3.870898, gl-2.204753]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:59:07]
2023.03.02-04:40:57:165:[step-13500/88900: 15.19%]--[loss-3.366316: wl-4.317025, gl-2.287060]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:24:25]
2023.03.02-04:42:19:265:[step-13600/88900: 15.30%]--[loss-3.368279: wl-3.930496, gl-2.385654]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:25:07]
2023.03.02-04:43:39:365:[step-13700/88900: 15.41%]--[loss-3.349092: wl-3.999879, gl-2.349122]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:23:02]
2023.03.02-04:45:00:465:[step-13800/88900: 15.52%]--[loss-3.263598: wl-4.176540, gl-2.219463]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:41:42]
2023.03.02-04:46:20:565:[step-13900/88900: 15.64%]--[loss-3.445590: wl-4.321674, gl-2.365172]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:39:17]
2023.03.02-04:47:41:665:[step-14000/88900: 15.75%]--[loss-3.528232: wl-4.158870, gl-2.488514]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:44:44]
2023.03.02-04:49:02:765:[step-14100/88900: 15.86%]--[loss-3.090261: wl-3.737351, gl-2.155923]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:56:52]
2023.03.02-04:50:24:865:[step-14200/88900: 15.97%]--[loss-2.988593: wl-3.590669, gl-2.090925]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:33:47]
End of epoch 16 / 100: train_loss: 3.426 	 time: 736 sec
Saving the model at the end of epoch 16, iters 14224
2023.03.02-04:52:00:76:[step-14300/88900: 16.09%]--[loss-3.541424: wl-4.884881, gl-2.320203]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:29:04]
2023.03.02-04:53:22:176:[step-14400/88900: 16.20%]--[loss-3.679643: wl-5.165400, gl-2.388293]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:38:55]
2023.03.02-04:54:43:276:[step-14500/88900: 16.31%]--[loss-3.301948: wl-4.085933, gl-2.280464]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:53:24]
2023.03.02-04:56:03:376:[step-14600/88900: 16.42%]--[loss-3.269964: wl-4.486742, gl-2.148279]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:14:11]
2023.03.02-04:57:24:476:[step-14700/88900: 16.54%]--[loss-3.676714: wl-4.672498, gl-2.508589]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:09:00]
2023.03.02-04:58:44:576:[step-14800/88900: 16.65%]--[loss-3.286234: wl-3.975279, gl-2.292415]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:41:31]
2023.03.02-05:00:03:676:[step-14900/88900: 16.76%]--[loss-3.645222: wl-5.340539, gl-2.310087]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:44:48]
2023.03.02-05:01:24:776:[step-15000/88900: 16.87%]--[loss-3.134353: wl-3.657494, gl-2.219980]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:17:24]
2023.03.02-05:02:44:876:[step-15100/88900: 16.99%]--[loss-3.151445: wl-3.827187, gl-2.194649]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:10:10]
End of epoch 17 / 100: train_loss: 3.414 	 time: 731 sec
Saving the model at the end of epoch 17, iters 15113
2023.03.02-05:04:21:87:[step-15200/88900: 17.10%]--[loss-3.629977: wl-4.441879, gl-2.519507]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:42:20]
2023.03.02-05:05:43:187:[step-15300/88900: 17.21%]--[loss-3.347475: wl-4.237505, gl-2.288099]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:07:50]
2023.03.02-05:07:05:287:[step-15400/88900: 17.32%]--[loss-3.346205: wl-4.279497, gl-2.276331]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:47:20]
2023.03.02-05:08:28:387:[step-15500/88900: 17.44%]--[loss-3.291733: wl-4.046527, gl-2.280102]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:27:04]
2023.03.02-05:09:50:487:[step-15600/88900: 17.55%]--[loss-3.071816: wl-3.910728, gl-2.094134]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:01:35]
2023.03.02-05:11:12:587:[step-15700/88900: 17.66%]--[loss-3.560030: wl-4.227454, gl-2.503166]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:21:06]
2023.03.02-05:12:32:687:[step-15800/88900: 17.77%]--[loss-3.488388: wl-4.237511, gl-2.429011]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:23:00]
2023.03.02-05:13:53:787:[step-15900/88900: 17.89%]--[loss-3.129982: wl-3.762525, gl-2.189351]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:01:41]
2023.03.02-05:15:13:887:[step-16000/88900: 18.00%]--[loss-3.303603: wl-3.957848, gl-2.314141]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:59:55]
End of epoch 18 / 100: train_loss: 3.398 	 time: 739 sec
Saving the model at the end of epoch 18, iters 16002
2023.03.02-05:16:51:98:[step-16100/88900: 18.11%]--[loss-3.595441: wl-4.749751, gl-2.408004]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:10:45]
2023.03.02-05:18:11:198:[step-16200/88900: 18.22%]--[loss-3.119477: wl-3.729628, gl-2.187070]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:46:17]
2023.03.02-05:19:30:298:[step-16300/88900: 18.34%]--[loss-3.328221: wl-4.326705, gl-2.246544]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:37:19]
2023.03.02-05:20:52:398:[step-16400/88900: 18.45%]--[loss-3.579654: wl-4.673287, gl-2.411333]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:23:11]
2023.03.02-05:22:12:498:[step-16500/88900: 18.56%]--[loss-3.162203: wl-3.745271, gl-2.225885]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:29:59]
2023.03.02-05:23:34:598:[step-16600/88900: 18.67%]--[loss-3.385166: wl-4.226027, gl-2.328659]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:48:45]
2023.03.02-05:24:55:698:[step-16700/88900: 18.79%]--[loss-3.361890: wl-4.421364, gl-2.256549]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:07:00]
2023.03.02-05:26:16:798:[step-16800/88900: 18.90%]--[loss-3.400943: wl-3.948719, gl-2.413763]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:20:42]
End of epoch 19 / 100: train_loss: 3.393 	 time: 734 sec
Saving the model at the end of epoch 19, iters 16891
2023.03.02-05:27:52:9:[step-16900/88900: 19.01%]--[loss-3.222601: wl-3.959154, gl-2.232813]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:34:04]
2023.03.02-05:29:14:109:[step-17000/88900: 19.12%]--[loss-3.126608: wl-3.951170, gl-2.138815]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:53:18]
2023.03.02-05:30:36:209:[step-17100/88900: 19.24%]--[loss-3.191539: wl-3.938596, gl-2.206890]--[lr: pb-0.000050, pf-0.000050]--[ETA-18:06:28]
2023.03.02-05:31:58:309:[step-17200/88900: 19.35%]--[loss-3.390781: wl-4.497194, gl-2.266483]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:46:54]
2023.03.02-05:33:20:409:[step-17300/88900: 19.46%]--[loss-3.489545: wl-4.229583, gl-2.432149]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:04:08]
2023.03.02-05:34:39:509:[step-17400/88900: 19.57%]--[loss-3.598461: wl-4.676719, gl-2.429281]--[lr: pb-0.000050, pf-0.000050]--[ETA-23:45:02]
2023.03.02-05:36:01:609:[step-17500/88900: 19.69%]--[loss-3.555874: wl-4.610220, gl-2.403319]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:21:34]
2023.03.02-05:37:21:709:[step-17600/88900: 19.80%]--[loss-3.234610: wl-3.821650, gl-2.279198]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:42:13]
2023.03.02-05:38:42:809:[step-17700/88900: 19.91%]--[loss-3.279429: wl-4.119311, gl-2.249602]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:11:35]
End of epoch 20 / 100: train_loss: 3.378 	 time: 736 sec
Saving the model at the end of epoch 20, iters 17780
2023.03.02-05:40:19:20:[step-17800/88900: 20.02%]--[loss-3.123912: wl-3.921581, gl-2.143517]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:41:51]
2023.03.02-05:41:39:120:[step-17900/88900: 20.13%]--[loss-3.256619: wl-4.377833, gl-2.162161]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:07:16]
2023.03.02-05:43:01:220:[step-18000/88900: 20.25%]--[loss-3.367607: wl-4.563333, gl-2.226773]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:46:48]
2023.03.02-05:44:23:320:[step-18100/88900: 20.36%]--[loss-3.246440: wl-3.981168, gl-2.251148]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:39:37]
2023.03.02-05:45:44:420:[step-18200/88900: 20.47%]--[loss-3.248768: wl-3.993026, gl-2.250511]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:36:25]
2023.03.02-05:47:06:520:[step-18300/88900: 20.58%]--[loss-3.281121: wl-4.368222, gl-2.189065]--[lr: pb-0.000050, pf-0.000050]--[ETA-20:52:04]
2023.03.02-05:48:27:620:[step-18400/88900: 20.70%]--[loss-3.288439: wl-4.083529, gl-2.267557]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:57:37]
2023.03.02-05:49:47:720:[step-18500/88900: 20.81%]--[loss-3.190083: wl-4.081257, gl-2.169769]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:37:15]
2023.03.02-05:51:08:820:[step-18600/88900: 20.92%]--[loss-3.438258: wl-4.450961, gl-2.325518]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:51:48]
End of epoch 21 / 100: train_loss: 3.364 	 time: 735 sec
Saving the model at the end of epoch 21, iters 18669
2023.03.02-05:52:45:31:[step-18700/88900: 21.03%]--[loss-3.333280: wl-4.107453, gl-2.306417]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:52:41]
2023.03.02-05:54:07:131:[step-18800/88900: 21.15%]--[loss-3.576858: wl-4.397160, gl-2.477568]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:28:37]
2023.03.02-05:55:29:231:[step-18900/88900: 21.26%]--[loss-3.477484: wl-4.840330, gl-2.267401]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:02:30]
2023.03.02-05:56:51:331:[step-19000/88900: 21.37%]--[loss-3.174496: wl-3.863605, gl-2.208595]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:52:53]
2023.03.02-05:58:13:431:[step-19100/88900: 21.48%]--[loss-3.226099: wl-4.148674, gl-2.188930]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:08:34]
2023.03.02-05:59:35:531:[step-19200/88900: 21.60%]--[loss-3.672181: wl-4.724044, gl-2.491169]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:11:00]
2023.03.02-06:00:55:631:[step-19300/88900: 21.71%]--[loss-3.403364: wl-4.467533, gl-2.286481]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:39:29]
2023.03.02-06:02:17:731:[step-19400/88900: 21.82%]--[loss-3.478987: wl-4.587053, gl-2.332224]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:15:41]
2023.03.02-06:03:38:831:[step-19500/88900: 21.93%]--[loss-3.586208: wl-5.589752, gl-2.188770]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:06:18]
End of epoch 22 / 100: train_loss: 3.356 	 time: 738 sec
Saving the model at the end of epoch 22, iters 19558
2023.03.02-06:05:15:42:[step-19600/88900: 22.05%]--[loss-3.271039: wl-4.171422, gl-2.228184]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:00:08]
2023.03.02-06:06:34:142:[step-19700/88900: 22.16%]--[loss-3.391422: wl-4.556435, gl-2.252313]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:03:54]
2023.03.02-06:07:57:242:[step-19800/88900: 22.27%]--[loss-3.401944: wl-4.113699, gl-2.373519]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:20:12]
2023.03.02-06:09:18:342:[step-19900/88900: 22.38%]--[loss-3.317078: wl-4.096769, gl-2.292886]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:44:46]
2023.03.02-06:10:40:442:[step-20000/88900: 22.50%]--[loss-3.447195: wl-4.410060, gl-2.344680]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:05:05]
2023.03.02-06:12:02:542:[step-20100/88900: 22.61%]--[loss-3.194620: wl-4.209894, gl-2.142147]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:23:41]
2023.03.02-06:13:25:642:[step-20200/88900: 22.72%]--[loss-3.274887: wl-4.545578, gl-2.138492]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:46:12]
2023.03.02-06:14:46:742:[step-20300/88900: 22.83%]--[loss-3.554045: wl-5.029907, gl-2.296569]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:02:40]
2023.03.02-06:16:07:842:[step-20400/88900: 22.95%]--[loss-3.525278: wl-4.494922, gl-2.401547]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:56:07]
End of epoch 23 / 100: train_loss: 3.349 	 time: 740 sec
Saving the model at the end of epoch 23, iters 20447
2023.03.02-06:17:44:53:[step-20500/88900: 23.06%]--[loss-3.032332: wl-3.722969, gl-2.101590]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:58:00]
2023.03.02-06:19:05:153:[step-20600/88900: 23.17%]--[loss-3.398536: wl-4.724834, gl-2.217328]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:35:56]
2023.03.02-06:20:27:253:[step-20700/88900: 23.28%]--[loss-3.066700: wl-3.906270, gl-2.090132]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:47:13]
2023.03.02-06:21:49:353:[step-20800/88900: 23.40%]--[loss-3.229221: wl-4.186518, gl-2.182592]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:42:10]
2023.03.02-06:23:09:453:[step-20900/88900: 23.51%]--[loss-3.276181: wl-4.272821, gl-2.207975]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:38:26]
2023.03.02-06:24:31:553:[step-21000/88900: 23.62%]--[loss-3.222507: wl-4.169536, gl-2.180123]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:05:03]
2023.03.02-06:25:53:653:[step-21100/88900: 23.73%]--[loss-3.508389: wl-4.538691, gl-2.373717]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:42:21]
2023.03.02-06:27:13:753:[step-21200/88900: 23.85%]--[loss-3.051010: wl-3.810730, gl-2.098328]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:13:46]
2023.03.02-06:28:34:853:[step-21300/88900: 23.96%]--[loss-3.346844: wl-3.936668, gl-2.362677]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:39:19]
End of epoch 24 / 100: train_loss: 3.336 	 time: 738 sec
Saving the model at the end of epoch 24, iters 21336
2023.03.02-06:30:10:64:[step-21400/88900: 24.07%]--[loss-3.358734: wl-4.316081, gl-2.279714]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:20:55]
2023.03.02-06:31:31:164:[step-21500/88900: 24.18%]--[loss-2.897178: wl-3.537524, gl-2.012797]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:56:04]
2023.03.02-06:32:52:264:[step-21600/88900: 24.30%]--[loss-3.379651: wl-4.575970, gl-2.235658]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:37:04]
2023.03.02-06:34:15:364:[step-21700/88900: 24.41%]--[loss-3.391376: wl-4.184661, gl-2.345211]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:55:00]
2023.03.02-06:35:35:464:[step-21800/88900: 24.52%]--[loss-3.754262: wl-4.405783, gl-2.652816]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:09:03]
2023.03.02-06:36:55:564:[step-21900/88900: 24.63%]--[loss-3.457212: wl-3.935012, gl-2.473459]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:47:34]
2023.03.02-06:38:14:664:[step-22000/88900: 24.75%]--[loss-3.719489: wl-4.746890, gl-2.532767]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:30:40]
2023.03.02-06:39:35:764:[step-22100/88900: 24.86%]--[loss-3.225835: wl-3.957416, gl-2.236481]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:18:04]
2023.03.02-06:40:57:864:[step-22200/88900: 24.97%]--[loss-3.032350: wl-4.016224, gl-2.028294]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:38:57]
End of epoch 25 / 100: train_loss: 3.326 	 time: 734 sec
Saving the model at the end of epoch 25, iters 22225
2023.03.02-06:42:33:75:[step-22300/88900: 25.08%]--[loss-3.351024: wl-4.322114, gl-2.270495]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:06:40]
2023.03.02-06:43:55:175:[step-22400/88900: 25.20%]--[loss-3.281468: wl-4.094215, gl-2.257915]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:18:56]
2023.03.02-06:45:16:275:[step-22500/88900: 25.31%]--[loss-3.279826: wl-4.246688, gl-2.218154]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:53:45]
2023.03.02-06:46:38:375:[step-22600/88900: 25.42%]--[loss-3.158177: wl-4.164660, gl-2.117012]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:24:04]
2023.03.02-06:47:58:475:[step-22700/88900: 25.53%]--[loss-3.836031: wl-5.085449, gl-2.564669]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:15:21]
2023.03.02-06:49:20:575:[step-22800/88900: 25.65%]--[loss-3.262343: wl-3.748725, gl-2.325161]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:58:26]
2023.03.02-06:50:40:675:[step-22900/88900: 25.76%]--[loss-3.351731: wl-4.353853, gl-2.263268]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:58:42]
2023.03.02-06:52:01:775:[step-23000/88900: 25.87%]--[loss-3.467432: wl-4.719875, gl-2.287463]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:40:52]
2023.03.02-06:53:22:875:[step-23100/88900: 25.98%]--[loss-3.678822: wl-5.118120, gl-2.399292]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:35:38]
End of epoch 26 / 100: train_loss: 3.327 	 time: 735 sec
Saving the model at the end of epoch 26, iters 23114
2023.03.02-06:55:00:86:[step-23200/88900: 26.10%]--[loss-3.377044: wl-4.159302, gl-2.337218]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:08:09]
2023.03.02-06:56:19:186:[step-23300/88900: 26.21%]--[loss-3.368292: wl-4.392932, gl-2.270059]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:26:57]
2023.03.02-06:57:40:286:[step-23400/88900: 26.32%]--[loss-3.354326: wl-4.482201, gl-2.233776]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:42:48]
2023.03.02-06:59:02:386:[step-23500/88900: 26.43%]--[loss-3.301160: wl-4.320234, gl-2.221102]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:57:11]
2023.03.02-07:00:22:486:[step-23600/88900: 26.55%]--[loss-3.368092: wl-4.559748, gl-2.228155]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:53:57]
2023.03.02-07:01:42:586:[step-23700/88900: 26.66%]--[loss-3.228715: wl-4.144329, gl-2.192633]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:51:52]
2023.03.02-07:03:03:686:[step-23800/88900: 26.77%]--[loss-3.455511: wl-4.577711, gl-2.311083]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:36:29]
2023.03.02-07:04:25:786:[step-23900/88900: 26.88%]--[loss-3.451850: wl-4.781867, gl-2.256383]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:14:22]
2023.03.02-07:05:45:886:[step-24000/88900: 27.00%]--[loss-3.236119: wl-4.219189, gl-2.181322]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:21:18]
End of epoch 27 / 100: train_loss: 3.312 	 time: 733 sec
Saving the model at the end of epoch 27, iters 24003
2023.03.02-07:07:22:97:[step-24100/88900: 27.11%]--[loss-3.117242: wl-4.056757, gl-2.103052]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:10:57]
2023.03.02-07:08:43:197:[step-24200/88900: 27.22%]--[loss-3.134073: wl-3.913169, gl-2.155781]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:30:50]
2023.03.02-07:10:02:297:[step-24300/88900: 27.33%]--[loss-3.290060: wl-4.085501, gl-2.268685]--[lr: pb-0.000050, pf-0.000050]--[ETA-21:14:01]
2023.03.02-07:11:21:397:[step-24400/88900: 27.45%]--[loss-3.286263: wl-4.020331, gl-2.281181]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:20:08]
2023.03.02-07:12:40:497:[step-24500/88900: 27.56%]--[loss-3.098353: wl-3.969183, gl-2.106057]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:22:03]
2023.03.02-07:14:00:597:[step-24600/88900: 27.67%]--[loss-3.454377: wl-4.474121, gl-2.335847]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:12:35]
2023.03.02-07:15:22:697:[step-24700/88900: 27.78%]--[loss-3.415405: wl-4.366022, gl-2.323900]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:34:43]
2023.03.02-07:16:43:797:[step-24800/88900: 27.90%]--[loss-3.390566: wl-4.213215, gl-2.337262]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:15:08]
End of epoch 28 / 100: train_loss: 3.299 	 time: 728 sec
Saving the model at the end of epoch 28, iters 24892
2023.03.02-07:18:20:8:[step-24900/88900: 28.01%]--[loss-3.104680: wl-4.262408, gl-2.039078]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:33:10]
2023.03.02-07:19:42:108:[step-25000/88900: 28.12%]--[loss-3.261298: wl-4.099402, gl-2.236448]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:58:47]
2023.03.02-07:21:02:208:[step-25100/88900: 28.23%]--[loss-3.525045: wl-4.531924, gl-2.392064]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:25:01]
2023.03.02-07:22:24:308:[step-25200/88900: 28.35%]--[loss-3.249546: wl-4.214616, gl-2.195892]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:23:03]
2023.03.02-07:23:45:408:[step-25300/88900: 28.46%]--[loss-3.309232: wl-4.314060, gl-2.230717]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:29:22]
2023.03.02-07:25:06:508:[step-25400/88900: 28.57%]--[loss-3.732181: wl-4.682888, gl-2.561459]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:33:15]
2023.03.02-07:26:28:608:[step-25500/88900: 28.68%]--[loss-3.157663: wl-4.043342, gl-2.146827]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:34:07]
2023.03.02-07:27:48:708:[step-25600/88900: 28.80%]--[loss-3.347840: wl-4.111274, gl-2.320021]--[lr: pb-0.000050, pf-0.000050]--[ETA-22:57:15]
2023.03.02-07:29:07:808:[step-25700/88900: 28.91%]--[loss-3.302549: wl-4.185994, gl-2.256050]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:44:19]
End of epoch 29 / 100: train_loss: 3.297 	 time: 734 sec
Saving the model at the end of epoch 29, iters 25781
2023.03.02-07:30:42:19:[step-25800/88900: 29.02%]--[loss-3.296274: wl-4.430513, gl-2.188646]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:13:28]
2023.03.02-07:32:03:119:[step-25900/88900: 29.13%]--[loss-3.466368: wl-4.841228, gl-2.256061]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:36:16]
2023.03.02-07:33:24:219:[step-26000/88900: 29.25%]--[loss-3.097130: wl-4.117728, gl-2.067698]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:33:42]
2023.03.02-07:34:45:319:[step-26100/88900: 29.36%]--[loss-3.208967: wl-3.960680, gl-2.218797]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:37:27]
2023.03.02-07:36:06:419:[step-26200/88900: 29.47%]--[loss-3.181409: wl-4.034694, gl-2.172735]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:07:15]
2023.03.02-07:37:27:519:[step-26300/88900: 29.58%]--[loss-3.289366: wl-4.228646, gl-2.232204]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:39:50]
2023.03.02-07:38:47:619:[step-26400/88900: 29.70%]--[loss-3.156024: wl-4.102358, gl-2.130435]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:22:02]
2023.03.02-07:40:07:719:[step-26500/88900: 29.81%]--[loss-3.295084: wl-4.421106, gl-2.189808]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:56:05]
2023.03.02-07:41:29:819:[step-26600/88900: 29.92%]--[loss-3.263786: wl-4.298593, gl-2.189138]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:54:22]
End of epoch 30 / 100: train_loss: 3.283 	 time: 730 sec
Saving the model at the end of epoch 30, iters 26670
2023.03.02-07:43:04:30:[step-26700/88900: 30.03%]--[loss-3.242175: wl-4.313473, gl-2.163806]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:22:00]
2023.03.02-07:44:26:130:[step-26800/88900: 30.15%]--[loss-3.318232: wl-4.370895, gl-2.225508]--[lr: pb-0.000050, pf-0.000050]--[ETA-19:56:57]
2023.03.02-07:45:46:230:[step-26900/88900: 30.26%]--[loss-3.170263: wl-4.056035, gl-2.156254]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:39:20]
2023.03.02-07:47:08:330:[step-27000/88900: 30.37%]--[loss-3.156520: wl-3.804108, gl-2.205493]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:47:39]
2023.03.02-07:48:30:430:[step-27100/88900: 30.48%]--[loss-3.247651: wl-4.201192, gl-2.197353]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:38:10]
2023.03.02-07:49:50:530:[step-27200/88900: 30.60%]--[loss-3.271496: wl-4.269333, gl-2.204163]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:22:42]
2023.03.02-07:51:12:630:[step-27300/88900: 30.71%]--[loss-3.428464: wl-5.293682, gl-2.105043]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:24:23]
2023.03.02-07:52:31:730:[step-27400/88900: 30.82%]--[loss-3.278950: wl-3.921134, gl-2.298666]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:16:14]
2023.03.02-07:53:51:830:[step-27500/88900: 30.93%]--[loss-3.146934: wl-3.837889, gl-2.187462]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:07:47]
End of epoch 31 / 100: train_loss: 3.277 	 time: 733 sec
Saving the model at the end of epoch 31, iters 27559
2023.03.02-07:55:27:41:[step-27600/88900: 31.05%]--[loss-3.211144: wl-4.100670, gl-2.185977]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:15:54]
2023.03.02-07:56:49:141:[step-27700/88900: 31.16%]--[loss-3.396140: wl-4.323892, gl-2.315167]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:29:42]
2023.03.02-07:58:11:241:[step-27800/88900: 31.27%]--[loss-3.357430: wl-4.300360, gl-2.282341]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:07:13]
2023.03.02-07:59:34:341:[step-27900/88900: 31.38%]--[loss-3.328428: wl-4.408562, gl-2.226287]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:04:44]
2023.03.02-08:01:01:441:[step-28000/88900: 31.50%]--[loss-3.223388: wl-4.078367, gl-2.203796]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:49:57]
2023.03.02-08:02:26:541:[step-28100/88900: 31.61%]--[loss-2.999364: wl-3.812361, gl-2.046274]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:54:09]
2023.03.02-08:03:55:641:[step-28200/88900: 31.72%]--[loss-3.227383: wl-4.112417, gl-2.199279]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:55:52]
2023.03.02-08:05:21:741:[step-28300/88900: 31.83%]--[loss-3.546806: wl-4.358703, gl-2.457130]--[lr: pb-0.000050, pf-0.000050]--[ETA-20:48:45]
2023.03.02-08:06:47:841:[step-28400/88900: 31.95%]--[loss-3.040580: wl-3.982590, gl-2.044932]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:23:27]
End of epoch 32 / 100: train_loss: 3.265 	 time: 772 sec
Saving the model at the end of epoch 32, iters 28448
2023.03.02-08:08:32:52:[step-28500/88900: 32.06%]--[loss-3.284122: wl-4.790303, gl-2.086546]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:43:28]
2023.03.02-08:09:57:152:[step-28600/88900: 32.17%]--[loss-3.530229: wl-4.756120, gl-2.341199]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:56:08]
2023.03.02-08:11:23:252:[step-28700/88900: 32.28%]--[loss-3.164444: wl-3.799297, gl-2.214620]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:27:13]
2023.03.02-08:12:51:352:[step-28800/88900: 32.40%]--[loss-3.037662: wl-4.008748, gl-2.035475]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:13:27]
2023.03.02-08:14:18:452:[step-28900/88900: 32.51%]--[loss-3.036196: wl-3.889407, gl-2.063845]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:42:39]
2023.03.02-08:15:45:552:[step-29000/88900: 32.62%]--[loss-3.281726: wl-4.214847, gl-2.228015]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:34:11]
2023.03.02-08:17:11:652:[step-29100/88900: 32.73%]--[loss-3.413496: wl-4.469016, gl-2.296242]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:05:54]
2023.03.02-08:18:36:752:[step-29200/88900: 32.85%]--[loss-3.117742: wl-4.138256, gl-2.083178]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:42:04]
2023.03.02-08:20:04:852:[step-29300/88900: 32.96%]--[loss-3.318849: wl-4.828274, gl-2.111780]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:13:46]
End of epoch 33 / 100: train_loss: 3.260 	 time: 785 sec
Saving the model at the end of epoch 33, iters 29337
2023.03.02-08:21:45:63:[step-29400/88900: 33.07%]--[loss-3.248709: wl-3.954242, gl-2.260149]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:12:40]
2023.03.02-08:23:12:163:[step-29500/88900: 33.18%]--[loss-3.106836: wl-3.780610, gl-2.161684]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:21:10]
2023.03.02-08:24:40:263:[step-29600/88900: 33.30%]--[loss-3.100753: wl-3.883329, gl-2.129920]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:29:10]
2023.03.02-08:26:08:363:[step-29700/88900: 33.41%]--[loss-3.329393: wl-4.205386, gl-2.278047]--[lr: pb-0.000050, pf-0.000050]--[ETA-17:30:03]
2023.03.02-08:27:34:463:[step-29800/88900: 33.52%]--[loss-3.292958: wl-3.955908, gl-2.303981]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:41:41]
2023.03.02-08:29:05:563:[step-29900/88900: 33.63%]--[loss-3.110359: wl-3.908650, gl-2.133196]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:25:38]
2023.03.02-08:30:34:663:[step-30000/88900: 33.75%]--[loss-3.568410: wl-4.472199, gl-2.450360]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:12:26]
2023.03.02-08:32:03:763:[step-30100/88900: 33.86%]--[loss-3.026156: wl-3.785540, gl-2.079772]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:45:42]
2023.03.02-08:33:31:863:[step-30200/88900: 33.97%]--[loss-3.166060: wl-4.244503, gl-2.104934]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:50:53]
End of epoch 34 / 100: train_loss: 3.253 	 time: 797 sec
Saving the model at the end of epoch 34, iters 30226
2023.03.02-08:35:15:74:[step-30300/88900: 34.08%]--[loss-3.331518: wl-4.124824, gl-2.300312]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:18:00]
2023.03.02-08:36:45:174:[step-30400/88900: 34.20%]--[loss-3.180539: wl-3.963157, gl-2.189750]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:54:18]
2023.03.02-08:38:14:274:[step-30500/88900: 34.31%]--[loss-3.417517: wl-4.590711, gl-2.269839]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:10:01]
2023.03.02-08:39:39:374:[step-30600/88900: 34.42%]--[loss-3.250612: wl-3.965224, gl-2.259306]--[lr: pb-0.000050, pf-0.000050]--[ETA-16:37:46]
2023.03.02-08:40:59:474:[step-30700/88900: 34.53%]--[loss-3.220176: wl-3.755819, gl-2.281221]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:45:21]
2023.03.02-08:42:19:574:[step-30800/88900: 34.65%]--[loss-3.022156: wl-3.786340, gl-2.075571]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:50:57]
2023.03.02-08:43:40:674:[step-30900/88900: 34.76%]--[loss-2.953131: wl-3.797444, gl-2.003770]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:44:38]
2023.03.02-08:44:57:774:[step-31000/88900: 34.87%]--[loss-3.303262: wl-4.115445, gl-2.274400]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:39:22]
2023.03.02-08:46:16:874:[step-31100/88900: 34.98%]--[loss-3.235684: wl-4.020642, gl-2.230524]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:33:07]
End of epoch 35 / 100: train_loss: 3.244 	 time: 752 sec
Saving the model at the end of epoch 35, iters 31115
2023.03.02-08:47:53:85:[step-31200/88900: 35.10%]--[loss-3.063107: wl-3.929837, gl-2.080647]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:23:34]
2023.03.02-08:49:13:185:[step-31300/88900: 35.21%]--[loss-3.295747: wl-4.235709, gl-2.236820]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:53:40]
2023.03.02-08:50:34:285:[step-31400/88900: 35.32%]--[loss-3.179846: wl-4.181579, gl-2.134452]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:50:48]
2023.03.02-08:51:56:385:[step-31500/88900: 35.43%]--[loss-3.132410: wl-4.271879, gl-2.064440]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:17:06]
2023.03.02-08:53:17:485:[step-31600/88900: 35.55%]--[loss-3.170380: wl-3.834229, gl-2.211823]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:22:07]
2023.03.02-08:54:38:585:[step-31700/88900: 35.66%]--[loss-3.197492: wl-4.114555, gl-2.168853]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:24:41]
2023.03.02-08:55:59:685:[step-31800/88900: 35.77%]--[loss-3.300608: wl-4.401949, gl-2.200121]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:30:28]
2023.03.02-08:57:19:785:[step-31900/88900: 35.88%]--[loss-3.496135: wl-4.415146, gl-2.392348]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:17:13]
2023.03.02-08:58:38:885:[step-32000/88900: 36.00%]--[loss-2.886763: wl-3.579282, gl-1.991942]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:20:45]
End of epoch 36 / 100: train_loss: 3.237 	 time: 732 sec
Saving the model at the end of epoch 36, iters 32004
2023.03.02-09:00:15:96:[step-32100/88900: 36.11%]--[loss-3.220315: wl-4.480167, gl-2.100274]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:12:46]
2023.03.02-09:01:33:196:[step-32200/88900: 36.22%]--[loss-3.383143: wl-4.277274, gl-2.313824]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:26:35]
2023.03.02-09:02:53:296:[step-32300/88900: 36.33%]--[loss-3.102284: wl-3.992970, gl-2.104042]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:14:20]
2023.03.02-09:04:13:396:[step-32400/88900: 36.45%]--[loss-3.369394: wl-4.570064, gl-2.226878]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:21:10]
2023.03.02-09:05:34:496:[step-32500/88900: 36.56%]--[loss-3.457752: wl-4.879688, gl-2.237830]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:19:04]
2023.03.02-09:06:53:596:[step-32600/88900: 36.67%]--[loss-3.439957: wl-4.112632, gl-2.411799]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:47:04]
2023.03.02-09:08:13:696:[step-32700/88900: 36.78%]--[loss-3.188125: wl-4.070681, gl-2.170455]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:14:02]
2023.03.02-09:09:33:796:[step-32800/88900: 36.90%]--[loss-3.380541: wl-5.174867, gl-2.086824]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:33:19]
End of epoch 37 / 100: train_loss: 3.235 	 time: 724 sec
Saving the model at the end of epoch 37, iters 32893
2023.03.02-09:11:07:7:[step-32900/88900: 37.01%]--[loss-2.963447: wl-3.715148, gl-2.034660]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:12:58]
2023.03.02-09:12:25:107:[step-33000/88900: 37.12%]--[loss-3.034107: wl-3.991023, gl-2.036352]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:20:13]
2023.03.02-09:13:46:207:[step-33100/88900: 37.23%]--[loss-3.078802: wl-3.745256, gl-2.142488]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:13:09]
2023.03.02-09:15:07:307:[step-33200/88900: 37.35%]--[loss-3.272541: wl-3.958396, gl-2.282942]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:54:42]
2023.03.02-09:16:28:407:[step-33300/88900: 37.46%]--[loss-3.248783: wl-4.234489, gl-2.190161]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:28:12]
2023.03.02-09:17:49:507:[step-33400/88900: 37.57%]--[loss-3.064985: wl-4.012233, gl-2.061927]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:21:18]
2023.03.02-09:19:11:607:[step-33500/88900: 37.68%]--[loss-3.359193: wl-4.772587, gl-2.166047]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:07:08]
2023.03.02-09:20:30:707:[step-33600/88900: 37.80%]--[loss-3.216884: wl-4.238311, gl-2.157306]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:26:33]
2023.03.02-09:21:51:807:[step-33700/88900: 37.91%]--[loss-3.231952: wl-4.233183, gl-2.173656]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:26:36]
End of epoch 38 / 100: train_loss: 3.223 	 time: 728 sec
Saving the model at the end of epoch 38, iters 33782
2023.03.02-09:23:26:18:[step-33800/88900: 38.02%]--[loss-3.445564: wl-4.489819, gl-2.323109]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:10:27]
2023.03.02-09:24:45:118:[step-33900/88900: 38.13%]--[loss-3.285433: wl-4.256214, gl-2.221379]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:01:01]
2023.03.02-09:26:04:218:[step-34000/88900: 38.25%]--[loss-3.184411: wl-4.033913, gl-2.175933]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:06:44]
2023.03.02-09:27:24:318:[step-34100/88900: 38.36%]--[loss-3.051817: wl-3.976947, gl-2.057580]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:17:07]
2023.03.02-09:28:45:418:[step-34200/88900: 38.47%]--[loss-3.181290: wl-4.120008, gl-2.151288]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:19:59]
2023.03.02-09:30:07:518:[step-34300/88900: 38.58%]--[loss-3.211292: wl-4.065860, gl-2.194827]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:05:09]
2023.03.02-09:31:27:618:[step-34400/88900: 38.70%]--[loss-3.484105: wl-4.970859, gl-2.241390]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:01:27]
2023.03.02-09:32:48:718:[step-34500/88900: 38.81%]--[loss-3.198352: wl-4.325702, gl-2.116926]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:06:32]
2023.03.02-09:34:08:818:[step-34600/88900: 38.92%]--[loss-3.246212: wl-4.382405, gl-2.150611]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:26:15]
End of epoch 39 / 100: train_loss: 3.217 	 time: 727 sec
Saving the model at the end of epoch 39, iters 34671
2023.03.02-09:35:43:29:[step-34700/88900: 39.03%]--[loss-3.108430: wl-4.040825, gl-2.098224]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:48:59]
2023.03.02-09:37:02:129:[step-34800/88900: 39.15%]--[loss-3.151454: wl-4.450050, gl-2.038941]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:06:43]
2023.03.02-09:38:23:229:[step-34900/88900: 39.26%]--[loss-3.397502: wl-4.305802, gl-2.321051]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:16:45]
2023.03.02-09:39:44:329:[step-35000/88900: 39.37%]--[loss-2.885644: wl-3.709096, gl-1.958370]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:06:01]
2023.03.02-09:41:15:429:[step-35100/88900: 39.48%]--[loss-3.410313: wl-4.275793, gl-2.341365]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:31:07]
2023.03.02-09:42:55:529:[step-35200/88900: 39.60%]--[loss-3.132538: wl-4.277093, gl-2.063265]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:20:51]
2023.03.02-09:44:41:629:[step-35300/88900: 39.71%]--[loss-2.964704: wl-3.736414, gl-2.030600]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:58:04]
2023.03.02-09:46:57:729:[step-35400/88900: 39.82%]--[loss-3.166346: wl-4.429250, gl-2.059033]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:53:21]
2023.03.02-09:48:16:829:[step-35500/88900: 39.93%]--[loss-3.164421: wl-3.843991, gl-2.203424]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:03:16]
End of epoch 40 / 100: train_loss: 3.208 	 time: 840 sec
Saving the model at the end of epoch 40, iters 35560
2023.03.02-09:50:02:40:[step-35600/88900: 40.04%]--[loss-3.398188: wl-4.456722, gl-2.284007]--[lr: pb-0.000050, pf-0.000050]--[ETA-1 day, 5:31:20]
2023.03.02-09:51:47:140:[step-35700/88900: 40.16%]--[loss-3.014052: wl-3.726121, gl-2.082522]--[lr: pb-0.000050, pf-0.000050]--[ETA-15:51:24]
2023.03.02-09:54:05:240:[step-35800/88900: 40.27%]--[loss-3.301913: wl-4.185512, gl-2.255536]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:39:45]
2023.03.02-09:55:26:340:[step-35900/88900: 40.38%]--[loss-3.246137: wl-4.463235, gl-2.130328]--[lr: pb-0.000050, pf-0.000050]--[ETA-14:37:05]
2023.03.02-09:56:45:440:[step-36000/88900: 40.49%]--[loss-2.950824: wl-3.893446, gl-1.977463]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:02:58]
2023.03.02-09:58:05:540:[step-36100/88900: 40.61%]--[loss-3.117796: wl-4.056137, gl-2.103762]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:22:09]
2023.03.02-09:59:25:640:[step-36200/88900: 40.72%]--[loss-3.010469: wl-3.858823, gl-2.045763]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:41:25]
2023.03.02-10:00:46:740:[step-36300/88900: 40.83%]--[loss-2.967525: wl-3.872540, gl-1.999390]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:48:41]
2023.03.02-10:02:06:840:[step-36400/88900: 40.94%]--[loss-3.531864: wl-4.839952, gl-2.321876]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:07:10]
End of epoch 41 / 100: train_loss: 3.210 	 time: 819 sec
Saving the model at the end of epoch 41, iters 36449
2023.03.02-10:03:42:51:[step-36500/88900: 41.06%]--[loss-3.210862: wl-4.521238, gl-2.080553]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:15:34]
2023.03.02-10:05:03:151:[step-36600/88900: 41.17%]--[loss-3.324427: wl-4.634699, gl-2.165752]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:55:10]
2023.03.02-10:06:24:251:[step-36700/88900: 41.28%]--[loss-3.179307: wl-4.910265, gl-1.951740]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:52:57]
2023.03.02-10:07:42:351:[step-36800/88900: 41.39%]--[loss-3.067832: wl-3.952070, gl-2.079815]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:16:29]
2023.03.02-10:09:02:451:[step-36900/88900: 41.51%]--[loss-3.074703: wl-4.092571, gl-2.051560]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:20:30]
2023.03.02-10:10:23:551:[step-37000/88900: 41.62%]--[loss-3.162123: wl-4.408626, gl-2.059967]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:06:24]
2023.03.02-10:11:43:651:[step-37100/88900: 41.73%]--[loss-2.972310: wl-3.969764, gl-1.979869]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:08:03]
2023.03.02-10:13:04:751:[step-37200/88900: 41.84%]--[loss-3.331697: wl-4.805384, gl-2.130350]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:42:51]
2023.03.02-10:14:26:851:[step-37300/88900: 41.96%]--[loss-3.212892: wl-4.190203, gl-2.165341]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:18:45]
End of epoch 42 / 100: train_loss: 3.201 	 time: 730 sec
Saving the model at the end of epoch 42, iters 37338
2023.03.02-10:16:02:62:[step-37400/88900: 42.07%]--[loss-2.961879: wl-3.789605, gl-2.014478]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:03:28]
2023.03.02-10:17:24:162:[step-37500/88900: 42.18%]--[loss-3.174159: wl-4.061848, gl-2.158697]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:51:41]
2023.03.02-10:18:44:262:[step-37600/88900: 42.29%]--[loss-2.907706: wl-3.896556, gl-1.933567]--[lr: pb-0.000050, pf-0.000050]--[ETA-13:23:39]
2023.03.02-10:20:03:362:[step-37700/88900: 42.41%]--[loss-3.441065: wl-4.655949, gl-2.277077]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:57:54]
2023.03.02-10:21:25:462:[step-37800/88900: 42.52%]--[loss-3.144962: wl-4.095438, gl-2.121103]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:20:34]
2023.03.02-10:22:46:562:[step-37900/88900: 42.63%]--[loss-3.080685: wl-4.146960, gl-2.043945]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:16:29]
2023.03.02-10:24:06:662:[step-38000/88900: 42.74%]--[loss-3.077320: wl-4.127830, gl-2.045363]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:16:48]
2023.03.02-10:25:27:762:[step-38100/88900: 42.86%]--[loss-2.999408: wl-3.940487, gl-2.014287]--[lr: pb-0.000050, pf-0.000050]--[ETA-12:27:42]
2023.03.02-10:26:48:862:[step-38200/88900: 42.97%]--[loss-3.052965: wl-4.103184, gl-2.027169]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:47:24]
End of epoch 43 / 100: train_loss: 3.187 	 time: 732 sec
Saving the model at the end of epoch 43, iters 38227
2023.03.02-10:28:24:73:[step-38300/88900: 43.08%]--[loss-3.244534: wl-4.448846, gl-2.132323]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:19:40]
2023.03.02-10:29:45:173:[step-38400/88900: 43.19%]--[loss-3.394097: wl-4.596539, gl-2.244962]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:33:49]
2023.03.02-10:31:06:273:[step-38500/88900: 43.31%]--[loss-2.866064: wl-3.889259, gl-1.893749]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:05:06]
2023.03.02-10:32:28:373:[step-38600/88900: 43.42%]--[loss-3.433621: wl-4.492284, gl-2.310550]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:16:51]
2023.03.02-10:33:46:473:[step-38700/88900: 43.53%]--[loss-3.066530: wl-4.052980, gl-2.053285]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:19:01]
2023.03.02-10:34:57:573:[step-38800/88900: 43.64%]--[loss-3.307041: wl-4.337571, gl-2.222649]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:19:09]
2023.03.02-10:36:07:673:[step-38900/88900: 43.76%]--[loss-3.146246: wl-4.160022, gl-2.106240]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:11:54]
2023.03.02-10:37:18:773:[step-39000/88900: 43.87%]--[loss-3.072195: wl-3.962321, gl-2.081614]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:35:44]
2023.03.02-10:38:28:873:[step-39100/88900: 43.98%]--[loss-3.082610: wl-3.821824, gl-2.127154]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:43:55]
End of epoch 44 / 100: train_loss: 3.178 	 time: 689 sec
Saving the model at the end of epoch 44, iters 39116
2023.03.02-10:39:52:84:[step-39200/88900: 44.09%]--[loss-3.032482: wl-3.750774, gl-2.094789]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:45:20]
2023.03.02-10:41:03:184:[step-39300/88900: 44.21%]--[loss-3.004334: wl-3.847888, gl-2.042362]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:46:22]
2023.03.02-10:42:14:284:[step-39400/88900: 44.32%]--[loss-3.069165: wl-3.930839, gl-2.086456]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:05:49]
2023.03.02-10:43:25:384:[step-39500/88900: 44.43%]--[loss-3.254020: wl-4.230490, gl-2.196398]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:58:20]
2023.03.02-10:44:36:484:[step-39600/88900: 44.54%]--[loss-3.338289: wl-4.717179, gl-2.158994]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:25:10]
2023.03.02-10:45:46:584:[step-39700/88900: 44.66%]--[loss-3.265783: wl-4.534726, gl-2.132102]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:11:36]
2023.03.02-10:46:56:684:[step-39800/88900: 44.77%]--[loss-2.981726: wl-3.947810, gl-1.994773]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:06:18]
2023.03.02-10:48:07:784:[step-39900/88900: 44.88%]--[loss-3.276622: wl-4.430079, gl-2.169102]--[lr: pb-0.000050, pf-0.000050]--[ETA-8:55:00]
2023.03.02-10:49:17:884:[step-40000/88900: 44.99%]--[loss-3.281811: wl-4.022947, gl-2.276074]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:32:42]
End of epoch 45 / 100: train_loss: 3.174 	 time: 641 sec
Saving the model at the end of epoch 45, iters 40005
2023.03.02-10:50:41:95:[step-40100/88900: 45.11%]--[loss-3.044090: wl-4.107476, gl-2.017221]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:50:43]
2023.03.02-10:51:51:195:[step-40200/88900: 45.22%]--[loss-3.171813: wl-4.135489, gl-2.137940]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:09:02]
2023.03.02-10:53:02:295:[step-40300/88900: 45.33%]--[loss-3.089885: wl-4.333706, gl-2.006459]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:16:06]
2023.03.02-10:54:13:395:[step-40400/88900: 45.44%]--[loss-3.012805: wl-3.749218, gl-2.075501]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:16:12]
2023.03.02-10:55:22:495:[step-40500/88900: 45.56%]--[loss-3.250119: wl-4.385837, gl-2.153660]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:51:36]
2023.03.02-10:56:33:595:[step-40600/88900: 45.67%]--[loss-3.089623: wl-4.150428, gl-2.052016]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:43:57]
2023.03.02-10:57:44:695:[step-40700/88900: 45.78%]--[loss-3.057188: wl-3.907099, gl-2.080413]--[lr: pb-0.000050, pf-0.000050]--[ETA-8:55:43]
2023.03.02-10:58:54:795:[step-40800/88900: 45.89%]--[loss-3.268778: wl-4.488420, gl-2.146673]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:44:53]
End of epoch 46 / 100: train_loss: 3.167 	 time: 638 sec
Saving the model at the end of epoch 46, iters 40894
2023.03.02-11:00:16:6:[step-40900/88900: 46.01%]--[loss-2.935223: wl-3.851782, gl-1.972277]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:51:53]
2023.03.02-11:01:26:106:[step-41000/88900: 46.12%]--[loss-3.127417: wl-4.098060, gl-2.102902]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:02:45]
2023.03.02-11:02:36:206:[step-41100/88900: 46.23%]--[loss-2.841710: wl-3.816000, gl-1.887709]--[lr: pb-0.000050, pf-0.000050]--[ETA-8:58:46]
2023.03.02-11:03:46:306:[step-41200/88900: 46.34%]--[loss-3.403130: wl-5.252617, gl-2.089975]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:09:22]
2023.03.02-11:04:57:406:[step-41300/88900: 46.46%]--[loss-2.973406: wl-3.722211, gl-2.042853]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:58:47]
2023.03.02-11:06:06:506:[step-41400/88900: 46.57%]--[loss-3.098226: wl-4.159054, gl-2.058463]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:43:25]
2023.03.02-11:07:17:606:[step-41500/88900: 46.68%]--[loss-2.976846: wl-3.872118, gl-2.008816]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:19:33]
2023.03.02-11:08:27:706:[step-41600/88900: 46.79%]--[loss-3.091762: wl-4.165301, gl-2.050437]--[lr: pb-0.000050, pf-0.000050]--[ETA-8:33:24]
2023.03.02-11:09:37:806:[step-41700/88900: 46.91%]--[loss-3.337270: wl-4.973502, gl-2.093894]--[lr: pb-0.000050, pf-0.000050]--[ETA-8:49:04]
End of epoch 47 / 100: train_loss: 3.166 	 time: 634 sec
Saving the model at the end of epoch 47, iters 41783
2023.03.02-11:11:01:17:[step-41800/88900: 47.02%]--[loss-2.942608: wl-3.911308, gl-1.964781]--[lr: pb-0.000050, pf-0.000050]--[ETA-8:52:01]
2023.03.02-11:12:12:117:[step-41900/88900: 47.13%]--[loss-3.047088: wl-4.010724, gl-2.044407]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:19:07]
2023.03.02-11:13:22:217:[step-42000/88900: 47.24%]--[loss-3.015135: wl-3.978780, gl-2.020440]--[lr: pb-0.000050, pf-0.000050]--[ETA-8:29:22]
2023.03.02-11:14:33:317:[step-42100/88900: 47.36%]--[loss-3.118535: wl-4.179292, gl-2.073712]--[lr: pb-0.000050, pf-0.000050]--[ETA-8:48:53]
2023.03.02-11:15:44:417:[step-42200/88900: 47.47%]--[loss-2.872591: wl-3.700587, gl-1.947444]--[lr: pb-0.000050, pf-0.000050]--[ETA-8:34:46]
2023.03.02-11:16:53:517:[step-42300/88900: 47.58%]--[loss-3.407722: wl-5.036773, gl-2.148529]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:42:34]
2023.03.02-11:18:03:617:[step-42400/88900: 47.69%]--[loss-3.111914: wl-4.272550, gl-2.043777]--[lr: pb-0.000050, pf-0.000050]--[ETA-8:40:15]
2023.03.02-11:19:13:717:[step-42500/88900: 47.81%]--[loss-3.520321: wl-5.098595, gl-2.245672]--[lr: pb-0.000050, pf-0.000050]--[ETA-8:54:41]
2023.03.02-11:20:23:817:[step-42600/88900: 47.92%]--[loss-3.173389: wl-4.331310, gl-2.090562]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:05:02]
End of epoch 48 / 100: train_loss: 3.157 	 time: 637 sec
Saving the model at the end of epoch 48, iters 42672
2023.03.02-11:21:45:28:[step-42700/88900: 48.03%]--[loss-3.411880: wl-4.895547, gl-2.187993]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:30:05]
2023.03.02-11:22:56:128:[step-42800/88900: 48.14%]--[loss-3.079917: wl-3.981524, gl-2.084536]--[lr: pb-0.000050, pf-0.000050]--[ETA-8:40:30]
2023.03.02-11:24:06:228:[step-42900/88900: 48.26%]--[loss-3.412538: wl-5.060743, gl-2.147352]--[lr: pb-0.000050, pf-0.000050]--[ETA-8:28:09]
2023.03.02-11:25:16:328:[step-43000/88900: 48.37%]--[loss-3.333456: wl-4.625177, gl-2.177161]--[lr: pb-0.000050, pf-0.000050]--[ETA-8:42:26]
2023.03.02-11:26:26:428:[step-43100/88900: 48.48%]--[loss-2.991821: wl-3.775267, gl-2.048004]--[lr: pb-0.000050, pf-0.000050]--[ETA-8:13:52]
2023.03.02-11:27:37:528:[step-43200/88900: 48.59%]--[loss-3.147373: wl-4.247593, gl-2.085474]--[lr: pb-0.000050, pf-0.000050]--[ETA-8:30:40]
2023.03.02-11:28:47:628:[step-43300/88900: 48.71%]--[loss-3.284076: wl-4.657520, gl-2.119696]--[lr: pb-0.000050, pf-0.000050]--[ETA-8:16:26]
2023.03.02-11:29:57:728:[step-43400/88900: 48.82%]--[loss-3.078940: wl-4.306376, gl-2.002346]--[lr: pb-0.000050, pf-0.000050]--[ETA-8:17:35]
2023.03.02-11:31:08:828:[step-43500/88900: 48.93%]--[loss-3.110816: wl-4.059480, gl-2.095946]--[lr: pb-0.000050, pf-0.000050]--[ETA-8:44:04]
End of epoch 49 / 100: train_loss: 3.149 	 time: 636 sec
Saving the model at the end of epoch 49, iters 43561
2023.03.02-11:32:31:39:[step-43600/88900: 49.04%]--[loss-3.419640: wl-4.599341, gl-2.269805]--[lr: pb-0.000050, pf-0.000050]--[ETA-8:25:19]
2023.03.02-11:33:41:139:[step-43700/88900: 49.16%]--[loss-3.464631: wl-5.411471, gl-2.111763]--[lr: pb-0.000050, pf-0.000050]--[ETA-8:32:24]
2023.03.02-11:34:52:239:[step-43800/88900: 49.27%]--[loss-3.211642: wl-4.057372, gl-2.197299]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:17:42]
2023.03.02-11:36:03:339:[step-43900/88900: 49.38%]--[loss-3.218257: wl-3.903399, gl-2.242408]--[lr: pb-0.000050, pf-0.000050]--[ETA-8:21:33]
2023.03.02-11:37:14:439:[step-44000/88900: 49.49%]--[loss-3.486286: wl-4.389389, gl-2.388938]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:53:53]
2023.03.02-11:38:25:539:[step-44100/88900: 49.61%]--[loss-3.372031: wl-3.782118, gl-2.426501]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:54:48]
2023.03.02-11:39:34:639:[step-44200/88900: 49.72%]--[loss-3.458281: wl-4.085500, gl-2.436906]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:01:11]
2023.03.02-11:40:45:739:[step-44300/88900: 49.83%]--[loss-3.464266: wl-4.279321, gl-2.394435]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:26:38]
2023.03.02-11:41:54:839:[step-44400/88900: 49.94%]--[loss-3.045058: wl-3.637043, gl-2.135797]--[lr: pb-0.000050, pf-0.000050]--[ETA-8:46:47]
End of epoch 50 / 100: train_loss: 3.392 	 time: 638 sec
Saving the model at the end of epoch 50, iters 44450
2023.03.02-11:43:16:50:[step-44500/88900: 50.06%]--[loss-3.206216: wl-3.924482, gl-2.225096]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:05:15]
2023.03.02-11:44:27:150:[step-44600/88900: 50.17%]--[loss-3.603772: wl-4.592498, gl-2.455648]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:58:57]
2023.03.02-11:45:37:250:[step-44700/88900: 50.28%]--[loss-3.160719: wl-3.813956, gl-2.207230]--[lr: pb-0.000050, pf-0.000050]--[ETA-8:06:16]
2023.03.02-11:46:48:350:[step-44800/88900: 50.39%]--[loss-3.110203: wl-3.771957, gl-2.167213]--[lr: pb-0.000050, pf-0.000050]--[ETA-9:26:27]
2023.03.02-11:47:59:450:[step-44900/88900: 50.51%]--[loss-3.202704: wl-4.412394, gl-2.099606]--[lr: pb-0.000050, pf-0.000050]--[ETA-8:57:57]
2023.03.02-11:49:09:550:[step-45000/88900: 50.62%]--[loss-3.397778: wl-4.125063, gl-2.366512]--[lr: pb-0.000050, pf-0.000050]--[ETA-10:07:22]
2023.03.02-11:50:29:650:[step-45100/88900: 50.73%]--[loss-3.425962: wl-4.886879, gl-2.204242]--[lr: pb-0.000050, pf-0.000050]--[ETA-11:27:13]
2023.03.02-11:52:05:750:[step-45200/88900: 50.84%]--[loss-3.203514: wl-4.206522, gl-2.151883]--[lr: pb-0.000050, pf-0.000050]--[ETA-8:30:52]
2023.03.02-11:53:31:850:[step-45300/88900: 50.96%]--[loss-3.127757: wl-4.060012, gl-2.112753]--[lr: pb-0.000050, pf-0.000050]--[ETA-8:52:06]
End of epoch 51 / 100: train_loss: 3.371 	 time: 688 sec
Saving the model at the end of epoch 51, iters 45339
2023.03.02-11:54:55:61:[step-45400/88900: 51.07%]--[loss-3.355286: wl-4.393057, gl-2.257021]--[lr: pb-0.000050, pf-0.000049]--[ETA-8:01:53]
2023.03.02-11:56:07:161:[step-45500/88900: 51.18%]--[loss-3.153006: wl-3.892428, gl-2.179899]--[lr: pb-0.000050, pf-0.000049]--[ETA-8:34:24]
2023.03.02-11:57:18:261:[step-45600/88900: 51.29%]--[loss-3.226003: wl-4.335520, gl-2.142123]--[lr: pb-0.000050, pf-0.000049]--[ETA-8:14:12]
2023.03.02-11:58:29:361:[step-45700/88900: 51.41%]--[loss-3.314444: wl-4.111639, gl-2.286534]--[lr: pb-0.000050, pf-0.000049]--[ETA-9:15:22]
2023.03.02-11:59:39:461:[step-45800/88900: 51.52%]--[loss-3.045471: wl-3.822155, gl-2.089932]--[lr: pb-0.000050, pf-0.000049]--[ETA-8:15:33]
2023.03.02-12:00:50:561:[step-45900/88900: 51.63%]--[loss-3.569255: wl-4.522495, gl-2.438631]--[lr: pb-0.000050, pf-0.000049]--[ETA-8:18:49]
2023.03.02-12:02:00:661:[step-46000/88900: 51.74%]--[loss-3.634621: wl-4.329991, gl-2.552124]--[lr: pb-0.000050, pf-0.000049]--[ETA-8:17:59]
2023.03.02-12:03:09:761:[step-46100/88900: 51.86%]--[loss-3.279746: wl-4.388216, gl-2.182692]--[lr: pb-0.000050, pf-0.000049]--[ETA-8:02:49]
2023.03.02-12:04:20:861:[step-46200/88900: 51.97%]--[loss-3.213669: wl-3.939780, gl-2.228724]--[lr: pb-0.000050, pf-0.000049]--[ETA-8:36:28]
End of epoch 52 / 100: train_loss: 3.358 	 time: 641 sec
Saving the model at the end of epoch 52, iters 46228
2023.03.02-12:05:42:72:[step-46300/88900: 52.08%]--[loss-3.175100: wl-4.092457, gl-2.151986]--[lr: pb-0.000050, pf-0.000048]--[ETA-8:42:42]
2023.03.02-12:06:52:172:[step-46400/88900: 52.19%]--[loss-3.394572: wl-4.089633, gl-2.372164]--[lr: pb-0.000050, pf-0.000048]--[ETA-8:47:03]
2023.03.02-12:08:04:272:[step-46500/88900: 52.31%]--[loss-3.370636: wl-4.704565, gl-2.194494]--[lr: pb-0.000050, pf-0.000048]--[ETA-8:25:48]
2023.03.02-12:09:14:372:[step-46600/88900: 52.42%]--[loss-3.281451: wl-4.258956, gl-2.216712]--[lr: pb-0.000050, pf-0.000048]--[ETA-8:47:49]
2023.03.02-12:10:25:472:[step-46700/88900: 52.53%]--[loss-3.462010: wl-4.053502, gl-2.448634]--[lr: pb-0.000050, pf-0.000048]--[ETA-8:42:03]
2023.03.02-12:11:36:572:[step-46800/88900: 52.64%]--[loss-3.500284: wl-4.342235, gl-2.414726]--[lr: pb-0.000050, pf-0.000048]--[ETA-7:58:29]
2023.03.02-12:12:46:672:[step-46900/88900: 52.76%]--[loss-3.546700: wl-4.810597, gl-2.344051]--[lr: pb-0.000050, pf-0.000048]--[ETA-7:48:05]
2023.03.02-12:14:11:772:[step-47000/88900: 52.87%]--[loss-3.238904: wl-4.061461, gl-2.223539]--[lr: pb-0.000050, pf-0.000048]--[ETA-10:40:17]
2023.03.02-12:15:49:872:[step-47100/88900: 52.98%]--[loss-3.398162: wl-4.329926, gl-2.315680]--[lr: pb-0.000050, pf-0.000048]--[ETA-1 day, 13:34:56]
End of epoch 53 / 100: train_loss: 3.340 	 time: 692 sec
Saving the model at the end of epoch 53, iters 47117
2023.03.02-12:17:24:83:[step-47200/88900: 53.09%]--[loss-3.168404: wl-4.145446, gl-2.132043]--[lr: pb-0.000050, pf-0.000047]--[ETA-7:32:00]
2023.03.02-12:18:34:183:[step-47300/88900: 53.21%]--[loss-3.526911: wl-4.547799, gl-2.389962]--[lr: pb-0.000050, pf-0.000047]--[ETA-7:27:03]
2023.03.02-12:19:45:283:[step-47400/88900: 53.32%]--[loss-3.466404: wl-4.391850, gl-2.368442]--[lr: pb-0.000050, pf-0.000047]--[ETA-7:45:43]
2023.03.02-12:20:55:383:[step-47500/88900: 53.43%]--[loss-3.460719: wl-4.264338, gl-2.394634]--[lr: pb-0.000050, pf-0.000047]--[ETA-8:10:29]
2023.03.02-12:22:06:483:[step-47600/88900: 53.54%]--[loss-3.264882: wl-4.479684, gl-2.144961]--[lr: pb-0.000050, pf-0.000047]--[ETA-7:32:08]
2023.03.02-12:23:18:583:[step-47700/88900: 53.66%]--[loss-3.278166: wl-4.179891, gl-2.233193]--[lr: pb-0.000050, pf-0.000047]--[ETA-9:24:23]
2023.03.02-12:24:29:683:[step-47800/88900: 53.77%]--[loss-3.068937: wl-3.868601, gl-2.101787]--[lr: pb-0.000050, pf-0.000047]--[ETA-7:30:21]
2023.03.02-12:25:41:783:[step-47900/88900: 53.88%]--[loss-3.462494: wl-4.511129, gl-2.334712]--[lr: pb-0.000050, pf-0.000047]--[ETA-7:48:17]
2023.03.02-12:26:52:883:[step-48000/88900: 53.99%]--[loss-3.282676: wl-4.138138, gl-2.248142]--[lr: pb-0.000050, pf-0.000047]--[ETA-7:34:19]
End of epoch 54 / 100: train_loss: 3.330 	 time: 644 sec
Saving the model at the end of epoch 54, iters 48006
2023.03.02-12:28:16:94:[step-48100/88900: 54.11%]--[loss-3.644853: wl-4.325954, gl-2.563365]--[lr: pb-0.000050, pf-0.000046]--[ETA-7:57:45]
2023.03.02-12:29:28:194:[step-48200/88900: 54.22%]--[loss-3.395613: wl-4.184096, gl-2.349589]--[lr: pb-0.000050, pf-0.000046]--[ETA-9:03:34]
2023.03.02-12:30:40:294:[step-48300/88900: 54.33%]--[loss-3.246065: wl-3.850013, gl-2.283561]--[lr: pb-0.000050, pf-0.000046]--[ETA-7:43:28]
2023.03.02-12:31:51:394:[step-48400/88900: 54.44%]--[loss-3.380402: wl-4.311792, gl-2.302454]--[lr: pb-0.000050, pf-0.000046]--[ETA-7:29:33]
2023.03.02-12:33:03:494:[step-48500/88900: 54.56%]--[loss-2.920518: wl-3.683645, gl-1.999606]--[lr: pb-0.000050, pf-0.000046]--[ETA-7:38:17]
2023.03.02-12:34:14:594:[step-48600/88900: 54.67%]--[loss-3.271366: wl-3.760695, gl-2.331192]--[lr: pb-0.000050, pf-0.000046]--[ETA-7:34:02]
2023.03.02-12:35:24:694:[step-48700/88900: 54.78%]--[loss-3.396304: wl-4.595436, gl-2.247445]--[lr: pb-0.000050, pf-0.000046]--[ETA-7:32:00]
2023.03.02-12:36:36:794:[step-48800/88900: 54.89%]--[loss-3.196428: wl-4.120692, gl-2.166255]--[lr: pb-0.000050, pf-0.000046]--[ETA-7:37:50]
End of epoch 55 / 100: train_loss: 3.322 	 time: 646 sec
Saving the model at the end of epoch 55, iters 48895
2023.03.02-12:38:01:5:[step-48900/88900: 55.01%]--[loss-3.100908: wl-3.910429, gl-2.123301]--[lr: pb-0.000050, pf-0.000045]--[ETA-7:54:36]
2023.03.02-12:39:13:105:[step-49000/88900: 55.12%]--[loss-3.437298: wl-4.849155, gl-2.225009]--[lr: pb-0.000050, pf-0.000045]--[ETA-8:17:37]
2023.03.02-12:40:24:205:[step-49100/88900: 55.23%]--[loss-3.548721: wl-4.483524, gl-2.427840]--[lr: pb-0.000050, pf-0.000045]--[ETA-7:31:52]
2023.03.02-12:41:35:305:[step-49200/88900: 55.34%]--[loss-3.268014: wl-3.859087, gl-2.303243]--[lr: pb-0.000050, pf-0.000045]--[ETA-8:08:14]
2023.03.02-12:42:47:405:[step-49300/88900: 55.46%]--[loss-3.142468: wl-3.705446, gl-2.216107]--[lr: pb-0.000050, pf-0.000045]--[ETA-7:06:22]
2023.03.02-12:43:55:505:[step-49400/88900: 55.57%]--[loss-3.227024: wl-4.194829, gl-2.178317]--[lr: pb-0.000050, pf-0.000045]--[ETA-6:58:32]
2023.03.02-12:45:00:605:[step-49500/88900: 55.68%]--[loss-3.443156: wl-4.896693, gl-2.218983]--[lr: pb-0.000050, pf-0.000045]--[ETA-7:07:54]
2023.03.02-12:46:04:705:[step-49600/88900: 55.79%]--[loss-3.177692: wl-3.976091, gl-2.183670]--[lr: pb-0.000050, pf-0.000045]--[ETA-7:00:16]
2023.03.02-12:47:09:805:[step-49700/88900: 55.91%]--[loss-3.611597: wl-4.653323, gl-2.448266]--[lr: pb-0.000050, pf-0.000045]--[ETA-6:59:59]
End of epoch 56 / 100: train_loss: 3.311 	 time: 619 sec
Saving the model at the end of epoch 56, iters 49784
2023.03.02-12:48:25:16:[step-49800/88900: 56.02%]--[loss-3.112571: wl-3.910114, gl-2.135042]--[lr: pb-0.000050, pf-0.000044]--[ETA-7:06:25]
2023.03.02-12:49:31:116:[step-49900/88900: 56.13%]--[loss-3.583794: wl-5.388613, gl-2.236641]--[lr: pb-0.000050, pf-0.000044]--[ETA-7:05:53]
2023.03.02-12:50:36:216:[step-50000/88900: 56.24%]--[loss-3.441319: wl-4.308937, gl-2.364085]--[lr: pb-0.000050, pf-0.000044]--[ETA-7:03:42]
2023.03.02-12:51:41:316:[step-50100/88900: 56.36%]--[loss-3.215908: wl-4.113796, gl-2.187459]--[lr: pb-0.000050, pf-0.000044]--[ETA-7:04:20]
2023.03.02-12:52:47:416:[step-50200/88900: 56.47%]--[loss-3.456712: wl-4.227399, gl-2.399862]--[lr: pb-0.000050, pf-0.000044]--[ETA-6:58:35]
2023.03.02-12:53:52:516:[step-50300/88900: 56.58%]--[loss-3.526219: wl-4.222395, gl-2.470620]--[lr: pb-0.000050, pf-0.000044]--[ETA-6:50:04]
2023.03.02-12:54:56:616:[step-50400/88900: 56.69%]--[loss-3.226028: wl-4.216814, gl-2.171824]--[lr: pb-0.000050, pf-0.000044]--[ETA-6:54:33]
2023.03.02-12:55:59:716:[step-50500/88900: 56.81%]--[loss-3.395784: wl-4.423471, gl-2.289916]--[lr: pb-0.000050, pf-0.000044]--[ETA-6:51:20]
2023.03.02-12:57:04:816:[step-50600/88900: 56.92%]--[loss-3.191309: wl-3.883461, gl-2.220444]--[lr: pb-0.000050, pf-0.000044]--[ETA-6:46:02]
End of epoch 57 / 100: train_loss: 3.301 	 time: 586 sec
Saving the model at the end of epoch 57, iters 50673
2023.03.02-12:58:19:27:[step-50700/88900: 57.03%]--[loss-3.516644: wl-4.558463, gl-2.377029]--[lr: pb-0.000050, pf-0.000043]--[ETA-6:52:34]
2023.03.02-12:59:24:127:[step-50800/88900: 57.14%]--[loss-3.127624: wl-3.991292, gl-2.129801]--[lr: pb-0.000050, pf-0.000043]--[ETA-6:53:11]
2023.03.02-13:00:29:227:[step-50900/88900: 57.26%]--[loss-3.313217: wl-4.505356, gl-2.186878]--[lr: pb-0.000050, pf-0.000043]--[ETA-6:49:44]
2023.03.02-13:01:34:327:[step-51000/88900: 57.37%]--[loss-3.285945: wl-4.498070, gl-2.161427]--[lr: pb-0.000050, pf-0.000043]--[ETA-6:44:46]
2023.03.02-13:02:38:427:[step-51100/88900: 57.48%]--[loss-3.374880: wl-4.355088, gl-2.286108]--[lr: pb-0.000050, pf-0.000043]--[ETA-6:43:15]
2023.03.02-13:03:42:527:[step-51200/88900: 57.59%]--[loss-3.137106: wl-3.935129, gl-2.153324]--[lr: pb-0.000050, pf-0.000043]--[ETA-6:41:40]
2023.03.02-13:04:46:627:[step-51300/88900: 57.71%]--[loss-3.187140: wl-3.883477, gl-2.216271]--[lr: pb-0.000050, pf-0.000043]--[ETA-6:38:55]
2023.03.02-13:05:51:727:[step-51400/88900: 57.82%]--[loss-3.338768: wl-3.856829, gl-2.374561]--[lr: pb-0.000050, pf-0.000043]--[ETA-6:43:48]
2023.03.02-13:06:55:827:[step-51500/88900: 57.93%]--[loss-3.547844: wl-4.459574, gl-2.432951]--[lr: pb-0.000050, pf-0.000043]--[ETA-6:39:48]
End of epoch 58 / 100: train_loss: 3.289 	 time: 585 sec
Saving the model at the end of epoch 58, iters 51562
2023.03.02-13:08:13:38:[step-51600/88900: 58.04%]--[loss-3.314816: wl-4.557479, gl-2.175447]--[lr: pb-0.000050, pf-0.000042]--[ETA-6:53:23]
2023.03.02-13:09:20:138:[step-51700/88900: 58.16%]--[loss-3.012263: wl-3.776963, gl-2.068022]--[lr: pb-0.000050, pf-0.000042]--[ETA-6:48:20]
2023.03.02-13:10:27:238:[step-51800/88900: 58.27%]--[loss-3.494062: wl-4.494869, gl-2.370345]--[lr: pb-0.000050, pf-0.000042]--[ETA-6:54:02]
2023.03.02-13:11:34:338:[step-51900/88900: 58.38%]--[loss-3.167127: wl-4.091713, gl-2.144199]--[lr: pb-0.000050, pf-0.000042]--[ETA-6:45:55]
2023.03.02-13:12:41:438:[step-52000/88900: 58.49%]--[loss-3.429944: wl-4.249613, gl-2.367540]--[lr: pb-0.000050, pf-0.000042]--[ETA-6:51:02]
2023.03.02-13:13:47:538:[step-52100/88900: 58.61%]--[loss-3.343663: wl-4.580085, gl-2.198642]--[lr: pb-0.000050, pf-0.000042]--[ETA-6:48:41]
2023.03.02-13:14:54:638:[step-52200/88900: 58.72%]--[loss-3.257750: wl-3.964488, gl-2.266628]--[lr: pb-0.000050, pf-0.000042]--[ETA-6:46:51]
2023.03.02-13:16:00:738:[step-52300/88900: 58.83%]--[loss-3.284362: wl-4.366294, gl-2.192789]--[lr: pb-0.000050, pf-0.000042]--[ETA-6:44:43]
2023.03.02-13:17:06:838:[step-52400/88900: 58.94%]--[loss-3.327421: wl-4.843661, gl-2.116506]--[lr: pb-0.000050, pf-0.000042]--[ETA-6:42:31]
End of epoch 59 / 100: train_loss: 3.284 	 time: 602 sec
Saving the model at the end of epoch 59, iters 52451
2023.03.02-13:18:25:49:[step-52500/88900: 59.06%]--[loss-3.448937: wl-4.169250, gl-2.406624]--[lr: pb-0.000050, pf-0.000041]--[ETA-6:45:41]
2023.03.02-13:19:32:149:[step-52600/88900: 59.17%]--[loss-3.376198: wl-4.435706, gl-2.267272]--[lr: pb-0.000050, pf-0.000041]--[ETA-6:39:59]
2023.03.02-13:20:38:249:[step-52700/88900: 59.28%]--[loss-3.391426: wl-4.883342, gl-2.170591]--[lr: pb-0.000050, pf-0.000041]--[ETA-6:45:46]
2023.03.02-13:21:45:349:[step-52800/88900: 59.39%]--[loss-3.340119: wl-4.163331, gl-2.299286]--[lr: pb-0.000050, pf-0.000041]--[ETA-6:41:19]
2023.03.02-13:22:51:449:[step-52900/88900: 59.51%]--[loss-2.874683: wl-3.615049, gl-1.970920]--[lr: pb-0.000050, pf-0.000041]--[ETA-6:41:54]
2023.03.02-13:23:56:549:[step-53000/88900: 59.62%]--[loss-3.043571: wl-3.740428, gl-2.108464]--[lr: pb-0.000050, pf-0.000041]--[ETA-6:28:01]
2023.03.02-13:25:01:649:[step-53100/88900: 59.73%]--[loss-3.272845: wl-4.399491, gl-2.172972]--[lr: pb-0.000050, pf-0.000041]--[ETA-6:24:16]
2023.03.02-13:26:05:749:[step-53200/88900: 59.84%]--[loss-3.089372: wl-3.822715, gl-2.133693]--[lr: pb-0.000050, pf-0.000041]--[ETA-6:23:00]
2023.03.02-13:27:09:849:[step-53300/88900: 59.96%]--[loss-3.161348: wl-4.014153, gl-2.157810]--[lr: pb-0.000050, pf-0.000041]--[ETA-8:03:54]
End of epoch 60 / 100: train_loss: 3.272 	 time: 595 sec
Saving the model at the end of epoch 60, iters 53340
2023.03.02-13:28:24:60:[step-53400/88900: 60.07%]--[loss-3.267214: wl-4.063625, gl-2.251308]--[lr: pb-0.000050, pf-0.000040]--[ETA-6:24:49]
2023.03.02-13:29:29:160:[step-53500/88900: 60.18%]--[loss-3.193334: wl-3.673904, gl-2.274858]--[lr: pb-0.000050, pf-0.000040]--[ETA-6:22:14]
2023.03.02-13:30:35:260:[step-53600/88900: 60.29%]--[loss-3.324860: wl-4.053980, gl-2.311365]--[lr: pb-0.000050, pf-0.000040]--[ETA-6:26:52]
2023.03.02-13:31:41:360:[step-53700/88900: 60.40%]--[loss-3.024725: wl-3.740804, gl-2.089524]--[lr: pb-0.000050, pf-0.000040]--[ETA-6:35:49]
2023.03.02-13:32:46:460:[step-53800/88900: 60.52%]--[loss-3.256726: wl-4.107874, gl-2.229758]--[lr: pb-0.000050, pf-0.000040]--[ETA-6:18:57]
2023.03.02-13:33:51:560:[step-53900/88900: 60.63%]--[loss-3.392319: wl-4.640485, gl-2.232198]--[lr: pb-0.000050, pf-0.000040]--[ETA-6:16:44]
2023.03.02-13:34:56:660:[step-54000/88900: 60.74%]--[loss-3.449785: wl-4.288213, gl-2.377732]--[lr: pb-0.000050, pf-0.000040]--[ETA-6:14:52]
2023.03.02-13:36:01:760:[step-54100/88900: 60.85%]--[loss-3.185274: wl-4.073284, gl-2.166953]--[lr: pb-0.000050, pf-0.000040]--[ETA-6:16:01]
2023.03.02-13:37:06:860:[step-54200/88900: 60.97%]--[loss-3.125564: wl-3.764175, gl-2.184520]--[lr: pb-0.000050, pf-0.000040]--[ETA-6:20:08]
End of epoch 61 / 100: train_loss: 3.260 	 time: 589 sec
Saving the model at the end of epoch 61, iters 54229
2023.03.02-13:38:21:71:[step-54300/88900: 61.08%]--[loss-3.099483: wl-3.910555, gl-2.121845]--[lr: pb-0.000050, pf-0.000039]--[ETA-6:09:52]
2023.03.02-13:39:26:171:[step-54400/88900: 61.19%]--[loss-3.481484: wl-4.315790, gl-2.402536]--[lr: pb-0.000050, pf-0.000039]--[ETA-6:16:25]
2023.03.02-13:40:30:271:[step-54500/88900: 61.30%]--[loss-3.151933: wl-4.343213, gl-2.066130]--[lr: pb-0.000050, pf-0.000039]--[ETA-6:09:53]
2023.03.02-13:41:35:371:[step-54600/88900: 61.42%]--[loss-3.523664: wl-5.036375, gl-2.264571]--[lr: pb-0.000050, pf-0.000039]--[ETA-6:08:18]
2023.03.02-13:42:41:471:[step-54700/88900: 61.53%]--[loss-3.099449: wl-4.302247, gl-2.023887]--[lr: pb-0.000050, pf-0.000039]--[ETA-6:05:21]
2023.03.02-13:43:45:571:[step-54800/88900: 61.64%]--[loss-3.233179: wl-4.218311, gl-2.178601]--[lr: pb-0.000050, pf-0.000039]--[ETA-5:59:23]
2023.03.02-13:44:49:671:[step-54900/88900: 61.75%]--[loss-3.255378: wl-3.980428, gl-2.260271]--[lr: pb-0.000050, pf-0.000039]--[ETA-6:05:53]
2023.03.02-13:45:53:771:[step-55000/88900: 61.87%]--[loss-3.670967: wl-5.165091, gl-2.379694]--[lr: pb-0.000050, pf-0.000039]--[ETA-5:58:14]
2023.03.02-13:46:57:871:[step-55100/88900: 61.98%]--[loss-3.430054: wl-4.660855, gl-2.264840]--[lr: pb-0.000050, pf-0.000039]--[ETA-6:02:23]
End of epoch 62 / 100: train_loss: 3.251 	 time: 583 sec
Saving the model at the end of epoch 62, iters 55118
2023.03.02-13:48:12:82:[step-55200/88900: 62.09%]--[loss-3.198545: wl-4.088422, gl-2.176439]--[lr: pb-0.000050, pf-0.000038]--[ETA-5:59:59]
2023.03.02-13:49:17:182:[step-55300/88900: 62.20%]--[loss-2.996777: wl-3.818704, gl-2.042100]--[lr: pb-0.000050, pf-0.000038]--[ETA-6:17:41]
2023.03.02-13:50:22:282:[step-55400/88900: 62.32%]--[loss-3.412876: wl-4.245474, gl-2.351507]--[lr: pb-0.000050, pf-0.000038]--[ETA-6:07:39]
2023.03.02-13:51:27:382:[step-55500/88900: 62.43%]--[loss-3.156693: wl-3.849349, gl-2.194356]--[lr: pb-0.000050, pf-0.000038]--[ETA-5:58:05]
2023.03.02-13:52:32:482:[step-55600/88900: 62.54%]--[loss-3.439826: wl-4.459407, gl-2.324975]--[lr: pb-0.000050, pf-0.000038]--[ETA-6:02:58]
2023.03.02-13:53:37:582:[step-55700/88900: 62.65%]--[loss-3.464444: wl-4.653100, gl-2.301169]--[lr: pb-0.000050, pf-0.000038]--[ETA-6:00:16]
2023.03.02-13:54:42:682:[step-55800/88900: 62.77%]--[loss-3.313747: wl-4.654529, gl-2.150115]--[lr: pb-0.000050, pf-0.000038]--[ETA-6:00:31]
2023.03.02-13:55:47:782:[step-55900/88900: 62.88%]--[loss-3.710536: wl-5.667707, gl-2.293609]--[lr: pb-0.000050, pf-0.000038]--[ETA-5:56:44]
2023.03.02-13:56:52:882:[step-56000/88900: 62.99%]--[loss-3.432438: wl-4.561527, gl-2.292056]--[lr: pb-0.000050, pf-0.000038]--[ETA-5:52:53]
End of epoch 63 / 100: train_loss: 3.246 	 time: 587 sec
Saving the model at the end of epoch 63, iters 56007
2023.03.02-13:58:09:93:[step-56100/88900: 63.10%]--[loss-3.052156: wl-3.898696, gl-2.077482]--[lr: pb-0.000050, pf-0.000037]--[ETA-6:00:13]
2023.03.02-13:59:14:193:[step-56200/88900: 63.22%]--[loss-3.756264: wl-4.957158, gl-2.516974]--[lr: pb-0.000050, pf-0.000037]--[ETA-5:53:21]
2023.03.02-14:00:20:293:[step-56300/88900: 63.33%]--[loss-3.141000: wl-3.971958, gl-2.148010]--[lr: pb-0.000050, pf-0.000037]--[ETA-5:54:21]
2023.03.02-14:01:25:393:[step-56400/88900: 63.44%]--[loss-3.080172: wl-3.565355, gl-2.188833]--[lr: pb-0.000050, pf-0.000037]--[ETA-5:43:43]
2023.03.02-14:02:29:493:[step-56500/88900: 63.55%]--[loss-3.194368: wl-4.190095, gl-2.146844]--[lr: pb-0.000050, pf-0.000037]--[ETA-5:44:12]
2023.03.02-14:03:33:593:[step-56600/88900: 63.67%]--[loss-3.151388: wl-4.146043, gl-2.114878]--[lr: pb-0.000050, pf-0.000037]--[ETA-5:51:55]
2023.03.02-14:04:37:693:[step-56700/88900: 63.78%]--[loss-2.967428: wl-3.694331, gl-2.043845]--[lr: pb-0.000050, pf-0.000037]--[ETA-5:41:13]
2023.03.02-14:05:41:793:[step-56800/88900: 63.89%]--[loss-3.599716: wl-4.910713, gl-2.372037]--[lr: pb-0.000050, pf-0.000037]--[ETA-5:43:36]
End of epoch 64 / 100: train_loss: 3.231 	 time: 585 sec
Saving the model at the end of epoch 64, iters 56896
2023.03.02-14:06:54:4:[step-56900/88900: 64.00%]--[loss-3.098708: wl-3.868445, gl-2.131597]--[lr: pb-0.000050, pf-0.000036]--[ETA-5:48:10]
2023.03.02-14:08:00:104:[step-57000/88900: 64.12%]--[loss-3.051072: wl-3.987674, gl-2.054154]--[lr: pb-0.000050, pf-0.000036]--[ETA-5:46:17]
2023.03.02-14:09:05:204:[step-57100/88900: 64.23%]--[loss-3.084811: wl-3.880105, gl-2.114785]--[lr: pb-0.000050, pf-0.000036]--[ETA-5:46:16]
2023.03.02-14:10:10:304:[step-57200/88900: 64.34%]--[loss-3.231330: wl-4.305738, gl-2.154895]--[lr: pb-0.000050, pf-0.000036]--[ETA-5:47:29]
2023.03.02-14:11:16:404:[step-57300/88900: 64.45%]--[loss-3.135508: wl-3.748963, gl-2.198267]--[lr: pb-0.000050, pf-0.000036]--[ETA-5:44:17]
2023.03.02-14:12:21:504:[step-57400/88900: 64.57%]--[loss-3.374414: wl-4.101395, gl-2.349066]--[lr: pb-0.000050, pf-0.000036]--[ETA-5:43:15]
2023.03.02-14:13:26:604:[step-57500/88900: 64.68%]--[loss-3.245998: wl-4.361603, gl-2.155597]--[lr: pb-0.000050, pf-0.000036]--[ETA-5:39:12]
2023.03.02-14:14:32:704:[step-57600/88900: 64.79%]--[loss-3.218875: wl-3.961298, gl-2.228551]--[lr: pb-0.000050, pf-0.000036]--[ETA-5:37:56]
2023.03.02-14:15:37:804:[step-57700/88900: 64.90%]--[loss-3.391663: wl-4.220668, gl-2.336496]--[lr: pb-0.000050, pf-0.000036]--[ETA-5:37:10]
End of epoch 65 / 100: train_loss: 3.226 	 time: 598 sec
Saving the model at the end of epoch 65, iters 57785
2023.03.02-14:17:06:15:[step-57800/88900: 65.02%]--[loss-3.109404: wl-3.837059, gl-2.150139]--[lr: pb-0.000050, pf-0.000035]--[ETA-8:15:06]
2023.03.02-14:18:29:115:[step-57900/88900: 65.13%]--[loss-3.338365: wl-4.202313, gl-2.287787]--[lr: pb-0.000050, pf-0.000035]--[ETA-5:38:31]
2023.03.02-14:19:59:215:[step-58000/88900: 65.24%]--[loss-3.280594: wl-4.106753, gl-2.253906]--[lr: pb-0.000050, pf-0.000035]--[ETA-8:55:50]
2023.03.02-14:21:19:315:[step-58100/88900: 65.35%]--[loss-3.306943: wl-4.648324, gl-2.144862]--[lr: pb-0.000050, pf-0.000035]--[ETA-5:47:45]
2023.03.02-14:22:29:415:[step-58200/88900: 65.47%]--[loss-3.032049: wl-3.869024, gl-2.064793]--[lr: pb-0.000050, pf-0.000035]--[ETA-5:50:19]
2023.03.02-14:24:04:515:[step-58300/88900: 65.58%]--[loss-3.259182: wl-4.665653, gl-2.092768]--[lr: pb-0.000050, pf-0.000035]--[ETA-1 day, 10:35:18]
2023.03.02-14:25:50:615:[step-58400/88900: 65.69%]--[loss-3.296501: wl-4.141812, gl-2.261048]--[lr: pb-0.000050, pf-0.000035]--[ETA-5:46:50]
2023.03.02-14:27:00:715:[step-58500/88900: 65.80%]--[loss-2.996988: wl-3.931184, gl-2.014192]--[lr: pb-0.000050, pf-0.000035]--[ETA-5:46:13]
2023.03.02-14:28:28:815:[step-58600/88900: 65.92%]--[loss-3.326665: wl-4.332676, gl-2.243496]--[lr: pb-0.000050, pf-0.000035]--[ETA-7:48:43]
End of epoch 66 / 100: train_loss: 3.216 	 time: 776 sec
Saving the model at the end of epoch 66, iters 58674
2023.03.02-14:30:50:26:[step-58700/88900: 66.03%]--[loss-3.160048: wl-3.921224, gl-2.179741]--[lr: pb-0.000050, pf-0.000034]--[ETA-5:51:18]
2023.03.02-14:32:00:126:[step-58800/88900: 66.14%]--[loss-3.035138: wl-3.930906, gl-2.052412]--[lr: pb-0.000050, pf-0.000034]--[ETA-5:47:46]
2023.03.02-14:33:13:226:[step-58900/88900: 66.25%]--[loss-3.334717: wl-4.868512, gl-2.117589]--[lr: pb-0.000050, pf-0.000034]--[ETA-9:11:16]
2023.03.02-14:34:52:326:[step-59000/88900: 66.37%]--[loss-3.207443: wl-3.757897, gl-2.267968]--[lr: pb-0.000050, pf-0.000034]--[ETA-7:29:49]
2023.03.02-14:36:25:426:[step-59100/88900: 66.48%]--[loss-3.264574: wl-4.079198, gl-2.244774]--[lr: pb-0.000050, pf-0.000034]--[ETA-8:13:15]
2023.03.02-14:38:33:526:[step-59200/88900: 66.59%]--[loss-2.945489: wl-3.868799, gl-1.978289]--[lr: pb-0.000050, pf-0.000034]--[ETA-1 day, 8:47:36]
2023.03.02-14:40:30:626:[step-59300/88900: 66.70%]--[loss-3.254516: wl-4.050690, gl-2.241843]--[lr: pb-0.000050, pf-0.000034]--[ETA-8:33:37]
2023.03.02-14:42:07:726:[step-59400/88900: 66.82%]--[loss-3.218316: wl-3.952461, gl-2.230200]--[lr: pb-0.000050, pf-0.000034]--[ETA-5:33:56]
2023.03.02-14:44:14:826:[step-59500/88900: 66.93%]--[loss-3.428891: wl-4.259802, gl-2.363940]--[lr: pb-0.000050, pf-0.000034]--[ETA-5:49:02]
End of epoch 67 / 100: train_loss: 3.203 	 time: 934 sec
Saving the model at the end of epoch 67, iters 59563
2023.03.02-14:46:04:37:[step-59600/88900: 67.04%]--[loss-3.032853: wl-4.181717, gl-1.987424]--[lr: pb-0.000050, pf-0.000033]--[ETA-7:53:24]
2023.03.02-14:48:23:137:[step-59700/88900: 67.15%]--[loss-3.437823: wl-4.928865, gl-2.205607]--[lr: pb-0.000050, pf-0.000033]--[ETA-5:28:59]
2023.03.02-14:49:32:237:[step-59800/88900: 67.27%]--[loss-3.272929: wl-4.140712, gl-2.237751]--[lr: pb-0.000050, pf-0.000033]--[ETA-5:35:08]
2023.03.02-14:50:42:337:[step-59900/88900: 67.38%]--[loss-3.046609: wl-3.606922, gl-2.144879]--[lr: pb-0.000050, pf-0.000033]--[ETA-5:34:09]
2023.03.02-14:51:50:437:[step-60000/88900: 67.49%]--[loss-3.424733: wl-4.290579, gl-2.352088]--[lr: pb-0.000050, pf-0.000033]--[ETA-5:39:02]
2023.03.02-14:52:59:537:[step-60100/88900: 67.60%]--[loss-3.229440: wl-3.898481, gl-2.254820]--[lr: pb-0.000050, pf-0.000033]--[ETA-5:16:39]
2023.03.02-14:55:11:637:[step-60200/88900: 67.72%]--[loss-3.164215: wl-4.141401, gl-2.128865]--[lr: pb-0.000050, pf-0.000033]--[ETA-5:56:09]
2023.03.02-14:56:20:737:[step-60300/88900: 67.83%]--[loss-2.879704: wl-3.818215, gl-1.925151]--[lr: pb-0.000050, pf-0.000033]--[ETA-5:28:43]
2023.03.02-14:57:29:837:[step-60400/88900: 67.94%]--[loss-3.214577: wl-4.206258, gl-2.163013]--[lr: pb-0.000050, pf-0.000033]--[ETA-5:26:43]
End of epoch 68 / 100: train_loss: 3.197 	 time: 770 sec
Saving the model at the end of epoch 68, iters 60452
2023.03.02-14:58:49:48:[step-60500/88900: 68.05%]--[loss-3.060899: wl-4.127315, gl-2.029070]--[lr: pb-0.000050, pf-0.000032]--[ETA-5:22:46]
2023.03.02-14:59:59:148:[step-60600/88900: 68.17%]--[loss-3.060032: wl-3.992911, gl-2.061805]--[lr: pb-0.000050, pf-0.000032]--[ETA-5:26:55]
2023.03.02-15:01:09:248:[step-60700/88900: 68.28%]--[loss-3.198303: wl-4.177155, gl-2.154015]--[lr: pb-0.000050, pf-0.000032]--[ETA-5:30:47]
2023.03.02-15:02:19:348:[step-60800/88900: 68.39%]--[loss-3.242282: wl-4.177821, gl-2.197826]--[lr: pb-0.000050, pf-0.000032]--[ETA-5:21:10]
2023.03.02-15:03:28:448:[step-60900/88900: 68.50%]--[loss-3.193673: wl-3.988457, gl-2.196559]--[lr: pb-0.000050, pf-0.000032]--[ETA-5:10:12]
2023.03.02-15:04:37:548:[step-61000/88900: 68.62%]--[loss-3.079374: wl-3.781958, gl-2.133885]--[lr: pb-0.000050, pf-0.000032]--[ETA-5:28:34]
2023.03.02-15:05:46:648:[step-61100/88900: 68.73%]--[loss-3.062706: wl-3.903821, gl-2.086751]--[lr: pb-0.000050, pf-0.000032]--[ETA-5:11:24]
2023.03.02-15:06:55:748:[step-61200/88900: 68.84%]--[loss-2.960300: wl-3.975676, gl-1.966381]--[lr: pb-0.000050, pf-0.000032]--[ETA-5:47:48]
2023.03.02-15:08:04:848:[step-61300/88900: 68.95%]--[loss-3.211892: wl-3.749468, gl-2.274525]--[lr: pb-0.000050, pf-0.000032]--[ETA-5:25:45]
End of epoch 69 / 100: train_loss: 3.188 	 time: 627 sec
Saving the model at the end of epoch 69, iters 61341
2023.03.02-15:09:26:59:[step-61400/88900: 69.07%]--[loss-3.355291: wl-4.423480, gl-2.249422]--[lr: pb-0.000050, pf-0.000031]--[ETA-5:32:07]
2023.03.02-15:10:35:159:[step-61500/88900: 69.18%]--[loss-3.499524: wl-4.897745, gl-2.275087]--[lr: pb-0.000050, pf-0.000031]--[ETA-5:11:46]
2023.03.02-15:11:45:259:[step-61600/88900: 69.29%]--[loss-3.031754: wl-3.893345, gl-2.058417]--[lr: pb-0.000050, pf-0.000031]--[ETA-5:11:40]
2023.03.02-15:12:55:359:[step-61700/88900: 69.40%]--[loss-3.580256: wl-4.473441, gl-2.461896]--[lr: pb-0.000050, pf-0.000031]--[ETA-5:03:07]
2023.03.02-15:14:04:459:[step-61800/88900: 69.52%]--[loss-3.390242: wl-4.547958, gl-2.253252]--[lr: pb-0.000050, pf-0.000031]--[ETA-5:12:33]
2023.03.02-15:15:13:559:[step-61900/88900: 69.63%]--[loss-3.225270: wl-3.968101, gl-2.233245]--[lr: pb-0.000050, pf-0.000031]--[ETA-5:03:51]
2023.03.02-15:16:22:659:[step-62000/88900: 69.74%]--[loss-3.320145: wl-4.339524, gl-2.235264]--[lr: pb-0.000050, pf-0.000031]--[ETA-5:06:01]
2023.03.02-15:17:31:759:[step-62100/88900: 69.85%]--[loss-3.094861: wl-4.048161, gl-2.082820]--[lr: pb-0.000050, pf-0.000031]--[ETA-5:03:30]
2023.03.02-15:18:40:859:[step-62200/88900: 69.97%]--[loss-3.123193: wl-4.110086, gl-2.095671]--[lr: pb-0.000050, pf-0.000031]--[ETA-5:07:29]
End of epoch 70 / 100: train_loss: 3.183 	 time: 627 sec
Saving the model at the end of epoch 70, iters 62230
2023.03.02-15:20:01:70:[step-62300/88900: 70.08%]--[loss-3.557113: wl-4.787528, gl-2.360231]--[lr: pb-0.000050, pf-0.000030]--[ETA-5:10:30]
2023.03.02-15:21:11:170:[step-62400/88900: 70.19%]--[loss-3.057078: wl-4.112747, gl-2.028891]--[lr: pb-0.000050, pf-0.000030]--[ETA-5:18:56]
2023.03.02-15:23:27:270:[step-62500/88900: 70.30%]--[loss-3.314765: wl-4.179946, gl-2.269778]--[lr: pb-0.000050, pf-0.000030]--[ETA-5:18:34]
2023.03.02-15:24:37:370:[step-62600/88900: 70.42%]--[loss-3.342432: wl-4.353488, gl-2.254060]--[lr: pb-0.000050, pf-0.000030]--[ETA-4:54:21]
2023.03.02-15:25:46:470:[step-62700/88900: 70.53%]--[loss-2.781185: wl-3.706465, gl-1.854568]--[lr: pb-0.000050, pf-0.000030]--[ETA-4:51:39]
2023.03.02-15:26:55:570:[step-62800/88900: 70.64%]--[loss-3.220419: wl-4.324477, gl-2.139300]--[lr: pb-0.000050, pf-0.000030]--[ETA-5:06:43]
2023.03.02-15:28:23:670:[step-62900/88900: 70.75%]--[loss-3.077874: wl-4.280465, gl-2.007758]--[lr: pb-0.000050, pf-0.000030]--[ETA-7:37:15]
2023.03.02-15:30:26:770:[step-63000/88900: 70.87%]--[loss-3.343578: wl-4.370939, gl-2.250843]--[lr: pb-0.000050, pf-0.000030]--[ETA-1 day, 1:55:47]
2023.03.02-15:32:04:870:[step-63100/88900: 70.98%]--[loss-3.225689: wl-4.609316, gl-2.073360]--[lr: pb-0.000050, pf-0.000030]--[ETA-4:55:36]
End of epoch 71 / 100: train_loss: 3.172 	 time: 795 sec
Saving the model at the end of epoch 71, iters 63119
2023.03.02-15:33:25:81:[step-63200/88900: 71.09%]--[loss-3.118588: wl-4.082180, gl-2.098043]--[lr: pb-0.000050, pf-0.000029]--[ETA-4:50:37]
2023.03.02-15:34:34:181:[step-63300/88900: 71.20%]--[loss-3.094884: wl-3.759750, gl-2.154946]--[lr: pb-0.000050, pf-0.000029]--[ETA-4:50:14]
2023.03.02-15:35:43:281:[step-63400/88900: 71.32%]--[loss-3.039856: wl-3.923170, gl-2.059063]--[lr: pb-0.000050, pf-0.000029]--[ETA-4:50:14]
2023.03.02-15:36:52:381:[step-63500/88900: 71.43%]--[loss-3.095241: wl-3.944975, gl-2.108997]--[lr: pb-0.000050, pf-0.000029]--[ETA-4:58:41]
2023.03.02-15:38:01:481:[step-63600/88900: 71.54%]--[loss-3.129628: wl-3.983643, gl-2.133717]--[lr: pb-0.000050, pf-0.000029]--[ETA-4:52:03]
2023.03.02-15:39:09:581:[step-63700/88900: 71.65%]--[loss-3.119005: wl-3.872495, gl-2.150882]--[lr: pb-0.000050, pf-0.000029]--[ETA-4:52:39]
2023.03.02-15:40:17:681:[step-63800/88900: 71.77%]--[loss-3.172688: wl-4.212665, gl-2.119522]--[lr: pb-0.000050, pf-0.000029]--[ETA-4:44:27]
2023.03.02-15:41:26:781:[step-63900/88900: 71.88%]--[loss-3.516420: wl-4.858954, gl-2.301682]--[lr: pb-0.000050, pf-0.000029]--[ETA-4:48:35]
2023.03.02-15:42:34:881:[step-64000/88900: 71.99%]--[loss-3.174555: wl-4.079993, gl-2.154557]--[lr: pb-0.000050, pf-0.000029]--[ETA-4:36:00]
End of epoch 72 / 100: train_loss: 3.168 	 time: 622 sec
Saving the model at the end of epoch 72, iters 64008
2023.03.02-15:43:54:92:[step-64100/88900: 72.10%]--[loss-3.087112: wl-4.317987, gl-2.007616]--[lr: pb-0.000050, pf-0.000028]--[ETA-4:40:50]
2023.03.02-15:45:02:192:[step-64200/88900: 72.22%]--[loss-3.381881: wl-3.984952, gl-2.385643]--[lr: pb-0.000050, pf-0.000028]--[ETA-4:36:03]
2023.03.02-15:46:11:292:[step-64300/88900: 72.33%]--[loss-3.211636: wl-4.228600, gl-2.154486]--[lr: pb-0.000050, pf-0.000028]--[ETA-4:32:28]
2023.03.02-15:47:20:392:[step-64400/88900: 72.44%]--[loss-2.920790: wl-3.600971, gl-2.020547]--[lr: pb-0.000050, pf-0.000028]--[ETA-4:36:06]
2023.03.02-15:48:28:492:[step-64500/88900: 72.55%]--[loss-3.062893: wl-3.927512, gl-2.081015]--[lr: pb-0.000050, pf-0.000028]--[ETA-4:34:27]
2023.03.02-15:49:58:592:[step-64600/88900: 72.67%]--[loss-2.989118: wl-4.093771, gl-1.965676]--[lr: pb-0.000050, pf-0.000028]--[ETA-6:24:17]
2023.03.02-15:52:10:692:[step-64700/88900: 72.78%]--[loss-3.480534: wl-5.367980, gl-2.138539]--[lr: pb-0.000050, pf-0.000028]--[ETA-4:24:44]
2023.03.02-15:53:18:792:[step-64800/88900: 72.89%]--[loss-3.170809: wl-4.119855, gl-2.140846]--[lr: pb-0.000050, pf-0.000028]--[ETA-4:37:14]
End of epoch 73 / 100: train_loss: 3.152 	 time: 710 sec
Saving the model at the end of epoch 73, iters 64897
2023.03.02-15:54:45:3:[step-64900/88900: 73.00%]--[loss-3.097177: wl-4.058447, gl-2.082565]--[lr: pb-0.000050, pf-0.000027]--[ETA-4:42:26]
2023.03.02-15:55:53:103:[step-65000/88900: 73.12%]--[loss-3.062966: wl-3.745123, gl-2.126685]--[lr: pb-0.000050, pf-0.000027]--[ETA-4:29:24]
2023.03.02-15:57:01:203:[step-65100/88900: 73.23%]--[loss-3.311791: wl-4.954090, gl-2.073269]--[lr: pb-0.000050, pf-0.000027]--[ETA-4:22:16]
2023.03.02-15:58:05:303:[step-65200/88900: 73.34%]--[loss-3.352886: wl-4.505721, gl-2.226456]--[lr: pb-0.000050, pf-0.000027]--[ETA-4:13:43]
2023.03.02-15:59:11:403:[step-65300/88900: 73.45%]--[loss-2.985102: wl-3.876361, gl-2.016012]--[lr: pb-0.000050, pf-0.000027]--[ETA-4:15:31]
2023.03.02-16:00:34:503:[step-65400/88900: 73.57%]--[loss-2.969529: wl-3.787002, gl-2.022779]--[lr: pb-0.000050, pf-0.000027]--[ETA-6:09:26]
2023.03.02-16:02:08:603:[step-65500/88900: 73.68%]--[loss-3.577334: wl-5.005608, gl-2.325932]--[lr: pb-0.000050, pf-0.000027]--[ETA-6:08:20]
2023.03.02-16:03:40:703:[step-65600/88900: 73.79%]--[loss-3.076397: wl-3.954452, gl-2.087784]--[lr: pb-0.000050, pf-0.000027]--[ETA-4:49:02]
2023.03.02-16:05:15:803:[step-65700/88900: 73.90%]--[loss-3.627137: wl-4.845340, gl-2.415802]--[lr: pb-0.000050, pf-0.000027]--[ETA-6:03:21]
End of epoch 74 / 100: train_loss: 3.148 	 time: 722 sec
Saving the model at the end of epoch 74, iters 65786
2023.03.02-16:06:57:14:[step-65800/88900: 74.02%]--[loss-3.226473: wl-4.042420, gl-2.215868]--[lr: pb-0.000050, pf-0.000026]--[ETA-6:05:45]
2023.03.02-16:08:30:114:[step-65900/88900: 74.13%]--[loss-3.101495: wl-4.079413, gl-2.081642]--[lr: pb-0.000050, pf-0.000026]--[ETA-4:35:51]
2023.03.02-16:10:04:214:[step-66000/88900: 74.24%]--[loss-3.054862: wl-4.076254, gl-2.035799]--[lr: pb-0.000050, pf-0.000026]--[ETA-6:03:11]
2023.03.02-16:11:37:314:[step-66100/88900: 74.35%]--[loss-3.248224: wl-4.402408, gl-2.147622]--[lr: pb-0.000050, pf-0.000026]--[ETA-6:02:20]
2023.03.02-16:13:08:414:[step-66200/88900: 74.47%]--[loss-3.081620: wl-3.936739, gl-2.097435]--[lr: pb-0.000050, pf-0.000026]--[ETA-5:57:47]
2023.03.02-16:14:42:514:[step-66300/88900: 74.58%]--[loss-3.045141: wl-3.967266, gl-2.053325]--[lr: pb-0.000050, pf-0.000026]--[ETA-4:26:12]
2023.03.02-16:16:13:614:[step-66400/88900: 74.69%]--[loss-3.081238: wl-3.804053, gl-2.130224]--[lr: pb-0.000050, pf-0.000026]--[ETA-6:01:58]
2023.03.02-16:17:45:714:[step-66500/88900: 74.80%]--[loss-2.873884: wl-3.536014, gl-1.989881]--[lr: pb-0.000050, pf-0.000026]--[ETA-5:51:11]
2023.03.02-16:19:17:814:[step-66600/88900: 74.92%]--[loss-2.881001: wl-3.555537, gl-1.992117]--[lr: pb-0.000050, pf-0.000026]--[ETA-3:58:35]
End of epoch 75 / 100: train_loss: 3.143 	 time: 818 sec
Saving the model at the end of epoch 75, iters 66675
2023.03.02-16:20:39:25:[step-66700/88900: 75.03%]--[loss-3.141284: wl-4.447713, gl-2.029356]--[lr: pb-0.000050, pf-0.000025]--[ETA-4:02:49]
2023.03.02-16:21:44:125:[step-66800/88900: 75.14%]--[loss-3.286344: wl-4.447681, gl-2.174424]--[lr: pb-0.000050, pf-0.000025]--[ETA-4:11:43]
2023.03.02-16:22:54:225:[step-66900/88900: 75.25%]--[loss-3.561749: wl-4.408100, gl-2.459725]--[lr: pb-0.000050, pf-0.000025]--[ETA-4:02:25]
2023.03.02-16:24:03:325:[step-67000/88900: 75.37%]--[loss-3.071548: wl-4.088181, gl-2.049503]--[lr: pb-0.000050, pf-0.000025]--[ETA-4:24:01]
2023.03.02-16:25:31:425:[step-67100/88900: 75.48%]--[loss-3.152036: wl-4.056024, gl-2.138030]--[lr: pb-0.000050, pf-0.000025]--[ETA-6:19:22]
2023.03.02-16:27:45:525:[step-67200/88900: 75.59%]--[loss-3.429930: wl-4.664821, gl-2.263725]--[lr: pb-0.000050, pf-0.000025]--[ETA-19:16:31]
2023.03.02-16:29:20:625:[step-67300/88900: 75.70%]--[loss-2.907728: wl-3.882681, gl-1.937058]--[lr: pb-0.000050, pf-0.000025]--[ETA-5:53:44]
2023.03.02-16:31:33:725:[step-67400/88900: 75.82%]--[loss-3.074722: wl-3.972569, gl-2.081580]--[lr: pb-0.000050, pf-0.000025]--[ETA-4:34:54]
2023.03.02-16:33:09:825:[step-67500/88900: 75.93%]--[loss-3.185597: wl-3.965200, gl-2.194297]--[lr: pb-0.000050, pf-0.000025]--[ETA-5:32:16]
End of epoch 76 / 100: train_loss: 3.132 	 time: 860 sec
Saving the model at the end of epoch 76, iters 67564
2023.03.02-16:35:21:36:[step-67600/88900: 76.04%]--[loss-2.988191: wl-4.082313, gl-1.967613]--[lr: pb-0.000050, pf-0.000024]--[ETA-5:29:17]
2023.03.02-16:36:54:136:[step-67700/88900: 76.15%]--[loss-3.373932: wl-4.115864, gl-2.344966]--[lr: pb-0.000050, pf-0.000024]--[ETA-4:03:31]
2023.03.02-16:39:04:236:[step-67800/88900: 76.27%]--[loss-3.256797: wl-4.317580, gl-2.177402]--[lr: pb-0.000050, pf-0.000024]--[ETA-5:30:23]
2023.03.02-16:41:14:336:[step-67900/88900: 76.38%]--[loss-2.983143: wl-3.894618, gl-2.009488]--[lr: pb-0.000050, pf-0.000024]--[ETA-4:30:52]
2023.03.02-16:42:50:436:[step-68000/88900: 76.49%]--[loss-2.948801: wl-3.502296, gl-2.073226]--[lr: pb-0.000050, pf-0.000024]--[ETA-5:20:07]
2023.03.02-16:44:53:536:[step-68100/88900: 76.60%]--[loss-3.362606: wl-4.281751, gl-2.292168]--[lr: pb-0.000050, pf-0.000024]--[ETA-5:11:53]
2023.03.02-16:46:30:636:[step-68200/88900: 76.72%]--[loss-3.531934: wl-4.814588, gl-2.328288]--[lr: pb-0.000050, pf-0.000024]--[ETA-5:48:06]
2023.03.02-16:48:39:736:[step-68300/88900: 76.83%]--[loss-2.704384: wl-3.220572, gl-1.899241]--[lr: pb-0.000050, pf-0.000024]--[ETA-5:14:37]
2023.03.02-16:50:12:836:[step-68400/88900: 76.94%]--[loss-2.981028: wl-4.018457, gl-1.976414]--[lr: pb-0.000050, pf-0.000024]--[ETA-4:01:24]
End of epoch 77 / 100: train_loss: 3.127 	 time: 1019 sec
Saving the model at the end of epoch 77, iters 68453
2023.03.02-16:52:32:47:[step-68500/88900: 77.05%]--[loss-3.228459: wl-4.314830, gl-2.149752]--[lr: pb-0.000050, pf-0.000023]--[ETA-5:27:03]
2023.03.02-16:54:38:147:[step-68600/88900: 77.17%]--[loss-2.964271: wl-4.000952, gl-1.964033]--[lr: pb-0.000050, pf-0.000023]--[ETA-5:08:05]
2023.03.02-16:56:15:247:[step-68700/88900: 77.28%]--[loss-3.392819: wl-5.131843, gl-2.109859]--[lr: pb-0.000050, pf-0.000023]--[ETA-5:02:32]
2023.03.02-16:58:24:347:[step-68800/88900: 77.39%]--[loss-3.101432: wl-4.240456, gl-2.041318]--[lr: pb-0.000050, pf-0.000023]--[ETA-3:51:13]
2023.03.02-16:59:32:447:[step-68900/88900: 77.50%]--[loss-3.583105: wl-6.029171, gl-2.075812]--[lr: pb-0.000050, pf-0.000023]--[ETA-3:36:43]
2023.03.02-17:00:40:547:[step-69000/88900: 77.62%]--[loss-3.045117: wl-3.652464, gl-2.132001]--[lr: pb-0.000050, pf-0.000023]--[ETA-3:45:57]
2023.03.02-17:01:49:647:[step-69100/88900: 77.73%]--[loss-3.108405: wl-4.374704, gl-2.014729]--[lr: pb-0.000050, pf-0.000023]--[ETA-3:43:54]
2023.03.02-17:02:57:747:[step-69200/88900: 77.84%]--[loss-3.242926: wl-4.026890, gl-2.236203]--[lr: pb-0.000050, pf-0.000023]--[ETA-3:48:35]
2023.03.02-17:04:06:847:[step-69300/88900: 77.95%]--[loss-3.167835: wl-4.494956, gl-2.044096]--[lr: pb-0.000050, pf-0.000023]--[ETA-3:41:06]
End of epoch 78 / 100: train_loss: 3.114 	 time: 781 sec
Saving the model at the end of epoch 78, iters 69342
2023.03.02-17:05:26:58:[step-69400/88900: 78.07%]--[loss-3.219811: wl-4.180840, gl-2.174601]--[lr: pb-0.000050, pf-0.000022]--[ETA-3:58:06]
2023.03.02-17:06:35:158:[step-69500/88900: 78.18%]--[loss-2.967371: wl-3.946499, gl-1.980746]--[lr: pb-0.000050, pf-0.000022]--[ETA-3:55:15]
2023.03.02-17:07:44:258:[step-69600/88900: 78.29%]--[loss-3.002144: wl-4.035038, gl-1.993384]--[lr: pb-0.000050, pf-0.000022]--[ETA-3:39:44]
2023.03.02-17:08:57:358:[step-69700/88900: 78.40%]--[loss-3.186839: wl-4.365176, gl-2.095545]--[lr: pb-0.000050, pf-0.000022]--[ETA-5:10:07]
2023.03.02-17:10:35:458:[step-69800/88900: 78.52%]--[loss-3.204572: wl-4.185206, gl-2.158271]--[lr: pb-0.000050, pf-0.000022]--[ETA-4:59:17]
2023.03.02-17:12:47:558:[step-69900/88900: 78.63%]--[loss-3.131913: wl-3.728684, gl-2.199742]--[lr: pb-0.000050, pf-0.000022]--[ETA-4:46:27]
2023.03.02-17:14:21:658:[step-70000/88900: 78.74%]--[loss-3.101575: wl-4.045362, gl-2.090235]--[lr: pb-0.000050, pf-0.000022]--[ETA-3:32:03]
2023.03.02-17:16:31:758:[step-70100/88900: 78.85%]--[loss-2.858339: wl-3.506779, gl-1.981644]--[lr: pb-0.000050, pf-0.000022]--[ETA-5:07:42]
2023.03.02-17:18:29:858:[step-70200/88900: 78.97%]--[loss-2.934221: wl-3.618364, gl-2.029630]--[lr: pb-0.000050, pf-0.000022]--[ETA-3:24:22]
End of epoch 79 / 100: train_loss: 3.110 	 time: 861 sec
Saving the model at the end of epoch 79, iters 70231
2023.03.02-17:20:17:69:[step-70300/88900: 79.08%]--[loss-3.030323: wl-4.091833, gl-2.007365]--[lr: pb-0.000050, pf-0.000021]--[ETA-4:57:57]
2023.03.02-17:22:23:169:[step-70400/88900: 79.19%]--[loss-3.173343: wl-4.417528, gl-2.068961]--[lr: pb-0.000050, pf-0.000021]--[ETA-5:06:41]
2023.03.02-17:24:02:269:[step-70500/88900: 79.30%]--[loss-2.850336: wl-3.721159, gl-1.920046]--[lr: pb-0.000050, pf-0.000021]--[ETA-5:17:36]
2023.03.02-17:26:11:369:[step-70600/88900: 79.42%]--[loss-3.178982: wl-4.110091, gl-2.151459]--[lr: pb-0.000050, pf-0.000021]--[ETA-4:56:39]
2023.03.02-17:28:00:469:[step-70700/88900: 79.53%]--[loss-3.124001: wl-3.876086, gl-2.154979]--[lr: pb-0.000050, pf-0.000021]--[ETA-16:49:29]
2023.03.02-17:29:56:569:[step-70800/88900: 79.64%]--[loss-3.028178: wl-3.669524, gl-2.110797]--[lr: pb-0.000050, pf-0.000021]--[ETA-4:43:01]
2023.03.02-17:31:57:669:[step-70900/88900: 79.75%]--[loss-3.303235: wl-4.384413, gl-2.207131]--[lr: pb-0.000050, pf-0.000021]--[ETA-3:43:04]
2023.03.02-17:33:33:769:[step-71000/88900: 79.87%]--[loss-3.022146: wl-3.833633, gl-2.063737]--[lr: pb-0.000050, pf-0.000021]--[ETA-4:49:21]
2023.03.02-17:35:37:869:[step-71100/88900: 79.98%]--[loss-3.112088: wl-3.814976, gl-2.158344]--[lr: pb-0.000050, pf-0.000021]--[ETA-4:48:03]
End of epoch 80 / 100: train_loss: 3.103 	 time: 1019 sec
Saving the model at the end of epoch 80, iters 71120
2023.03.02-17:37:25:80:[step-71200/88900: 80.09%]--[loss-3.103742: wl-4.057921, gl-2.089262]--[lr: pb-0.000050, pf-0.000020]--[ETA-3:29:32]
2023.03.02-17:39:42:180:[step-71300/88900: 80.20%]--[loss-3.011876: wl-3.971577, gl-2.018982]--[lr: pb-0.000050, pf-0.000020]--[ETA-4:30:24]
2023.03.02-17:41:42:280:[step-71400/88900: 80.31%]--[loss-3.135171: wl-4.178251, gl-2.090609]--[lr: pb-0.000050, pf-0.000020]--[ETA-5:41:04]
2023.03.02-17:43:02:380:[step-71500/88900: 80.43%]--[loss-3.217852: wl-4.458408, gl-2.103250]--[lr: pb-0.000050, pf-0.000020]--[ETA-3:16:07]
2023.03.02-17:44:11:480:[step-71600/88900: 80.54%]--[loss-3.003519: wl-3.971514, gl-2.010640]--[lr: pb-0.000050, pf-0.000020]--[ETA-3:09:38]
2023.03.02-17:45:20:580:[step-71700/88900: 80.65%]--[loss-3.150548: wl-4.038577, gl-2.140904]--[lr: pb-0.000050, pf-0.000020]--[ETA-3:26:55]
2023.03.02-17:46:29:680:[step-71800/88900: 80.76%]--[loss-3.021643: wl-3.953901, gl-2.033168]--[lr: pb-0.000050, pf-0.000020]--[ETA-3:11:09]
2023.03.02-17:47:37:780:[step-71900/88900: 80.88%]--[loss-3.011867: wl-3.869517, gl-2.044488]--[lr: pb-0.000050, pf-0.000020]--[ETA-3:07:01]
2023.03.02-17:49:05:880:[step-72000/88900: 80.99%]--[loss-3.094002: wl-4.121046, gl-2.063740]--[lr: pb-0.000050, pf-0.000020]--[ETA-4:22:48]
End of epoch 81 / 100: train_loss: 3.097 	 time: 796 sec
Saving the model at the end of epoch 81, iters 72009
2023.03.02-17:51:20:91:[step-72100/88900: 81.10%]--[loss-2.889556: wl-3.620477, gl-1.984437]--[lr: pb-0.000050, pf-0.000019]--[ETA-4:23:28]
2023.03.02-17:52:58:191:[step-72200/88900: 81.21%]--[loss-2.947403: wl-3.817985, gl-1.992907]--[lr: pb-0.000050, pf-0.000019]--[ETA-4:11:59]
2023.03.02-17:54:58:291:[step-72300/88900: 81.33%]--[loss-2.928661: wl-3.704967, gl-2.002419]--[lr: pb-0.000050, pf-0.000019]--[ETA-4:45:19]
2023.03.02-17:56:36:391:[step-72400/88900: 81.44%]--[loss-2.930699: wl-3.708889, gl-2.003477]--[lr: pb-0.000050, pf-0.000019]--[ETA-3:22:08]
2023.03.02-17:58:39:491:[step-72500/88900: 81.55%]--[loss-3.375451: wl-4.605535, gl-2.224067]--[lr: pb-0.000050, pf-0.000019]--[ETA-4:12:11]
2023.03.02-18:00:12:591:[step-72600/88900: 81.66%]--[loss-3.185366: wl-4.823800, gl-1.979416]--[lr: pb-0.000050, pf-0.000019]--[ETA-3:06:34]
2023.03.02-18:02:31:691:[step-72700/88900: 81.78%]--[loss-2.911624: wl-3.571488, gl-2.018753]--[lr: pb-0.000050, pf-0.000019]--[ETA-4:18:21]
2023.03.02-18:04:41:791:[step-72800/88900: 81.89%]--[loss-3.029163: wl-4.156772, gl-1.989970]--[lr: pb-0.000050, pf-0.000019]--[ETA-16:05:52]
End of epoch 82 / 100: train_loss: 3.089 	 time: 1019 sec
Saving the model at the end of epoch 82, iters 72898
2023.03.02-18:06:28:2:[step-72900/88900: 82.00%]--[loss-3.176184: wl-3.899207, gl-2.201382]--[lr: pb-0.000050, pf-0.000018]--[ETA-4:23:11]
2023.03.02-18:08:46:102:[step-73000/88900: 82.11%]--[loss-3.038286: wl-3.982491, gl-2.042663]--[lr: pb-0.000050, pf-0.000018]--[ETA-4:07:43]
2023.03.02-18:10:25:202:[step-73100/88900: 82.23%]--[loss-2.798708: wl-3.681682, gl-1.878287]--[lr: pb-0.000050, pf-0.000018]--[ETA-4:13:18]
2023.03.02-18:12:45:302:[step-73200/88900: 82.34%]--[loss-3.019493: wl-3.587847, gl-2.122531]--[lr: pb-0.000050, pf-0.000018]--[ETA-3:51:49]
2023.03.02-18:14:23:402:[step-73300/88900: 82.45%]--[loss-2.847136: wl-3.821965, gl-1.891645]--[lr: pb-0.000050, pf-0.000018]--[ETA-4:26:01]
2023.03.02-18:16:44:502:[step-73400/88900: 82.56%]--[loss-3.401659: wl-4.896219, gl-2.177604]--[lr: pb-0.000050, pf-0.000018]--[ETA-4:27:22]
2023.03.02-18:18:19:602:[step-73500/88900: 82.68%]--[loss-2.933434: wl-3.931349, gl-1.950597]--[lr: pb-0.000050, pf-0.000018]--[ETA-2:51:18]
2023.03.02-18:20:45:702:[step-73600/88900: 82.79%]--[loss-2.972929: wl-4.083333, gl-1.952096]--[lr: pb-0.000050, pf-0.000018]--[ETA-4:00:30]
2023.03.02-18:22:35:802:[step-73700/88900: 82.90%]--[loss-3.288082: wl-4.472453, gl-2.169969]--[lr: pb-0.000050, pf-0.000018]--[ETA-13:37:41]
End of epoch 83 / 100: train_loss: 3.080 	 time: 1094 sec
Saving the model at the end of epoch 83, iters 73787
2023.03.02-18:24:55:13:[step-73800/88900: 83.01%]--[loss-3.013767: wl-3.811517, gl-2.060888]--[lr: pb-0.000050, pf-0.000017]--[ETA-3:46:39]
2023.03.02-18:27:17:113:[step-73900/88900: 83.13%]--[loss-3.178184: wl-3.895303, gl-2.204359]--[lr: pb-0.000050, pf-0.000017]--[ETA-3:05:20]
2023.03.02-18:28:55:213:[step-74000/88900: 83.24%]--[loss-3.331156: wl-4.584942, gl-2.184920]--[lr: pb-0.000050, pf-0.000017]--[ETA-3:50:30]
2023.03.02-18:31:00:313:[step-74100/88900: 83.35%]--[loss-3.199160: wl-4.730605, gl-2.016509]--[lr: pb-0.000050, pf-0.000017]--[ETA-3:37:47]
2023.03.02-18:32:38:413:[step-74200/88900: 83.46%]--[loss-2.922113: wl-3.740125, gl-1.987082]--[lr: pb-0.000050, pf-0.000017]--[ETA-3:58:59]
2023.03.02-18:34:53:513:[step-74300/88900: 83.58%]--[loss-3.297956: wl-4.756423, gl-2.108851]--[lr: pb-0.000050, pf-0.000017]--[ETA-4:03:27]
2023.03.02-18:36:28:613:[step-74400/88900: 83.69%]--[loss-3.042704: wl-3.925910, gl-2.061226]--[lr: pb-0.000050, pf-0.000017]--[ETA-2:41:24]
2023.03.02-18:38:52:713:[step-74500/88900: 83.80%]--[loss-3.143419: wl-3.636020, gl-2.234414]--[lr: pb-0.000050, pf-0.000017]--[ETA-3:37:03]
2023.03.02-18:40:30:813:[step-74600/88900: 83.91%]--[loss-3.096301: wl-3.722477, gl-2.165682]--[lr: pb-0.000050, pf-0.000017]--[ETA-14:36:28]
End of epoch 84 / 100: train_loss: 3.077 	 time: 1066 sec
Saving the model at the end of epoch 84, iters 74676
2023.03.02-18:42:54:24:[step-74700/88900: 84.03%]--[loss-3.209600: wl-4.020717, gl-2.204421]--[lr: pb-0.000050, pf-0.000016]--[ETA-3:55:29]
2023.03.02-18:45:05:124:[step-74800/88900: 84.14%]--[loss-3.089797: wl-4.035645, gl-2.080886]--[lr: pb-0.000050, pf-0.000016]--[ETA-3:27:41]
2023.03.02-18:46:44:224:[step-74900/88900: 84.25%]--[loss-2.892395: wl-3.873407, gl-1.924043]--[lr: pb-0.000050, pf-0.000016]--[ETA-3:56:01]
2023.03.02-18:49:00:324:[step-75000/88900: 84.36%]--[loss-2.956879: wl-4.103212, gl-1.931076]--[lr: pb-0.000050, pf-0.000016]--[ETA-3:42:24]
2023.03.02-18:50:37:424:[step-75100/88900: 84.48%]--[loss-3.038601: wl-4.246655, gl-1.976938]--[lr: pb-0.000050, pf-0.000016]--[ETA-4:10:05]
2023.03.02-18:52:49:524:[step-75200/88900: 84.59%]--[loss-3.021034: wl-4.100580, gl-1.995890]--[lr: pb-0.000050, pf-0.000016]--[ETA-3:37:44]
2023.03.02-18:54:22:624:[step-75300/88900: 84.70%]--[loss-2.900633: wl-3.867295, gl-1.933809]--[lr: pb-0.000050, pf-0.000016]--[ETA-2:55:20]
2023.03.02-18:56:48:724:[step-75400/88900: 84.81%]--[loss-3.051939: wl-4.307551, gl-1.975051]--[lr: pb-0.000050, pf-0.000016]--[ETA-3:56:14]
2023.03.02-18:59:07:824:[step-75500/88900: 84.93%]--[loss-3.075704: wl-4.095679, gl-2.051785]--[lr: pb-0.000050, pf-0.000016]--[ETA-12:11:13]
End of epoch 85 / 100: train_loss: 3.068 	 time: 1065 sec
Saving the model at the end of epoch 85, iters 75565
2023.03.02-19:00:41:35:[step-75600/88900: 85.04%]--[loss-2.980859: wl-3.828308, gl-2.023782]--[lr: pb-0.000050, pf-0.000015]--[ETA-2:33:24]
2023.03.02-19:02:01:135:[step-75700/88900: 85.15%]--[loss-3.082293: wl-4.023500, gl-2.076417]--[lr: pb-0.000050, pf-0.000015]--[ETA-2:47:33]
2023.03.02-19:03:30:235:[step-75800/88900: 85.26%]--[loss-2.962245: wl-3.703134, gl-2.036462]--[lr: pb-0.000050, pf-0.000015]--[ETA-3:02:38]
2023.03.02-19:04:48:335:[step-75900/88900: 85.38%]--[loss-3.014205: wl-4.032156, gl-2.006166]--[lr: pb-0.000050, pf-0.000015]--[ETA-2:41:38]
2023.03.02-19:06:03:435:[step-76000/88900: 85.49%]--[loss-3.154035: wl-3.891263, gl-2.181219]--[lr: pb-0.000050, pf-0.000015]--[ETA-2:39:47]
2023.03.02-19:07:18:535:[step-76100/88900: 85.60%]--[loss-3.066703: wl-4.041928, gl-2.056221]--[lr: pb-0.000050, pf-0.000015]--[ETA-2:30:30]
2023.03.02-19:08:34:635:[step-76200/88900: 85.71%]--[loss-3.016797: wl-3.958483, gl-2.027176]--[lr: pb-0.000050, pf-0.000015]--[ETA-2:47:47]
2023.03.02-19:09:49:735:[step-76300/88900: 85.83%]--[loss-2.802530: wl-3.616969, gl-1.898288]--[lr: pb-0.000050, pf-0.000015]--[ETA-2:38:20]
2023.03.02-19:11:04:835:[step-76400/88900: 85.94%]--[loss-3.106650: wl-3.701866, gl-2.181184]--[lr: pb-0.000050, pf-0.000015]--[ETA-2:40:25]
End of epoch 86 / 100: train_loss: 3.062 	 time: 702 sec
Saving the model at the end of epoch 86, iters 76454
2023.03.02-19:12:33:46:[step-76500/88900: 86.05%]--[loss-2.799413: wl-3.432148, gl-1.941376]--[lr: pb-0.000050, pf-0.000014]--[ETA-2:40:06]
2023.03.02-19:13:49:146:[step-76600/88900: 86.16%]--[loss-3.107183: wl-4.179820, gl-2.062228]--[lr: pb-0.000050, pf-0.000014]--[ETA-2:37:29]
2023.03.02-19:15:05:246:[step-76700/88900: 86.28%]--[loss-3.386699: wl-4.344948, gl-2.300462]--[lr: pb-0.000050, pf-0.000014]--[ETA-2:21:14]
2023.03.02-19:16:21:346:[step-76800/88900: 86.39%]--[loss-2.892910: wl-3.752770, gl-1.954718]--[lr: pb-0.000050, pf-0.000014]--[ETA-2:31:21]
2023.03.02-19:17:37:446:[step-76900/88900: 86.50%]--[loss-3.011579: wl-3.966578, gl-2.019934]--[lr: pb-0.000050, pf-0.000014]--[ETA-2:15:07]
2023.03.02-19:18:51:546:[step-77000/88900: 86.61%]--[loss-3.042783: wl-4.192884, gl-1.994562]--[lr: pb-0.000050, pf-0.000014]--[ETA-2:24:09]
2023.03.02-19:20:02:646:[step-77100/88900: 86.73%]--[loss-3.076075: wl-4.142969, gl-2.040333]--[lr: pb-0.000050, pf-0.000014]--[ETA-2:24:17]
2023.03.02-19:21:13:746:[step-77200/88900: 86.84%]--[loss-2.806764: wl-3.659994, gl-1.891766]--[lr: pb-0.000050, pf-0.000014]--[ETA-2:10:33]
2023.03.02-19:22:25:846:[step-77300/88900: 86.95%]--[loss-3.053720: wl-4.533490, gl-1.920348]--[lr: pb-0.000050, pf-0.000014]--[ETA-2:22:21]
End of epoch 87 / 100: train_loss: 3.056 	 time: 670 sec
Saving the model at the end of epoch 87, iters 77343
2023.03.02-19:23:49:57:[step-77400/88900: 87.06%]--[loss-3.121025: wl-4.053952, gl-2.107537]--[lr: pb-0.000050, pf-0.000013]--[ETA-2:22:10]
2023.03.02-19:25:03:157:[step-77500/88900: 87.18%]--[loss-2.886927: wl-4.126592, gl-1.855279]--[lr: pb-0.000050, pf-0.000013]--[ETA-2:18:50]
2023.03.02-19:26:16:257:[step-77600/88900: 87.29%]--[loss-3.138019: wl-4.282826, gl-2.067312]--[lr: pb-0.000050, pf-0.000013]--[ETA-2:19:18]
2023.03.02-19:27:28:357:[step-77700/88900: 87.40%]--[loss-3.373914: wl-4.466457, gl-2.257299]--[lr: pb-0.000050, pf-0.000013]--[ETA-2:16:06]
2023.03.02-19:28:41:457:[step-77800/88900: 87.51%]--[loss-3.183121: wl-4.506058, gl-2.056606]--[lr: pb-0.000050, pf-0.000013]--[ETA-2:12:57]
2023.03.02-19:29:54:557:[step-77900/88900: 87.63%]--[loss-2.954796: wl-4.018436, gl-1.950186]--[lr: pb-0.000050, pf-0.000013]--[ETA-2:12:07]
2023.03.02-19:31:06:657:[step-78000/88900: 87.74%]--[loss-2.752132: wl-3.774425, gl-1.808526]--[lr: pb-0.000050, pf-0.000013]--[ETA-2:12:44]
2023.03.02-19:32:19:757:[step-78100/88900: 87.85%]--[loss-2.915749: wl-3.921252, gl-1.935436]--[lr: pb-0.000050, pf-0.000013]--[ETA-2:11:46]
2023.03.02-19:33:33:857:[step-78200/88900: 87.96%]--[loss-3.018078: wl-4.253158, gl-1.954789]--[lr: pb-0.000050, pf-0.000013]--[ETA-2:08:54]
End of epoch 88 / 100: train_loss: 3.050 	 time: 659 sec
Saving the model at the end of epoch 88, iters 78232
2023.03.02-19:34:56:68:[step-78300/88900: 88.08%]--[loss-2.719782: wl-3.448626, gl-1.857625]--[lr: pb-0.000050, pf-0.000012]--[ETA-2:09:55]
2023.03.02-19:36:10:168:[step-78400/88900: 88.19%]--[loss-3.022872: wl-4.015166, gl-2.019081]--[lr: pb-0.000050, pf-0.000012]--[ETA-2:09:49]
2023.03.02-19:37:22:268:[step-78500/88900: 88.30%]--[loss-2.927504: wl-3.743885, gl-1.991533]--[lr: pb-0.000050, pf-0.000012]--[ETA-2:07:28]
2023.03.02-19:38:36:368:[step-78600/88900: 88.41%]--[loss-3.114871: wl-4.251788, gl-2.051924]--[lr: pb-0.000050, pf-0.000012]--[ETA-2:08:23]
2023.03.02-19:39:49:468:[step-78700/88900: 88.53%]--[loss-2.737720: wl-3.453907, gl-1.874243]--[lr: pb-0.000050, pf-0.000012]--[ETA-2:03:24]
2023.03.02-19:41:00:568:[step-78800/88900: 88.64%]--[loss-2.899849: wl-4.036756, gl-1.890661]--[lr: pb-0.000050, pf-0.000012]--[ETA-2:03:49]
2023.03.02-19:42:13:668:[step-78900/88900: 88.75%]--[loss-3.187531: wl-4.273196, gl-2.119232]--[lr: pb-0.000050, pf-0.000012]--[ETA-2:01:00]
2023.03.02-19:43:26:768:[step-79000/88900: 88.86%]--[loss-3.067976: wl-3.769671, gl-2.125559]--[lr: pb-0.000050, pf-0.000012]--[ETA-1:59:35]
2023.03.02-19:44:38:868:[step-79100/88900: 88.98%]--[loss-3.010506: wl-3.831278, gl-2.052686]--[lr: pb-0.000050, pf-0.000012]--[ETA-1:56:29]
End of epoch 89 / 100: train_loss: 3.044 	 time: 657 sec
Saving the model at the end of epoch 89, iters 79121
2023.03.02-19:46:02:79:[step-79200/88900: 89.09%]--[loss-2.986548: wl-4.087763, gl-1.964607]--[lr: pb-0.000050, pf-0.000011]--[ETA-1:57:41]
2023.03.02-19:47:15:179:[step-79300/88900: 89.20%]--[loss-3.004836: wl-4.370925, gl-1.912105]--[lr: pb-0.000050, pf-0.000011]--[ETA-1:57:45]
2023.03.02-19:48:28:279:[step-79400/88900: 89.31%]--[loss-3.187914: wl-4.031990, gl-2.179917]--[lr: pb-0.000050, pf-0.000011]--[ETA-1:55:12]
2023.03.02-19:49:41:379:[step-79500/88900: 89.43%]--[loss-3.073597: wl-3.932683, gl-2.090426]--[lr: pb-0.000050, pf-0.000011]--[ETA-1:53:53]
2023.03.02-19:50:54:479:[step-79600/88900: 89.54%]--[loss-2.870489: wl-3.976938, gl-1.876254]--[lr: pb-0.000050, pf-0.000011]--[ETA-1:51:30]
2023.03.02-19:52:06:579:[step-79700/88900: 89.65%]--[loss-3.205235: wl-4.462845, gl-2.089523]--[lr: pb-0.000050, pf-0.000011]--[ETA-1:53:46]
2023.03.02-19:53:19:679:[step-79800/88900: 89.76%]--[loss-3.078836: wl-3.968387, gl-2.086740]--[lr: pb-0.000050, pf-0.000011]--[ETA-1:50:27]
2023.03.02-19:54:31:779:[step-79900/88900: 89.88%]--[loss-3.067966: wl-3.880915, gl-2.097737]--[lr: pb-0.000050, pf-0.000011]--[ETA-1:48:08]
2023.03.02-19:55:43:879:[step-80000/88900: 89.99%]--[loss-3.025324: wl-4.035264, gl-2.016508]--[lr: pb-0.000050, pf-0.000011]--[ETA-1:46:54]
End of epoch 90 / 100: train_loss: 3.041 	 time: 656 sec
Saving the model at the end of epoch 90, iters 80010
2023.03.02-19:57:08:90:[step-80100/88900: 90.10%]--[loss-3.013237: wl-4.049376, gl-2.000892]--[lr: pb-0.000050, pf-0.000010]--[ETA-1:47:10]
2023.03.02-19:58:21:190:[step-80200/88900: 90.21%]--[loss-3.169914: wl-4.108768, gl-2.142722]--[lr: pb-0.000050, pf-0.000010]--[ETA-1:46:19]
2023.03.02-19:59:34:290:[step-80300/88900: 90.33%]--[loss-3.039695: wl-4.063143, gl-2.023910]--[lr: pb-0.000050, pf-0.000010]--[ETA-1:44:52]
2023.03.02-20:00:46:390:[step-80400/88900: 90.44%]--[loss-3.183629: wl-4.456161, gl-2.069588]--[lr: pb-0.000050, pf-0.000010]--[ETA-1:44:17]
2023.03.02-20:02:00:490:[step-80500/88900: 90.55%]--[loss-3.131780: wl-4.128238, gl-2.099720]--[lr: pb-0.000050, pf-0.000010]--[ETA-1:44:36]
2023.03.02-20:03:12:590:[step-80600/88900: 90.66%]--[loss-3.317653: wl-4.481120, gl-2.197373]--[lr: pb-0.000050, pf-0.000010]--[ETA-1:40:32]
2023.03.02-20:04:24:690:[step-80700/88900: 90.78%]--[loss-2.942908: wl-3.827223, gl-1.986102]--[lr: pb-0.000050, pf-0.000010]--[ETA-1:40:15]
2023.03.02-20:05:37:790:[step-80800/88900: 90.89%]--[loss-3.245389: wl-4.330762, gl-2.162699]--[lr: pb-0.000050, pf-0.000010]--[ETA-1:36:46]
End of epoch 91 / 100: train_loss: 3.034 	 time: 657 sec
Saving the model at the end of epoch 91, iters 80899
2023.03.02-20:07:00:1:[step-80900/88900: 91.00%]--[loss-2.996179: wl-3.716192, gl-2.067131]--[lr: pb-0.000050, pf-0.000009]--[ETA-2:18:47]
2023.03.02-20:08:13:101:[step-81000/88900: 91.11%]--[loss-2.961129: wl-4.036993, gl-1.951881]--[lr: pb-0.000050, pf-0.000009]--[ETA-1:37:02]
2023.03.02-20:09:26:201:[step-81100/88900: 91.23%]--[loss-3.183506: wl-3.941142, gl-2.198221]--[lr: pb-0.000050, pf-0.000009]--[ETA-1:35:11]
2023.03.02-20:10:40:301:[step-81200/88900: 91.34%]--[loss-3.000238: wl-4.359502, gl-1.910363]--[lr: pb-0.000050, pf-0.000009]--[ETA-1:33:13]
2023.03.02-20:11:52:401:[step-81300/88900: 91.45%]--[loss-3.049043: wl-3.926749, gl-2.067356]--[lr: pb-0.000050, pf-0.000009]--[ETA-1:33:04]
2023.03.02-20:13:05:501:[step-81400/88900: 91.56%]--[loss-3.035555: wl-3.867139, gl-2.068770]--[lr: pb-0.000050, pf-0.000009]--[ETA-1:31:06]
2023.03.02-20:14:18:601:[step-81500/88900: 91.68%]--[loss-2.921550: wl-4.029779, gl-1.914105]--[lr: pb-0.000050, pf-0.000009]--[ETA-1:30:07]
2023.03.02-20:15:28:701:[step-81600/88900: 91.79%]--[loss-2.913431: wl-4.072678, gl-1.895261]--[lr: pb-0.000050, pf-0.000009]--[ETA-1:17:36]
2023.03.02-20:16:32:801:[step-81700/88900: 91.90%]--[loss-3.044486: wl-3.839727, gl-2.084554]--[lr: pb-0.000050, pf-0.000009]--[ETA-1:16:24]
End of epoch 92 / 100: train_loss: 3.031 	 time: 638 sec
Saving the model at the end of epoch 92, iters 81788
2023.03.02-20:17:46:12:[step-81800/88900: 92.01%]--[loss-3.185631: wl-4.708295, gl-2.008557]--[lr: pb-0.000050, pf-0.000008]--[ETA-1:17:54]
2023.03.02-20:18:51:112:[step-81900/88900: 92.13%]--[loss-3.041972: wl-4.217239, gl-1.987662]--[lr: pb-0.000050, pf-0.000008]--[ETA-1:15:56]
2023.03.02-20:19:56:212:[step-82000/88900: 92.24%]--[loss-3.212854: wl-4.325787, gl-2.131407]--[lr: pb-0.000050, pf-0.000008]--[ETA-1:14:57]
2023.03.02-20:21:01:312:[step-82100/88900: 92.35%]--[loss-2.793032: wl-3.307633, gl-1.966124]--[lr: pb-0.000050, pf-0.000008]--[ETA-1:13:30]
2023.03.02-20:22:05:412:[step-82200/88900: 92.46%]--[loss-2.867574: wl-3.799467, gl-1.917708]--[lr: pb-0.000050, pf-0.000008]--[ETA-1:12:37]
2023.03.02-20:23:10:512:[step-82300/88900: 92.58%]--[loss-2.914701: wl-3.890333, gl-1.942118]--[lr: pb-0.000050, pf-0.000008]--[ETA-1:10:24]
2023.03.02-20:24:14:612:[step-82400/88900: 92.69%]--[loss-2.873308: wl-3.872925, gl-1.905077]--[lr: pb-0.000050, pf-0.000008]--[ETA-1:09:55]
2023.03.02-20:25:18:712:[step-82500/88900: 92.80%]--[loss-2.993895: wl-4.083977, gl-1.972900]--[lr: pb-0.000050, pf-0.000008]--[ETA-1:08:13]
2023.03.02-20:26:23:812:[step-82600/88900: 92.91%]--[loss-2.930956: wl-3.750416, gl-1.993352]--[lr: pb-0.000050, pf-0.000008]--[ETA-1:07:05]
End of epoch 93 / 100: train_loss: 3.022 	 time: 592 sec
Saving the model at the end of epoch 93, iters 82677
2023.03.02-20:27:54:23:[step-82700/88900: 93.03%]--[loss-3.010268: wl-3.983486, gl-2.014397]--[lr: pb-0.000050, pf-0.000007]--[ETA-1:36:44]
2023.03.02-20:29:28:123:[step-82800/88900: 93.14%]--[loss-3.077819: wl-4.247355, gl-2.015980]--[lr: pb-0.000050, pf-0.000007]--[ETA-1:29:30]
2023.03.02-20:31:01:223:[step-82900/88900: 93.25%]--[loss-3.073991: wl-4.085355, gl-2.052653]--[lr: pb-0.000050, pf-0.000007]--[ETA-1:04:26]
2023.03.02-20:32:36:323:[step-83000/88900: 93.36%]--[loss-3.328317: wl-4.178209, gl-2.283765]--[lr: pb-0.000050, pf-0.000007]--[ETA-1:34:00]
2023.03.02-20:34:09:423:[step-83100/88900: 93.48%]--[loss-2.811241: wl-3.736757, gl-1.877052]--[lr: pb-0.000050, pf-0.000007]--[ETA-1:30:04]
2023.03.02-20:35:40:523:[step-83200/88900: 93.59%]--[loss-2.677305: wl-3.421344, gl-1.821969]--[lr: pb-0.000050, pf-0.000007]--[ETA-1:04:17]
2023.03.02-20:37:16:623:[step-83300/88900: 93.70%]--[loss-3.381589: wl-4.364250, gl-2.290527]--[lr: pb-0.000050, pf-0.000007]--[ETA-1:27:44]
2023.03.02-20:38:48:723:[step-83400/88900: 93.81%]--[loss-2.983466: wl-4.136391, gl-1.949368]--[lr: pb-0.000050, pf-0.000007]--[ETA-1:26:26]
2023.03.02-20:40:19:823:[step-83500/88900: 93.93%]--[loss-3.227974: wl-4.930483, gl-1.995354]--[lr: pb-0.000050, pf-0.000007]--[ETA-0:59:30]
End of epoch 94 / 100: train_loss: 3.017 	 time: 840 sec
Saving the model at the end of epoch 94, iters 83566
2023.03.02-20:42:05:34:[step-83600/88900: 94.04%]--[loss-2.839015: wl-3.708162, gl-1.911975]--[lr: pb-0.000050, pf-0.000006]--[ETA-1:24:07]
2023.03.02-20:43:39:134:[step-83700/88900: 94.15%]--[loss-2.775189: wl-3.787313, gl-1.828361]--[lr: pb-0.000050, pf-0.000006]--[ETA-1:21:24]
2023.03.02-20:45:16:234:[step-83800/88900: 94.26%]--[loss-3.201302: wl-4.382865, gl-2.105585]--[lr: pb-0.000050, pf-0.000006]--[ETA-1:01:55]
2023.03.02-20:46:49:334:[step-83900/88900: 94.38%]--[loss-2.863473: wl-3.764401, gl-1.922372]--[lr: pb-0.000050, pf-0.000006]--[ETA-1:19:30]
2023.03.02-20:48:23:434:[step-84000/88900: 94.49%]--[loss-3.008950: wl-4.241323, gl-1.948619]--[lr: pb-0.000050, pf-0.000006]--[ETA-1:15:24]
2023.03.02-20:49:56:534:[step-84100/88900: 94.60%]--[loss-3.270997: wl-4.296087, gl-2.196975]--[lr: pb-0.000050, pf-0.000006]--[ETA-0:57:05]
2023.03.02-20:51:30:634:[step-84200/88900: 94.71%]--[loss-2.817176: wl-3.416810, gl-1.962973]--[lr: pb-0.000050, pf-0.000006]--[ETA-1:14:50]
2023.03.02-20:53:03:734:[step-84300/88900: 94.83%]--[loss-2.758983: wl-3.376044, gl-1.914972]--[lr: pb-0.000050, pf-0.000006]--[ETA-1:12:58]
2023.03.02-20:54:35:834:[step-84400/88900: 94.94%]--[loss-3.176818: wl-4.036716, gl-2.167639]--[lr: pb-0.000050, pf-0.000006]--[ETA-1:11:43]
End of epoch 95 / 100: train_loss: 3.010 	 time: 845 sec
Saving the model at the end of epoch 95, iters 84455
2023.03.02-20:56:19:45:[step-84500/88900: 95.05%]--[loss-3.009107: wl-4.230837, gl-1.951398]--[lr: pb-0.000050, pf-0.000005]--[ETA-0:48:36]
2023.03.02-20:57:54:145:[step-84600/88900: 95.16%]--[loss-2.904561: wl-3.716548, gl-1.975423]--[lr: pb-0.000050, pf-0.000005]--[ETA-1:06:42]
2023.03.02-20:59:27:245:[step-84700/88900: 95.28%]--[loss-2.960248: wl-3.778214, gl-2.015694]--[lr: pb-0.000050, pf-0.000005]--[ETA-1:05:31]
2023.03.02-21:00:57:345:[step-84800/88900: 95.39%]--[loss-2.769676: wl-3.594736, gl-1.870992]--[lr: pb-0.000050, pf-0.000005]--[ETA-0:44:50]
2023.03.02-21:02:35:445:[step-84900/88900: 95.50%]--[loss-3.136856: wl-4.686485, gl-1.965235]--[lr: pb-0.000050, pf-0.000005]--[ETA-1:02:10]
2023.03.02-21:04:08:545:[step-85000/88900: 95.61%]--[loss-3.339894: wl-5.323542, gl-2.009009]--[lr: pb-0.000050, pf-0.000005]--[ETA-0:59:49]
2023.03.02-21:05:43:645:[step-85100/88900: 95.73%]--[loss-3.129287: wl-4.742325, gl-1.943705]--[lr: pb-0.000050, pf-0.000005]--[ETA-3:27:56]
2023.03.02-21:07:16:745:[step-85200/88900: 95.84%]--[loss-3.087007: wl-4.600677, gl-1.936837]--[lr: pb-0.000050, pf-0.000005]--[ETA-0:58:19]
2023.03.02-21:08:48:845:[step-85300/88900: 95.95%]--[loss-2.787842: wl-3.708281, gl-1.860771]--[lr: pb-0.000050, pf-0.000005]--[ETA-0:56:43]
End of epoch 96 / 100: train_loss: 3.009 	 time: 841 sec
Saving the model at the end of epoch 96, iters 85344
2023.03.02-21:10:32:56:[step-85400/88900: 96.06%]--[loss-3.075781: wl-4.191279, gl-2.027961]--[lr: pb-0.000050, pf-0.000004]--[ETA-0:42:17]
2023.03.02-21:12:07:156:[step-85500/88900: 96.18%]--[loss-2.738030: wl-3.554455, gl-1.849416]--[lr: pb-0.000050, pf-0.000004]--[ETA-0:52:56]
2023.03.02-21:13:25:256:[step-85600/88900: 96.29%]--[loss-2.840585: wl-3.539980, gl-1.955590]--[lr: pb-0.000050, pf-0.000004]--[ETA-0:35:20]
2023.03.02-21:14:30:356:[step-85700/88900: 96.40%]--[loss-3.015750: wl-3.782025, gl-2.070244]--[lr: pb-0.000050, pf-0.000004]--[ETA-0:34:29]
2023.03.02-21:15:34:456:[step-85800/88900: 96.51%]--[loss-3.129891: wl-4.563630, gl-1.988983]--[lr: pb-0.000050, pf-0.000004]--[ETA-0:33:18]
2023.03.02-21:16:50:556:[step-85900/88900: 96.63%]--[loss-3.083750: wl-4.265728, gl-2.017318]--[lr: pb-0.000050, pf-0.000004]--[ETA-0:47:26]
2023.03.02-21:18:23:656:[step-86000/88900: 96.74%]--[loss-2.959193: wl-4.057193, gl-1.944895]--[lr: pb-0.000050, pf-0.000004]--[ETA-0:34:16]
2023.03.02-21:19:56:756:[step-86100/88900: 96.85%]--[loss-2.699492: wl-3.343182, gl-1.863696]--[lr: pb-0.000050, pf-0.000004]--[ETA-0:43:57]
2023.03.02-21:21:28:856:[step-86200/88900: 96.96%]--[loss-2.824934: wl-3.792727, gl-1.876753]--[lr: pb-0.000050, pf-0.000004]--[ETA-0:42:40]
End of epoch 97 / 100: train_loss: 3.006 	 time: 749 sec
Saving the model at the end of epoch 97, iters 86233
2023.03.02-21:23:12:67:[step-86300/88900: 97.08%]--[loss-2.805005: wl-3.673949, gl-1.886518]--[lr: pb-0.000050, pf-0.000003]--[ETA-0:40:43]
2023.03.02-21:24:44:167:[step-86400/88900: 97.19%]--[loss-3.152834: wl-4.724195, gl-1.971785]--[lr: pb-0.000050, pf-0.000003]--[ETA-0:27:41]
2023.03.02-21:26:19:267:[step-86500/88900: 97.30%]--[loss-2.938660: wl-3.917893, gl-1.959187]--[lr: pb-0.000050, pf-0.000003]--[ETA-0:37:53]
2023.03.02-21:27:53:367:[step-86600/88900: 97.41%]--[loss-3.083493: wl-4.003061, gl-2.082728]--[lr: pb-0.000050, pf-0.000003]--[ETA-0:36:07]
2023.03.02-21:29:28:467:[step-86700/88900: 97.53%]--[loss-3.158200: wl-4.364992, gl-2.066952]--[lr: pb-0.000050, pf-0.000003]--[ETA-1:57:57]
2023.03.02-21:30:58:567:[step-86800/88900: 97.64%]--[loss-2.885961: wl-4.173856, gl-1.842497]--[lr: pb-0.000050, pf-0.000003]--[ETA-0:33:04]
2023.03.02-21:32:30:667:[step-86900/88900: 97.75%]--[loss-3.034328: wl-4.045887, gl-2.022856]--[lr: pb-0.000050, pf-0.000003]--[ETA-0:31:58]
2023.03.02-21:34:04:767:[step-87000/88900: 97.86%]--[loss-2.853485: wl-3.520442, gl-1.973375]--[lr: pb-0.000050, pf-0.000003]--[ETA-0:22:15]
2023.03.02-21:35:37:867:[step-87100/88900: 97.98%]--[loss-3.140092: wl-4.011532, gl-2.137209]--[lr: pb-0.000050, pf-0.000003]--[ETA-0:28:39]
End of epoch 98 / 100: train_loss: 3.000 	 time: 835 sec
Saving the model at the end of epoch 98, iters 87122
2023.03.02-21:37:18:78:[step-87200/88900: 98.09%]--[loss-3.168363: wl-4.216417, gl-2.114259]--[lr: pb-0.000050, pf-0.000002]--[ETA-0:26:54]
2023.03.02-21:38:52:178:[step-87300/88900: 98.20%]--[loss-2.926736: wl-3.945197, gl-1.940437]--[lr: pb-0.000050, pf-0.000002]--[ETA-0:25:09]
2023.03.02-21:40:24:278:[step-87400/88900: 98.31%]--[loss-3.038513: wl-4.008440, gl-2.036403]--[lr: pb-0.000050, pf-0.000002]--[ETA-0:16:35]
2023.03.02-21:41:59:378:[step-87500/88900: 98.43%]--[loss-3.082354: wl-4.186645, gl-2.035693]--[lr: pb-0.000050, pf-0.000002]--[ETA-0:21:55]
2023.03.02-21:43:32:478:[step-87600/88900: 98.54%]--[loss-2.889265: wl-3.757952, gl-1.949777]--[lr: pb-0.000050, pf-0.000002]--[ETA-0:20:24]
2023.03.02-21:45:08:578:[step-87700/88900: 98.65%]--[loss-3.146558: wl-4.466688, gl-2.029886]--[lr: pb-0.000050, pf-0.000002]--[ETA-0:12:48]
2023.03.02-21:46:40:678:[step-87800/88900: 98.76%]--[loss-2.935294: wl-3.935352, gl-1.951456]--[lr: pb-0.000050, pf-0.000002]--[ETA-0:17:18]
2023.03.02-21:48:13:778:[step-87900/88900: 98.88%]--[loss-2.951627: wl-3.825624, gl-1.995221]--[lr: pb-0.000050, pf-0.000002]--[ETA-0:15:51]
2023.03.02-21:49:45:878:[step-88000/88900: 98.99%]--[loss-3.167568: wl-4.417025, gl-2.063312]--[lr: pb-0.000050, pf-0.000002]--[ETA-0:10:34]
End of epoch 99 / 100: train_loss: 2.994 	 time: 838 sec
Saving the model at the end of epoch 99, iters 88011
2023.03.02-21:51:31:89:[step-88100/88900: 99.10%]--[loss-3.061817: wl-4.123607, gl-2.030915]--[lr: pb-0.000050, pf-0.000001]--[ETA-0:12:29]
2023.03.02-21:53:02:189:[step-88200/88900: 99.21%]--[loss-3.052726: wl-3.966842, gl-2.061016]--[lr: pb-0.000050, pf-0.000001]--[ETA-0:10:54]
2023.03.02-21:54:34:289:[step-88300/88900: 99.33%]--[loss-2.928892: wl-3.713472, gl-2.000524]--[lr: pb-0.000050, pf-0.000001]--[ETA-0:09:20]
2023.03.02-21:56:08:389:[step-88400/88900: 99.44%]--[loss-3.020458: wl-4.111097, gl-1.992684]--[lr: pb-0.000050, pf-0.000001]--[ETA-0:05:42]
2023.03.02-21:57:41:489:[step-88500/88900: 99.55%]--[loss-3.270531: wl-4.430772, gl-2.162838]--[lr: pb-0.000050, pf-0.000001]--[ETA-0:06:17]
2023.03.02-21:59:12:589:[step-88600/88900: 99.66%]--[loss-3.033902: wl-4.294013, gl-1.960399]--[lr: pb-0.000050, pf-0.000001]--[ETA-0:04:42]
2023.03.02-22:00:43:689:[step-88700/88900: 99.78%]--[loss-2.901562: wl-3.730478, gl-1.968943]--[lr: pb-0.000050, pf-0.000001]--[ETA-0:02:09]
2023.03.02-22:01:55:789:[step-88800/88900: 99.89%]--[loss-3.026515: wl-4.075320, gl-2.007685]--[lr: pb-0.000050, pf-0.000001]--[ETA-0:01:04]
2023.03.02-22:03:19:889:[step-88900/88900: 100.00%]--[loss-3.034661: wl-4.537758, gl-1.900221]--[lr: pb-0.000050, pf-0.000001]--[ETA-0:11:40]
End of epoch 100 / 100: train_loss: 2.995 	 time: 805 sec
Saving the model at the end of epoch 100, iters 88900
